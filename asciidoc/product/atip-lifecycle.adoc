[#atip-lifecycle]
== Lifecycle actions
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

This section covers the lifecycle management actions of deployed ATIP clusters.

=== Management cluster upgrades

Upgrading the management cluster involves upgrading several of its components. The following sections cover the upgrade process for each of the components.

*Updating the operating system*

SUSE releases new versions of `SLE Micro` at regular intervals. To make it easy for customers to migrate to a new minor version and minimize downtime, SUSE supports migrating online while the system is running.
SLE Micro uses transactional updates to upgrade from one version to the next. This has the following advantages:

- The system is always in a defined state until the first RPM is updated.
- Canceling is possible until the first RPM is updated.
- Simple recovery if there is an error.
- It is possible to do a rollback via system toolsâ€”no backup or restore needed.
- Use of all active repositories.

[,shell]
----
transactional-update
----

To revert the migration, use the following command:

[,shell]
----
transactional-update rollback last
----

[NOTE]
====
For more information about the update process, see https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/sec-transactional-udate.html#sec-command-list[Transactional Update].
====

*Upgrading the RKE2 cluster*

To upgrade `RKE2` from an older version, you can re-run the installation script using the new version, for example:

[,shell]
----
curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=vX.Y.Z+rke2rN sh -
----

After that, restart the `rke2-server` service:

[,shell]
----
# server nodes
systemctl restart rke2-server
----
or the agent service:

[,shell]
----
# agent nodes
systemctl restart rke2-agent
----

[NOTE]
====
For more information about the upgrade process, refer to the https://docs.rke2.io/upgrade[RKE2 documentation].
====

*Upgrading Rancher Prime*

To upgrade `Rancher Prime`, use the following command to update the Helm repository cache and fetch the latest chart to install Rancher from the Helm chart repository:

[,shell]
----
helm repo update
helm fetch rancher-prime/rancher
----

After that, the easy way to upgrade is to export your current configurations to a file, and then upgrade the Rancher Prime version using that previous file.
If any change is required in the new version, the file can be edited before the upgrade.

[,shell]
----
helm get values rancher -n cattle-system -o yaml > rancher-values.yaml
helm upgrade rancher rancher-prime/rancher \
  --namespace cattle-system \
  -f values.yaml \
  --version=2.x.y
----

[NOTE]
====
For more information about the upgrade process, refer to the https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades[Upgrade Rancher Prime] documentation.
====

*Upgrading Metal^3^*

To upgrade `Metal^3^`, use the following command to update the Helm repository cache and fetch the latest chart to install `Metal^3^` from the Helm chart repository:

[,shell]
----
helm repo update
helm fetch suse-edge/metal3
----

After that, the easy way to upgrade is to export your current configurations to a file, and then upgrade the `Metal^3^` version using that previous file.
If any change is required in the new version, the file can be edited before the upgrade.

[,shell]
----
helm get values metal3 -n metal3-system -o yaml > metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.7.0
----

=== Downstream cluster upgrades

Upgrading downstream clusters involves updating several components. The following sections cover the upgrade process for each of the components.

*Upgrading the operating system*

For this process, check the following <<atip-automated-provisioning#ztp-eib-edge-image,reference>> to build the new image with a new operating system version.
With this new image generated by `EIB`, the next provision phase uses the new operating version provided.
In the following step, the new image is used to upgrade the nodes.

*Upgrading the RKE2 cluster*

The changes required to upgrade the `RKE2` cluster using the automated workflow are the following:

* Change the block `RKE2ControlPlane` in the `capi-provisioning-example.yaml` shown in the following <<atip-automated-provisioning#ztp-single-node-provision,section>>:

  ** Add the rollout strategy in the spec file.
  ** Change the version of the `RKE2` cluster to the new version replacing `$\{RKE2_NEW_VERSION\}`.

[,yaml]
----
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" >> /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_NEW_VERSION}
    nodeName: "localhost.localdomain"
----

* Change the block `Metal3MachineTemplate` in the `capi-provisioning-example.yaml` shown in the following <<atip-automated-provisioning#ztp-single-node-provision,section>>:

  ** Change the image name and checksum to the new version generated in the previous step.
  ** Add the directive `nodeReuse` to `true` to avoid creating a new node.
  ** Add the directive `automatedCleaningMode` to `metadata` to enable the automated cleaning for the node.

[,yaml]
----
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw
----

After making these changes, the `capi-provisioning-example.yaml` file can be applied to the cluster using the following command:

[,shell]
----
kubectl apply -f capi-provisioning-example.yaml
----

