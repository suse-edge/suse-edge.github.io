[#components-system-upgrade-controller]
= System Upgrade Controller

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

See the link:https://github.com/rancher/system-upgrade-controller[System Upgrade Controller documentation].

[quote]
____
The System Upgrade Controller (SUC) aims to provide a general-purpose, Kubernetes-native upgrade controller (for nodes). It introduces a new CRD, the Plan, for defining any and all of your upgrade policies/requirements. A Plan is an outstanding intent to mutate nodes in your cluster.
____

== How does SUSE Edge use System Upgrade Controller?

At SUSE Edge, *SUC* is used to assist in the various `Day 2` operations that need to be executed in order to upgrade management/downstream clusters from one Edge platform version to another. `Day 2` operations are defined in the form of *SUC Plans*. Based on the SUC Plan, SUC deploys workloads on each node that execute the specific `Day 2` opeartion.

[#components-system-upgrade-controller-install]
== Installing the System Upgrade Controller

We recommend that you install *SUC* through <<components-fleet, Fleet>> using our fleet that is located in our link:https://github.com/suse-edge/fleet-examples[suse-edge/fleet-examples] repository.

[NOTE]
====
The resources offered by the `suse-edge/fleet-examples` repository *must* always be used from a valid link:https://github.com/suse-edge/fleet-examples/releases[fleet-examples release]. To determine which release you need to use, refer to the <<release-notes, Release Notes>>.
====

If using <<components-fleet, Fleet>> for the installation of *SUC* is not possible, you can install it through Rancher's Helm chart repository, or incorporate the Rancher's Helm chart in your own third-party GitOps workflow.

This section covers:

* <<components-system-upgrade-controller-fleet, Fleet installation>>

* <<components-system-upgrade-controller-helm, Helm installation>>

[#components-system-upgrade-controller-fleet]
=== System Upgrade Controller Fleet installation

Using *Fleet* there are two possible resources that can be used to deploy *SUC*:

* link:https://fleet.rancher.io/ref-gitrepo[GitRepo] resource - for use-cases where an external/local Git server is available. For installation instructions, see <<components-system-upgrade-controller-fleet-gitrepo, System Upgrade Controller installation - GitRepo>>.

* link:https://fleet.rancher.io/bundle-add[Bundle] resource - for air-gapped use-cases that do not support a local Git server option. For installation instructions, see <<components-system-upgrade-controller-fleet-bundle, System Upgrade Controller installation - Bundle>>.

[#components-system-upgrade-controller-fleet-gitrepo]
==== System Upgrade Controller installation - GitRepo

[NOTE]
====
This process can also be done through the Rancher UI, if such is available. For more information, see link:https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui[Accessing Fleet in the Rancher UI].
====

In your *management* cluster:

. Determine on which clusters you want to deploy *SUC*. This is done by deploying the *SUC GitRepo* in the correct Fleet workspace inside your *management* cluster. By default, Fleet has two workspaces:

** `fleet-local` - for resources that need to be deployed on the *management* cluster.

** `fleet-default` - for resources that need to be deployed on *downstream* clusters.
+
For more information on Fleet workspaces, see the link:https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups[upstream] documentation.

. Deploy the *GitRepo* resource:

** To deploy *SUC* on your *management* cluster:
+
[,bash]
----
kubectl apply -n fleet-local -f - <<EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.1.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF
----

** To deploy *SUC* on your *downstream* clusters:
+
[NOTE]
====
Before deploying the resource below, you *must* provide a valid *targets* configuration, so that Fleet knows on which downstream clusters to deploy your resource. For information on how to map to downstream clusters, see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters].
====
+
[,bash]
----
kubectl apply -n fleet-local -f - <<EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.1.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF
----

. Validate that the *GitRepo* is deployed:
+
[,bash]
----
kubectl get gitrepo system-upgrade-controller -n fleet-local
NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.1.0   1/1                       
----

. Validate the *System Upgrade Controller* deployment:
+
[,bash]
----
kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s
----

[#components-system-upgrade-controller-fleet-bundle]
==== System Upgrade Controller installation - Bundle

This section illustrates how to build and deploy a *Bundle* resource from a standard Fleet configuration using the link:https://fleet.rancher.io/cli/fleet-cli/fleet[fleet-cli].

On a machine with network access:

. Download the *fleet-cli*:
+
[NOTE]
====
Make sure that the version of the *fleet-cli* you download matches the version of Fleet that has been deployed on your cluster.
====

** For Mac users there is a link:https://formulae.brew.sh/formula/fleet-cli[fleet-cli] Homebrew Formulae.

** For Linux and Windows users the binaries are present as *assets* to each Fleet link:https://github.com/rancher/fleet/releases[release].

*** Linux AMD:
+
[,bash]
----
curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/<FLEET_VERSION>/fleet-linux-amd64
----

*** Linux ARM:
+
[,bash]
----
curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/<FLEET_VERSION>/fleet-linux-arm64
----

*** Windows:
+
[,bash]
----
curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/<FLEET_VERSION>/fleet-windows-amd64.exe
----

. Make *fleet-cli* executable:
+
[,bash]
----
chmod +x fleet-cli
----

. Clone the *suse-edge/fleet-examples* link:https://github.com/suse-edge/fleet-examples/releases[release] that you wish to use:
+
[,bash]
----
git clone -b release-3.1.0 https://github.com/suse-edge/fleet-examples.git
----

. Navigate to the *SUC* fleet, located in the *fleet-examples* repo:
+
[,bash]
----
cd fleet-examples/fleets/day2/system-upgrade-controller
----

. Determine on which clusters you want to deploy *SUC*. This is done by deploying the *SUC Bundle* in the correct Fleet workspace inside your *management* cluster. By default, Fleet has two workspaces:

** `fleet-local` - for resources that need to be deployed on the *management* cluster.

** `fleet-default` - for resources that need to be deployed on *downstream* clusters.
+
For more information on Fleet workspaces, see the link:https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups[upstream] documentation.

. *Only if you intend to deploy SUC on downstream clusters*, create a *targets.yaml* file that matches the downstream clusters to which you wish to deploy *SUC* to:
+
[,bash]
----
cat > targets.yaml <<EOF
targets:
- clusterSelector: CHANGEME
EOF
----
+
For information on how to map to downstream clusters, see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters]

. Proceed to building the Bundle:
+
[NOTE]
====
Make sure you did *not* download the *fleet-cli* in the `fleet-examples/fleets/day2/system-upgrade-controller` directory, otherwise it will be packaged with the Bundle, which is not advised.
====

** To deploy *SUC* on your *management* cluster, execute:
+
[,bash]
----
fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . > system-upgrade-controller-bundle.yaml
----

** To deploy *SUC* on your *downstream* clusters, execute:
+
[,bash]
----
fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . > system-upgrade-controller-bundle.yaml
----
+
For more information about this process, see link:https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle[Convert a Helm Chart into a Bundle].
+
For more information about the `fleet-cli apply` command, see link:https://fleet.rancher.io/cli/fleet-cli/fleet_apply[fleet apply].

. Transfer the *system-upgrade-controller-bundle.yaml* Bundle to your *management* cluster machine:
+
[,bash]
----
scp system-upgrade-controller-bundle.yaml ..
----

. On your *management* cluster, deploy the *system-upgrade-controller-bundle.yaml* Bundle:
+
[,bash]
----
kubectl apply -f system-upgrade-controller-bundle.yaml
----

. On your *management* cluster, validate the *Bundle* has been deployed:
+
[,bash]
----
kubectl get bundle system-upgrade-controller -n fleet-local
NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1 
----

. Based on the Fleet workspace that you deployed your *Bundle* to, navigate to the cluster and validate the *SUC* deployment:
+
[NOTE]
====
*SUC* is always deployed in the *cattle-system* namespace. 
====
+
[,bash]
----
kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s
----

[#components-system-upgrade-controller-helm]
=== System Upgrade Controller Helm installation

. Add the Rancher chart repository:
+
[,bash]
----
helm repo add rancher-charts https://charts.rancher.io/
----

. Deploy the *SUC* chart:
+
[,bash]
----
helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 104.0.0+up0.7.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace
----
+
This will install *SUC* `0.13.4` version which is needed by the Edge 3.1 platform.

. Validate the *SUC* deployment:
+
[,bash]
----
kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s
----

[#components-system-upgrade-controller-monitor-plans]
== Monitoring System Upgrade Controller Plans

Depending on your setup, *SUC* Plans can be viewed in the following ways:

* Through the <<components-system-upgrade-controller-monitor-plans-rancher, Rancher UI>>.

* Through <<components-system-upgrade-controller-monitor-plans-manual, manual monitoring>> inside of the cluster.

[IMPORTANT]
====
Pods deployed for *SUC Plans* are kept alive *15* minutes after a successful execution. After that they are removed by the corresponding Job that created them. To have access to the Pod's logs after this time period, you should enable logging for your cluster. For information on how to do this in Rancher, see link:https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/logging[Rancher Integration with Logging Services].
====

[#components-system-upgrade-controller-monitor-plans-rancher]
=== Monitoring System Upgrade Controller Plans - Rancher UI

To check *Pod* logs for the specific *SUC* plan:

. In the upper left corner, *☰ -> <your-cluster-name>*

. Select *Workloads -> Pods*

. Under the namespace drop down menu select the `cattle-system` namespace
+
image::day2-monitor-suc-deployment-1.png[]

. In the Pod filter bar, write the name for your *SUC Plan* Pod. The name will be in the following template format: `apply-<plan_name>-on-<node_name>`
+
.Example Kubernetes upgrade plan pods
image::day2-k8s-plan-monitor.png[]
+
Note how we have one Pod in *Completed* and one in *Unknown* state. This is expected and has happened due to the Kubernetes version upgrade on the node.
+
.Example OS upgrade plan pods
image::day2-os-pkg-plan-monitor.png[]
+
Note how we have one Pod in *Completed* and one in *Unknown* state. This is expected and has happened due to the OS reboot.

. Select the pod that you want to review the logs of and navigate to *⋮ → View Logs*

[#components-system-upgrade-controller-monitor-plans-manual]
=== Monitoring System Upgrade Controller Plans - Manual monitoring

. Navigate to the desired cluster:
+
[,bash]
----
ssh user@node
----

. List deployed *SUC* Plans:
+
[,bash]
----
kubectl get plans -n cattle-system
----

. Get Pod for *SUC* Plan:
+
[,bash]
----
kubectl get pods -l upgrade.cattle.io/plan=<plan_name> -n cattle-system
----

. Get logs for the Pod:
+
[,bash]
----
kubectl logs <pod_name> -n cattle-system
----
