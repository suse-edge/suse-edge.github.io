[#quickstart-elemental]
= Remote host onboarding with Elemental
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

This section documents the "phone home network provisioning" solution as part of SUSE Edge, where we use Elemental to assist with node onboarding. Elemental is a software stack enabling remote host registration and centralized full cloud-native OS management with Kubernetes. In the SUSE Edge stack we use the registration feature of Elemental to enable remote host onboarding into Rancher so that hosts can be integrated into a centralized management platform and from there, deploy and manage Kubernetes clusters along with layered components, applications, and their lifecycle, all from a common place. 

This approach can be useful in scenarios where the devices that you want to control are not on the same network as the management cluster or do not have a out-of-band management controller onboard to allow more direct control, and where you're booting many different "unknown" systems at the edge, and need to securely onboard and manage them at scale. This is a common scenario for use cases in retail, industrial IoT, or other spaces where you have little control over the network your devices are being installed in.

== High-level architecture

image::quickstart-elemental-architecture.svg[]

== Resources needed

The following describes the minimum system and environmental requirements to run through this quickstart:

* A host for the centralized management cluster (the one hosting Rancher and Elemental):
 ** Minimum 8 GB RAM and 20 GB disk space for development or testing (see https://ranchermanager.docs.rancher.com/{rancher-docs-version}/getting-started/installation-and-upgrade/installation-requirements#hardware-requirements[here] for production use)
* A target node to be provisioned, i.e. the edge device (a virtual machine can be used for demoing or testing purposes)
 ** Minimum 4GB RAM, 2 CPU cores, and 20 GB disk
* A resolvable host name for the management cluster or a static IP address to use with a service like sslip.io
* A host to build the installation media via Edge Image Builder
 ** Running SLES 15 SP6, openSUSE Leap 15.6, or another compatible operating system that supports Podman.
 ** With https://kubernetes.io/docs/reference/kubectl/kubectl/[Kubectl], https://podman.io[Podman], and https://helm.sh[Helm] installed
* A USB flash drive to boot from (if using physical hardware)
* A downloaded copy of the latest SUSE Linux Micro {version-operatingsystem} SelfInstall "GM2" ISO image found https://www.suse.com/download/sle-micro/[here].

NOTE: Existing data found on target machines will be overwritten as part of the process, please make sure you backup any data on any USB storage devices and disks attached to target deployment nodes.

This guide is created using a Digital Ocean droplet to host the upstream cluster and an Intel NUC as the downstream device. For building the installation media, SUSE Linux Enterprise Server is used.

== Build bootstrap cluster [[build-bootstrap-cluster]]

Start by creating a cluster capable of hosting Rancher and Elemental. This cluster needs to be routable from the network that the downstream nodes are connected to.

=== Create Kubernetes cluster

If you are using a hyperscaler (such as Azure, AWS or Google Cloud), the easiest way to set up a cluster is using their built-in tools. For the sake of conciseness in this guide, we do not detail the process of each of these options.

If you are installing onto bare-metal or another hosting service where you need to also provide the Kubernetes distribution itself, we recommend using https://docs.rke2.io/install/quickstart[RKE2].

=== Set up DNS

Before continuing, you need to set up access to your cluster. As with the setup of the cluster itself, how you configure DNS will be different depending on where it is being hosted.

[TIP]
====
If you do not want to handle setting up DNS records (for example, this is just an ephemeral test server), you can use a service like https://sslip.io[sslip.io] instead. With this service, you can resolve any IP address with `<address>.sslip.io`.
====

== Install Rancher [[install-rancher]]

To install Rancher, you need to get access to the Kubernetes API of the cluster you just created. This looks different depending on what distribution of Kubernetes is being used.

For RKE2, the kubeconfig file will have been written to `/etc/rancher/rke2/rke2.yaml`.
Save this file as `~/.kube/config` on your local system.
You may need to edit the file to include the correct externally routable IP address or host name.

Install Rancher easily with the commands from the https://ranchermanager.docs.rancher.com/{rancher-docs-version}/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster[Rancher Documentation]:

Install https://cert-manager.io[cert-manager]:
[,bash]
----
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true
----

Then install Rancher itself:

[,bash,subs="attributes,specialchars"]
----
helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=<DNS or sslip from above> \
  --set replicas=1 \
  --set bootstrapPassword=<PASSWORD_FOR_RANCHER_ADMIN> \
  --version {version-rancher-prime}
----

[NOTE]
====
If this is intended to be a production system, please use cert-manager to configure a real certificate (such as one from Let's Encrypt).
====

Browse to the host name you set up and log in to Rancher with the `bootstrapPassword` you used. You will be guided through a short setup process.

== Install Elemental [[install-elemental]]

With Rancher installed, you can now install the Elemental operator and required CRD's. The Helm chart for Elemental is published as an OCI artifact so the installation is a little simpler than other charts.
It can be installed from either the same shell you used to install Rancher or in the browser from within Rancher's shell.

[,bash,subs="attributes"]
----
helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version {version-elemental-operator}
 
helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version {version-elemental-operator}
----

=== (Optionally) Install the Elemental UI extension

. To use the Elemental UI, log in to your Rancher instance, click the three-line menu in the upper left:
+
image::installing-elemental-extension-1.png[Installing Elemental extension1]
+
. From the "Available" tab on this page, click "Install" on the Elemental card:
+
image::installing-elemental-extension-2.png[Installing Elemental extension 2]
+
. Confirm that you want to install the extension:
+
image::installing-elemental-extension-3.png[Installing Elemental extension 3]
+
. After it installs, you will be prompted to reload the page.
+
image::installing-elemental-extension-4.png[Installing Elemental extension 4]
+
. Once you reload, you can access the Elemental extension through the "OS Management" global app.
+
image::accessing-elemental-extension.png[Accessing Elemental extension]

== Configure Elemental [[configure-elemental]]

For simplicity, we recommend setting the variable `$ELEM` to the full path of where you want the configuration directory:

[,shell]
----
export ELEM=$HOME/elemental
mkdir -p $ELEM
----

To allow machines to register to Elemental, we need to create a `MachineRegistration` object in the `fleet-default` namespace.

Let us create a basic version of this object:

[,shell]
----
cat << EOF > $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml
----

[NOTE]
====
The `cat` command escapes each `$` with a backslash (`\`) so that Bash does not template them. Remove the backslashes if copying manually.
====

Once the object is created, find and note the endpoint that gets assigned:

[,bash]
----
REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')
----

Alternatively, this can also be done from the UI.

UI Extension::
+
. From the OS Management extension, click "Create Registration Endpoint":
+
image::click-create-registration.png[Click Create Registration]
+
. Give this configuration a name.
+
image::create-registration-name.png[Add Name]
+
[NOTE]
====
You can ignore the Cloud Configuration field as the data here is overridden by the following steps with Edge Image Builder.
====
. Next, scroll down and click "Add Label" for each label you want to be on the resource that gets created when a machine registers. This is useful for distinguishing machines.
+
image::create-registration-labels.png[Add Labels]
+
. Click "Create" to save the configuration.

. Once the registration is created, you should see the Registration URL listed and can click "Copy" to copy the address:
+
image::get-registration-url.png[Copy URL]
+
[TIP]
====
If you clicked away from that screen, you can click "Registration Endpoints" in the left menu, then click the name of the endpoint you just created.
====
+
This URL is used in the next step.

== Build the image [[build-installation-media]]

While the current version of Elemental has a way to build its own installation media, in SUSE Edge {version-edge} we do this with the Edge Image Builder instead, so the resulting system is built with https://www.suse.com/products/micro/[SUSE Linux Micro] as the base Operating System.

[TIP]
====
For more details on the Edge Image Builder, check out the <<quickstart-eib,Getting Started Guide for it>> and also the <<components-eib,Component Documentation>>.
====

From a Linux system with Podman installed, create the directories and place the base image:

[,bash]
----
mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/downloads/SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental
----

[,bash]
----
curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml
----

[,bash,subs="attributes,specialchars"]
----
cat << EOF > $ELEM/eib_quickstart/eib-config.yaml
apiVersion: {version-eib-api-latest}
image:
    imageType: iso
    arch: x86_64
    baseImage: {micro-base-image-iso}
    outputImageName: elemental-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: XXX
EOF
----

[NOTE]
====
* The unencoded password is `eib`.
* The `sccRegistrationCode` is needed to download and install the necessary RPMs from the official sources (alternatively, the `elemental-register` and `elemental-system-agent` RPMs can be manually side-loaded instead)
* The `cat` command escapes each `$` with a backslash (`\`) so that Bash does not template them. Remove the backslashes if copying manually.
* The installation device will be wiped during the installation.
====

[,bash,subs="attributes"]
----
podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/{version-edge-registry}/edge-image-builder:{version-eib} \
 build --definition-file eib-config.yaml
----

If you are booting a physical device, we need to burn the image to a USB flash drive. This can be done with:

[,bash]
----
sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/<PATH_TO_DISK_DEVICE> status=progress
----

== Boot the downstream nodes [[boot-downstream-nodes]]

Now that we have created the installation media, we can boot our downstream nodes with it.

For each of the systems that you want to control with Elemental, add the installation media and boot the device. After installation, it will reboot and register itself.

If you are using the UI extension, you should see your node appear in the "Inventory of Machines."

NOTE: Do not remove the installation medium until you've seen the login prompt; during first-boot files are still accessed on the USB stick.

== Create downstream clusters [[create-downstream-clusters]]

There are two objects we need to create when provisioning a new cluster using Elemental.

[.tabs]
Linux::
The first is the `MachineInventorySelectorTemplate`. This object allows us to specify a mapping between clusters and the machines in the inventory.
+
. Create a selector which will match any machine in the inventory with a label:
+
[,yaml]
----
cat << EOF > $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF
----
+
. Apply the resource to the cluster:
+
[,bash]
----
kubectl apply -f $ELEM/selector.yaml
----
+
. Obtain the name of the machine and add the matching label:
+
[,bash]
----
MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR>1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123
----
+
. Create a simple single-node K3s cluster resource and apply it to the cluster:
+
[,bash,subs="attributes,specialchars"]
----
cat << EOF > $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: {version-kubernetes-k3s}
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml
----
+
UI Extension::
The UI extension allows for a few shortcuts to be taken. Note that managing multiple locations may involve too much manual work.
+
. As before, open the left three-line menu and select "OS Management." This brings you back to the main screen for managing your Elemental systems.
. On the left sidebar, click "Inventory of Machines." This opens the inventory of machines that have registered.
. To create a cluster from these machines, select the systems you want, click the "Actions" drop-down list, then "Create Elemental Cluster." This opens the Cluster Creation dialog while also creating a MachineSelectorTemplate to use in the background.
. On this screen, configure the cluster you want to be built. For this quick start, K3s v1.30.5+k3s1 is selected and the rest of the options are left as is.
+
[TIP]
====
You may need to scroll down to see more options.
====

After creating these objects, you should see a new Kubernetes cluster spin up using the new node you just installed with.

== Node Reset (Optional)

SUSE Rancher Elemental supports the ability to perform a "node reset" which can optionally trigger when either a whole cluster is deleted from Rancher, a single node is deleted from a cluster, or a node is manually deleted from the machine inventory. This is useful when you want to reset and clean-up any orphaned resources and want to automatically bring the cleaned node back into the machine inventory so it can be reused. This is not enabled by default, and thus any system that is removed, will not be cleaned up (i.e. data will not be removed, and any Kubernetes cluster resources will continue to operate on the downstream clusters) and it will require manual intervention to wipe data and re-register the machine to Rancher via Elemental.

If you wish for this functionality to be enabled by default, you need to make sure that your `MachineRegistration` explicitly enables this by adding `config.elemental.reset.enabled: true`, for example:

[,yaml]
----
config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true
----

Then, all systems registered with this `MachineRegistration` will automatically receive the `elemental.cattle.io/resettable: 'true'` annotation in their configuration. If you wish to do this manually on individual nodes, e.g. because you've got an existing `MachineInventory` that doesn't have this annotation, or you have already deployed nodes, you can modify the `MachineInventory` and add the `resettable` configuration, for example:

[,yaml]
----
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'
----

In SUSE Edge 3.1, the Elemental Operator puts down a marker on the operating system that will trigger the cleanup process automatically; it will stop all Kubernetes services, remove all persistent data, uninstall all Kubernetes services, cleanup any remaining Kubernetes/Rancher directories, and force a re-registration to Rancher via the original Elemental `MachineRegistration` configuration. This happens automatically, there is no need for any manual intervention. The script that gets called can be found in `/opt/edge/elemental_node_cleanup.sh` and is triggered via `systemd.path` upon the placement of the marker, so its execution is immediate.

[WARNING]
====
Using the `resettable` functionality assumes that the desired behavior when removing a node/cluster from Rancher is to wipe data and force a re-registration. Data loss is guaranteed in this situation, so only use this if you're sure that you want automatic reset to be performed.
====

== Next steps

Here are some recommended resources to research after using this guide:

* End-to-end automation in <<components-fleet>>
* Additional network configuration options in <<components-nmc>>
