[#day2-k8s-upgrade]
== Kubernetes version upgrade
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc: auto

[IMPORTANT]
====
This section covers Kubernetes upgrades for downstream clusters that have *NOT* been created through a <<components-rancher,Rancher>> instance. For information on how to upgrade the Kubernetes version of `Rancher` created clusters, see link:https://ranchermanager.docs.rancher.com/v2.8/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version[Upgrading and Rolling Back Kubernetes].
====

=== Components

This section covers the custom components that the `Kubernetes upgrade` process uses over the default `Day 2` <<day2-downstream-components, components>>.

==== rke2-upgrade

Image responsible for upgrading the RKE2 version of a specific node.

Shipped through a Pod created by *SUC* based on a *SUC Plan*. The Plan should be located on each *downstream cluster* that is in need of a RKE2 upgrade.

For more information regarding how the `rke2-upgrade` image performs the upgrade, see the link:https://github.com/rancher/rke2-upgrade/tree/master[upstream] documentation.

==== k3s-upgrade

Image responsible for upgrading the K3s version of a specific node.

Shipped through a Pod created by *SUC* based on a *SUC Plan*. The Plan should be located on each *downstream cluster* that is in need of a K3s upgrade.

For more information regarding how the `k3s-upgrade` image performs the upgrade, see the link:https://github.com/k3s-io/k3s-upgrade[upstream] documentation.

=== Requirements

. *Backup your Kubernetes distribution:*

.. For *imported RKE2 clusters*, see the link:https://docs.rke2.io/backup_restore[RKE2 Backup and Restore] documentation.

.. For *imported K3s clusters*, see the link:https://docs.k3s.io/datastore/backup-restore[K3s Backup and Restore] documentation.

. *Make sure that SUC Plan tolerations match node tolerations* - If your Kubernetes cluster nodes have custom *taints*, make sure to add link:https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[tolerations] for those taints in the *SUC Plans*. By default *SUC Plans* have tolerations only for *control-plane* nodes. Default tolerations include: 

* _CriticalAddonsOnly=true:NoExecute_

* _node-role.kubernetes.io/control-plane:NoSchedule_

* _node-role.kubernetes.io/etcd:NoExecute_
+
[NOTE]
====
Any additional tolerations must be added under the `.spec.tolerations` section of each Plan. *SUC Plans* related to the Kubernetes version upgrade can be found in the link:https://github.com/suse-edge/fleet-examples[suse-edge/fleet-examples] repository under:

* For *RKE2* - `fleets/day2/system-upgrade-controller-plans/rke2-upgrade`
* For *K3s*  - `fleets/day2/system-upgrade-controller-plans/k3s-upgrade`

*Make sure you use the Plans from a valid repository link:https://github.com/suse-edge/fleet-examples/releases[release] tag.*

An example of defining custom tolerations for the RKE2 *control-plane* SUC Plan, would look like this:
[,yaml]
----
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...
----
====

=== Upgrade procedure

[NOTE]
====
This section assumes you will be deploying *SUC Plans* using <<components-fleet,Fleet>>. If you intend to deploy the *SUC Plan* using a different approach, refer to <<k8s-upgrade-suc-plan-deployment-third-party>>.
====

[IMPORTANT]
====
For environments previously upgraded using this procedure, users should ensure that *one* of the following steps is completed:

* `Remove any previously deployed SUC Plans related to older Edge release versions from the downstream cluster` - can be done by removing the desired _downstream_ cluster from the existing `GitRepo/Bundle` target configuration, or removing the `GitRepo/Bundle` resource altogether.

* `Reuse the existing GitRepo/Bundle resource` - can be done by pointing the resource's revision to a new tag that holds the correct fleets for the desired `suse-edge/fleet-examples` link:https://github.com/suse-edge/fleet-examples/releases[release].

This is done in order to avoid clashes between `SUC Plans` for older Edge release versions.

If users attempt to upgrade, while there are existing `SUC Plans` on the _downstream_ cluster, they will see the following fleet error:

[,bash]
----
Not installed: Unable to continue with install: Plan <plan_name> in namespace <plan_namespace> exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..
----
====

The `Kubernetes version upgrade procedure` revolves around deploying *SUC Plans* to downstream clusters. These plans hold information that instructs the *SUC* on which nodes to create Pods which run the `rke2/k3s-upgrade` images. For information regarding the structure of a *SUC Plan*, refer to the https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans[upstream] documentation.

`Kubernetes upgrade` Plans are shipped in the following ways:

* Through a `GitRepo` resources - <<k8s-upgrade-suc-plan-deployment-git-repo>>

* Through a `Bundle` resource - <<k8s-upgrade-suc-plan-deployment-bundle>>

To determine which resource you should use, refer to <<day2-determine-use-case>>.

For a full overview of what happens during the _update procedure_, refer to the <<k8s-version-upgrade-overview>> section.

[#k8s-version-upgrade-overview]
==== Overview

This section aims to describe the full workflow that the *_Kubernetes version upgrade process_* goes through from start to finish.

.Kubernetes version upgrade workflow
image::day2_k8s_version_upgrade_diagram.png[]

Kubernetes version upgrade steps:

. Based on his use-case, the user determines whether to use a *GitRepo* or a *Bundle* resource for the deployment of the `Kubernetes upgrade SUC Plans` to the desired downstream clusters. For information on how to map a *GitRepo/Bundle* to a specific set of downstream clusters, see https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters].

.. If you are unsure whether you should use a *GitRepo* or a *Bundle* resource for the *SUC Plan* deployment, refer to <<day2-determine-use-case>>.

.. For *GitRepo/Bundle* configuration options, refer to <<k8s-upgrade-suc-plan-deployment-git-repo>> or <<k8s-upgrade-suc-plan-deployment-bundle>>.

. The user deploys the configured *GitRepo/Bundle* resource to the `fleet-default` namespace in his `management cluster`. This is done either *manually* or through the *Rancher UI* if such is available.

. <<components-fleet,Fleet>> constantly monitors the `fleet-default` namespace and immediately detects the newly deployed *GitRepo/Bundle* resource. For more information regarding what namespaces does Fleet monitor, refer to Fleet's https://fleet.rancher.io/namespaces[Namespaces] documentation.

. If the user has deployed a *GitRepo* resource, `Fleet` will reconcile the *GitRepo* and based on its *paths* and *fleet.yaml* configurations it will deploy a *Bundle* resource in the `fleet-default` namespace. For more information, refer to Fleet's https://fleet.rancher.io/gitrepo-content[GitRepo Contents] documentation.

. `Fleet` then proceeds to deploy the `Kubernetes resources` from this *Bundle* to all the targeted `downstream clusters`. In the context of the `Kubernetes version upgrade`, Fleet deploys the following resources from the *Bundle* (depending on the Kubernetes distribution):

.. `rke2-upgrade-worker`/`k3s-upgrade-worker` - instructs *SUC* on how to do a Kubernetes upgrade on cluster *_worker_* nodes. Will *not* be interpreted if the cluster consists only from _control-plane_ nodes.

.. `rke2-upgrade-control-plane`/`k3s-upgrade-control-plane` - instructs *SUC* on how to do a Kubernetes upgrade on cluster *_control-plane_* nodes.
+
[NOTE]
====
The above *SUC Plans* will be deployed in the `cattle-system` namespace of each downstream cluster.
====

. On the downstream cluster, *SUC* picks up the newly deployed *SUC Plans* and deploys an *_Update Pod_* on each node that matches the *node selector* defined in the *SUC Plan*. For information how to monitor the *SUC Plan Pod*, refer to <<components-system-upgrade-controller-monitor-plans>>.

. Depending on which *SUC Plans* you have deployed, the *Update Pod* will run either a https://hub.docker.com/r/rancher/rke2-upgrade/tags[rke2-upgrade] or a https://hub.docker.com/r/rancher/k3s-upgrade/tags[k3s-upgrade] image and will execute the following workflow on *each* cluster node:

.. https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/[Cordon] cluster node - to ensure that no pods are scheduled accidentally on this node while it is being upgraded, we mark it as `unschedulable`.

.. Replace the `rke2/k3s` binary that is installed on the node OS with the binary shipped by the `rke2-upgrade/k3s-upgrade` image that the Pod is currently running.

.. Kill the `rke2/k3s` process that is running on the node OS - this instructs the *supervisor* to automatically restart the `rke2/k3s` process using the new version.

.. https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/[Uncordon] cluster node - after the successful Kubernetes distribution upgrade, the node is again marked as `schedulable`.
+
[NOTE]
====
For further information regarding how the `rke2-upgrade` and `k3s-upgrade` images work, see the https://github.com/rancher/rke2-upgrade[rke2-upgrade] and https://github.com/k3s-io/k3s-upgrade[k3s-upgrade] upstream projects.
====

With the above steps executed, the Kubernetes version of each cluster node should have been upgraded to the desired Edge compatible link:https://github.com/suse-edge/fleet-examples/releases[release].

[#k8s-upgrade-suc-plan-deployment]
=== Kubernetes version upgrade - SUC Plan deployment

This section describes how to orchestrate the deployment of *SUC Plans* related Kubernetes upgrades using Fleet's *GitRepo* and *Bundle* resources.

[#k8s-upgrade-suc-plan-deployment-git-repo]
==== SUC Plan deployment - GitRepo resource

A *GitRepo* resource, that ships the needed `Kubernetes upgrade` *SUC Plans*, can be deployed in one of the following ways:

. Through the `Rancher UI` - <<k8s-upgrade-suc-plan-deployment-git-repo-rancher>> (when `Rancher` is available).

. By <<k8s-upgrade-suc-plan-deployment-git-repo-manual, manually deploying>> the resource to your `management cluster`.

Once deployed, to monitor the Kubernetes upgrade process of the nodes of your targeted cluster, refer to the <<components-system-upgrade-controller-monitor-plans>> documentation.

[#k8s-upgrade-suc-plan-deployment-git-repo-rancher]
===== GitRepo creation - Rancher UI

To create a `GitRepo` resource through the Rancher UI, follow their official link:https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui[documentation].

The Edge team maintains ready to use fleets for both link:https://github.com/suse-edge/fleet-examples/tree/release-3.1.2/fleets/day2/system-upgrade-controller-plans/rke2-upgrade[rke2] and link:https://github.com/suse-edge/fleet-examples/tree/release-3.1.2/fleets/day2/system-upgrade-controller-plans/k3s-upgrade[k3s] Kubernetes distributions, that users can add as a `path` for their GitRepo resource.

[IMPORTANT]
====
Always use this fleets from a valid Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag.
====

For use-cases where no custom tolerations need to be included to the `SUC plans` that these fleets ship, users can directly refer the fleets from the `suse-edge/fleet-examples` repository.

In cases where custom tolerations are needed, users should refer the fleets from a separate repository, allowing them to add the tolerations to the SUC plans as required.

Configuration examples for a `GitRepo` resource using the fleets from `suse-edge/fleet-examples` repository:

* link:https://github.com/suse-edge/fleet-examples/blob/release-3.1.2/gitrepos/day2/rke2-upgrade-gitrepo.yaml[RKE2]

* link:https://github.com/suse-edge/fleet-examples/blob/release-3.1.2/gitrepos/day2/k3s-upgrade-gitrepo.yaml[K3s]

[#k8s-upgrade-suc-plan-deployment-git-repo-manual]
===== GitRepo creation - manual

. Pull the *GitRepo* resource:

** For *RKE2* clusters:
+
[,bash]
----
curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.2/gitrepos/day2/rke2-upgrade-gitrepo.yaml
----

** For *K3s* clusters:
+
[,bash]
----
curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.2/gitrepos/day2/k3s-upgrade-gitrepo.yaml
----

. Edit the *GitRepo* configuration, under `spec.targets` specify your desired target list. By default the `GitRepo` resources from the `suse-edge/fleet-examples` are *NOT* mapped to any downstream clusters.

** To match all clusters change the default `GitRepo` *target* to:
+
[, yaml]
----
spec:
  targets:
  - clusterSelector: {}
----

** Alternatively, if you want a more granular cluster selection see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters]


. Apply the *GitRepo* resources to your `management cluster`:
+
[,bash]
----
# RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml 

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml
----

. View the created *GitRepo* resource under the `fleet-default` namespace:
+
[,bash]
----
# RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   release-3.1.2   0/0                       
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   release-3.1.2   0/0                       
----

[#k8s-upgrade-suc-plan-deployment-bundle]
==== SUC Plan deployment - Bundle resource

A *Bundle* resource, that ships the needed `Kubernetes upgrade` *SUC Plans*, can be deployed in one of the following ways:

. Through the `Rancher UI` - <<k8s-upgrade-suc-plan-deployment-bundle-rancher>> (when `Rancher` is available).

. By <<k8s-upgrade-suc-plan-deployment-bundle-manual, manually deploying>> the resource to your `management cluster`.

Once deployed, to monitor the Kubernetes upgrade process of the nodes of your targeted cluster, refer to the <<components-system-upgrade-controller-monitor-plans>> documentation.

[#k8s-upgrade-suc-plan-deployment-bundle-rancher]
===== Bundle creation - Rancher UI

The Edge team maintains ready to use bundles for both link:https://github.com/suse-edge/fleet-examples/blob/release-3.1.2/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml[rke2] and link:https://github.com/suse-edge/fleet-examples/blob/release-3.1.2/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml[k3s] Kubernetes distributions that can be used in the below steps.

[IMPORTANT]
====
Always use this bundle from a valid Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag.
====

To create a bundle through Rancher's UI:

. In the upper left corner, click *☰ -> Continuous Delivery*

. Go to *Advanced* > *Bundles*

. Select *Create from YAML*

. From here you can create the Bundle in one of the following ways:
+
[NOTE]
====
There might be use-cases where you would need to include custom tolerations to the `SUC plans` that the bundle ships. Make sure to include those tolerations in the bundle that will be generated by the below steps.
====

.. By manually copying the bundle content for link:https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.2/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml[RKE2] or link:https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.2/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml[K3s] from `suse-edge/fleet-examples` to the *Create from YAML* page.

.. By cloning the link:https://github.com/suse-edge/fleet-examples.git[suse-edge/fleet-examples] repository from the desired link:https://github.com/suse-edge/fleet-examples/releases[release] tag and selecting the *Read from File* option in the *Create from YAML* page. From there, navigate to the bundle that you need (`bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml` for RKE2 and `bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml` for K3s). This will auto-populate the *Create from YAML* page with the bundle content.

. Change the *target* clusters for the `Bundle`:

** To match all downstream clusters change the default Bundle `.spec.targets` to:
+
[, yaml]
----
spec:
  targets:
  - clusterSelector: {}
----

** For a more granular downstream cluster mappings, see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters].

. *Create*

[#k8s-upgrade-suc-plan-deployment-bundle-manual]
===== Bundle creation - manual

. Pull the *Bundle* resources:

** For *RKE2* clusters:
+
[,bash]
----
curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.2/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml
----

** For *K3s* clusters:
+
[,bash]
----
curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.2/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml
----

. Edit the `Bundle` *target* configurations, under `spec.targets` provide your desired target list. By default the `Bundle` resources from the `suse-edge/fleet-examples` are *NOT* mapped to any downstream clusters.

** To match all clusters change the default `Bundle` *target* to:
+
[, yaml]
----
spec:
  targets:
  - clusterSelector: {}
----

** Alternatively, if you want a more granular cluster selection see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters]


. Apply the *Bundle* resources to your `management cluster`:
+
[,bash]
----
# For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml
----

. View the created *Bundle* resource under the `fleet-default` namespace:
+
[,bash]
----
# For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0                       
rke2-upgrade   0/0                       
----

[#k8s-upgrade-suc-plan-deployment-third-party]
==== SUC Plan deployment - third-party GitOps workflow

There might be use-cases where users would like to incorporate the Kubernetes upgrade resources to their own third-party GitOps workflow (e.g. `Flux`).

To get the upgrade resources that you need, first determine the Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag of the link:https://github.com/suse-edge/fleet-examples.git[suse-edge/fleet-examples] repository that you would like to use.

After that, the resources can be found at:

* For a RKE2 cluster upgrade:

** For `control-plane` nodes - `fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml`

** For `worker` nodes - `fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml`

* For a K3s cluster upgrade:

** For `control-plane` nodes - `fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml`

** For `worker` nodes - `fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml`

[IMPORTANT]
====
These `Plan` resources are interpreted by the `system-upgrade-controller` and should be deployed on each downstream cluster that you wish to upgrade. For information on how to deploy the `system-upgrade-controller`, see <<components-system-upgrade-controller-install>>.
====

To better understand how your GitOps workflow can be used to deploy the *SUC Plans* for Kubernetes version upgrade, it can be beneficial to take a look at the <<k8s-version-upgrade-overview,overview>> of the update procedure using `Fleet`.
