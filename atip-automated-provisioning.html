<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | Fully automated directed network provisioning</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Fully automated directed network provisioning"/>
<meta name="description" content="Directed network provisioning is a feature that allows you to automate the provisioning of downstream clusters. This feature is useful when you have …"/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 43. Fully automated directed network provisioning"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Fully automated directed network provisioning"/>
<meta property="og:description" content="Directed network provisioning is a feature that allows you …"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fully automated directed network provisioning"/>
<meta name="twitter:description" content="Directed network provisioning is a feature that allows you …"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    

    "headline": "Fully automated directed network provisioning",
  
    "description": "Fully automated directed network provisioning",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="atip-features.html" title="Chapter 42. Telco features configuration"/><link rel="next" href="atip-lifecycle.html" title="Chapter 44. Lifecycle actions"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-suse-telco-cloud-documentation.html">SUSE Telco Cloud Documentation</a><span> / </span><a class="crumb" href="atip-automated-provisioning.html">Fully automated directed network provisioning</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge 3.4 Documentation</span></a></li><li><a href="id-quick-starts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quick Starts</span></a><ol><li><a href="quickstart-metal3.html" class=" "><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone clusters with Edge Image Builder</span></a></li><li><a href="quickstart-suma.html" class=" "><span class="title-number">4 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-components.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">5 </span><span class="title-name">Rancher</span></a></li><li><a href="components-rancher-dashboard-extensions.html" class=" "><span class="title-number">6 </span><span class="title-name">Rancher Dashboard Extensions</span></a></li><li><a href="components-rancher-turtles.html" class=" "><span class="title-number">7 </span><span class="title-name">Rancher Turtles</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">8 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">9 </span><span class="title-name">SUSE Linux Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">10 </span><span class="title-name">Metal<sup>3</sup></span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">11 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-nmc.html" class=" "><span class="title-number">12 </span><span class="title-name">Edge Networking</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">13 </span><span class="title-name">Elemental</span></a></li><li><a href="components-akri.html" class=" "><span class="title-number">14 </span><span class="title-name">Akri</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">15 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">16 </span><span class="title-name">RKE2</span></a></li><li><a href="components-suse-storage.html" class=" "><span class="title-number">17 </span><span class="title-name">SUSE Storage</span></a></li><li><a href="components-suse-security.html" class=" "><span class="title-number">18 </span><span class="title-name">SUSE Security</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">19 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-eco.html" class=" "><span class="title-number">20 </span><span class="title-name">Endpoint Copier Operator</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">21 </span><span class="title-name">Edge Virtualization</span></a></li><li><a href="components-system-upgrade-controller.html" class=" "><span class="title-number">22 </span><span class="title-name">System Upgrade Controller</span></a></li><li><a href="components-upgrade-controller.html" class=" "><span class="title-number">23 </span><span class="title-name">Upgrade Controller</span></a></li><li><a href="components-suma.html" class=" "><span class="title-number">24 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How-To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">25 </span><span class="title-name">MetalLB on K3s (using Layer 2 Mode)</span></a></li><li><a href="guides-metallb-k3s-l3.html" class=" "><span class="title-number">26 </span><span class="title-name">MetalLB on K3s (using Layer 3 Mode)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">27 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li><li><a href="id-air-gapped-deployments-with-edge-image-builder.html" class=" "><span class="title-number">28 </span><span class="title-name">Air-gapped deployments with Edge Image Builder</span></a></li><li><a href="guides-kiwi-builder-images.html" class=" "><span class="title-number">29 </span><span class="title-name">Building Updated SUSE Linux Micro Images with Kiwi</span></a></li><li><a href="guides-clusterclass-example.html" class=" "><span class="title-number">30 </span><span class="title-name">Using clusterclass to deploy downstream clusters</span></a></li></ol></li><li><a href="tips-and-tricks.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Tips and Tricks</span></a><ol><li><a href="tips-edge-image-builder.html" class=" "><span class="title-number">31 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="tips-elemental.html" class=" "><span class="title-number">32 </span><span class="title-name">Elemental</span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">V </span><span class="title-name">Third-Party Integration</span></a><ol><li><a href="integrations-nats.html" class=" "><span class="title-number">33 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-suse-linux-micro.html" class=" "><span class="title-number">34 </span><span class="title-name">NVIDIA GPUs on SUSE Linux Micro</span></a></li></ol></li><li><a href="day-2-operations.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Day 2 Operations</span></a><ol><li><a href="day2-migration.html" class=" "><span class="title-number">35 </span><span class="title-name">Edge 3.3 migration</span></a></li><li><a href="day2-mgmt-cluster.html" class=" "><span class="title-number">36 </span><span class="title-name">Management Cluster</span></a></li><li><a href="day2-downstream-clusters.html" class=" "><span class="title-number">37 </span><span class="title-name">Downstream clusters</span></a></li></ol></li><li class="active"><a href="id-suse-telco-cloud-documentation.html" class="has-children you-are-here"><span class="title-number">VII </span><span class="title-name">SUSE Telco Cloud Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">38 </span><span class="title-name">SUSE Telco Cloud</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">39 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">40 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" "><span class="title-number">41 </span><span class="title-name">Setting up the management cluster</span></a></li><li><a href="atip-features.html" class=" "><span class="title-number">42 </span><span class="title-name">Telco features configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" you-are-here"><span class="title-number">43 </span><span class="title-name">Fully automated directed network provisioning</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">44 </span><span class="title-name">Lifecycle actions</span></a></li></ol></li><li><a href="id-troubleshooting-3.html" class="has-children "><span class="title-number">VIII </span><span class="title-name">Troubleshooting</span></a><ol><li><a href="general-troubleshooting-principles.html" class=" "><span class="title-number">45 </span><span class="title-name">General Troubleshooting Principles</span></a></li><li><a href="troubleshooting-kiwi.html" class=" "><span class="title-number">46 </span><span class="title-name">Troubleshooting Kiwi</span></a></li><li><a href="troubleshooting-edge-image-builder.html" class=" "><span class="title-number">47 </span><span class="title-name">Troubleshooting Edge Image Builder (EIB)</span></a></li><li><a href="troubleshooting-edge-networking.html" class=" "><span class="title-number">48 </span><span class="title-name">Troubleshooting Edge Networking (NMC)</span></a></li><li><a href="troubleshooting-phone-home-scenarios.html" class=" "><span class="title-number">49 </span><span class="title-name">Troubleshooting Phone-Home scenarios</span></a></li><li><a href="troubleshooting-directed-network-provisioning.html" class=" "><span class="title-number">50 </span><span class="title-name">Troubleshooting Directed-network provisioning</span></a></li><li><a href="troubleshooting-other-components.html" class=" "><span class="title-number">51 </span><span class="title-name">Troubleshooting Other components</span></a></li><li><a href="collecting-diagnostics-for-support.html" class=" "><span class="title-number">52 </span><span class="title-name">Collecting Diagnostics for Support</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">IX </span><span class="title-name">Appendix</span></a><ol><li><a href="id-release-notes.html" class=" "><span class="title-number">53 </span><span class="title-name">Release Notes</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="atip-automated-provisioning" data-id-title="Fully automated directed network provisioning"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">43 </span><span class="title-name">Fully automated directed network provisioning</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="id-introduction-3" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-introduction-3">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Directed network provisioning is a feature that allows you to automate the provisioning of downstream clusters. This feature is useful when you have many downstream clusters to provision, and you want to automate the process.</p><p>A management cluster (<a class="xref" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster">Chapter 41, <em>Setting up the management cluster</em></a>) automates deployment of the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">SUSE Linux Micro RT</code> as the OS. Depending on the use case, configurations like networking, storage, users and kernel arguments can be customized.</p></li><li class="listitem"><p><code class="literal">RKE2</code> as the Kubernetes cluster. The default <code class="literal">CNI</code> plug-in is <code class="literal">Cilium</code>. Depending on the use case, certain <code class="literal">CNI</code> plug-ins can be used, such as <code class="literal">Cilium+Multus</code>.</p></li><li class="listitem"><p><code class="literal">SUSE Storage</code></p></li><li class="listitem"><p><code class="literal">SUSE Security</code></p></li><li class="listitem"><p><code class="literal">MetalLB</code> can be used as the load balancer for highly available multi-node clusters.</p></li></ul></div><div id="id-1.9.8.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">SUSE Linux Micro</code>, see <a class="xref" href="components-slmicro.html" title="Chapter 9. SUSE Linux Micro">Chapter 9, <em>SUSE Linux Micro</em></a>
For more information about <code class="literal">RKE2</code>, see <a class="xref" href="components-rke2.html" title="Chapter 16. RKE2">Chapter 16, <em>RKE2</em></a>
For more information about <code class="literal">SUSE Storage</code>, see <a class="xref" href="components-suse-storage.html" title="Chapter 17. SUSE Storage">Chapter 17, <em>SUSE Storage</em></a>
For more information about <code class="literal">SUSE Security</code>, see <a class="xref" href="components-suse-security.html" title="Chapter 18. SUSE Security">Chapter 18, <em>SUSE Security</em></a></p></div><p>The following sections describe the different directed network provisioning workflows and some additional features that can be added to the provisioning process:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#eib-edge-image-connected" title="43.2. Prepare downstream cluster image for connected scenarios">Section 43.2, “Prepare downstream cluster image for connected scenarios”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#eib-edge-image-airgap" title="43.3. Prepare downstream cluster image for air-gap scenarios">Section 43.3, “Prepare downstream cluster image for air-gap scenarios”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#single-node" title="43.4. Downstream cluster provisioning with Directed network provisioning (single-node)">Section 43.4, “Downstream cluster provisioning with Directed network provisioning (single-node)”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#multi-node" title="43.5. Downstream cluster provisioning with Directed network provisioning (multi-node)">Section 43.5, “Downstream cluster provisioning with Directed network provisioning (multi-node)”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#advanced-network-configuration" title="43.6. Advanced Network Configuration">Section 43.6, “Advanced Network Configuration”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#add-telco" title="43.7. Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)">Section 43.7, “Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#atip-private-registry" title="43.8. Private registry">Section 43.8, “Private registry”</a></p></li><li class="listitem"><p><a class="xref" href="atip-automated-provisioning.html#airgap-deployment" title="43.9. Downstream cluster provisioning in air-gapped scenarios">Section 43.9, “Downstream cluster provisioning in air-gapped scenarios”</a></p></li></ul></div><div id="id-1.9.8.2.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The following sections show how to prepare the different scenarios for the directed network provisioning workflow using SUSE Telco Cloud.
For examples of the different configurations options for deployment (incl. air-gapped environments, DHCP and DHCP-less networks, private container registries, etc.), see the <a class="link" href="https://github.com/suse-edge/atip/tree/release-3.4/telco-examples/edge-clusters" target="_blank">SUSE Telco Cloud repository</a>.</p></div></section><section class="sect1" id="eib-edge-image-connected" data-id-title="Prepare downstream cluster image for connected scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.2 </span><span class="title-name">Prepare downstream cluster image for connected scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#eib-edge-image-connected">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Edge Image Builder (<a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a>) is used to prepare a modified SLEMicro base image which is provisioned on downstream cluster hosts.</p><p>Much of the configuration via Edge Image Builder is possible, but in this guide, we cover the minimal configurations necessary to set up the downstream cluster.</p><section class="sect2" id="id-prerequisites-for-connected-scenarios" data-id-title="Prerequisites for connected scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.2.1 </span><span class="title-name">Prerequisites for connected scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-prerequisites-for-connected-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>A container runtime such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a> is required to run Edge Image Builder.</p></li><li class="listitem"><p>The base image will be built using the following guide <a class="xref" href="guides-kiwi-builder-images.html" title="Chapter 29. Building Updated SUSE Linux Micro Images with Kiwi">Chapter 29, <em>Building Updated SUSE Linux Micro Images with Kiwi</em></a> with the profile <code class="literal">Base-SelfInstall</code> (or <code class="literal">Base-RT-SelfInstall</code> for the Real-Time kernel). The process is the same for both architectures (x86-64 and aarch64).</p></li></ul></div><div id="id-1.9.8.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>It is required to use a build host with the same architecture of the images being built. In other words, to build an <code class="literal">aarch64</code> image, it is required to use an <code class="literal">aarch64</code> build host, and vice-versa for <code class="literal">x86-64</code> (cross-builds are not supported at this time).</p></div></section><section class="sect2" id="id-image-configuration-for-connected-scenarios" data-id-title="Image configuration for connected scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.2.2 </span><span class="title-name">Image configuration for connected scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-image-configuration-for-connected-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When running Edge Image Builder, a directory is mounted from the host, so it is necessary to create a directory structure to store the configuration files used to define the target image.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">downstream-cluster-config.yaml</code> is the image definition file, see <a class="xref" href="quickstart-eib.html" title="Chapter 3. Standalone clusters with Edge Image Builder">Chapter 3, <em>Standalone clusters with Edge Image Builder</em></a> for more details.</p></li><li class="listitem"><p>The base image folder will contain the output raw image generated following the guide <a class="xref" href="guides-kiwi-builder-images.html" title="Chapter 29. Building Updated SUSE Linux Micro Images with Kiwi">Chapter 29, <em>Building Updated SUSE Linux Micro Images with Kiwi</em></a> with the profile <code class="literal">Base-SelfInstall</code> (or <code class="literal">Base-RT-SelfInstall</code> for the Real-Time kernel) must be copied/moved under the <code class="literal">base-images</code> folder.</p></li><li class="listitem"><p>The <code class="literal">network</code> folder is optional, see <a class="xref" href="atip-automated-provisioning.html#add-network-eib" title="43.2.2.6. Additional script for Advanced Network Configuration">Section 43.2.2.6, “Additional script for Advanced Network Configuration”</a> for more details.</p></li><li class="listitem"><p>The <code class="literal">custom/scripts</code> directory contains scripts to be run on first-boot:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><code class="literal">01-fix-growfs.sh</code> script is required to resize the OS root partition on deployment</p></li><li class="listitem"><p><code class="literal">02-performance.sh</code> script is optional and can be used to configure the system for performance tuning.</p></li><li class="listitem"><p><code class="literal">03-sriov.sh</code> script is optional and can be used to configure the system for SR-IOV.</p></li></ol></div></li><li class="listitem"><p>The <code class="literal">custom/files</code> directory contains the <code class="literal">performance-settings.sh</code> and <code class="literal">sriov-auto-filler.sh</code> files to be copied to the image during the image creation process.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</pre></div><section class="sect3" id="id-downstream-cluster-image-definition-file-2" data-id-title="Downstream cluster image definition file"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.2.2.1 </span><span class="title-name">Downstream cluster image definition file</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-downstream-cluster-image-definition-file-2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">downstream-cluster-config.yaml</code> file is the main configuration file for the downstream cluster image. The following is a minimal example for deployment via Metal<sup>3</sup>:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</pre></div><p>Where <code class="literal">$SCC_REGISTRATION_CODE</code> is the registration code copied from <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>, and the package list contains <code class="literal">jq</code> which is required.</p><p><code class="literal">$ROOT_PASSWORD</code> is the encrypted password for the root user, which can be useful for test/debugging.  It can be generated with the <code class="literal">openssl passwd -6 PASSWORD</code> command</p><p>For the production environments, it is recommended to use the SSH keys that can be added to the users block replacing the <code class="literal">$USERKEY1</code> with the real SSH keys.</p><div id="id-1.9.8.3.5.5.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p><code class="literal">arch: x86_64</code> is the architecture of the image. For arm64 architecture, use <code class="literal">arch: aarch64</code>.</p><p><code class="literal">net.ifnames=1</code> enables <a class="link" href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html" target="_blank">Predictable Network Interface Naming</a></p><p>This matches the default configuration for the metal3 chart, but the setting must match the configured chart <code class="literal">predictableNicNames</code> value.</p><p>Also note <code class="literal">ignition.platform.id=openstack</code> is mandatory, without this argument SLEMicro configuration via ignition will fail in the Metal<sup>3</sup> automated flow.</p></div></section><section class="sect3" id="add-custom-script-growfs" data-id-title="Growfs script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.2.2.2 </span><span class="title-name">Growfs script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-custom-script-growfs">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Currently, a custom script (<code class="literal">custom/scripts/01-fix-growfs.sh</code>) is required to grow the file system to match the disk size on first-boot after provisioning. The <code class="literal">01-fix-growfs.sh</code> script contains the following information:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</pre></div></section><section class="sect3" id="add-custom-script-performance" data-id-title="Performance script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.2.2.3 </span><span class="title-name">Performance script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-custom-script-performance">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following optional script (<code class="literal">custom/scripts/02-performance.sh</code>) can be used to configure the system for performance tuning:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</pre></div><p>The content of <code class="literal">custom/files/performance-settings.sh</code> is a script that can be used to configure the system for performance tuning and can be downloaded from the following <a class="link" href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh" target="_blank">link</a>.</p></section><section class="sect3" id="add-custom-script-sriov" data-id-title="SR-IOV script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.2.2.4 </span><span class="title-name">SR-IOV script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-custom-script-sriov">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following optional script (<code class="literal">custom/scripts/03-sriov.sh</code>) can be used to configure the system for SR-IOV:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</pre></div><p>The content of <code class="literal">custom/files/sriov-auto-filler.sh</code> is a script that can be used to configure the system for SR-IOV and can be downloaded from the following <a class="link" href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh" target="_blank">link</a>.</p><div id="id-1.9.8.3.5.8.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Add your own custom scripts to be executed during the provisioning process using the same approach.
For more information, see <a class="xref" href="quickstart-eib.html" title="Chapter 3. Standalone clusters with Edge Image Builder">Chapter 3, <em>Standalone clusters with Edge Image Builder</em></a>.</p></div></section><section class="sect3" id="add-telco-feature-eib" data-id-title="Additional configuration for Telco workloads"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.2.2.5 </span><span class="title-name">Additional configuration for Telco workloads</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-telco-feature-eib">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To enable Telco features like <code class="literal">dpdk</code>, <code class="literal">sr-iov</code> or <code class="literal">FEC</code>, additional packages may be required as shown in the following example.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $user1Key1
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    sccRegistrationCode: $SCC_REGISTRATION_CODE</pre></div><p>Where <code class="literal">$SCC_REGISTRATION_CODE</code> is the registration code copied from <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>, and the package list contains the minimum packages to be used for the Telco profiles.</p><div id="id-1.9.8.3.5.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p><code class="literal">arch: x86_64</code> is the architecture of the image. For arm64 architecture, use <code class="literal">arch: aarch64</code>.</p></div></section><section class="sect3" id="add-network-eib" data-id-title="Additional script for Advanced Network Configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.2.2.6 </span><span class="title-name">Additional script for Advanced Network Configuration</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-network-eib">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>If you need to configure static IPs or more advanced networking scenarios as described in <a class="xref" href="atip-automated-provisioning.html#advanced-network-configuration" title="43.6. Advanced Network Configuration">Section 43.6, “Advanced Network Configuration”</a>, the following additional configuration is required.</p><p>In the <code class="literal">network</code> folder, create the following <code class="literal">configure-network.sh</code> file - this consumes configuration drive data on first-boot, and configures the
host networking using the <a class="link" href="https://github.com/suse-edge/nm-configurator" target="_blank">NM Configurator tool</a>.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</pre></div></section></section><section class="sect2" id="id-image-creation-2" data-id-title="Image creation"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.2.3 </span><span class="title-name">Image creation</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-image-creation-2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Once the directory structure is prepared following the previous sections, run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</pre></div><p>This creates the output ISO image file named <code class="literal">eibimage-output-telco.raw</code>, based on the definition described above.</p><p>The output image must then be made available via a webserver, either the media-server container enabled via the Management Cluster Documentation (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>)
or some other locally accessible server.  In the examples below, we refer to this server as <code class="literal">imagecache.local:8080</code></p></section></section><section class="sect1" id="eib-edge-image-airgap" data-id-title="Prepare downstream cluster image for air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.3 </span><span class="title-name">Prepare downstream cluster image for air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#eib-edge-image-airgap">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Edge Image Builder (<a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a>) is used to prepare a modified SLEMicro base image which is provisioned on downstream cluster hosts.</p><p>Much of the configuration is possible with Edge Image Builder, but in this guide, we cover the minimal configurations necessary to set up the downstream cluster for air-gap scenarios.</p><section class="sect2" id="id-prerequisites-for-air-gap-scenarios" data-id-title="Prerequisites for air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.3.1 </span><span class="title-name">Prerequisites for air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-prerequisites-for-air-gap-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>A container runtime such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a> is required to run Edge Image Builder.</p></li><li class="listitem"><p>The base image will be built using the following guide <a class="xref" href="guides-kiwi-builder-images.html" title="Chapter 29. Building Updated SUSE Linux Micro Images with Kiwi">Chapter 29, <em>Building Updated SUSE Linux Micro Images with Kiwi</em></a> with the profile <code class="literal">Base-SelfInstall</code> (or <code class="literal">Base-RT-SelfInstall</code> for the Real-Time kernel). The process is the same for both architectures (x86-64 and aarch64).</p></li><li class="listitem"><p>If you want to use SR-IOV or any other workload which require a container image, a local private registry must be deployed and already configured (with/without TLS and/or authentication). This registry will be used to store the images and the helm chart OCI images.</p></li></ul></div><div id="id-1.9.8.4.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>It is required to use a build host with the same architecture of the images being built. In other words, to build an <code class="literal">aarch64</code> image, it is required to use an <code class="literal">aarch64</code> build host, and vice-versa for <code class="literal">x86-64</code> (cross-builds are not supported at this time).</p></div></section><section class="sect2" id="id-image-configuration-for-air-gap-scenarios" data-id-title="Image configuration for air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.3.2 </span><span class="title-name">Image configuration for air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-image-configuration-for-air-gap-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When running Edge Image Builder, a directory is mounted from the host, so it is necessary to create a directory structure to store the configuration files used to define the target image.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">downstream-cluster-airgap-config.yaml</code> is the image definition file, see <a class="xref" href="quickstart-eib.html" title="Chapter 3. Standalone clusters with Edge Image Builder">Chapter 3, <em>Standalone clusters with Edge Image Builder</em></a> for more details.</p></li><li class="listitem"><p>The base image folder will contain the output raw image generated following the guide <a class="xref" href="guides-kiwi-builder-images.html" title="Chapter 29. Building Updated SUSE Linux Micro Images with Kiwi">Chapter 29, <em>Building Updated SUSE Linux Micro Images with Kiwi</em></a> with the profile <code class="literal">Base-SelfInstall</code> (or <code class="literal">Base-RT-SelfInstall</code> for the Real-Time kernel) must be copied/moved under the <code class="literal">base-images</code> folder.</p></li><li class="listitem"><p>The <code class="literal">network</code> folder is optional, see <a class="xref" href="atip-automated-provisioning.html#add-network-eib" title="43.2.2.6. Additional script for Advanced Network Configuration">Section 43.2.2.6, “Additional script for Advanced Network Configuration”</a> for more details.</p></li><li class="listitem"><p>The <code class="literal">custom/scripts</code> directory contains scripts to be run on first-boot:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><code class="literal">01-fix-growfs.sh</code> script is required to resize the OS root partition on deployment.</p></li><li class="listitem"><p><code class="literal">02-airgap.sh</code> script is required to copy the images to the right place during the image creation process for air-gapped environments.</p></li><li class="listitem"><p><code class="literal">03-performance.sh</code> script is optional and can be used to configure the system for performance tuning.</p></li><li class="listitem"><p><code class="literal">04-sriov.sh</code> script is optional and can be used to configure the system for SR-IOV.</p></li></ol></div></li><li class="listitem"><p>The <code class="literal">custom/files</code> directory contains the <code class="literal">rke2</code> and the <code class="literal">cni</code> images to be copied to the image during the image creation process. Also, the optional <code class="literal">performance-settings.sh</code> and <code class="literal">sriov-auto-filler.sh</code> files can be included.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</pre></div><section class="sect3" id="id-downstream-cluster-image-definition-file-3" data-id-title="Downstream cluster image definition file"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.1 </span><span class="title-name">Downstream cluster image definition file</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-downstream-cluster-image-definition-file-3">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">downstream-cluster-airgap-config.yaml</code> file is the main configuration file for the downstream cluster image and the content has been described in the previous section (<a class="xref" href="atip-automated-provisioning.html#add-telco-feature-eib" title="43.2.2.5. Additional configuration for Telco workloads">Section 43.2.2.5, “Additional configuration for Telco workloads”</a>).</p></section><section class="sect3" id="id-growfs-script" data-id-title="Growfs script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.2 </span><span class="title-name">Growfs script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-growfs-script">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Currently, a custom script (<code class="literal">custom/scripts/01-fix-growfs.sh</code>) is required to grow the file system to match the disk size on first-boot after provisioning. The <code class="literal">01-fix-growfs.sh</code> script contains the following information:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</pre></div></section><section class="sect3" id="id-air-gap-script" data-id-title="Air-gap script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.3 </span><span class="title-name">Air-gap script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-air-gap-script">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following script (<code class="literal">custom/scripts/02-airgap.sh</code>) is required to copy the images to the right place during the image creation process:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</pre></div></section><section class="sect3" id="add-custom-script-performance2" data-id-title="Performance script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.4 </span><span class="title-name">Performance script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-custom-script-performance2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following optional script (<code class="literal">custom/scripts/03-performance.sh</code>) can be used to configure the system for performance tuning:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</pre></div><p>The content of <code class="literal">custom/files/performance-settings.sh</code> is a script that can be used to configure the system for performance tuning and can be downloaded from the following <a class="link" href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh" target="_blank">link</a>.</p></section><section class="sect3" id="add-custom-script-sriov2" data-id-title="SR-IOV script"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.5 </span><span class="title-name">SR-IOV script</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-custom-script-sriov2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following optional script (<code class="literal">custom/scripts/04-sriov.sh</code>) can be used to configure the system for SR-IOV:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</pre></div><p>The content of <code class="literal">custom/files/sriov-auto-filler.sh</code> is a script that can be used to configure the system for SR-IOV and can be downloaded from the following <a class="link" href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh" target="_blank">link</a>.</p></section><section class="sect3" id="id-custom-files-for-air-gap-scenarios" data-id-title="Custom files for air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.6 </span><span class="title-name">Custom files for air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-custom-files-for-air-gap-scenarios">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">custom/files</code> directory contains the <code class="literal">rke2</code> and the <code class="literal">cni</code> images to be copied to the image during the image creation process.
To easily generate the images, prepare them locally using following <a class="link" href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh" target="_blank">script</a> and the list of images <a class="link" href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-release-rke2-images.txt" target="_blank">here</a> to generate the artifacts required to be included in <code class="literal">custom/files</code>.
Also, you can download the latest <code class="literal">rke2-install</code> script from <a class="link" href="https://get.rke2.io/" target="_blank">here</a>.</p><div class="verbatim-wrap"><pre class="screen">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</pre></div><p>After downloading the images, the directory structure should look like this:</p><div class="verbatim-wrap"><pre class="screen">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</pre></div></section><section class="sect3" id="preload-private-registry" data-id-title="Preload your private registry with images required for air-gap scenarios and SR-IOV (optional)"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">43.3.2.7 </span><span class="title-name">Preload your private registry with images required for air-gap scenarios and SR-IOV (optional)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#preload-private-registry">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>If you want to use SR-IOV in your air-gap scenario or any other workload images, you must preload your local private registry with the images following the next steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Download, extract, and push the helm-chart OCI images to the private registry</p></li><li class="listitem"><p>Download, extract, and push the rest of images required to the private registry</p></li></ul></div><p>The following scripts can be used to download, extract, and push the images to the private registry. We will show an example to preload the SR-IOV images, but you can also use the same approach to preload any other custom images:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Preload with helm-chart OCI images for SR-IOV:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>You must create a list with the helm-chart OCI images required:</p><div class="verbatim-wrap"><pre class="screen">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:304.0.2+up1.5.0
edge/sriov-crd-chart:304.0.2+up1.5.0
EOF</pre></div></li><li class="listitem"><p>Generate a local tarball file using the following <a class="link" href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-oci-artefacts.sh" target="_blank">script</a> and the list created above:</p><div class="verbatim-wrap"><pre class="screen">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/charts/sriov-network-operator:304.0.2+up1.5.0
Pulled: registry.suse.com/edge/charts/sriov-crd:304.0.2+up1.5.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-304.0.2+up1.5.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-304.0.2+up1.5.0.tgz</pre></div></li><li class="listitem"><p>Upload your tarball file to your private registry (e.g. <code class="literal">myregistry:5000</code>) using the following <a class="link" href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-oci-artefacts.sh" target="_blank">script</a> to preload your registry with the helm chart OCI images downloaded in the previous step:</p><div class="verbatim-wrap"><pre class="screen">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</pre></div></li></ol></div></li><li class="listitem"><p>Preload with the rest of the images required for SR-IOV:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>In this case, we must include the `sr-iov container images for telco workloads (e.g. as a reference, you could get them from <a class="link" href="https://github.com/suse-edge/charts/blob/main/charts/sriov-network-operator/1.5.0/values.yaml" target="_blank">helm-chart values</a>)</p><div class="verbatim-wrap"><pre class="screen">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</pre></div></li><li class="listitem"><p>Using the following <a class="link" href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh" target="_blank">script</a> and the list created above, you must generate locally the tarball file with the images required:</p><div class="verbatim-wrap"><pre class="screen">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</pre></div></li><li class="listitem"><p>Upload your tarball file to your private registry (e.g. <code class="literal">myregistry:5000</code>) using the following <a class="link" href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-images.sh" target="_blank">script</a> to preload your private registry with the images downloaded in the previous step:</p><div class="verbatim-wrap"><pre class="screen">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</pre></div></li></ol></div></li></ol></div></section></section><section class="sect2" id="id-image-creation-for-air-gap-scenarios" data-id-title="Image creation for air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.3.3 </span><span class="title-name">Image creation for air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-image-creation-for-air-gap-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Once the directory structure is prepared following the previous sections, run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-airgap-config.yaml</pre></div><p>This creates the output ISO image file named <code class="literal">eibimage-output-telco.raw</code>, based on the definition described above.</p><p>The output image must then be made available via a webserver, either the media-server container enabled via the Management Cluster Documentation (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>)
or some other locally accessible server.  In the examples below, we refer to this server as <code class="literal">imagecache.local:8080</code>.</p></section></section><section class="sect1" id="single-node" data-id-title="Downstream cluster provisioning with Directed network provisioning (single-node)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.4 </span><span class="title-name">Downstream cluster provisioning with Directed network provisioning (single-node)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#single-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes the workflow used to automate the provisioning of a single-node downstream cluster using directed network provisioning.
This is the simplest way to automate the provisioning of a downstream cluster.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code>, as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#eib-edge-image-connected" title="43.2. Prepare downstream cluster image for connected scenarios">Section 43.2, “Prepare downstream cluster image for connected scenarios”</a>), with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>).</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. For more information, refer to the Management Cluster section <a class="xref" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster">Chapter 41, <em>Setting up the management cluster</em></a>.</p></li></ul></div><p><span class="strong"><strong>Workflow</strong></span></p><p>The following diagram shows the workflow used to automate the provisioning of a single-node downstream cluster using directed network provisioning:</p><div class="informalfigure"><div class="mediaobject"><a href="images/atip-automated-singlenode1.png"><img src="images/atip-automated-singlenode1.png" width="100%" alt="atip automated singlenode1" title="atip automated singlenode1"/></a></div></div><p>There are two different steps to automate the provisioning of a single-node downstream cluster using directed network provisioning:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enroll the bare-metal host to make it available for the provisioning process.</p></li><li class="listitem"><p>Provision the bare-metal host to install and configure the operating system and the Kubernetes cluster.</p></li></ol></div><p id="enroll-bare-metal-host"><span class="strong"><strong>Enroll the bare-metal host</strong></span></p><p>The first step is to enroll the new bare-metal host in the management cluster to make it available to be provisioned.
To do that, the following file (<code class="literal">bmh-example.yaml</code>) has to be created in the management cluster, to specify the <code class="literal">BMC</code> credentials to be used and the <code class="literal">BaremetalHost</code> object to be enrolled:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</pre></div><p>where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${BMC_USERNAME}</code> — The user name for the <code class="literal">BMC</code> of the new bare-metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_PASSWORD}</code> — The password for the <code class="literal">BMC</code> of the new bare-metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_MAC}</code> — The <code class="literal">MAC</code> address of the new bare-metal host to be used.</p></li><li class="listitem"><p><code class="literal">${BMC_ADDRESS}</code> — The <code class="literal">URL</code> for the bare-metal host <code class="literal">BMC</code> (for example, <code class="literal">redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</code>). To learn more about the different options available depending on your hardware provider, check the following <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">link</a>.</p></li></ul></div><div id="id-1.9.8.5.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Architecture must be either <code class="literal">x86_64</code> or <code class="literal">aarch64</code>, depending on the architecture of the bare-metal host to be enrolled.</p></li><li class="listitem"><p>If no network configuration for the host has been specified, either at image build time or through the <code class="literal">BareMetalHost</code> definition, an autoconfiguration mechanism (DHCP, DHCPv6, SLAAC) will be used. For more details or complex configurations, check the <a class="xref" href="atip-automated-provisioning.html#advanced-network-configuration" title="43.6. Advanced Network Configuration">Section 43.6, “Advanced Network Configuration”</a>.</p></li></ul></div></div><p>Once the file is created, the following command has to be executed in the management cluster to start enrolling the new bare-metal host in the management cluster:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f bmh-example.yaml</pre></div><p>The new bare-metal host object will be enrolled, changing its state from registering to inspecting and available. The changes can be checked using the following command:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get bmh</pre></div><div id="id-1.9.8.5.20" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">BaremetalHost</code> object is in the <code class="literal">registering</code> state until the <code class="literal">BMC</code> credentials are validated. Once the credentials are validated, the <code class="literal">BaremetalHost</code> object changes its state to <code class="literal">inspecting</code>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. Check the information using the following command: <code class="literal">kubectl get bmh -o yaml</code>.</p></div><p id="single-node-provision"><span class="strong"><strong>Provision step</strong></span></p><p>Once the bare-metal host is enrolled and available, the next step is to provision the bare-metal host to install and configure the operating system and the Kubernetes cluster.
To do that, the following file (<code class="literal">capi-provisioning-example.yaml</code>) has to be created in the management-cluster with the following information (the <code class="literal">capi-provisioning-example.yaml</code> can be generated by joining the following blocks).</p><div id="id-1.9.8.5.23" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Only values between <code class="literal">$\{…​\}</code> must be replaced with the real values.</p></div><p>The following block is the cluster definition, where the networking can be configured using the <code class="literal">pods</code> and the <code class="literal">services</code> blocks. Also, it contains the references to the control plane and the infrastructure (using the <code class="literal">Metal<sup>3</sup></code> provider) objects to be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:bad:cafe::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:bad:bad:cafe::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</pre></div><div id="id-1.9.8.5.26" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Both single-stack and dual-stack deployments are possible, remove the IPv6 CIDRs from the above definition for an IPv4 only cluster.</p></li><li class="listitem"><p>Single-stack IPv6 deployments are in tech preview status and not yet officially supported.</p></li></ul></div></div><p>The <code class="literal">Metal3Cluster</code> object specifies the control-plane endpoint (replacing the <code class="literal">${DOWNSTREAM_CONTROL_PLANE_IPV4}</code>) to be configured and the <code class="literal">noCloudProvider</code> because a bare-metal node is used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IPV4}
    port: 6443
  noCloudProvider: true</pre></div><p>The <code class="literal">RKE2ControlPlane</code> object specifies the control-plane configuration to be used and the <code class="literal">Metal3MachineTemplate</code> object specifies the control-plane image to be used.
Also, it contains the information about the number of replicas to be used (in this case, one) and the <code class="literal">CNI</code> plug-in to be used (in this case, <code class="literal">Cilium</code>).
The agentConfig block contains the <code class="literal">Ignition</code> format to be used and the <code class="literal">additionalUserData</code> to be used to configure the <code class="literal">RKE2</code> node with information like a systemd named <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.
To enable multus with cilium a file is created in the <code class="literal">rke2</code> server manifests directory named <code class="literal">rke2-cilium-config.yaml</code> with the configuration to be used.
The last block of information contains the Kubernetes version to be used. <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (for example, <code class="literal">v1.33.3+rke2r1</code>).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object specifies the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">dataTemplate</code> to be used as a reference to the template.</p></li><li class="listitem"><p>The <code class="literal">hostSelector</code> to be used matching with the label created during the enrollment process.</p></li><li class="listitem"><p>The <code class="literal">image</code> to be used as a reference to the image generated using <code class="literal">EIB</code> on the previous section (<a class="xref" href="atip-automated-provisioning.html#eib-edge-image-connected" title="43.2. Prepare downstream cluster image for connected scenarios">Section 43.2, “Prepare downstream cluster image for connected scenarios”</a>), and the <code class="literal">checksum</code> and <code class="literal">checksumType</code> to be used to validate the image.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object specifies the <code class="literal">metaData</code> for the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once the file is created by joining the previous blocks, the following command must be executed in the management cluster to start provisioning the new bare-metal host:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect1" id="multi-node" data-id-title="Downstream cluster provisioning with Directed network provisioning (multi-node)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.5 </span><span class="title-name">Downstream cluster provisioning with Directed network provisioning (multi-node)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#multi-node">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes the workflow used to automate the provisioning of a multi-node downstream cluster using directed network provisioning and <code class="literal">MetalLB</code> as a load-balancer strategy.
This is the simplest way to automate the provisioning of a downstream cluster. The following diagram shows the workflow used to automate the provisioning of a multi-node downstream cluster using directed network provisioning and <code class="literal">MetalLB</code>.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code>, as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#eib-edge-image-connected" title="43.2. Prepare downstream cluster image for connected scenarios">Section 43.2, “Prepare downstream cluster image for connected scenarios”</a>), with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>).</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. For more information, refer to the Management Cluster section: <a class="xref" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster">Chapter 41, <em>Setting up the management cluster</em></a>.</p></li></ul></div><p><span class="strong"><strong>Workflow</strong></span></p><p>The following diagram shows the workflow used to automate the provisioning of a multi-node downstream cluster using directed network provisioning:</p><div class="informalfigure"><div class="mediaobject"><a href="images/atip-automate-multinode1.png"><img src="images/atip-automate-multinode1.png" width="100%" alt="atip automate multinode1" title="atip automate multinode1"/></a></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Enroll the three bare-metal hosts to make them available for the provisioning process.</p></li><li class="listitem"><p>Provision the three bare-metal hosts to install and configure the operating system and the Kubernetes cluster using <code class="literal">MetalLB</code>.</p></li></ol></div><p><span class="strong"><strong>Enroll the bare-metal hosts</strong></span></p><p>The first step is to enroll the three bare-metal hosts in the management cluster to make them available to be provisioned.
To do that, the following files (<code class="literal">bmh-example-node1.yaml</code>, <code class="literal">bmh-example-node2.yaml</code> and <code class="literal">bmh-example-node3.yaml</code>) must be created in the management cluster, to specify the <code class="literal">BMC</code> credentials to be used and the <code class="literal">BaremetalHost</code> object to be enrolled in the management cluster.</p><div id="id-1.9.8.6.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Only the values between <code class="literal">$\{…​\}</code> have to be replaced with the real values.</p></li><li class="listitem"><p>We will walk you through the process for only one host. The same steps apply to the other two nodes.</p></li></ul></div></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</pre></div><p>Where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${BMC_NODE1_USERNAME}</code> — The username for the BMC of the first bare-metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_PASSWORD}</code> — The password for the BMC of the first bare-metal host.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_MAC}</code> — The MAC address of the first bare-metal host to be used.</p></li><li class="listitem"><p><code class="literal">${BMC_NODE1_ADDRESS}</code> — The URL for the first bare-metal host BMC (for example, <code class="literal">redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</code>). The host part of the URL can be an IP address (v4 or v6) or a domain name, where the existing infrastructure allows. To learn more about the different options available depending on your hardware provider, check the following <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">link</a>.</p></li></ul></div><div id="id-1.9.8.6.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>If no network configuration for the host has been specified, either at image build time or through the <code class="literal">BareMetalHost</code> definition, an autoconfiguration mechanism (DHCP, DHCPv6, SLAAC) will be used. For more details or complex configurations, check the <a class="xref" href="atip-automated-provisioning.html#advanced-network-configuration" title="43.6. Advanced Network Configuration">Section 43.6, “Advanced Network Configuration”</a>.</p></li><li class="listitem"><p>Single-stack IPv6 clusters are in tech preview status and not yet officially supported.</p></li><li class="listitem"><p>Architecture must be either <code class="literal">x86_64</code> or <code class="literal">aarch64</code>, depending on the architecture of the bare-metal host to be enrolled.</p></li><li class="listitem"><p>All modern servers come with a dual-stack capable BMC, however IPv6 support (and possibly the option of using hostnames for the VirtualMedia capability) should be verified before use in production in a dual-stack environment.</p></li></ul></div></div><p>Once the file is created, the following command must be executed in the management cluster to start enrolling the bare-metal hosts in the management cluster:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</pre></div><p>The new bare-metal host objects are enrolled, changing their state from registering to inspecting and available. The changes can be checked using the following command:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get bmh -o wide</pre></div><div id="id-1.9.8.6.20" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">BaremetalHost</code> object is in the <code class="literal">registering</code> state until the <code class="literal">BMC</code> credentials are validated. Once the credentials are validated, the <code class="literal">BaremetalHost</code> object changes its state to <code class="literal">inspecting</code>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. Check the information using the following command: <code class="literal">kubectl get bmh -o yaml</code>.</p></div><p><span class="strong"><strong>Provision step</strong></span></p><p>Once the three bare-metal hosts are enrolled and available, the next step is to provision the bare-metal hosts to install and configure the operating system and the Kubernetes cluster, creating a load balancer to manage them.
To do that, the following file (<code class="literal">capi-provisioning-example.yaml</code>) must be created in the management cluster with the following information (the `capi-provisioning-example.yaml can be generated by joining the following blocks).</p><div id="id-1.9.8.6.23" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Only values between <code class="literal">$\{…​\}</code> must be replaced with the real values.</p></li><li class="listitem"><p>The <code class="literal">VIP</code> address is a reserved IP address that is not assigned to any node and is used to configure the load balancer. In a dual-stack cluster, both an IPv4 and IPv6 can be specified, but in the following examples priority will be given to the IPv4 address.</p></li></ul></div></div><p>Below is the cluster definition, where the cluster network can be configured using the <code class="literal">pods</code> and the <code class="literal">services</code> blocks. Also, it contains the references to the control plane and the infrastructure (using the <code class="literal">Metal<sup>3</sup></code> provider) objects to be used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:1234:4321::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:5678:8765:4321::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</pre></div><div id="id-1.9.8.6.26" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Both single-stack and dual-stack deployments are possible, remove the IPv6 CIDRs and IPv6 VIP addresses (in the subsequent sections) for an IPv4 only cluster</p></div><p>The <code class="literal">Metal3Cluster</code> object specifies the control-plane endpoint that uses the <code class="literal">VIP</code> address already reserved (replacing the <code class="literal">${EDGE_VIP_ADDRESS_IPV4}</code>) to be configured and the <code class="literal">noCloudProvider</code> because the three bare-metal nodes are used.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS_IPV4}
    port: 6443
  noCloudProvider: true</pre></div><p>The <code class="literal">RKE2ControlPlane</code> object specifies the control-plane configuration to be used, and the <code class="literal">Metal3MachineTemplate</code> object specifies the control-plane image to be used.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The number of replicas to be used (in this case, three).</p></li><li class="listitem"><p>The advertisement mode to be used by the Load Balancer (<code class="literal">address</code> uses the L2 implementation), as well as the address to be used (replacing the <code class="literal">${EDGE_VIP_ADDRESS}</code> with the <code class="literal">VIP</code> address).</p></li><li class="listitem"><p>The <code class="literal">serverConfig</code> with the <code class="literal">CNI</code> plug-in to be used (in this case, <code class="literal">Cilium</code>), and the additional <code class="literal">VIP</code> address(es) and name(s) to be listed under <code class="literal">tlsSan</code>.</p></li><li class="listitem"><p>The agentConfig block contains the <code class="literal">Ignition</code> format to be used and the <code class="literal">additionalUserData</code> to be used to configure the <code class="literal">RKE2</code> node with information like:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The systemd service named <code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.</p></li><li class="listitem"><p>The <code class="literal">storage</code> block which contains the Helm charts to be used to install the <code class="literal">MetalLB</code> and the <code class="literal">endpoint-copier-operator</code>.</p></li><li class="listitem"><p>The <code class="literal">metalLB</code> custom resource file with the <code class="literal">IPaddressPool</code> and the <code class="literal">L2Advertisement</code> to be used (replacing <code class="literal">${EDGE_VIP_ADDRESS_IPV4}</code> with the <code class="literal">VIP</code> address).</p></li><li class="listitem"><p>The <code class="literal">endpoint-svc.yaml</code> file to be used to configure the <code class="literal">kubernetes-vip</code> service to be used by the <code class="literal">MetalLB</code> to manage the <code class="literal">VIP</code> address.</p></li></ul></div></li><li class="listitem"><p>The last block of information contains the Kubernetes version to be used. The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (for example, <code class="literal">v1.33.3+rke2r1</code>).</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS_IPV4}
      - ${EDGE_VIP_ADDRESS_IPV6}
      - https://${EDGE_VIP_ADDRESS_IPV4}.sslip.io
      - https://${EDGE_VIP_ADDRESS_IPV6}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS_IPV4}/32
                      - ${EDGE_VIP_ADDRESS_IPV6}/128
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ipFamilyPolicy: PreferDualStack
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "Node-multinode-cluster"</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object specifies the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">dataTemplate</code> to be used as a reference to the template.</p></li><li class="listitem"><p>The <code class="literal">hostSelector</code> to be used matching with the label created during the enrollment process.</p></li><li class="listitem"><p>The <code class="literal">image</code> to be used as a reference to the image generated using <code class="literal">EIB</code> on the previous section (<a class="xref" href="atip-automated-provisioning.html#eib-edge-image-connected" title="43.2. Prepare downstream cluster image for connected scenarios">Section 43.2, “Prepare downstream cluster image for connected scenarios”</a>), and <code class="literal">checksum</code> and <code class="literal">checksumType</code> to be used to validate the image.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object specifies the <code class="literal">metaData</code> for the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>The following yaml files are an example configuration for the worker nodes.</p><p>A <code class="literal">MachineDeployment</code>:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: multinode-cluster
    nodepool: nodepool-0
  name: multinode-cluster-workers
  namespace: default
spec:
  clusterName: multinode-cluster
  replicas: 3
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: multinode-cluster
      nodepool: nodepool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: multinode-cluster
        nodepool: nodepool-0
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: RKE2ConfigTemplate
          name: multinode-cluster-workers
      clusterName: multinode-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: multinode-cluster-workers
      nodeDrainTimeout: 0s
      version: ${RKE2_VERSION}</pre></div><p>The RKE2ConfigTemplate` object specifies the configuration template to be used for multinode cluster worker nodes.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: RKE2ConfigTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "Node-multinode-cluster-worker"
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target</pre></div><p>The <code class="literal">Metal3MachineTemplate</code> object contain references to <code class="literal">dataTemplate</code>, <code class="literal">hostSelector</code>, and <code class="literal">image</code> for the worker nodes:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw</pre></div><p>The <code class="literal">Metal3DataTemplate</code> object specifies the <code class="literal">metaData</code> for the downstream cluster for the worker nodes:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-workers-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once the file is created by joining the previous blocks, run the following command in the management cluster to start provisioning the new three bare-metal hosts:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect1" id="advanced-network-configuration" data-id-title="Advanced Network Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.6 </span><span class="title-name">Advanced Network Configuration</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#advanced-network-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The directed network provisioning workflow allows for specific network configurations in downstream clusters, such as static IPs, bonding, VLANs, IPv6, etc.</p><p>The following sections describe the additional steps required to enable provisioning downstream clusters using advanced network configuration.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code> has to include the network folder and the script following this section (<a class="xref" href="atip-automated-provisioning.html#add-network-eib" title="43.2.2.6. Additional script for Advanced Network Configuration">Section 43.2.2.6, “Additional script for Advanced Network Configuration”</a>).</p></li></ul></div><p><span class="strong"><strong>Configuration</strong></span></p><p>Before proceeding refer to one of the following sections for guidance on the steps required to enroll and provision the host(s):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Downstream cluster provisioning with Directed network provisioning (single-node) (<a class="xref" href="atip-automated-provisioning.html#single-node" title="43.4. Downstream cluster provisioning with Directed network provisioning (single-node)">Section 43.4, “Downstream cluster provisioning with Directed network provisioning (single-node)”</a>)</p></li><li class="listitem"><p>Downstream cluster provisioning with Directed network provisioning (multi-node) (<a class="xref" href="atip-automated-provisioning.html#multi-node" title="43.5. Downstream cluster provisioning with Directed network provisioning (multi-node)">Section 43.5, “Downstream cluster provisioning with Directed network provisioning (multi-node)”</a>)</p></li></ul></div><p>Any advanced network configuration must be applied at enrollment time through the <code class="literal">BareMetalHost</code> host definition and an associated Secret containing an <code class="literal">nmstate</code> formatted <code class="literal">networkData</code> block. The following example file defines a secret containing the required <code class="literal">networkData</code> that requests a static <code class="literal">IP</code> and <code class="literal">VLAN</code> for the downstream cluster host:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      identifier: mac-address
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</pre></div><p>As you can see, the example shows the configuration to enable the interface with static IPs, as well as the configuration to enable the VLAN using the base interface, once the following variables are replaced with the actual values, according to your infrastructure:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${CONTROLPLANE_INTERFACE}</code> — The control-plane interface to be used for the edge cluster (for example, <code class="literal">eth0</code>). Including <code class="literal">identifier: mac-address</code> the naming is inspected automatically by the MAC address so any interface name can be used.</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_IP}</code> — The IP address to be used as an endpoint for the edge cluster (must match with the kubeapi-server endpoint).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_PREFIX}</code> — The CIDR to be used for the edge cluster (for example, <code class="literal">24</code> if you want <code class="literal">/24</code> or <code class="literal">255.255.255.0</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_GATEWAY}</code> — The gateway to be used for the edge cluster (for example, <code class="literal">192.168.100.1</code>).</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_MAC}</code> — The MAC address to be used for the control-plane interface (for example, <code class="literal">00:0c:29:3e:3e:3e</code>).</p></li><li class="listitem"><p><code class="literal">${DNS_SERVER}</code> — The DNS to be used for the edge cluster (for example, <code class="literal">192.168.100.2</code>).</p></li><li class="listitem"><p><code class="literal">${VLAN_ID}</code> — The VLAN ID to be used for the edge cluster (for example, <code class="literal">100</code>).</p></li></ul></div><p>Any other <code class="literal">nmstate</code>-compliant definition can be used to configure the network for the downstream cluster to adapt to the specific requirements. For example, it is possible to specify a static dual-stack configuration:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: ${CONTROLPLANE_MAC}
      ipv4:
        enabled: true
        dhcp: false
        address:
        - ip: ${CONTROLPLANE_IP_V4}
          prefix-length: ${CONTROLPLANE_PREFIX_V4}
      ipv6:
        enabled: true
        dhcp: false
        autoconf: false
        address:
        - ip: ${CONTROLPLANE_IP_V6}
          prefix-length: ${CONTROLPLANE_PREFIX_V6}
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V4}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
      - destination: ::/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V6}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
    dns-resolver:
      config:
        server:
        - ${DNS_SERVER_V4}
        - ${DNS_SERVER_V6}</pre></div><p>As for the previous example, replace the following variables with actual values, according to your infrastructure:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${CONTROLPLANE_IP_V4}</code> - the IPv4 address to assign to the host</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_PREFIX_V4}</code> - the IPv4 prefix of the network to which the host IP belongs</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_IP_V6}</code> - the IPv6 address to assign to the host</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_PREFIX_V6}</code> - the IPv6 prefix of the network to which the host IP belongs</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_GATEWAY_V4}</code> - the IPv4 address of the gateway for the traffic matching the default route</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_GATEWAY_V6}</code> - the IPv6 address of the gateway for the traffic matching the default route</p></li><li class="listitem"><p><code class="literal">${CONTROLPLANE_INTERFACE}</code> - the name of the interface to assign the addresses to and to use for egress traffic matching the default route, for both IPv4 and IPv6</p></li><li class="listitem"><p><code class="literal">${DNS_SERVER_V4}</code> and/or <code class="literal">${DNS_SERVER_V6}</code> - the IP address(es) of the DNS server(s) to use, which can be specified as single or multiple entries. Both IPv4 and/or IPv6 addresses are supported</p></li></ul></div><div id="id-1.9.8.7.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You can refer to <a class="link" href="https://github.com/suse-edge/atip/tree/main/telco-examples/edge-clusters" target="_blank">SUSE Telco Cloud examples repo</a> for more complex examples, including IPv6 only and dual-stack configurations.</p></li><li class="listitem"><p>Single-stack IPv6 deployments are in tech preview status and not yet officially supported.</p></li></ul></div></div><p>Lastly, regardless of the network configuration details, ensure that the secret is referenced by appending <code class="literal">preprovisioningNetworkDataName</code> to the <code class="literal">BaremetalHost</code> object to successfully enroll the host in the management cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</pre></div><div id="id-1.9.8.7.20" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>If you need to deploy a multi-node cluster, the same process must be done for each node.</p></li><li class="listitem"><p>The <code class="literal">Metal3DataTemplate</code>, <code class="literal">networkData</code> and <code class="literal">Metal3 IPAM</code> are currently not supported; only the configuration via static secrets is fully supported.</p></li><li class="listitem"><p>Architecture must be either <code class="literal">x86_64</code> or <code class="literal">aarch64</code>, depending on the architecture of the bare-metal host to be enrolled.</p></li></ul></div></div></section><section class="sect1" id="add-telco" data-id-title="Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.7 </span><span class="title-name">Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#add-telco">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The directed network provisioning workflow allows to automate the Telco features to be used in the downstream clusters to run Telco workloads on top of those servers.</p><p><span class="strong"><strong>Requirements</strong></span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The image generated using <code class="literal">EIB</code>, as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#eib-edge-image-connected" title="43.2. Prepare downstream cluster image for connected scenarios">Section 43.2, “Prepare downstream cluster image for connected scenarios”</a>),  has to be located in the management cluster exactly on the path you configured on this section (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>).</p></li><li class="listitem"><p>The image generated using <code class="literal">EIB</code> has to include the specific Telco packages following this section (<a class="xref" href="atip-automated-provisioning.html#add-telco-feature-eib" title="43.2.2.5. Additional configuration for Telco workloads">Section 43.2.2.5, “Additional configuration for Telco workloads”</a>).</p></li><li class="listitem"><p>The management server created and available to be used on the following sections. For more information, refer to the Management Cluster section: <a class="xref" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster">Chapter 41, <em>Setting up the management cluster</em></a>.</p></li></ul></div><p><span class="strong"><strong>Configuration</strong></span></p><p>Use the following two sections as the base to enroll and provision the hosts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Downstream cluster provisioning with Directed network provisioning (single-node) (<a class="xref" href="atip-automated-provisioning.html#single-node" title="43.4. Downstream cluster provisioning with Directed network provisioning (single-node)">Section 43.4, “Downstream cluster provisioning with Directed network provisioning (single-node)”</a>)</p></li><li class="listitem"><p>Downstream cluster provisioning with Directed network provisioning (multi-node) (<a class="xref" href="atip-automated-provisioning.html#multi-node" title="43.5. Downstream cluster provisioning with Directed network provisioning (multi-node)">Section 43.5, “Downstream cluster provisioning with Directed network provisioning (multi-node)”</a>)</p></li></ul></div><p>The Telco features covered in this section are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>DPDK and VFs creation</p></li><li class="listitem"><p>SR-IOV and VFs allocation to be used by the workloads</p></li><li class="listitem"><p>CPU isolation and performance tuning</p></li><li class="listitem"><p>Huge pages configuration</p></li><li class="listitem"><p>Kernel parameters tuning</p></li></ul></div><div id="id-1.9.8.8.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the Telco features, see <a class="xref" href="atip-features.html" title="Chapter 42. Telco features configuration">Chapter 42, <em>Telco features configuration</em></a>.</p></div><p>The changes required to enable the Telco features shown above are all inside the <code class="literal">RKE2ControlPlane</code> block in the provision file <code class="literal">capi-provisioning-example.yaml</code>. The rest of the information inside the file <code class="literal">capi-provisioning-example.yaml</code> is the same as the information provided in the provisioning section (<a class="xref" href="atip-automated-provisioning.html#single-node-provision">Section 43.4, “Downstream cluster provisioning with Directed network provisioning (single-node)”</a>).</p><p>To make the process clear, the changes required on that block (<code class="literal">RKE2ControlPlane</code>) to enable the Telco features are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">preRKE2Commands</code> to be used to execute the commands before the <code class="literal">RKE2</code> installation process. In this case, use the <code class="literal">modprobe</code> command to enable the <code class="literal">vfio-pci</code> and the <code class="literal">SR-IOV</code> kernel modules.</p></li><li class="listitem"><p>The ignition file <code class="literal">/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</code> to be used to define the interfaces, drivers and the number of <code class="literal">VFs</code> to be created and exposed to the workloads.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The values inside the config map <code class="literal">sriov-custom-auto-config</code> are the only values to be replaced with real values.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${RESOURCE_NAME1}</code> — The resource name to be used for the first <code class="literal">PF</code> interface (for example, <code class="literal">sriov-resource-du1</code>). It is added to the prefix <code class="literal">rancher.io</code> to be used as a label to be used by the workloads (for example, <code class="literal">rancher.io/sriov-resource-du1</code>).</p></li><li class="listitem"><p><code class="literal">${SRIOV-NIC-NAME1}</code> — The name of the first <code class="literal">PF</code> interface to be used (for example, <code class="literal">eth0</code>).</p></li><li class="listitem"><p><code class="literal">${PF_NAME1}</code> — The name of the first physical function <code class="literal">PF</code> to be used. Generate more complex filters using this (for example, <code class="literal">eth0#2-5</code>).</p></li><li class="listitem"><p><code class="literal">${DRIVER_NAME1}</code> — The driver name to be used for the first <code class="literal">VF</code> interface (for example, <code class="literal">vfio-pci</code>).</p></li><li class="listitem"><p><code class="literal">${NUM_VFS1}</code> — The number of <code class="literal">VFs</code> to be created for the first <code class="literal">PF</code> interface (for example, <code class="literal">8</code>).</p></li></ul></div></li></ul></div></li><li class="listitem"><p>The <code class="literal">/var/sriov-auto-filler.sh</code> to be used as a translator between the high-level config map <code class="literal">sriov-custom-auto-config</code> and the <code class="literal">sriovnetworknodepolicy</code> which contains the low-level hardware information. This script has been created to abstract the user from the complexity to know in advance the hardware information. No changes are required in this file, but it should be present if we need to enable <code class="literal">sr-iov</code> and create <code class="literal">VFs</code>.</p></li><li class="listitem"><p>The kernel arguments to be used to enable the following features:</p></li></ul></div><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Parameter</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>Value</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Description</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>isolcpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>domain,nohz,managed_irq,1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Isolate the cores 1-30 and 33-62.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>skew_tick</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to skew the timer interrupts across the isolated CPUs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the timer tick on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz_full</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocbs</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the RCU callbacks on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>irqaffinity</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows the kernel to run the interrupts on a single CPU when the system is idle.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>idle</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>poll</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Minimizes the latency of exiting the idle state.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows to use vfio for the dpdk interfaces.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>intel_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Enables the use of vfio for VFs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Allows to set the size of huge pages to 1 G.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepages</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>40</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Number of huge pages defined before.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>default_hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Default value to enable huge pages.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nowatchdog</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Disables the watchdog.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>nmi_watchdog</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>0</p></td><td style="text-align: left; vertical-align: top; "><p>Disables the NMI watchdog.</p></td></tr></tbody></table></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The following systemd services are used to enable the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">rke2-preinstall.service</code> to replace automatically the <code class="literal">BAREMETALHOST_UUID</code> and <code class="literal">node-name</code> during the provisioning process using the Ironic information.</p></li><li class="listitem"><p><code class="literal">cpu-partitioning.service</code> to enable the isolation cores of the <code class="literal">CPU</code> (for example, <code class="literal">1-30,33-62</code>).</p></li><li class="listitem"><p><code class="literal">performance-settings.service</code> to enable the CPU performance tuning.</p></li><li class="listitem"><p><code class="literal">sriov-custom-auto-vfs.service</code> to install the <code class="literal">sriov</code> Helm chart, wait until custom resources are created and run the <code class="literal">/var/sriov-auto-filler.sh</code> to replace the values in the config map <code class="literal">sriov-custom-auto-config</code> and create the <code class="literal">sriovnetworknodepolicy</code> to be used by the workloads.</p></li></ul></div></li><li class="listitem"><p>The <code class="literal">${RKE2_VERSION}</code> is the version of <code class="literal">RKE2</code> to be used replacing this value (for example, <code class="literal">v1.33.3+rke2r1</code>).</p></li></ul></div><p>With all these changes mentioned, the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> will look like the following:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-crd
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-network-operator
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</pre></div><p>Once the file is created by joining the previous blocks, the following command must be executed in the management cluster to start provisioning the new downstream cluster using the Telco features:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl apply -f capi-provisioning-example.yaml</pre></div></section><section class="sect1" id="atip-private-registry" data-id-title="Private registry"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.8 </span><span class="title-name">Private registry</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#atip-private-registry">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>It is possible to configure a private registry as a mirror for images used by workloads.</p><p>To do this we create the secret containing the information about the private registry to be used by the downstream cluster.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</pre></div><p>The <code class="literal">tls.crt</code>, <code class="literal">tls.key</code> and <code class="literal">ca.crt</code> are the certificates to be used to authenticate the private registry. The <code class="literal">username</code> and <code class="literal">password</code> are the credentials to be used to authenticate the private registry.</p><div id="id-1.9.8.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">tls.crt</code>, <code class="literal">tls.key</code>, <code class="literal">ca.crt</code> , <code class="literal">username</code> and <code class="literal">password</code> have to be encoded in base64 format before to be used in the secret.</p></div><p>With all these changes mentioned, the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> will look like the following:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</pre></div><p>Where the <code class="literal">registry.example.com</code> is the example name of the private registry to be used by the downstream cluster, and it should be replaced with the real values.</p></section><section class="sect1" id="airgap-deployment" data-id-title="Downstream cluster provisioning in air-gapped scenarios"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">43.9 </span><span class="title-name">Downstream cluster provisioning in air-gapped scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#airgap-deployment">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The directed network provisioning workflow allows to automate the provisioning of downstream clusters in air-gapped scenarios.</p><section class="sect2" id="id-requirements-for-air-gapped-scenarios" data-id-title="Requirements for air-gapped scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.9.1 </span><span class="title-name">Requirements for air-gapped scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-requirements-for-air-gapped-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>The <code class="literal">raw</code> image generated using <code class="literal">EIB</code> must include the specific container images (helm-chart OCI and container images) required to run the downstream cluster in an air-gapped scenario. For more information, refer to this section (<a class="xref" href="atip-automated-provisioning.html#eib-edge-image-airgap" title="43.3. Prepare downstream cluster image for air-gap scenarios">Section 43.3, “Prepare downstream cluster image for air-gap scenarios”</a>).</p></li><li class="listitem"><p>In case of using SR-IOV or any other custom workload, the images required to run the workloads must be preloaded in your private registry following the preload private registry section (<a class="xref" href="atip-automated-provisioning.html#preload-private-registry" title="43.3.2.7. Preload your private registry with images required for air-gap scenarios and SR-IOV (optional)">Section 43.3.2.7, “Preload your private registry with images required for air-gap scenarios and SR-IOV (optional)”</a>).</p></li></ol></div></section><section class="sect2" id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios" data-id-title="Enroll the bare-metal hosts in air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.9.2 </span><span class="title-name">Enroll the bare-metal hosts in air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The process to enroll the bare-metal hosts in the management cluster is the same as described in the previous section (<a class="xref" href="atip-automated-provisioning.html#enroll-bare-metal-host">Section 43.4, “Downstream cluster provisioning with Directed network provisioning (single-node)”</a>).</p></section><section class="sect2" id="id-provision-the-downstream-cluster-in-air-gap-scenarios" data-id-title="Provision the downstream cluster in air-gap scenarios"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">43.9.3 </span><span class="title-name">Provision the downstream cluster in air-gap scenarios</span></span> <a title="Permalink" class="permalink" href="atip-automated-provisioning.html#id-provision-the-downstream-cluster-in-air-gap-scenarios">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>There are some important changes required to provision the downstream cluster in air-gapped scenarios:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>The <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-example.yaml</code> file must include the <code class="literal">spec.agentConfig.airGapped: true</code> directive.</p></li><li class="listitem"><p>The private registry configuration must be included in the <code class="literal">RKE2ControlPlane</code> block in the <code class="literal">capi-provisioning-airgap-example.yaml</code> file following the private registry section (<a class="xref" href="atip-automated-provisioning.html#atip-private-registry" title="43.8. Private registry">Section 43.8, “Private registry”</a>).</p></li><li class="listitem"><p>If you are using SR-IOV or any other <code class="literal">AdditionalUserData</code> configuration (combustion script) which requires the helm-chart installation, you must modify the content to reference the private registry instead of using the public registry.</p></li></ol></div><p>The following example shows the SR-IOV configuration in the <code class="literal">AdditionalUserData</code> block in the <code class="literal">capi-provisioning-airgap-example.yaml</code> file with the modifications required to reference the private registry</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Private Registry secrets references</p></li><li class="listitem"><p>Helm-Chart definition using the private registry instead of the public OCI images.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=1800
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --timeout=30m --all ; do sleep 10 ; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</pre></div></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="atip-features.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 42 </span>Telco features configuration</span></a> </div><div><a class="pagination-link next" href="atip-lifecycle.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 44 </span>Lifecycle actions</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="atip-automated-provisioning.html#id-introduction-3"><span class="title-number">43.1 </span><span class="title-name">Introduction</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#eib-edge-image-connected"><span class="title-number">43.2 </span><span class="title-name">Prepare downstream cluster image for connected scenarios</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#eib-edge-image-airgap"><span class="title-number">43.3 </span><span class="title-name">Prepare downstream cluster image for air-gap scenarios</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#single-node"><span class="title-number">43.4 </span><span class="title-name">Downstream cluster provisioning with Directed network provisioning (single-node)</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#multi-node"><span class="title-number">43.5 </span><span class="title-name">Downstream cluster provisioning with Directed network provisioning (multi-node)</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#advanced-network-configuration"><span class="title-number">43.6 </span><span class="title-name">Advanced Network Configuration</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#add-telco"><span class="title-number">43.7 </span><span class="title-name">Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#atip-private-registry"><span class="title-number">43.8 </span><span class="title-name">Private registry</span></a></span></li><li><span class="section"><a href="atip-automated-provisioning.html#airgap-deployment"><span class="title-number">43.9 </span><span class="title-name">Downstream cluster provisioning in air-gapped scenarios</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2025</span></div></div></footer></body></html>