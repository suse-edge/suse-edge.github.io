<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | Telco features configuration</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Telco features configuration"/>
<meta name="description" content="This section documents and explains the configuration of Telco-specific features on clusters deployed via SUSE Telco Cloud."/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 42. Telco features configuration"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Telco features configuration"/>
<meta property="og:description" content="This section documents and explains the configuration of Te…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Telco features configuration"/>
<meta name="twitter:description" content="This section documents and explains the configuration of Te…"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    

    "headline": "Telco features configuration",
  
    "description": "Telco features configuration",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster"/><link rel="next" href="atip-automated-provisioning.html" title="Chapter 43. Fully automated directed network provisioning"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-suse-telco-cloud-documentation.html">SUSE Telco Cloud Documentation</a><span> / </span><a class="crumb" href="atip-features.html">Telco features configuration</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge 3.5 Documentation</span></a></li><li><a href="id-quick-starts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quick Starts</span></a><ol><li><a href="quickstart-metal3.html" class=" "><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone clusters with Edge Image Builder</span></a></li><li><a href="quickstart-suma.html" class=" "><span class="title-number">4 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-components.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">5 </span><span class="title-name">Rancher</span></a></li><li><a href="components-rancher-dashboard-extensions.html" class=" "><span class="title-number">6 </span><span class="title-name">Rancher Dashboard Extensions</span></a></li><li><a href="components-rancher-turtles.html" class=" "><span class="title-number">7 </span><span class="title-name">Rancher Turtles</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">8 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">9 </span><span class="title-name">SUSE Linux Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">10 </span><span class="title-name">Metal<sup>3</sup></span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">11 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-nmc.html" class=" "><span class="title-number">12 </span><span class="title-name">Edge Networking</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">13 </span><span class="title-name">Elemental</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">14 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">15 </span><span class="title-name">RKE2</span></a></li><li><a href="components-suse-storage.html" class=" "><span class="title-number">16 </span><span class="title-name">SUSE Storage</span></a></li><li><a href="components-suse-security.html" class=" "><span class="title-number">17 </span><span class="title-name">SUSE Security</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">18 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-eco.html" class=" "><span class="title-number">19 </span><span class="title-name">Endpoint Copier Operator</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">20 </span><span class="title-name">Edge Virtualization</span></a></li><li><a href="components-system-upgrade-controller.html" class=" "><span class="title-number">21 </span><span class="title-name">System Upgrade Controller</span></a></li><li><a href="components-upgrade-controller.html" class=" "><span class="title-number">22 </span><span class="title-name">Upgrade Controller</span></a></li><li><a href="components-suma.html" class=" "><span class="title-number">23 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How-To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">24 </span><span class="title-name">MetalLB on K3s (using Layer 2 Mode)</span></a></li><li><a href="guides-metallb-k3s-l3.html" class=" "><span class="title-number">25 </span><span class="title-name">MetalLB on K3s (using Layer 3 Mode)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">26 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li><li><a href="id-air-gapped-deployments-with-edge-image-builder.html" class=" "><span class="title-number">27 </span><span class="title-name">Air-gapped deployments with Edge Image Builder</span></a></li><li><a href="guides-kiwi-builder-images.html" class=" "><span class="title-number">28 </span><span class="title-name">Building Updated SUSE Linux Micro Images with Kiwi</span></a></li><li><a href="guides-clusterclass-example.html" class=" "><span class="title-number">29 </span><span class="title-name">Using clusterclass to deploy downstream clusters</span></a></li></ol></li><li><a href="tips-and-tricks.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Tips and Tricks</span></a><ol><li><a href="tips-edge-image-builder.html" class=" "><span class="title-number">30 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="tips-elemental.html" class=" "><span class="title-number">31 </span><span class="title-name">Elemental</span></a></li><li><a href="tips-metal3.html" class=" "><span class="title-number">32 </span><span class="title-name"><span class="strong"><strong>Metal<sup>3</sup></strong></span></span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">V </span><span class="title-name">Third-Party Integration</span></a><ol><li><a href="integrations-nats.html" class=" "><span class="title-number">33 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-suse-linux-micro.html" class=" "><span class="title-number">34 </span><span class="title-name">NVIDIA GPUs on SUSE Linux Micro</span></a></li></ol></li><li><a href="day-2-operations.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Day 2 Operations</span></a><ol><li><a href="day2-migration.html" class=" "><span class="title-number">35 </span><span class="title-name">Edge 3.5 migration</span></a></li><li><a href="day2-mgmt-cluster.html" class=" "><span class="title-number">36 </span><span class="title-name">Management Cluster</span></a></li><li><a href="day2-downstream-clusters.html" class=" "><span class="title-number">37 </span><span class="title-name">Downstream clusters</span></a></li></ol></li><li class="active"><a href="id-suse-telco-cloud-documentation.html" class="has-children you-are-here"><span class="title-number">VII </span><span class="title-name">SUSE Telco Cloud Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">38 </span><span class="title-name">SUSE Telco Cloud</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">39 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">40 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" "><span class="title-number">41 </span><span class="title-name">Setting up the management cluster</span></a></li><li><a href="atip-features.html" class=" you-are-here"><span class="title-number">42 </span><span class="title-name">Telco features configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" "><span class="title-number">43 </span><span class="title-name">Fully automated directed network provisioning</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">44 </span><span class="title-name">Lifecycle actions</span></a></li></ol></li><li><a href="id-troubleshooting-3.html" class="has-children "><span class="title-number">VIII </span><span class="title-name">Troubleshooting</span></a><ol><li><a href="general-troubleshooting-principles.html" class=" "><span class="title-number">45 </span><span class="title-name">General Troubleshooting Principles</span></a></li><li><a href="troubleshooting-kiwi.html" class=" "><span class="title-number">46 </span><span class="title-name">Troubleshooting Kiwi</span></a></li><li><a href="troubleshooting-edge-image-builder.html" class=" "><span class="title-number">47 </span><span class="title-name">Troubleshooting Edge Image Builder (EIB)</span></a></li><li><a href="troubleshooting-edge-networking.html" class=" "><span class="title-number">48 </span><span class="title-name">Troubleshooting Edge Networking (NMC)</span></a></li><li><a href="troubleshooting-phone-home-scenarios.html" class=" "><span class="title-number">49 </span><span class="title-name">Troubleshooting Phone-Home scenarios</span></a></li><li><a href="troubleshooting-directed-network-provisioning.html" class=" "><span class="title-number">50 </span><span class="title-name">Troubleshooting Directed-network provisioning</span></a></li><li><a href="troubleshooting-other-components.html" class=" "><span class="title-number">51 </span><span class="title-name">Troubleshooting Other components</span></a></li><li><a href="collecting-diagnostics-for-support.html" class=" "><span class="title-number">52 </span><span class="title-name">Collecting Diagnostics for Support</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">IX </span><span class="title-name">Appendix</span></a><ol><li><a href="id-release-notes.html" class=" "><span class="title-number">53 </span><span class="title-name">Release Notes</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="atip-features" data-id-title="Telco features configuration"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">42 </span><span class="title-name">Telco features configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section documents and explains the configuration of Telco-specific features on clusters deployed via SUSE Telco Cloud.</p><p>The directed network provisioning deployment method is used, as described in the Automated Provisioning (<a class="xref" href="atip-automated-provisioning.html" title="Chapter 43. Fully automated directed network provisioning">Chapter 43, <em>Fully automated directed network provisioning</em></a>) section.</p><p>The following topics are covered in this section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel image for real time (<a class="xref" href="atip-features.html#kernel-image-for-real-time" title="42.1. Kernel image for real time">Section 42.1, “Kernel image for real time”</a>): Kernel image to be used by the real-time kernel.</p></li><li class="listitem"><p>Kernel arguments for low latency and high performance (<a class="xref" href="atip-features.html#kernel-args" title="42.2. Kernel arguments for low latency and high performance">Section 42.2, “Kernel arguments for low latency and high performance”</a>): Kernel arguments to be used by the real-time kernel for maximum performance and low latency running telco workloads.</p></li><li class="listitem"><p>CPU Pinning on Host (<a class="xref" href="atip-features.html#cpu-pinning-host" title="42.3. CPU Pinning on Host">Section 42.3, “CPU Pinning on Host”</a>): Isolating the CPUs via kernel arguments and TuneD profile.</p></li><li class="listitem"><p>CPU Pinning on Kubernetes (<a class="xref" href="atip-features.html#cpu-pinning-k8s" title="42.4. CPU Pinning on Kubernetes">Section 42.4, “CPU Pinning on Kubernetes”</a>): Isolating the CPUs on Kubernetes via Kubelet configuration.</p></li><li class="listitem"><p>CNI configuration (<a class="xref" href="atip-features.html#cni-configuration" title="42.5. CNI Configuration">Section 42.5, “CNI Configuration”</a>): CNI configuration to be used by the Kubernetes cluster.</p></li><li class="listitem"><p>SR-IOV configuration (<a class="xref" href="atip-features.html#sriov" title="42.6. SR-IOV">Section 42.6, “SR-IOV”</a>): SR-IOV configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>DPDK configuration (<a class="xref" href="atip-features.html#dpdk" title="42.7. DPDK">Section 42.7, “DPDK”</a>): DPDK configuration to be used by the system.</p></li><li class="listitem"><p>vRAN Acceleration (<a class="xref" href="atip-features.html#acceleration" title="42.8. vRAN Acceleration (Intel ACC100/ACC200)">Section 42.8, “vRAN Acceleration (<code class="literal">Intel ACC100/ACC200</code>)”</a>): Offloading FEC algorithm computation to vRAN Acceleration card.</p></li><li class="listitem"><p>Huge pages (<a class="xref" href="atip-features.html#huge-pages" title="42.9. Huge pages">Section 42.9, “Huge pages”</a>): Huge pages configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>NUMA-aware scheduling configuration (<a class="xref" href="atip-features.html#numa-aware-scheduling" title="42.10. NUMA-aware scheduling">Section 42.10, “NUMA-aware scheduling”</a>): NUMA-aware scheduling configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Metal LB configuration (<a class="xref" href="atip-features.html#metal-lb-configuration" title="42.11. Metal LB">Section 42.11, “Metal LB”</a>): Metal LB configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Private registry configuration (<a class="xref" href="atip-features.html#private-registry" title="42.12. Private registry configuration">Section 42.12, “Private registry configuration”</a>): Private registry configuration to be used by the Kubernetes workloads.</p></li><li class="listitem"><p>Precision Time Protocol configuration (<a class="xref" href="atip-features.html#ptp-configuration" title="42.13. Precision Time Protocol">Section 42.13, “Precision Time Protocol”</a>): Configuration files for running PTP telco profiles.</p></li></ul></div><section class="sect1" id="kernel-image-for-real-time" data-id-title="Kernel image for real time"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.1 </span><span class="title-name">Kernel image for real time</span></span> <a title="Permalink" class="permalink" href="atip-features.html#kernel-image-for-real-time">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The real-time kernel image is not necessarily better than a standard kernel.
It is a different kernel tuned to a specific use case. The real-time kernel is tuned for lower latency at the cost of throughput. The real-time kernel is not recommended for general purpose use, but in our case, this is the recommended kernel for Telco Workloads where latency is a key factor.</p><p>There are four top features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Deterministic execution:</p><p>Get greater predictability — ensure critical business processes complete in time, every time and deliver high-quality service, even under heavy system loads. By shielding key system resources for high-priority processes, you can ensure greater predictability for time-sensitive applications.</p></li><li class="listitem"><p>Low jitter:</p><p>The low jitter built upon the highly deterministic technology helps to keep applications synchronized with the real world. This helps services that need ongoing and repeated calculation.</p></li><li class="listitem"><p>Priority inheritance:</p><p>Priority inheritance refers to the ability of a lower priority process to assume a higher priority when there is a higher priority process that requires the lower priority process to finish before it can accomplish its task. SUSE Linux Enterprise Real Time solves these priority inversion problems for mission-critical processes.</p></li><li class="listitem"><p>Thread interrupts:</p><p>Processes running in interrupt mode in a general-purpose operating system are not preemptible. With SUSE Linux Enterprise Real Time, these interrupts have been encapsulated by kernel threads, which are interruptible, and allow the hard and soft interrupts to be preempted by user-defined higher priority processes.</p><p>In our case, if you have installed a real-time image like <code class="literal">SUSE Linux Micro RT</code>, kernel real time is already installed. From the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>, you can download the real-time kernel image.</p><div id="id-1.9.7.6.4.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about the real-time kernel, visit <a class="link" href="https://www.suse.com/products/realtime/" target="_blank">SUSE Real Time</a>.</p></div></li></ul></div></section><section class="sect1" id="kernel-args" data-id-title="Kernel arguments for low latency and high performance"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.2 </span><span class="title-name">Kernel arguments for low latency and high performance</span></span> <a title="Permalink" class="permalink" href="atip-features.html#kernel-args">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The kernel arguments are important to be configured to enable the real-time kernel to work properly giving the best performance and low latency to run telco workloads.  There are some important concepts to keep in mind when configuring the kernel arguments for this use case:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Remove <code class="literal">kthread_cpus</code> when using SUSE real-time kernel. This parameter controls on which CPUs kernel threads are created. It also controls which CPUs are allowed for PID 1 and for loading kernel modules (the kmod user-space helper). This parameter is not
recognized and does not have any effect.</p></li><li class="listitem"><p>Isolate the CPU cores using <code class="literal">isolcpus</code>, <code class="literal">nohz_full</code>, <code class="literal">rcu_nocbs</code>, and <code class="literal">irqaffinity</code>. For a comprehensive list of CPU pinning techniques, refer to  CPU Pinning on Host (<a class="xref" href="atip-features.html#cpu-pinning-host" title="42.3. CPU Pinning on Host">Section 42.3, “CPU Pinning on Host”</a>) chapter.</p></li><li class="listitem"><p>Add <code class="literal">domain,nohz,managed_irq</code> flags to <code class="literal">isolcpus</code> kernel argument. Without any flags, <code class="literal">isolcpus</code> is equivalent to specifying only the <code class="literal">domain</code> flag. This isolates the specified CPUs from scheduling, including kernel tasks. The <code class="literal">nohz</code> flag stops the scheduler tick on the specified CPUs (if only one task is runnable on a CPU), and the <code class="literal">managed_irq</code> flag avoids routing managed external (device) interrupts at the specified CPUs. Note that the IRQ lines of NVMe devices are fully managed by the kernel and will be routed to the non-isolated (housekeeping) cores as a consequence. For example, the command line provided at the end of this section will result in only four queues (plus an admin/control queue) allocated on the system:</p><div class="verbatim-wrap"><pre class="screen">for I in $(grep nvme0 /proc/interrupts | cut -d ':' -f1); do cat /proc/irq/${I}/effective_affinity_list; done | column
39      0       19      20      39</pre></div><p>This behavior prevents any disruption caused by disk I/O to any time sensitive application running on the isolated cores, but might require attention and careful design for storage focused workloads.</p></li><li class="listitem"><p>Tune the ticks (kernel’s periodic timer interrupts):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">skew_tick=1</code>: ticks can sometimes happen simultaneously. Instead of all CPUs receiving their timer tick at the exact same moment, <code class="literal">skew_tick=1</code> makes them occur at slightly offset times. This helps reduce system jitter, resulting in more consistent and lower interrupt response times (an essential requirement for latency-sensitive applications).</p></li><li class="listitem"><p><code class="literal">nohz=on</code>: stops the periodic timer tick on idle CPUs.</p></li><li class="listitem"><p><code class="literal">nohz_full=&lt;cpu-cores&gt;</code>: Stops the periodic timer tick on specified CPUs that are dedicated for real-time applications.</p></li></ul></div></li><li class="listitem"><p>Disable Machine Check Exception (MCE) handling by specifying <code class="literal">mce=off</code>. MCEs are hardware errors detected by the processor and disabling them can avoid noisy logs.</p></li><li class="listitem"><p>Add <code class="literal">nowatchdog</code> to disable the soft-lockup watchdog which is implemented as a timer running in the timer hard-interrupt context. When it expires (i.e. a soft lockup is detected), it will print a warning (in the hard interrupt context), running any latency targets. Even if it never expires, it goes onto the timer list, slightly increasing the overhead of every timer interrupt. This option also disables the NMI watchdog, so NMIs cannot interfere.</p></li><li class="listitem"><p><code class="literal">nmi_watchdog=0</code> disables the NMI (Non-Maskable Interrupt) watchdog. This can be omitted when <code class="literal">nowatchdog</code> is used.</p></li><li class="listitem"><p>RCU (Read-Copy-Update) is a kernel mechanism that enables concurrent, lock-free access for many readers to shared data. An RCU callback, a function triggered after a 'grace period', ensures all previous readers have finished so old data can be safely reclaimed. We fine-tune RCU, particularly for sensitive workloads, to offload these callbacks from dedicated (pinned) CPUs, preventing kernel operations from interfering with critical, time-sensitive tasks.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Specify the pinned CPUs in <code class="literal">rcu_nocbs</code> so that RCU callbacks do not run on them. This helps reducing jitter and latency for the real-time workloads.</p></li><li class="listitem"><p><code class="literal">rcu_nocb_poll</code> makes the no-callback CPUs regularly 'poll' to see if callback handling is required. This can reduce the interrupt overhead.</p></li><li class="listitem"><p><code class="literal">rcupdate.rcu_cpu_stall_suppress=1</code> suppresses RCU CPU stall warnings, which can sometimes be false positives in heavily loaded real-time systems</p></li><li class="listitem"><p><code class="literal">rcupdate.rcu_expedited=1</code> speeds up the grace period for RCU operations, making read-side critical sections more responsive</p></li><li class="listitem"><p><code class="literal">rcupdate.rcu_normal_after_boot=1</code> When used with rcu_expedited, it allows RCU to revert to normal (non-expedited) operation after the system boot.</p></li><li class="listitem"><p><code class="literal">rcupdate.rcu_task_stall_timeout=0</code> disables the RCU task stall detector, preventing potential warnings or system halts from long-running RCU tasks.</p></li><li class="listitem"><p><code class="literal">rcutree.kthread_prio=99</code> sets the priority of the RCU callback kernel thread to the highest possible (99), ensuring it gets scheduled and handles RCU callbacks promptly, when needed.</p></li></ul></div></li><li class="listitem"><p>Add <code class="literal">ignition.platform.id=openstack</code> for Metal3 and Cluster API to successfully provision/deprovision the cluster. This is used by Metal3 Python agent, which originated from Openstack Ironic.</p></li><li class="listitem"><p>Remove <code class="literal">intel_pstate=passive</code>. This option configures <code class="literal">intel_pstate</code> to work with generic cpufreq governors, but to make this work, it disables hardware-managed P-states (<code class="literal">HWP</code>) as a side effect. To reduce the hardware latency, this option is not recommended for real-time workloads.</p></li><li class="listitem"><p>Replace <code class="literal">intel_idle.max_cstate=0 processor.max_cstate=1</code> with <code class="literal">idle=poll</code>. To avoid C-State transitions, the <code class="literal">idle=poll</code> option is used to disable the C-State transitions and keep the CPU in the highest C-State. The <code class="literal">intel_idle.max_cstate=0</code> option disables <code class="literal">intel_idle</code>, so <code class="literal">acpi_idle</code> is used, and <code class="literal">acpi_idle.max_cstate=1</code> then sets max C-state for acpi_idle.
On AMD64/Intel 64 architectures, the first ACPI C-State is always <code class="literal">POLL</code>, but it uses a <code class="literal">poll_idle()</code> function, which may introduce some tiny latency by reading the clock periodically, and restarting the main loop in <code class="literal">do_idle()</code> after a timeout (this also involves clearing and setting the <code class="literal">TIF_POLL</code> task flag).
In contrast, <code class="literal">idle=poll</code> runs in a tight loop, busy-waiting for a task to be rescheduled. This minimizes the latency of exiting the idle state, but at the cost of keeping the CPU running at full speed in the idle thread.</p></li><li class="listitem"><p>Disable C1E in BIOS. This option is important to disable the C1E state in the BIOS to avoid the CPU from entering the C1E state when idle. The C1E state is a low-power state that can introduce latency when the CPU is idle.</p></li></ul></div><p>The rest of this documentation covers additional parameters, including huge pages and IOMMU.</p><p>This provides an example of kernel arguments for a 32-core Intel server, including the aforementioned adjustments:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</pre></div><p>Here is another configuration example for a 64-core AMD server. Among the 128 logical processors (<code class="literal">0-127</code>), first 8 cores (<code class="literal">0-7</code>) are designated for housekeeping, while the remaining 120 cores (<code class="literal">8-127</code>) are pinned for the applications:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=575291cf-74e8-42cf-8f2c-408a20dc00b8 skew_tick=1 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack amd_iommu=on iommu=pt irqaffinity=0-7 isolcpus=domain,nohz,managed_irq,8-127 nohz_full=8-127 rcu_nocbs=8-127 mce=off nohz=on nowatchdog nmi_watchdog=0 nosoftlockup quiet rcu_nocb_poll rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</pre></div></section><section class="sect1" id="cpu-pinning-host" data-id-title="CPU Pinning on Host"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.3 </span><span class="title-name">CPU Pinning on Host</span></span> <a title="Permalink" class="permalink" href="atip-features.html#cpu-pinning-host">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>CPU pinning, also known as processor affinity, is the technique of binding a process or thread to a specific CPU core, preventing the operating system’s scheduler from moving it. By ensuring a process always runs on the same core, it benefits from faster access to data that remains in that core’s cache memory. This practice is common in high-performance computing environments because it dramatically improves performance and reduces overhead.</p><section class="sect2" id="id-isolating-cpus-via-tuned" data-id-title="Isolating CPUs via TuneD"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.3.1 </span><span class="title-name">Isolating CPUs via TuneD</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-isolating-cpus-via-tuned">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">tuned</code> is a system tuning tool that monitors system conditions to optimize performance using various predefined profiles. A key feature is its ability to isolate CPU cores for specific workloads, like real-time applications. This prevents the OS from utilizing these cores and potentially increasing latency.</p><p>To enable and configure this feature, the first thing is to create a profile for the CPU cores we want to isolate. In this example, among 64 cores, we dedicate 60 cores (<code class="literal">1-30,33-62</code>) for the application and remaining 4 cores are used for housekeeping. Note that the design of isolated CPUs heavily depends on the real-time applications.</p><div class="verbatim-wrap"><pre class="screen">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</pre></div></section><section class="sect2" id="id-isolating-cpus-via-kernel-arguments" data-id-title="Isolating CPUs via kernel arguments"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.3.2 </span><span class="title-name">Isolating CPUs via kernel arguments</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-isolating-cpus-via-kernel-arguments">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Then we need to modify the GRUB option to isolate CPU cores and other important parameters for CPU usage.
The following options are important to be customized with your current hardware specifications:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>isolcpus</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>domain,nohz,managed_irq,1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>Isolate the cores 1-30 and 33-62. <code class="literal">domain</code> indicates these CPUs are part of isolation domain. <code class="literal">nohz</code> enables tickless operation on these isolated CPUs when they are idle, to reduce interruptions. <code class="literal">managed_irq</code> isolates pinned CPUs from being targeted by IRQs. This contemplates <code class="literal">irqaffinity=0-7</code>, which already directs mosts IRQs to the housekeeping cores.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>skew_tick</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to skew the timer interrupts across the isolated CPUs.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>When enabled, kernel’s periodic timer interrupt (the 'tick') will stop on any CPU core that is idle. This primary benefits the housekeeping CPUs (<code class="literal">0,31,32,63</code>). This conserves power and reduces unnecessary wake-ups on those general-purpose cores.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nohz_full</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>For the isolated cores, this stops the tick and it does so even when the CPU is running a single active task. It means it makes the CPU run in full tickless mode (or 'dyntick'). The kernel will only deliver timer interrupts when they are actually needed.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocbs</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1-30,33-62</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option offloads the RCU callback processing from specified CPU cores.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>rcu_nocb_poll</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "> </td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>When this option is set, no-RCU-callback CPUs will regularly 'poll' to see if callback handling is required, rather than being explicitly woken up by other CPUs. This can reduce the interrupt overhead.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>irqaffinity</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0,31,32,63</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows the kernel to run the interrupts to the housekeeping cores.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>idle</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>poll</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This minimizes the latency of exiting the idle state, but at the cost of keeping the CPU running at full speed in the idle thread.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>nmi_watchdog</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>0</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option disables only the NMI watchdog. This can be omitted when <code class="literal">nowatchdog</code> is set.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>nowatchdog</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "> </td><td style="text-align: left; vertical-align: top; "><p>This option disables the soft-lockup watchdog which is implemented as a timer running in the timer hard-interrupt context.</p></td></tr></tbody></table></div><p>The following commands modify the GRUB configuration and apply the changes mentioned above to be present on the next boot:</p><p>Edit the <code class="literal">/etc/default/grub</code> file with above parameters and the file will look like this:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</pre></div><p>Update the GRUB configuration:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after the reboot, the following command can be used to check the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><p>There is another script that can be used to tune the CPU configuration, which basically is doing the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Set the CPU governor to <code class="literal">performance</code>.</p></li><li class="listitem"><p>Unset the timer migration to the isolated CPUs.</p></li><li class="listitem"><p>Migrate the kdaemon threads to the housekeeping CPUs.</p></li><li class="listitem"><p>Set the isolated CPUs latency to the lowest possible value.</p></li><li class="listitem"><p>Delay the vmstat updates to 300 seconds.</p></li></ul></div><p>The script is available at <a class="link" href="https://raw.githubusercontent.com/suse-edge/telco-cloud-examples/refs/heads/release-3.5/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh" target="_blank">SUSE Telco Cloud Examples repository</a>.</p></section></section><section class="sect1" id="cpu-pinning-k8s" data-id-title="CPU Pinning on Kubernetes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.4 </span><span class="title-name">CPU Pinning on Kubernetes</span></span> <a title="Permalink" class="permalink" href="atip-features.html#cpu-pinning-k8s">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-rke2-versions-v1-32" data-id-title="RKE2 Versions v1.32"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.4.1 </span><span class="title-name">RKE2 Versions &lt; v1.32</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-rke2-versions-v1-32">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Enable CPU pinning in your RKE2 cluster by editing RKE2 config file. Add below kubelet arguments in <code class="literal">/etc/rancher/rke2/config.yaml</code> file. Make sure specifying the housekeeping CPU cores in <code class="literal">kubelet-reserved</code> and <code class="literal">system-reserved</code> arguments:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">kubelet-arg:
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=0,31,32,63"
- "system-reserved=cpu=0,31,32,63"</pre></div></section><section class="sect2" id="id-rke2-versions-v1-32-2" data-id-title="RKE2 Versions = v1.32"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.4.2 </span><span class="title-name">RKE2 Versions &gt;= v1.32</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-rke2-versions-v1-32-2">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>If your RKE2 version is v1.32 or higher, command-line arguments cannot be used to configure kubelet, following upstream Kubernetes practice. To set up CPU pinning, a kubelet config file needs to be created. Refer to <a class="link" href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/configuration.html#_kubelet_configuration" target="_blank">RKE2 documentation</a>.</p><p>Create a new kubelet config file such as <code class="literal">01-cpu-pinning.conf</code> and place it in the <code class="literal">/var/lib/rancher/rke2/agent/etc/kubelet.conf.d/</code> directory:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cpuManagerPolicy: static
reservedSystemCPUs: 0,31,32,63
topologyManagerPolicy: single-numa-node</pre></div><p>For configuration changes to take effect, a restart of the appropriate RKE2 service (server or agent) is required. This action will briefly interrupt RKE2 service on the host. Run only one of the following commands, depending on the node type:</p><div class="verbatim-wrap"><pre class="screen"># If the node is RKE2 agent
systemctl restart rke2-agent
# Else if the node is RKE2 server
systemctl restart rke2-server</pre></div></section><section class="sect2" id="id-deploy-workloads-leveraging-pinned-cpus" data-id-title="Deploy Workloads Leveraging Pinned CPUs"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.4.3 </span><span class="title-name">Deploy Workloads Leveraging Pinned CPUs</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-deploy-workloads-leveraging-pinned-cpus">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>There are three ways to use the feature using the <code class="literal">Static Policy</code> defined in kubelet depending on the requests and limits you define on your workload:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><code class="literal">BestEffort</code> QoS Class: If you do not define any request or limit for <code class="literal">CPU</code>, the pod is scheduled on the first <code class="literal">CPU</code> available on the system.</p><p>An example of using the <code class="literal">BestEffort</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx</pre></div></li><li class="listitem"><p><code class="literal">Burstable</code> QoS Class: If you define a request for CPU, which is not equal to the limits, or there is no CPU request.</p><p>Examples of using the <code class="literal">Burstable</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</pre></div><p>or</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</pre></div></li><li class="listitem"><p><code class="literal">Guaranteed</code> QoS Class: If you define a request for CPU, which is equal to the limits.</p><p>An example of using the <code class="literal">Guaranteed</code> QoS Class could be:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</pre></div></li></ol></div></section></section><section class="sect1" id="cni-configuration" data-id-title="CNI Configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.5 </span><span class="title-name">CNI Configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#cni-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-cilium" data-id-title="Cilium"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.5.1 </span><span class="title-name">Cilium</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-cilium">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Cilium</code> is the default CNI plug-in for SUSE Telco Cloud.
To enable Cilium on RKE2 cluster as the default plug-in, the following configuration is required in the <code class="literal">/etc/rancher/rke2/config.yaml</code> file:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- cilium</pre></div><p>This can also be specified with command-line arguments, that is, <code class="literal">--cni=cilium</code> into the server line in <code class="literal">/etc/systemd/system/rke2-server</code> file.</p><p>To use the <code class="literal">SR-IOV</code> network described in <a class="xref" href="atip-features.html#option2-sriov-helm" title="42.6.2. Option 2 (Recommended): SR-IOV Network Operator">Section 42.6.2, “Option 2 (Recommended): SR-IOV Network Operator”</a> along with Cilium, deploy <code class="literal">multus</code> meta plugin. Make sure <code class="literal">multus</code> is listed before other CNIs.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- multus
- cilium</pre></div></section><section class="sect2" id="id-calico" data-id-title="Calico"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.5.2 </span><span class="title-name">Calico</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-calico">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Calico</code> is another CNI plug-in for SUSE Edge for Telco.
To enable Calico on RKE2 cluster as the default plug-in, the following
configuration is required in the <code class="literal">/etc/rancher/rke2/config.yaml</code> file:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- calico</pre></div><p>This can also be specified with command-line arguments, that is, <code class="literal">--cni=calico</code> into the server line in <code class="literal">/etc/systemd/system/rke2-server</code> file.</p><p>To use the <code class="literal">SR-IOV</code> network described in <a class="xref" href="atip-features.html#option2-sriov-helm" title="42.6.2. Option 2 (Recommended): SR-IOV Network Operator">Section 42.6.2, “Option 2 (Recommended): SR-IOV Network Operator”</a> along with Calico, deploy <code class="literal">multus</code> meta plugin. Make sure <code class="literal">multus</code> is listed before other CNIs.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- multus
- calico</pre></div><div id="id-1.9.7.10.3.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about CNI plug-ins, see <a class="link" href="https://docs.rke2.io/install/network_options" target="_blank">Network Options</a>.</p></div></section><section class="sect2" id="id-bond-cni" data-id-title="Bond CNI"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.5.3 </span><span class="title-name">Bond CNI</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-bond-cni">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>In general terms, bonding provides a method for aggregating multiple network
interfaces into a single logical "bonded" interface. This is typically used to
increase service availability by introducing redundant networking paths, but can
also be used to increase bandwidth with certain bond modes.
The following CNI plug-ins are supported for the Bond CNI plugin in combination with multus:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>MACVLAN</p></li><li class="listitem"><p>Host Device</p></li><li class="listitem"><p>SR-IOV</p></li></ul></div><section class="sect3" id="id-bond-cni-with-macvlan" data-id-title="Bond CNI with MACVLAN"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">42.5.3.1 </span><span class="title-name">Bond CNI with MACVLAN</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-bond-cni-with-macvlan">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To use the Bond CNI plugin with MACVLAN two free interfaces are needed in the
container. The following example uses 'enp8s0' and 'enp9s0'. Start by creating network
attachment definitions for them:</p><p><span class="strong"><strong>NetworkAttachmentDefinition enp8s0</strong></span></p><div class="verbatim-wrap"><pre class="screen">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp8s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</pre></div><p><span class="strong"><strong>NetworkAttachmentDefinition enp9s0</strong></span></p><div class="verbatim-wrap"><pre class="screen">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp9s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</pre></div><p>After this, add a network attachment definition for the bond itself.</p><p><span class="strong"><strong>NetworkAttachmentDefinition bond</strong></span></p><div class="verbatim-wrap"><pre class="screen">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</pre></div><p>The IP address assignment here is static and defines the address of the bond as
'192.168.200.100' on a /24 network, with a gateway residing on the network’s first
available address. In the bond’s network attachment we also define the type of
bond we want. In this case it is active-backup.</p><p>To use this bond, the pod needs to know about all interfaces. An example pod
definition might look like this:</p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-conf",
"interface": "net1"
},
{"name": "enp9s0-conf",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</pre></div><p>Note how the annotation refers to all networks and how it defines the mapping
between the interfaces 'enp8s0 → net1', and 'enp9s0→net2'.</p></section><section class="sect3" id="id-bond-cni-with-host-device" data-id-title="Bond CNI with Host Device"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">42.5.3.2 </span><span class="title-name">Bond CNI with Host Device</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-bond-cni-with-host-device">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To use the Bond CNI plugin with host device, two free interfaces are needed on
the host. These interfaces are then mapped through to the container. The
following example uses 'enp8s0' and 'enp9s0'. Start by creating network
attachment definitions for them:</p><p><span class="strong"><strong>NetworkAttachmentDefinition enp8s0</strong></span></p><div class="verbatim-wrap"><pre class="screen">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp8s0",
          "ipam": {}
        }]
    }'</pre></div><p><span class="strong"><strong>NetworkAttachmentDefinition enp9s0</strong></span></p><div class="verbatim-wrap"><pre class="screen">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp9s0",
          "ipam": {}
        }]
    }'</pre></div><p>After this, add network attachment definition for the bond itself. This is
similar to the MACVLAN use case.</p><p><span class="strong"><strong>NetworkAttachmentDefinition bond</strong></span></p><div class="verbatim-wrap"><pre class="screen">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</pre></div><p>The IP address assignment here is static and defines the address of the bond as
'192.168.200.100' on a /24 network, with a gateway residing on the network’s
first available address. In the bond’s network attachment, define the type of
bond. In this case it is active-backup.</p><p>To use this bond, the pod needs to know about all interfaces. An example pod
definition for bond with host devices might look like this:</p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-hostdev",
"interface": "net1"
},
{"name": "enp9s0-hostdev",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</pre></div></section><section class="sect3" id="id-bond-cni-with-sr-iov" data-id-title="Bond CNI with SR-IOV"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">42.5.3.3 </span><span class="title-name">Bond CNI with SR-IOV</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-bond-cni-with-sr-iov">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Using the Bond CNI with SR-IOV is fairly straight forward. For more details on how to set up SR-IOV, see <a class="xref" href="atip-features.html#sriov" title="42.6. SR-IOV">Section 42.6, “SR-IOV”</a>. As described there, you have to create <code class="literal">SriovNetworkNodePolicies</code> that defines <code class="literal">resourceNames</code>, as well as number of virtual functions and such. The <code class="literal">resourceNames</code> are being used by the <code class="literal">SriovNetwork</code> which is used as interfaces in the pod definition. The bond definition is exactly the same as for the MACVLAN and host device cases.</p><div id="id-1.9.7.10.4.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Bond CNI with SR-IOV is only applicable to SRIOV Virtual Functions (VF) using the kernel driver. Userspace driver VFs - such as those used in DPDK workloads - can not be bonded with the Bond CNI.</p></div></section></section></section><section class="sect1" id="sriov" data-id-title="SR-IOV"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.6 </span><span class="title-name">SR-IOV</span></span> <a title="Permalink" class="permalink" href="atip-features.html#sriov">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>SR-IOV (Single Root I/O Virtualization) allows a single physical device, such as a network adapter, to separate its resources across multiple <code class="literal">PCIe</code> hardware functions. This enables direct resource access for various applications.</p><p>We provide two distinct methods for deploying <code class="literal">SR-IOV</code> in your cluster:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="xref" href="atip-features.html#option1-sriov-deviceplugin" title="42.6.1. Option 1: SR-IOV Network Device Plugin Daemonset and configMap">Section 42.6.1, “Option 1: SR-IOV Network Device Plugin Daemonset and configMap”</a>: This method supports both network devices and vRAN accelerator.</p></li><li class="listitem"><p><a class="xref" href="atip-features.html#option2-sriov-helm" title="42.6.2. Option 2 (Recommended): SR-IOV Network Operator">Section 42.6.2, “Option 2 (Recommended): SR-IOV Network Operator”</a>: This automated method provides simpler deployment. This method is only for network devices.</p></li></ul></div><p>In rare cases where you need both solutions - using the Network Operator for network devices and the Device Plugin for vRAN Accelerators - you must deploy them into separate Kubernetes namespaces. This separation is essential to prevent conflicts between two deployments.</p><section class="sect2" id="option1-sriov-deviceplugin" data-id-title="Option 1: SR-IOV Network Device Plugin Daemonset and configMap"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.6.1 </span><span class="title-name">Option 1: SR-IOV Network Device Plugin Daemonset and configMap</span></span> <a title="Permalink" class="permalink" href="atip-features.html#option1-sriov-deviceplugin">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><a class="link" href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin" target="_blank">SR-IOV Network Device Plugin</a> discovers and advertises network resources, such as PCI physical functions (PFs), and their virtual functions (VFs), on a Kubernetes host.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Prepare the config map for the device plugin</p></li></ul></div><p>We need to create a config map that defines SR-IOV resource pools. Run <code class="literal">lspci</code> command to retrieve the information:</p><div class="verbatim-wrap"><pre class="screen">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</pre></div><p>The SR-IOV Device Plugin uses a configMap containing a JSON file to define which hardware resources Kubernetes should expose. This configuration is based on two core concepts: <code class="literal">selectors</code> (for hardware discovery) and <code class="literal">resources</code> (for Kubernetes exposure).</p><p>A <code class="literal">resource</code> is the named entity that pods consume (e.g. <code class="literal">rancher.io/intel_fec_5g</code>). Resources can be defined as one of two types:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">accelerator</code>: Used for vRAN accelerator cards (like ACC100/vRAN Boost).</p></li><li class="listitem"><p><code class="literal">netdevice</code>: Used for standard network interfaces (NICs).</p></li></ul></div><p>You define the target devices using <code class="literal">selectors</code> to filter the hardware on the node:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">vendors</code>: <code class="literal">8086</code> (Intel)</p></li><li class="listitem"><p><code class="literal">devices</code>: <code class="literal">0d5d</code> (FEC card), <code class="literal">1889</code> (NIC)</p></li><li class="listitem"><p><code class="literal">drivers</code>: <code class="literal">vfio-pci</code></p></li><li class="listitem"><p><code class="literal">pfNames</code>: <code class="literal">p2p1</code> (physical interface name)</p></li></ul></div><p>For network cards, you can also select a subset of Virtual Functions (VFs) from a Physical Function:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">pfNames</code>: <code class="literal">["eth1#1,2,3,4,5,6"]</code> or <code class="literal">[eth1#1-6]</code></p></li></ul></div><p>To allow pods to request the devices, each resource must have a name, which is composed of a prefix and a name:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">resourceName</code>: <code class="literal">pci_sriov_net_bh_dpdk</code></p></li><li class="listitem"><p><code class="literal">resourcePrefix</code>: <code class="literal">rancher.io</code></p></li></ul></div><p>Pods would then request the combined resource name: <code class="literal">rancher.io/pci_sriov_net_bh_dpdk</code> .</p><div id="id-1.9.7.11.6.16" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>This document does not list all possible selectors. Different resource types use different sets of selectors. For comprehensive details, refer to the <a class="link" href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin" target="_blank">SR-IOV Network Device Plugin repository</a>.</p></div><p>The ConfigMap below is an example that creates three resources: one for the vRAN Accelerator card (FEC) and two for two different NIC ports.</p><p>For FEC card, you must first retrieve the device ID and VFIO token. Follow the instructions in <a class="xref" href="atip-features.html#acceleration" title="42.8. vRAN Acceleration (Intel ACC100/ACC200)">Section 42.8, “vRAN Acceleration (<code class="literal">Intel ACC100/ACC200</code>)”</a> chapter for prerequisites.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
            	"resourcePrefix": "rancher.io",
                "resourceName": "intel_fec_5g",
                "deviceType": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                },
        		"additionalInfo": {
          			"*": {
            			"VFIO_TOKEN": "00112233-4455-6677-8899-aabbccddeeff"
          			}
          		}
            },
            {
            	"resourcePrefix": "rancher.io",
                "resourceName": "intel_sriov_odu",
                "deviceType": "netdevice",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
            	"resourcePrefix": "rancher.io",
                "resourceName": "intel_sriov_oru",
                "deviceType": "netdevice",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Prepare the <code class="literal">daemonset</code> file to deploy the device plugin.</p></li></ul></div><p>The device plugin supports several architectures (<code class="literal">arm</code>, <code class="literal">amd</code>, <code class="literal">ppc64le</code>), so the same file can be used for different architectures by deploying several <code class="literal">daemonset</code> for each architecture.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.9.0-build20250425
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>After applying the configMap and the <code class="literal">daemonset</code>, the device plugin will be deployed and the interfaces will be discovered and available for the pods.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</pre></div></li><li class="listitem"><p>Verify all nodes if interfaces were discovered and became available for the pods:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, allocatable: .status.allocatable}'
{
  "name": "node1.suse.edge.com",
  "allocatable": {
	  "cpu": "64",
	  "ephemeral-storage": "256196109726",
	  "hugepages-1Gi": "40Gi",
	  "hugepages-2Mi": "0",
	  "rancher.io/intel_fec_5g": "16",
	  "rancher.io/intel_sriov_odu": "4",
	  "rancher.io/intel_sriov_oru": "4",
	  "memory": "221396384Ki",
	  "pods": "110"
  }
}</pre></div></li><li class="listitem"><p>The resourceName for <code class="literal">FEC</code> accelerator is <code class="literal">rancher.io/intel_fec_5g</code> and 16 VFs are available for use.</p></li><li class="listitem"><p>The resourceName for NIC cards are <code class="literal">rancher.io/intel_sriov_odu</code> and <code class="literal">rancher.io/intel_sriov_oru</code>. Each resource provides 4 VFs.</p></li></ul></div><div id="id-1.9.7.11.6.24" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>If no interfaces are detected as allocatable resources in the kubernetes nodes, it is essential to resolve this issue. One common cause is ill-formed configMap spec, so better review the configMap and its selectors.</p></div></section><section class="sect2" id="option2-sriov-helm" data-id-title="Option 2 (Recommended): SR-IOV Network Operator"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.6.2 </span><span class="title-name">Option 2 (Recommended): SR-IOV Network Operator</span></span> <a title="Permalink" class="permalink" href="atip-features.html#option2-sriov-helm">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Get Helm if not present:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install SR-IOV Network Operator on <code class="literal">sriov-network-operator</code> namespace:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">helm install sriov-crd oci://registry.suse.com/edge/charts/sriov-crd -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/charts/sriov-network-operator -n sriov-network-operator</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the deployed CRDs and pods:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check if SR-IOV label is applied to the nodes.</p></li></ul></div><p>With all resources running, the label appears automatically in your node:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Review the <code class="literal">daemonset</code> to see the new <code class="literal">sriov-network-config-daemon</code> and <code class="literal">sriov-rancher-nfd-worker</code> as active and ready:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get daemonset -n sriov-network-operator
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m</pre></div><p>In a few minutes, the nodes will be detected and fully configured with <code class="literal">SR-IOV</code> capabilities. The update can sometimes take up to 10 minutes:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get sriovnetworknodestates -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check if the interfaces were detected.</p></li></ul></div><p>The interfaces discovered should be the PCI address of the network device. Check this information with the <code class="literal">lspci</code> command in the host.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get sriovnetworknodestates -n sriov-network-operator -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</pre></div><div id="id-1.9.7.11.7.18" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>If your interface is not detected here, ensure that it is present in the next config map:</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</pre></div><p>If your device is not listed, edit the config map by adding the right values to be discovered. Then restart the <code class="literal">sriov-network-config-daemon</code> pods on each node for update to take effect.</p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the <code class="literal">SriovNetworkNodePolicy</code> to configure the <code class="literal">VFs</code></p></li></ul></div><p>This policy creates the resource <code class="literal">intelnicsDpdk</code> for pod consumption. It also binds <code class="literal">vfio-pci</code> driver to the provided PCI device and creates 8 VFs with an MTU size of 1500:</p><div id="id-1.9.7.11.7.21" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">resourceName</code> field must not contain any special characters and must be unique across the cluster.
The example uses the <code class="literal">deviceType: vfio-pci</code> because DPDK is used in combination with SR-IOV. If you don’t use DPDK, configure <code class="literal">deviceType: netdevice</code> (default value).</p></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Validate configurations on all nodes:</p></li></ul></div><p>With the predefined resourcePrefix <code class="literal">rancher.io</code>, a resource <code class="literal">rancher.io/intelnicsDpdk</code> with 8 VFs should be discovered.</p><div class="verbatim-wrap"><pre class="screen">$ kubectl get nodes -o jsonpath='{"items": [ { "name": @.metadata.name, "allocatable": @.status.allocatable } ]}' | jq
{
  "name": "node1.suse.edge.com",
  "allocatable": {
	  "cpu": "64",
	  "ephemeral-storage": "256196109726",
	  "hugepages-1Gi": "60Gi",
	  "hugepages-2Mi": "0",
	  "rancher.io/intel_fec_5g": "16",
	  "memory": "200424836Ki",
	  "pods": "110",
	  "rancher.io/intelnicsDpdk": "8"
  }
}</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>(Optional) Create the <code class="literal">sriovnetwork</code></p></li></ul></div><p>This step is optional and only required for custom network definitions. Specify the <code class="literal">resourceName</code> to bind to the previously created node policy.</p><p>If the <code class="literal">networkNamespace</code> is set, the network is exposed to pods in that namespace. Otherwise, the network becomes available in the Network Operator’s installation namespace.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator		# where SRIOV Operator is installed
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk
  networkNamespace: default 		# where workloads are deployed</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>If the update is successful, a NetworkAttachmentDefinition (NAD) is created in target cluster.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get net-attach-def -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: default
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</pre></div><p>The workload pods could use the resourceName <code class="literal">rancher.io/intelnicsDpdk</code> to use the VFs of the network interface.</p></section></section><section class="sect1" id="dpdk" data-id-title="DPDK"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.7 </span><span class="title-name">DPDK</span></span> <a title="Permalink" class="permalink" href="atip-features.html#dpdk">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">DPDK</code> (Data Plane Development Kit) is a set of libraries and drivers for fast packet processing. It is used to accelerate packet processing workloads running on a wide variety of CPU architectures.
The DPDK includes data plane libraries and optimized network interface controller (<code class="literal">NIC</code>) drivers for the following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>A queue manager implements lockless queues.</p></li><li class="listitem"><p>A buffer manager pre-allocates fixed size buffers.</p></li><li class="listitem"><p>A memory manager allocates pools of objects in memory and uses a ring to store free objects; ensures that objects are spread equally on all <code class="literal">DRAM</code> channels.</p></li><li class="listitem"><p>Poll mode drivers (<code class="literal">PMD</code>) are designed to work without asynchronous notifications, reducing overhead.</p></li><li class="listitem"><p>A packet framework as a set of libraries that are helpers to develop packet processing.</p></li></ol></div><p>The following steps will show how to enable <code class="literal">DPDK</code> and how to create <code class="literal">VFs</code> from the <code class="literal">NICs</code> to be used by the <code class="literal">DPDK</code> interfaces:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install the <code class="literal">DPDK</code> package:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters:</p></li></ul></div><p>To use DPDK, employ some drivers to enable certain parameters in the kernel:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option enables the use  of the <code class="literal">vfio</code> driver for the DPDK interfaces.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_iommu or amd_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; "><p>This option enables the use of <code class="literal">vfio</code> for <code class="literal">VFs</code>.</p></td></tr></tbody></table></div><p>To enable the parameters, add them to the <code class="literal">/etc/default/grub</code> file:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</pre></div><p>Update the GRUB configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Load <code class="literal">vfio-pci</code> kernel module and enable <code class="literal">SR-IOV</code> on the NICs. First argument indicates <code class="literal">vfio-pci</code> driver to support SR-IOV, and second argument prevents the PCI device from entering a low-power state when it’s idle:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create some virtual functions (<code class="literal">VFs</code>) from the <code class="literal">NICs</code>.</p></li></ul></div><p>To create for <code class="literal">VFs</code>, for example, for two different <code class="literal">NICs</code>, the following commands are required:</p><div class="verbatim-wrap"><pre class="screen">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Bind the new VFs with the <code class="literal">vfio-pci</code> driver:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Review the configuration is correctly applied:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</pre></div></section><section class="sect1" id="acceleration" data-id-title="vRAN Acceleration (Intel ACC100/ACC200)"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.8 </span><span class="title-name">vRAN Acceleration (<code class="literal">Intel ACC100/ACC200</code>)</span></span> <a title="Permalink" class="permalink" href="atip-features.html#acceleration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>As communications service providers move from 4G to 5G networks, many are adopting virtualized radio access network (vRAN) architectures for higher channel capacity and easier deployment of edge-based services and applications. vRAN solutions are ideally located to deliver low-latency services with the flexibility to increase or decrease capacity based on the volume of real-time traffic and demand on the network.</p><p>One of the most compute-intensive 4G and 5G workloads is RAN layer 1 (<code class="literal">L1</code>) <code class="literal">FEC</code>, which resolves data transmission errors over unreliable or noisy communication channels. <code class="literal">FEC</code> technology detects and corrects a limited number of errors in 4G or 5G data, eliminating the need for retransmission. Since the <code class="literal">FEC</code> acceleration transaction does not contain cell state information, it can be easily virtualized, enabling pooling benefits and easy cell migration.</p><p>Historically, Intel provided the ACC100 vRAN Accelerator card to rapidly execute Layer 1 FEC algorithms, freeing up host processing power for the main CPU. Intel has since integrated this technology directly into newer CPUs, starting with Sapphire Rapids, under the name <code class="literal">Intel vRAN Boost</code> (also known as ACC200). Intel vRAN Boost act as an offload accelerator on the CPU itself, eliminating the need for a separate hardware card. This section details configuration of SUSE Telco Cloud for workloads to leverage ACC100 or Intel vRAN Boost.</p><section class="sect2" id="id-kernel-parameters" data-id-title="Kernel parameters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.8.1 </span><span class="title-name">Kernel parameters</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-kernel-parameters">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To enable the <code class="literal">vRAN</code> acceleration, we need to enable the following kernel parameters (if not present yet):</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>pt</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option enables the use of vfio for the DPDK interfaces.</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>intel_iommu or amd_iommu</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>on</p></td><td style="text-align: left; vertical-align: top; "><p>This option enables the use of vfio for VFs.</p></td></tr></tbody></table></div><p>Modify the GRUB file <code class="literal">/etc/default/grub</code> to add them to the kernel command line:</p><div class="verbatim-wrap"><pre class="screen">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</pre></div><p>Update the GRUB configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To verify that the parameters are applied after the reboot, check the command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div></section><section class="sect2" id="id-configure-sr-iov-on-fec-accelerators" data-id-title="Configure SR-IOV on FEC Accelerators"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.8.2 </span><span class="title-name">Configure SR-IOV on FEC Accelerators</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-configure-sr-iov-on-fec-accelerators">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Load <code class="literal">vfio-pci</code> kernel module to enable vRAN acceleration. First argument indicates <code class="literal">vfio-pci</code> module to support SR-IOV, and second argument prevents the PCI device from entering a low-power state when it’s idle.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Retrieve the PCI device address of FEC accelerator:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Bind the physical interface (<code class="literal">PF</code>) with <code class="literal">vfio-pci</code> driver:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the virtual functions (<code class="literal">VFs</code>) from the physical interface (<code class="literal">PF</code>).</p></li></ul></div><p>Check the maximum <code class="literal">VF</code> capacity of the accelerator card. Next, configure the card to expose the desired number of VFs, not exceeding the maximum number. In this example, we configure the card for its full capacity of 16 VFs:</p><div class="verbatim-wrap"><pre class="screen">$ cat /sys/bus/pci/devices/0000:8a:00.0/sriov_totalvfs
16
$ echo 16 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Configure the accelerator card and its virtual functions with a 4G or 5G profile. A unique VF token (UUID) must be provided. The workload would consume this VF token to utilize the card’s FEC acceleration capabilities:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ pf_bb_config ACC200 -c /opt/pf-bb-config/acc200_config_vf_5g.cfg -v 00112233-4455-6677-8899-aabbccddeeff
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC200 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC200 PF [0000:8a:00.0] configuration complete!</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Verify the new VFs created from the FEC PF. Note that the VFs got <code class="literal">0d5d</code> device ID. This information is required in next step to expose these VFs as Kubernetes resource:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</pre></div></section><section class="sect2" id="id-configure-kubernetes-for-fec-acceleration" data-id-title="Configure Kubernetes for FEC Acceleration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.8.3 </span><span class="title-name">Configure Kubernetes for FEC Acceleration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-configure-kubernetes-for-fec-acceleration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The final step is exposing the VFs to Kubernetes with help of SR-IOV device plugin. Create a ConfigMap using the VFs' deviceID gathered from previous step, and install SR-IOV device plugin. Once Kubernetes nodes display the FEC VFs as <code class="literal">Allocatable</code> resources, the cluster is ready for the workloads to enjoy the FEC acceleration.</p><p>Follow the steps of Option 1 in <a class="xref" href="atip-features.html#sriov" title="42.6. SR-IOV">Section 42.6, “SR-IOV”</a> chapter. SRIOV Network Operator isn’t applicable for FEC Accelerator, so Option 2 is not applicable.</p></section></section><section class="sect1" id="huge-pages" data-id-title="Huge pages"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.9 </span><span class="title-name">Huge pages</span></span> <a title="Permalink" class="permalink" href="atip-features.html#huge-pages">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When a process uses <code class="literal">RAM</code>, the <code class="literal">CPU</code> marks it as used by that process. For efficiency, the <code class="literal">CPU</code> allocates <code class="literal">RAM</code> in chunks <code class="literal">4K</code> bytes is the default value on many platforms. Those chunks are named pages. Pages can be swapped to disk, etc.</p><p>Since the process address space is virtual, the <code class="literal">CPU</code> and the operating system need to remember which pages belong to which process, and where each page is stored. The greater the number of pages, the longer the search for memory mapping. When a process uses <code class="literal">1 GB</code> of memory, that is 262144 entries to look up (<code class="literal">1 GB</code> / <code class="literal">4 K</code>). If a page table entry consumes 8 bytes, that is <code class="literal">2 MB</code> (262144 * 8) to look up.</p><p>Most current <code class="literal">CPU</code> architectures support larger-than-default pages, which give the <code class="literal">CPU/OS</code> fewer entries to look up.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kernel parameters</p></li></ul></div><p>To enable the huge pages, we should add the following kernel parameters. In this example, we configure 40 1G pages, though the huge page size and exact number should be tailored to your application’s memory requirements:</p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col class="col_1"/><col class="col_2"/><col class="col_3"/></colgroup><thead><tr><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">parameter</th><th style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; ">value</th><th style="text-align: left; vertical-align: top; border-bottom: 1px solid ; ">description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This option allows to set the size of huge pages to 1 G</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>hugepages</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; border-bottom: 1px solid ; "><p>40</p></td><td style="text-align: left; vertical-align: top; border-bottom: 1px solid ; "><p>This is the number of huge pages defined before</p></td></tr><tr><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>default_hugepagesz</p></td><td style="text-align: left; vertical-align: top; border-right: 1px solid ; "><p>1G</p></td><td style="text-align: left; vertical-align: top; "><p>This is the default value to get the huge pages</p></td></tr></tbody></table></div><p>Modify the GRUB file <code class="literal">/etc/default/grub</code> to add these parameters in <code class="literal">GRUB_CMDLINE_LINUX</code>:</p><div class="verbatim-wrap"><pre class="screen">default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0</pre></div><p>Update the GRUB configuration and reboot the system to apply the changes:</p><div class="verbatim-wrap"><pre class="screen">$ transactional-update grub.cfg
$ reboot</pre></div><p>To validate that the parameters are applied after the reboot, you can check the command line:</p><div class="verbatim-wrap"><pre class="screen">$ cat /proc/cmdline</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Using huge pages</p></li></ul></div><p>To use the huge pages, we need to mount them:</p><div class="verbatim-wrap"><pre class="screen">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</pre></div><p>Deploy a Kubernetes workload, creating the resources and the volumes:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</pre></div><div class="verbatim-wrap highlight yaml"><pre class="screen">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</pre></div></section><section class="sect1" id="numa-aware-scheduling" data-id-title="NUMA-aware scheduling"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.10 </span><span class="title-name">NUMA-aware scheduling</span></span> <a title="Permalink" class="permalink" href="atip-features.html#numa-aware-scheduling">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Non-Uniform Memory Access or Non-Uniform Memory Architecture (<code class="literal">NUMA</code>) is a physical memory design used in <code class="literal">SMP</code> (multiprocessors) architecture, where the memory access time depends on the memory location relative to a processor. Under <code class="literal">NUMA</code>, a processor can access its own local memory faster than non-local memory, that is, memory local to another processor or memory shared between processors.</p><section class="sect2" id="id-identifying-numa-nodes" data-id-title="Identifying NUMA nodes"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.10.1 </span><span class="title-name">Identifying NUMA nodes</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-identifying-numa-nodes">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To identify the <code class="literal">NUMA</code> nodes, on your system use the following command:</p><div class="verbatim-wrap"><pre class="screen">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</pre></div><div id="id-1.9.7.15.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For this example, we have only one <code class="literal">NUMA</code> node showing 64 <code class="literal">CPUs</code>.</p><p><code class="literal">NUMA</code> needs to be enabled in the <code class="literal">BIOS</code>. If <code class="literal">dmesg</code> does not have records of NUMA initialization during the bootup, then <code class="literal">NUMA</code>-related messages in the kernel ring buffer might have been overwritten.</p></div></section></section><section class="sect1" id="metal-lb-configuration" data-id-title="Metal LB"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.11 </span><span class="title-name">Metal LB</span></span> <a title="Permalink" class="permalink" href="atip-features.html#metal-lb-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">MetalLB</code> is a load-balancer implementation for bare-metal Kubernetes clusters, using standard routing protocols like <code class="literal">L2</code> and <code class="literal">BGP</code> as advertisement protocols. It is a network load balancer that can be used to expose services in a Kubernetes cluster to the outside world due to the need to use Kubernetes Services type <code class="literal">LoadBalancer</code> with bare-metal.</p><p>To enable <code class="literal">MetalLB</code> in the <code class="literal">RKE2</code> cluster, the following steps are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Install <code class="literal">MetalLB</code> using the following command:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/metallb
  targetNamespace: metallb-system
  version: 305.0.1+up0.15.2
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
  version: 305.0.1+up0.3.0
  createNamespace: true
EOF</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the <code class="literal">IpAddressPool</code> and the <code class="literal">L2advertisement</code> configuration:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Create the endpoint service to expose the <code class="literal">VIP</code>:</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Check the <code class="literal">VIP</code> is created and the <code class="literal">MetalLB</code> pods are running:</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">$ kubectl get svc -n default
$ kubectl get pods -n default</pre></div></section><section class="sect1" id="private-registry" data-id-title="Private registry configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.12 </span><span class="title-name">Private registry configuration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#private-registry">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p><code class="literal">Containerd</code> can be configured to connect to private registries and use them to pull private images on each node.</p><p>Upon startup, <code class="literal">RKE2</code> checks if a <code class="literal">registries.yaml</code> file exists at <code class="literal">/etc/rancher/rke2/</code> and instructs <code class="literal">containerd</code> to use any registries defined in the file. If you wish to use a private registry, create this file as root on each node that will use the registry.</p><p>To add the private registry, create the file <code class="literal">/etc/rancher/rke2/registries.yaml</code> with the following content:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</pre></div><p>or without authentication:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</pre></div><p>For the registry changes to take effect, you need to either configure this file before starting RKE2 on the node, or restart RKE2 on each configured node.</p><div id="id-1.9.7.17.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about this, please check <a class="link" href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html#_registries_configuration_file" target="_blank">containerd registry configuration rke2</a>.</p></div></section><section class="sect1" id="ptp-configuration" data-id-title="Precision Time Protocol"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">42.13 </span><span class="title-name">Precision Time Protocol</span></span> <a title="Permalink" class="permalink" href="atip-features.html#ptp-configuration">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Precision Time Protocol (PTP) is a network protocol developed by the Institute of Electrical and Electronics Engineers (IEEE) to enable sub-microsecond time synchronization in a computer network. Since its inception and for a couple of decades now, PTP has been in use in many industries. It has recently seen a growing adoption in the telecommunication networks as a vital element to 5G networks. While being a relatively simple protocol, its configuration can change significantly depending on the application. For this reason, multiple profiles have been defined and standardized.</p><p>In this section, only telco-specific profiles will be covered. Consequently time-stamping capability and a PTP hardware clock (PHC) in the NIC will be assumed. Nowadays, all telco-grade network adapters come with PTP support in hardware, but you can verify such capabilities with the following command:</p><div class="verbatim-wrap"><pre class="screen"># ethtool -T p1p1
Time stamping parameters for p1p1:
Capabilities:
        hardware-transmit
        software-transmit
        hardware-receive
        software-receive
        software-system-clock
        hardware-raw-clock
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
        off
        on
Hardware Receive Filter Modes:
        none
        all</pre></div><p>Replace <code class="literal">p1p1</code> with name of the interface to be used for PTP.</p><p>The following sections will provide guidance on how to install and configure PTP on SUSE Telco Cloud specifically, but familiarity with basic PTP concepts is expected. For a brief overview of PTP and the implementation included in SUSE Telco Cloud, refer to <a class="link" href="https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html" target="_blank">https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html</a>.</p><section class="sect2" id="id-install-ptp-software-components" data-id-title="Install PTP software components"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.13.1 </span><span class="title-name">Install PTP software components</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-install-ptp-software-components">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>In SUSE Telco Cloud, the PTP implementation is provided by the <code class="literal">linuxptp</code> package, which includes two components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">ptp4l</code>: a daemon that controls the PHC on the NIC and runs the PTP protocol</p></li><li class="listitem"><p><code class="literal">phc2sys</code>: a daemon that keeps the system clock in sync with the PTP-synchronized PHC on the NIC</p></li></ul></div><p>Both daemons are required for the system synchronization to fully work and must be correctly configured according to your setup. This is covered in <a class="xref" href="atip-features.html#ptp-telco-config" title="42.13.2. Configure PTP for telco deployments">Section 42.13.2, “Configure PTP for telco deployments”</a>.</p><p>The easiest and best way to integrate PTP in your downstream cluster is to add the <code class="literal">linuxptp</code> package under <code class="literal">packageList</code> in the Edge Image Builder (EIB) definition file. This way the PTP control plane software will be installed automatically during the cluster provisioning. See the EIB documentation (<a class="xref" href="quickstart-eib.html#eib-configuring-rpm-packages" title="3.3.6. Configuring RPM packages">Section 3.3.6, “Configuring RPM packages”</a>) for more information on installing packages.</p><p>Below find a sample EIB manifest with <code class="literal">linuxptp</code>:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  imageType: RAW
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.2-Base-RT-GM.raw
  outputImageName: eibimage-slmicrort-telco.raw
operatingSystem:
  time:
    timezone: America/New_York
  kernelArgs:
    - ignition.platform.id=openstack
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
    enable:
      - ptp4l
      - phc2sys
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
      - open-iscsi
      - tuned
      - cpupower
      - linuxptp
    sccRegistrationCode: $SCC_REGISTRATION_CODE</pre></div><div id="id-1.9.7.18.7.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">linuxptp</code> package included in SUSE Telco Cloud does not enable <code class="literal">ptp4l</code> and <code class="literal">phc2sys</code> by default. If their system-specific configuration files are deployed at provisioning time (see <a class="xref" href="atip-features.html#ptp-capi" title="42.13.3. Cluster API integration">Section 42.13.3, “Cluster API integration”</a>), they should be enabled. Do so by adding them to the <code class="literal">systemd</code> section of the manifest, as in the example above.</p></div><p>Follow the usual process to build the image as described in the EIB Documentation (<a class="xref" href="quickstart-eib.html#eib-how-to-build-image" title="3.4. Building the image">Section 3.4, “Building the image”</a>) and use it to deploy your cluster. If you are new to EIB, start from <a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a> instead.</p></section><section class="sect2" id="ptp-telco-config" data-id-title="Configure PTP for telco deployments"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.13.2 </span><span class="title-name">Configure PTP for telco deployments</span></span> <a title="Permalink" class="permalink" href="atip-features.html#ptp-telco-config">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Many telco applications require strict phase and time synchronization with little deviance, which resulted in a definition of two telco-oriented profiles: the ITU-T G.8275.1 and ITU-T G.8275.2. They both have a high rate of sync messages and other distinctive traits, such as the use of an alternative Best Master Clock Algorithm (BMCA). Such behavior mandates specific settings in the configuration file consumed by <code class="literal">ptp4l</code>, provided in the following sections as a reference.</p><div id="id-1.9.7.18.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Both sections only cover the case of an ordinary clock in Time Receiver configuration.</p></li><li class="listitem"><p>Any such profile must be used in a well-planned PTP infrastructure.</p></li><li class="listitem"><p>Your specific PTP network may require additional configuration tuning, make sure to review and adapt the provided examples if needed.</p></li></ul></div></div><section class="sect3" id="id-ptp-profile-itu-t-g-8275-1" data-id-title="PTP profile ITU-T G.8275.1"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">42.13.2.1 </span><span class="title-name">PTP profile ITU-T G.8275.1</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-ptp-profile-itu-t-g-8275-1">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The G.8275.1 profile has the following specifics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Runs directly on Ethernet and requires full network support (adjacent nodes/switches must support PTP).</p></li><li class="listitem"><p>The default domain setting is 24.</p></li><li class="listitem"><p>Dataset comparison is based on the G.8275.x algorithm and its <code class="literal">localPriority</code> values after <code class="literal">priority2</code>.</p></li></ul></div><p>Copy the following content to a file named <code class="literal">/etc/ptp4l-G.8275.1.conf</code>:</p><div class="verbatim-wrap"><pre class="screen"># Telecom G.8275.1 example configuration
[global]
domainNumber                    24
priority2			255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             -3
logSyncInterval                 -4
logMinDelayReqInterval          -4
announceReceiptTimeout		3
serverOnly                      0
ptp_dst_mac                     01:80:C2:00:00:0E
network_transport               L2</pre></div><p>Once the file has been created, it must be referenced in <code class="literal">/etc/sysconfig/ptp4l</code> for the daemon to start correctly. This can be done by changing the <code class="literal">OPTIONS=</code> line to:</p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"</pre></div><p>More precisely:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">-f</code> requires the file name of the configuration file to use; <code class="literal">/etc/ptp4l-G.8275.1.conf</code> in this case</p></li><li class="listitem"><p><code class="literal">-i</code> requires the name of the interface to use, replace <code class="literal">$IFNAME</code> with a real interface name.</p></li><li class="listitem"><p><code class="literal">--message_tag</code> allows to better identify the ptp4l output in the system logs and is optional.</p></li></ul></div><p>Once the steps above are complete, the <code class="literal">ptp4l</code> daemon must be (re)started:</p><div class="verbatim-wrap"><pre class="screen"># systemctl restart ptp4l</pre></div><p>Check the synchronization status by observing the logs with:</p><div class="verbatim-wrap"><pre class="screen"># journalctl -e -u ptp4l</pre></div></section><section class="sect3" id="id-ptp-profile-itu-t-g-8275-2" data-id-title="PTP profile ITU-T G.8275.2"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">42.13.2.2 </span><span class="title-name">PTP profile ITU-T G.8275.2</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-ptp-profile-itu-t-g-8275-2">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The G.8275.2 profile has the following specifics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Runs on IP and does not require full network support (adjacent nodes/switches may not support PTP).</p></li><li class="listitem"><p>The default domain setting is 44.</p></li><li class="listitem"><p>Dataset comparison is based on the G.8275.x algorithm and its <code class="literal">localPriority</code> values after <code class="literal">priority2</code>.</p></li></ul></div><p>Copy the following content to a file named <code class="literal">/etc/ptp4l-G.8275.2.conf</code>:</p><div class="verbatim-wrap"><pre class="screen"># Telecom G.8275.2 example configuration
[global]
domainNumber                    44
priority2			255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             0
serverOnly                      0
hybrid_e2e                      1
inhibit_multicast_service       1
unicast_listen                  1
unicast_req_duration            60
logSyncInterval                 -5
logMinDelayReqInterval          -4
announceReceiptTimeout		2
#
# Customize the following for slave operation:
#
[unicast_master_table]
table_id                        1
logQueryInterval                2
UDPv4                           $PEER_IP_ADDRESS
[$IFNAME]
unicast_master_table            1</pre></div><p>Make sure to replace the following placeholders:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">$PEER_IP_ADDRESS</code> - the IP address of the next PTP node to communicate with, such as the master or boundary clock that will provide synchronization.</p></li><li class="listitem"><p><code class="literal">$IFNAME</code> - tells <code class="literal">ptp4l</code> what interface to use for PTP.</p></li></ul></div><p>Once the file has been created, it must be referenced, along with the name of the interface to use for PTP, in <code class="literal">/etc/sysconfig/ptp4l</code> for the daemon to start correctly. This can be done by changing the <code class="literal">OPTIONS=</code> line to:</p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-f /etc/ptp4l-G.8275.2.conf --message_tag ptp-8275.2"</pre></div><p>More precisely:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">-f</code> requires the file name of the configuration file to use. In this case, it is <code class="literal">/etc/ptp4l-G.8275.2.conf</code>.</p></li><li class="listitem"><p><code class="literal">--message_tag</code> allows to better identify the ptp4l output in the system logs and is optional.</p></li></ul></div><p>Once the steps above are complete, the <code class="literal">ptp4l</code> daemon must be (re)started:</p><div class="verbatim-wrap"><pre class="screen"># systemctl restart ptp4l</pre></div><p>Check the synchronization status by observing the logs with:</p><div class="verbatim-wrap"><pre class="screen"># journalctl -e -u ptp4l</pre></div></section><section class="sect3" id="id-configuration-of-phc2sys" data-id-title="Configuration of phc2sys"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">42.13.2.3 </span><span class="title-name">Configuration of phc2sys</span></span> <a title="Permalink" class="permalink" href="atip-features.html#id-configuration-of-phc2sys">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Although not required, it is recommended that you fully complete the configuration of <code class="literal">ptp4l</code> before moving to <code class="literal">phc2sys</code>.
<code class="literal">phc2sys</code> does not require a configuration file and its execution parameters can be solely controlled through the <code class="literal">OPTIONS=</code> variable present in <code class="literal">/etc/sysconfig/ptp4l</code>, in a similar fashion to <code class="literal">ptp4l</code>:</p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-s $IFNAME -w"</pre></div><p>Where <code class="literal">$IFNAME</code> is the name of the interface already set up in ptp4l that will be used as the source for the system clock. This is used to identify the source PHC.</p></section></section><section class="sect2" id="ptp-capi" data-id-title="Cluster API integration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">42.13.3 </span><span class="title-name">Cluster API integration</span></span> <a title="Permalink" class="permalink" href="atip-features.html#ptp-capi">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Whenever a cluster is deployed through a management cluster and directed network provisioning, both the configuration file and the two configuration variables in <code class="literal">/etc/sysconfig</code> can be deployed on the host at provisioning time. Below is an excerpt from a cluster definition, focusing on a modified <code class="literal">RKE2ControlPlane</code> object that deploys the same G.8275.1 configuration file on all hosts:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  serverConfig:
    cni: canal
  agentConfig:
    format: ignition
    cisProfile: cis
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /etc/ptp4l-G.8275.1.conf
              overwrite: true
              contents:
                inline: |
                  # Telecom G.8275.1 example configuration
                  [global]
                  domainNumber                    24
                  priority2                       255
                  dataset_comparison              G.8275.x
                  G.8275.portDS.localPriority     128
                  G.8275.defaultDS.localPriority  128
                  maxStepsRemoved                 255
                  logAnnounceInterval             -3
                  logSyncInterval                 -4
                  logMinDelayReqInterval          -4
                  announceReceiptTimeout          3
                  serverOnly                      0
                  ptp_dst_mac                     01:80:C2:00:00:0E
                  network_transport               L2
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/ptp4l
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): ptp4l settings
                  ## Type:           string
                  ## Default:        "-i eth0 -f /etc/ptp4l.conf"
                  ## ServiceRestart: ptp4l
                  #
                  # Arguments when starting ptp4l(8).
                  #
                  OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/phc2sys
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): phc2sys settings
                  ## Type:           string
                  ## Default:        "-s eth0 -w"
                  ## ServiceRestart: phc2sys
                  #
                  # Arguments when starting phc2sys(8).
                  #
                  OPTIONS="-s $IFNAME -w"
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</pre></div><p>Besides other variables, the above definition must be completed with the interface name and with the other Cluster API objects, as described in <a class="xref" href="atip-automated-provisioning.html" title="Chapter 43. Fully automated directed network provisioning">Chapter 43, <em>Fully automated directed network provisioning</em></a>.</p><div id="id-1.9.7.18.9.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>This approach is convenient only if the hardware in the cluster is uniform and the same configuration is needed on all hosts, interface name included.</p></li><li class="listitem"><p>Alternative approaches are possible and will be covered in future releases.</p></li></ul></div></div><p>At this point, your hosts should have a working and running PTP stack and will start negotiating their PTP role.</p></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="atip-management-cluster.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 41 </span>Setting up the management cluster</span></a> </div><div><a class="pagination-link next" href="atip-automated-provisioning.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 43 </span>Fully automated directed network provisioning</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="atip-features.html#kernel-image-for-real-time"><span class="title-number">42.1 </span><span class="title-name">Kernel image for real time</span></a></span></li><li><span class="section"><a href="atip-features.html#kernel-args"><span class="title-number">42.2 </span><span class="title-name">Kernel arguments for low latency and high performance</span></a></span></li><li><span class="section"><a href="atip-features.html#cpu-pinning-host"><span class="title-number">42.3 </span><span class="title-name">CPU Pinning on Host</span></a></span></li><li><span class="section"><a href="atip-features.html#cpu-pinning-k8s"><span class="title-number">42.4 </span><span class="title-name">CPU Pinning on Kubernetes</span></a></span></li><li><span class="section"><a href="atip-features.html#cni-configuration"><span class="title-number">42.5 </span><span class="title-name">CNI Configuration</span></a></span></li><li><span class="section"><a href="atip-features.html#sriov"><span class="title-number">42.6 </span><span class="title-name">SR-IOV</span></a></span></li><li><span class="section"><a href="atip-features.html#dpdk"><span class="title-number">42.7 </span><span class="title-name">DPDK</span></a></span></li><li><span class="section"><a href="atip-features.html#acceleration"><span class="title-number">42.8 </span><span class="title-name">vRAN Acceleration (<code class="literal">Intel ACC100/ACC200</code>)</span></a></span></li><li><span class="section"><a href="atip-features.html#huge-pages"><span class="title-number">42.9 </span><span class="title-name">Huge pages</span></a></span></li><li><span class="section"><a href="atip-features.html#numa-aware-scheduling"><span class="title-number">42.10 </span><span class="title-name">NUMA-aware scheduling</span></a></span></li><li><span class="section"><a href="atip-features.html#metal-lb-configuration"><span class="title-number">42.11 </span><span class="title-name">Metal LB</span></a></span></li><li><span class="section"><a href="atip-features.html#private-registry"><span class="title-number">42.12 </span><span class="title-name">Private registry configuration</span></a></span></li><li><span class="section"><a href="atip-features.html#ptp-configuration"><span class="title-number">42.13 </span><span class="title-name">Precision Time Protocol</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2026</span></div></div></footer></body></html>