"use strict";(self.webpackChunksuse_edge_docs=self.webpackChunksuse_edge_docs||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"Linkerd","metadata":{"permalink":"/blog/Linkerd","source":"@site/blog/2023-10-26-edge-computing-reference-architecture-with-rancher-and-linkerd.md","title":"Edge Computing Reference Architecture with Rancher and Linkerd","description":"With the exponential growth of connected devices, Edge computing is becoming a game changer. Edge computing is a model that processes data near the network edge where data is generated. It addresses the issues of latency, bandwidth, and data privacy more effectively than centralized cloud architectures. However, managing and orchestrating applications and services at the Edge is no easy task. Robust, lightweight, and reliable tools are needed \u2014 a challenge some open source tools are prepared to tackle. By combining Rancher Prime, Buoyant\u2019s Linkerd, RKE2, and K3s, users get a state-of-the-art, highly secure, highly performant solution for unique edge requirements.","date":"2023-10-26T00:00:00.000Z","formattedDate":"October 26, 2023","tags":[{"label":"edge","permalink":"/blog/tags/edge"},{"label":"linkerd","permalink":"/blog/tags/linkerd"},{"label":"rancher","permalink":"/blog/tags/rancher"}],"readingTime":9.945,"hasTruncateMarker":false,"authors":[{"name":"Vince Matev","title":"Partner Alliance Manager","url":"https://github.com/vmatev","image_url":"github.com/vmatev.png","imageURL":"github.com/vmatev.png"}],"frontMatter":{"slug":"Linkerd","title":"Edge Computing Reference Architecture with Rancher and Linkerd","authors":{"name":"Vince Matev","title":"Partner Alliance Manager","url":"https://github.com/vmatev","image_url":"github.com/vmatev.png","imageURL":"github.com/vmatev.png"},"tags":["edge","linkerd","rancher"]},"unlisted":false,"nextItem":{"title":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","permalink":"/blog/Flexran"}},"content":"With the exponential growth of connected devices, Edge computing is becoming a game changer. Edge computing is a model that processes data near the network edge where data is generated. It addresses the issues of latency, bandwidth, and data privacy more effectively than centralized cloud architectures. However, managing and orchestrating applications and services at the Edge is no easy task. Robust, lightweight, and reliable tools are needed \u2014 a challenge some open source tools are prepared to tackle. By combining Rancher Prime, Buoyant\u2019s Linkerd, RKE2, and K3s, users get a state-of-the-art, highly secure, highly performant solution for unique edge requirements.\\n\\n![](./2023-10-26-edge-computing-ref-architecture-linkerd/Buoyant-SUSE-logo.png)\\n\\n\\n## Introducing the architecture\\n\\nBefore we get into the \\"how,\\" let\'s introduce the edge computing stack and examine why these tools work so well together for an edge computing scenario. If you are running Rancher, we recommend combining Rancher Prime, Buoyant\'s Linkerd, RKE2, and K3s (for an overview of what each piece does, please refer to the table below).\\n\\n![](./2023-10-26-edge-computing-ref-architecture-linkerd/Buoyant-SUSE-Edge-Computing.png)\\n\\nWhy this stack for edge computing? First and foremost, they integrate seamlessly, which will save you a lot of headaches. Additionally, Linkerd and RKE2 significantly improve security, while Linkerd provides an additional layer of reliability. This stack is lightweight and resource efficient, making it ideal for resource-constrained environments. And lastly, all these tools focus on operational simplicity. And when it comes to edge computing, that is incredibly important. With multiple disparate devices, you need a unified way to operate them all.\\n\\n|Project Name|What it is|Why for the edge?|\\n|---|---|---|\\n|[Buoyant\u2019s Linkerd](https://buoyant.io/cloud)|Open-source, security-first service mesh for Kubernetes|Provides security, reliability, and observability without any code changes. Is ultra-lightweight and easy to install with a small runtime footprint (this is key in edge computing where managing communication must be efficient)|\\n|[Rancher Prime](https://www.rancher.com/products/rancher)|Open-source multi-cluster Kubernetes orchestration platform|Flexible and compatible with any CNCF Kubernetes distribution, including K3s and RKE2, Rancher Prime proactively monitors cluster health and performance.|\\n|[RKE2](https://docs.rke2.io/)|CNCF-certified Kubernetes distribution optimized for air-gapped, offline, or edge environments deployed at the core or near the edge.|Fully CNCF-certified, RKE2 improves security and simplicity of your Kubernetes deployment. It is designed to be secure, reliable, and lightweight, ideal for general-purpose computing and near-edge use cases.|\\n|[K3s](https://docs.rke2.io/)|CNCF-certified ultra-lightweight Kubernetes distribution providing the best choice for clusters running at the edge.|Ideal for edge applications, allowing for simple deployment and management while still fully CNCF-certified. It is ultra-lightweight and optimized for resource-constrained environments and functions even in remote and disconnected areas.|\\n\\nSecurity, reliability, and observability are all critical concerns for edge computing, and it\u2019s therefore important to choose an architecture that helps, rather than hinders, accomplishing these goals. An effective architecture will be simple to deploy and operate, using technologies in ways that play to their strengths, as described above. With Rancher and Linkerd, we can adopt an extremely simple architecture that nevertheless brings an enormous amount of functionality to the table:\\n\\n![](./2023-10-26-edge-computing-ref-architecture-linkerd/Buoyant-SUSE-Joint-reference-architecture-1.png)\\n\\nHere, our instruments (on the left of the diagram) are connected to IoT \u201cgateway\u201d systems running Linux. By deploying k3s clusters with Linkerd all the way out on the edge gateways, we can use Linkerd\u2019s secure multicluster capabilities to extend the secure service mesh all the way from the central cluster (shown on the right, running RKE) to the edge gateways themselves.\\n\\nThese tools all integrate seamlessly, providing a secure, reliable, observable edge platform that is lightweight and resource efficient. Now let\'s explore why we believe these technologies are a perfect match for the edge.\\n\\n## Why Rancher and Buoyant\'s Linkerd?\\n\\n### Seamless Integration\\nOne of the most significant advantages of combining Rancher Prime, Linkerd, RKE2, and K3s is their compatibility. These tools are designed to work hand-in-hand, providing a seamless experience. Rancher provides the overarching platform to manage your Kubernetes clusters, including RKE2, K3s, EKS, AKS, GKE, etc. And Linkerd easily integrates with any Kubernetes distribution, adding a service mesh layer to your Rancher-managed clusters.\\n\\n### Reliability and Robustness\\nLinkerd adds a layer of reliability and robustness to your Kubernetes clusters by providing traffic splitting, retries, and timeouts capabilities for your applications. With its fault-tolerance feature, Linkerd ensures your applications continue running smoothly, even in the event of a network failure.\\n\\nRKE2 takes Kubernetes security to the next level. It includes several enhancements like CIS benchmark compliance, security by default, and defense in depth. These features, along with Linkerd\'s ability to automate mutual TLS for all service-to-service communication, provide a secure environment for your Edge computing needs.\\n\\n### Lightweight and Resource Efficient\\nEdge environments often have limited resources. K3s is designed explicitly for such situations. It is a fully compliant Kubernetes distribution with a significantly smaller footprint, consuming less than half the memory of a typical Kubernetes installation. This lightweight nature extends to Linkerd as well, which maintains a small runtime footprint, making it an ideal service mesh for Edge environments.\\n\\n### Comprehensive Observability\\nWith numerous devices and applications running in various locations, clearly understanding their performance and issues is vital. The Rancher Prime, Linkerd, RKE2, and K3s stack addresses that by providing comprehensive observability capabilities.\\n\\n**Rancher** enables users to monitor and manage clusters from a unified interface, regardless of where they are deployed. With built-in monitoring and alerting tools, users get detailed insight into cluster health, allowing for quick identification and resolution of potential issues. **Linkerd** provides deep real-time data into your applications\' performance and includes features such as request volume, success rates, and latency distributions for all meshed services. Users get a more granular level of observability into microservices communication, which is crucial in Edge computing scenarios where the network is notorious for being unstable and latency-sensitive. Linkerd also automatically adds mTLS to all service-to-service communication, adding security with no code changes, which is particularly valuable in Edge computing.\\n\\n### Operational Simplicity \\nWhen it comes to Edge computing, operational simplicity is key. Edge environments involve managing numerous devices, often spread across multiple geographical locations, making traditional management methods impractical. Rancher simplifies Kubernetes cluster management with an intuitive user interface and robust API. **Rancher** allows users to manage all their Kubernetes clusters from a single control point, whether in the cloud, data center, or Edge environments. This unified approach simplifies operations and reduces the complexity of managing multiple clusters. \\n\\nThe **Linkerd** service mesh requires minimal configuration and comes with zero-config features such as load balancing, retries, timeouts, and more. With no time-consuming setup, developers have more time to build business logic. That being said, edge devices will require some initial setup work and configuration. Due to their resource limitations, these devices typically require deployment optimization to ensure they run efficiently.\\n\\n**Linkerd Architecture:**\\n\\n![](./2023-10-26-edge-computing-ref-architecture-linkerd/Buoyant-SUSE-Linkerd-architecture.png)\\n\\nThe Linkerd architecture is fairly simple. The Linkerd control plane manages Linkerd\u2019s operation; it also has a CLI that the user can use to configure, and examine, the mesh. Application Pods that are part of the mesh have the ultra-fast, ultra-lightweight Linkerd proxy, purpose-built in Rust, injected into them as a sidecar container. Once the proxy is injected, all application communication goes through the proxy, which manages mTLS, workload authentication and authorization, retries, circuit breaking, metrics collection, and much more. Having these critical features uniformly applied across the entire application at the platform level eliminates the need to change the application itself, meaning that the application developers are free to focus on the business needs of the application rather than on the complexities of the network.\\n\\n## Edge computing use case examples \\n\\nIndustries such as manufacturing, healthcare, transportation, retail, and energy are all increasingly taking advantage of edge computing to optimize their operations. Let\'s look at some examples. But keep in mind that the stack is vertical agnostic. The role of Rancher, Linkerd, K3s, and RKE2 is always the same. The examples below put them in industry-specific context. \\n\\n|Industry Use Case|Retail Industry - Point of Sale (POS) Systems|Manufacturing - Predictive Maintenance|Healthcare - Remote Patient Monitoring|Transportation - Fleet Management|Summing it up|\\n|---|---|---|---|---|---|\\n|**Specific challenge**|In retail environments the people setting up and maintaining the physical devices are more likely to be store managers than technicians. This leads to fragile physical systems.|Predictive maintenance is often critical. Manufacturing equipment sensors send data to the central system that predicts potential equipment failures.|In remote patient monitoring scenarios, patient health data is often collected by various devices and sent to a central system for analysis and monitoring.|Modern fleet management uses real-time vehicle data for route optimization, improved fuel efficiency, and predictive maintenance.|Edge devices must process and analyze data in real-time to ensure business continuity (manufacturing, transportation), save lives (healthcare), or save money (retail).|\\n|**Rancher**|Multi-cluster management enables easy containerized app deployment and management across stores in various geo locations\u200b\u200b.|Manages edge deployments, providing a central point of control for all the clusters running on the factory floor.|Helps manage the deployment of these applications across various devices and locations, ensuring uniformity and ease of management.|Manages edge deployments across various vehicles, providing a central point of control for all the clusters.|Centralized management of distributed containerized apps on the edge.|\\n|**Linkerd**|Guarantees secure, reliable communication between store POS systems and cloud-based central inventory management systems. Provides real-time inventory updates and transaction processing. Seamlessly merges multiple clusters into a single secure mesh.|Guarantees secure, reliable communication between sensors and the applications processing the data. Seamlessly merges multiple clusters into a single secure mesh.|Guarantees secure and reliable communication between patient devices and central health monitoring system. Seamlessly merges multiple clusters into a single secure mesh.|Guarantees secure and reliable communication between the onboard devices and the central fleet management system. Seamlessly merges multiple clusters into a single secure mesh.|Guarantees secure and reliable communication from the edge to the central processing and analysis system.|\\n|**RKE2**|For store backend systems, ensuring reliable and secure operation of the POS system.|Provides secure, reliable Kubernetes runtime for the central systems processing and analyzing the sensor data.|Provides secure, reliable Kubernetes runtime for central health monitoring systems, so patient data is processed accurately and securely.|Provides secure, reliable Kubernetes runtime for central fleet management systems, ensuring real-time fleet data is processed accurately and securely.|Provides secure, reliable Kubernetes runtime for the central system.|\\n|**K3s**|Efficiently deploy and manage containerized apps across multiple stores.|Run data processing apps at the edge, close to data source, reducing latency and network load\u200b\u200b.|Efficiently processes data at the edge, reducing latency and ensuring timely alerts in case of any health anomalies\u200b.|Processes data at the edge, providing real-time insights and reducing network load\u200b\u200b.|Efficiently processes data at the edge.|\\n\\n**Edge Computing stack:**\\n\\n![](./2023-10-26-edge-computing-ref-architecture-linkerd/Buoyant-SUSE-Joint-reference-architecture-2.png)\\n\\n## Accelerate time-to-value with the Rancher and Buoyant teams\\n\\nAs Edge computing use cases rapidly expand, the Rancher Prime, Linkerd, RKE2, and K3s toolkit offers a state-of-the-art, highly secure, and highly performant to these unique challenges. It provides organizations and developers with the tools and strategies they need to deliver fast, reliable performance and robust, secure communication between microservices and applications, all while efficiently managing and orchestrating Kubernetes clusters.\\n\\nPractical use cases showcase how these open source tools synergize to create robust, efficient, and flexible Edge computing solutions. From Retail industry POS systems to remote patient healthcare monitoring, this stack has clear advantages. An easy integration streamlines your Edge computing implementation, and enables you to process data at the Edge, while ensuring reliable and secure data transfer, reducing latency, and providing scalability and flexibility. \\n\\nAs with any implementation, there are some challenges, however. The initial setup and configuration on the edge can be complex. A deep understanding of these tools and Kubernetes is required. If you need help and want to accelerate your time-to-value, the Buoyant and SUSE teams can help. Reach out, and let\'s chat!\\n\\nContact the Buoyant team at: https://buoyant.io/contact\\n\\nContact the SUSE team at: https://www.suse.com/contact/\\n\\nTo sum it up, combining **Rancher Prime**, **Linkerd**, **RKE2**, and **K3s** delivers a robust, observable, and easy-to-manage Edge computing solution. Organizations gain a powerful set of capabilities to improve edge computing performance and tackle the complexities and challenges of managing edge environments. As edge computing applications across  industries continue to proliferate, these tools will play an increasingly critical role in shaping the future of how we process, manage, and utilize data in an increasingly decentralized world."},{"id":"Flexran","metadata":{"permalink":"/blog/Flexran","source":"@site/blog/2023-07-28-flexran.md","title":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","description":"Introduction","date":"2023-07-28T00:00:00.000Z","formattedDate":"July 28, 2023","tags":[{"label":"edge","permalink":"/blog/tags/edge"},{"label":"telco","permalink":"/blog/tags/telco"},{"label":"flexran","permalink":"/blog/tags/flexran"}],"readingTime":37.84,"hasTruncateMarker":false,"authors":[{"name":"Alberto Morgante Medina","title":"Sr. Edge Telco Engineer","url":"https://github.com/alknopfler","image_url":"https://github.com/alknopfler.png","imageURL":"https://github.com/alknopfler.png"}],"frontMatter":{"slug":"Flexran","title":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","authors":{"name":"Alberto Morgante Medina","title":"Sr. Edge Telco Engineer","url":"https://github.com/alknopfler","image_url":"https://github.com/alknopfler.png","imageURL":"https://github.com/alknopfler.png"},"tags":["edge","telco","flexran"]},"unlisted":false,"prevItem":{"title":"Edge Computing Reference Architecture with Rancher and Linkerd","permalink":"/blog/Linkerd"},"nextItem":{"title":"Welcome to SUSE Edge","permalink":"/blog/welcome"}},"content":"## Introduction\\n\\nIntel\xae FlexRAN is a reference implementation for a virtualized 4G and 5G RAN stack. It\'s main purpose is to illustrate how to achieve real-time performance of a virtualized 4G or 5G PHY and MAC layer on an Intel-based hardware architecture, using Intel\xae Xeon\xae Scalable processors, the Intel\xae Advanced Vector Extensions 512 `Intel\xae AVX 512` instruction set and a number of other hardware acceleration features and technologies found in modern Intel Xeon family processors. While some vendors of CloudRAN and in particular O-RAN software merely use it as inspiration, some vendors adopt Intel FlexRAN directly as part of their product.\\nThis article describes how to deploy FlexRAN on the SUSE Adaptive Telco Infrastructure Platform (ATIP) running on bare-metal infrastructure as a Containerized Network Function (CNF). In the ATIP architecture, the runtime stack consists of [SUSE Linux Enterprise Micro](https://www.suse.com/products/micro/) and Rancher Kubernetes Engine v2 (RKE2) and we are using [Rancher](https://www.rancher.com/) to perform cluster and application provisioning, life cycle management, application resource management and collection of telemetry data with role-based access control.\\nThe example RAN deployment option considered here is the [ORAN Alliance 7-2x lower-level split](https://www.o-ran.org/blog/20-new-o-ran-specifications-have-been-released-since-june-2020), which in a nutshell means that the lower layer PHY processing for uplink and downlink (such as Cyclic Prefix addition/removal, iFFT/FFT, calculation of beamforming coefficients and PRACH pre-filtering) is performed in the Radio Unit (RU), and upper layer PHY processing as well as all MAC layer processing is performed in the Distributed Unit (DU). When DU and RU are deployed in different Kubernetes pods, this constitutes an excellent test for a telco cloud platform, as it requires near-realtime processing and transmission of fronthaul traffic over physical network interfaces, which is only possible with careful orchestration of a number of telco-grade platform features, as we will detail further below.\\n\\nWith this cloud-native setup, we can identify the appropriate configuration of the `Suse Edge ATIP` (Adaptive Telco Infrastructure Platform) stack. Also, it demostrates that telecom containerised applications like DU/CU can be run on RKE2 cluster on top of the SLE Micro RT OS taking advantage of the accelerator cards. In addition, we show that we are applying the full power and potential of the accelerator card and CPU.\\n\\n> For more information about Intel FlexRan, see the official [Intel FlexRan website](https://www.intel.com/content/www/us/en/developer/topic-technology/edge-5g/tools/flexran.html)\\n>\\n> For more infomration about Suse ATIP, see the official [Suse Edge ATIP documentation](https://suse-edge.github.io/docs/product/atip/introduction)\\n\\n\\n## Architecture\\n\\nThe architecture is based on the following components:\\n\\n- Rancher Management Cluster: This component will be used to manage the lifecycle of the RKE2 clusters hosting the FlexRAN solution, as well as the monitoring platform based on the Rancher monitoring stack (Grafana and Prometheus installed on the downstream cluster). This cluster is based on a single-node RKE2 cluster on top of SLE Micro.\\n- Edge Cluster 1: This cluster will be used to deploy the `FlexRan Timer Mode tests`. This cluster is based on a single-node RKE2 cluster on top of SLE Micro RT Operating System.\\n- Edge Cluster 2: This cluster will be used to deploy the `FlexRan Xran Mode tests`. This cluster is based on a single-node RKE2 cluster on top of SLE Micro RT Operating System.\\n\\n> Note: The FlexRan tests could be deployed on the same edge cluster, but just for clarity and simplicity in the article, we will deploy them on different clusters.\\n\\n![img.png](2023-07-29-flexran-images/architecture-components.png)\\n\\n\\n\\n## Hardware and Software components\\n\\n### Hardware\\nThe hardware used for this article is based on the following components:\\n\\n- Dell PowerEdge XR11 servers\\n- Intel XEON Gold 6338N 2.2G\\n- 8 x 32GB RDIMM 3200MT/S\\n- 2 x 480GB SSD SATA Disk\\n- Intel E810-CQDA2 Dual Port 100GbE QSFP28\\n- ACC100 FEC Accelerator card\\n\\n### Software\\nThe software used for this article is based on the following components:\\n\\n- Operating system: `SLE Micro 5.4`\\n- Kernel Real Time: `5.14.21-150400.15.11-rt` \\n- RKE2 version: `v1.25.9+rke2r1`\\n- CNI plugins: `Multus 0.3.1` and  `Calico v3.25.0`\\n- Rancher release: `rancher-stable v2.7.5`\\n- Dpdk version: `22.11`\\n- SRIOV upstream `v3.5.1`\\n\\n### Flexran\\nThe FlexRan deployment used for this article is based on the following components:\\n\\n- FlexRAN version: `22.07` using [pre-defined containers](https://hub.docker.com/r/intel/flexran_vdu) from Intel.\\n- Intel OneAPI Base Toolkit: `2022.1.2.146`\\n\\n## Rancher Management Cluster\\n\\nIn our case, the Rancher management cluster will be used to manage the lifecycle of the RKE2 edge clusters deployed for the FlexRAN solution, as well as the monitoring platform based on Grafana and Prometheus.\\n\\n![img.png](2023-07-29-flexran-images/mgmt-component.png)\\n\\n> The operating system installation steps have been omitted for the sake of brevity. There are no special performance-related configuration settings required for the OS on the management cluster.\\n> For more information about how to install the Operating System, see the next [link](https://suse-edge.github.io/docs/product/atip/management-cluster)\\n\\n### RKE2 Cluster Installation on the Management Server\\n\\nOnce you have the Operating System installed, you can proceed with the Rancher installation. First, we will install a RKE2 cluster and then, the Rancher Helm chart to install the Rancher management cluster.\\n\\n1. Run the RKE2 installer:\\n\\n`curl -sfL https://get.rke2.io | sh -`\\n\\nif you want to install a particular version, the `INSTALL_RKE2_VERSION` variable can be used as:\\n\\n`curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\\"v1.25.9+rke2r1\\" sh -`\\n\\n> For more information about the installation, please refer to the documentation: https://docs.rke2.io/install/install_options/\\n\\n2. Enable and start the rke2-server service:\\n\\n`systemctl enable --now rke2-server.service`\\n\\nIn case you want to run the RKE2 agent (in case you need to add more nodes designated to run your apps and services), you can follow the next steps:\\n\\n1. Run the RKE2 installer:\\n\\n`curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\\"agent\\" sh -`\\n\\n2. Configure the config.yaml file located in `/etc/rancher/rke2/` with the following content:\\n```yaml\\nserver: https://<server>:9345\\ntoken: <token from server node>\\n```\\n\\n3. Enable and start the service:\\n\\n`systemctl enable --now rke2-agent.service`\\n\\n### Rancher Manager Install\\n\\nRancher is installed using the Helm package manager for Kubernetes.\\nHelm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm, we can create configurable deployments instead of just using static files.\\n\\n> This section covers the deployment of Rancher on the management cluster.\\n>\\n> For more information about the Rancher manager installation, please refer to the documentation: https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster\\n\\n1. Add the Helm repository\\n\\nThere are three Rancher manager releases available to be added as a Helm repository for Rancher. In our case, we will use the `rancher-stable` because it\'s the release recommended for production environments, but you could use `rancher-latest` or `rancher-alpha` if you want. Also, there is a `rancher primer` release that is the enterprise version of Rancher.\\n\\n`helm repo add rancher-stable https://releases.rancher.com/server-charts/stable`\\n\\n> If you don\'t have `helm` installed previously, you could install it using the following command:\\n>\\n> `curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 |bash`\\n>\\n\\n2. Choose your SSL Configuration\\n\\nThe Rancher management server is designed to be secure by default and requires SSL/TLS configuration.\\n\\nThere are three recommended options for SSL/TLS configuration:\\n- Rancher-generated TLS certificate\\n- Let\'s Encrypt\\n- Bring your own certificate\\n\\n> For more information about the SSL/TLS configuration, please refer to the documentation: https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster/#3-choose-your-ssl-configuration\\n\\nIn our case we will use the Rancher-generated TLS certificate. This requires `cert-manager` to be deployed in the cluster as::\\n\\n```shell \\nhelm repo add jetstack https://charts.jetstack.io\\nhelm repo update\\nhelm install cert-manager jetstack/cert-manager \\\\\\n\\t\\t--namespace cert-manager \\\\\\n\\t\\t--create-namespace \\\\\\n\\t\\t--set installCRDs=true \\\\\\n\\t\\t--version v1.11.1 \\n```\\n\\nOnce you\'ve installed cert-manager, you can verify the pods are running:\\n`kubectl get pods --namespace cert-manager`\\n\\n\\n3. Install Rancher with Helm using the following command modifying the `<hostname>` and `<bootstrapPassword>` values to fit your environment:\\n\\n```shell \\nhelm install rancher rancher-stable/rancher \\\\\\n        --namespace cattle-system \\\\\\n        --create-namespace \\\\\\n        --set hostname=<hostname> \\\\\\n        --set bootstrapPassword=<bootstrapPassword> \\\\\\n        --set replicas=1 \\\\\\n        --set global.cattle.psp.enabled=false\\n```\\n\\n4. Verify the Rancher installation\\n\\nYou should wait a few minutes for Rancher to be rolled out:\\n\\n`kubectl -n cattle-system rollout status deploy/rancher`\\n\\n## Edge Cluster deployment\\n\\nThis section covers the bare-metal provisioning and installation of the edge cluster nodes in order to deploy the FlexRan tests. In our case, we will deploy two edge clusters, one for the FlexRan Timer Mode tests and another one for the FlexRan Xran Mode tests.\\nThe configuration of the edge cluster nodes is based on the following components, and it will be the same for the two edge clusters:\\n\\n![img.png](2023-07-29-flexran-images/edge.png)\\n\\n### BIOS Configuration for Dell PowerEdge XR11\\n\\nThe BIOS and NIC configuration used for this article based on the [hardware](./2023-07-28-flexran.md#hardware) shown above, could be imported directly using the [Dell PowerEdge XR11 BIOS Exported Configuration File](2023-07-29-flexran-images/system-config.xml).\\nIn order to import the configuration file, you need to do it using the idrac web interface:\\n\\n![img.png](2023-07-29-flexran-images/import-bios.png)\\n\\n> *Note*: This configuration depends on the hardware vendor, so please, check with your hardware vendor the best configuration to be used.\\n\\nPay special attention to the parameters suggested here as some of them could dramatically increase or decrease the performance of the tests being executed.\\nThe next table shows the recommended configuration as a reference for the most common hardware vendors:\\n\\n| Option                                         | Value            | Description |\\n|------------------------------------------------|------------------| ----------- |\\n| Workload Profile                               | Telco Optimized  | Telco profile to optimize the performance in the hardware. |\\n| Boot Performance Mode                          | Max. Performance | Maximize the performance in the boot process. |\\n| Hyper- Threading (Logical Proccesor)           | Enable           | This option enables Intel\xae Hyper-Threading Technology for logical processor enabling and converting processor cores (pCores) to logical cores (lCores). |\\n| Virtualization Technology (XAPIC)              | Enable           | This option is for Extended Advanced Programmable Interrupt Controller (xAPIC) support for the Intel\xae Virtualization Technology for Directed I/O (Intel\xae VT-d) feature. |\\n| uncore frequency scaling                       | Disable          | If enabled, Uncore Frequency Scaling (UFS) allows the uncore to operate at a lower frequency when the Power Control Unit (PCU) has detected low utilization. Conversely, UFS allows the uncore to operate at a higher frequency when the PCU has detected high utilization.|\\n| CPU P-State Control (EIST PSD Function         | HW_ALL           | optimization of the voltage and CPU fequency during operation|\\n| CPU C-State Control                            | Disable          | This option is for the CPU C-State Control feature, which provides power savings by placing the processor into lower power states when the processor is idle. |\\n| CPU C1E Support                                | Disable          | This option is for the CPU Enhanced Halt (C1E) feature, which provides power savings by placing the processor into a low power state when the processor is idle. |\\n| AVX License Pre-Grant                          | Enable           | If enabled, this option enables the pre-grant license level selection based on workload with the AVX ICCP Pre-Grant Level option.|\\n| AVX ICCP Pre- Grant Level                      | Level 5          | This option selects a workload level for the Intel\xae Advanced Vector Extensions (Intel\xae AVX): Intel\xae AVX-512 Heavy |\\n| AVX P1                                         | Level 2          |This option serves a dual purpose: 1 -Specifies the base P1 ratio for Intel\xae Streaming SIMD Extensions (Intel\xae SSE) or Intel\xae AVX workloads. 2- Pre-grants a license level based on the workload level.|\\n| Energy Efficient Turbo                         | Disable          | This option allows entry into the Intel\xae Turbo Boost Technology frequency when the Power Control Unit (PCU) has detected high utilization.|\\n| Turbo Mode                                     | Enable | Enabling this Intel\xae Turbo Boost Technology mode setting allows the CPU cores to operate at higher than the rated frequency.|\\n| GPSS timer                                     | 0us              | This option allows the reduction of the Global P-State Selection (GPSS) timer to be set from: 0 \u03bcs to 500 \u03bcs|\\n| LLC prefetch                                   | Enable | This option enables Last Level Cache (LLC) hardware prefetch logic. |\\n| Frequency Prioritization (RAPL Prioritization) | Disable | This setting controls whether the Running Average Power Limit (RAPL) balancer is enabled. If enabled, it activates per core power budgeting.|\\n| Hardware P-States                              | Native with no Legacy Support | When enabled, this option allows the hardware to choose a Performance State (P-State) based on an OS request (that is, a legacy P-State).| \\n| EPP enable3                                    | Disable|When this option is enabled, the system uses the energy performance bias register for the Energy Performance Preference (EPP) input to make decision on Performance State (P-State) or Processor Core Idle State (C-State) transitions.|\\n| APS Rocketing                                  | Disable | Rocketing mechanism in the HWP p-state selection for pcode algorithm. Rocketing enables the core ratio to jump to max turbo instantaneously as opposed to a smooth ramp| \\n| Scalability                                    | Disable | Core Performance to frequency scalability based on optimizations in the CPU.|\\n| Native ASPM                                    | Disable | ASPM off not controlled by BIOS or OS.|\\n| Power Performance Tuning                       | OS Controls EPB | This option selects the BIOS or OS that controls the Energy Performance Bias (EPB) functionality.|\\n| Workload Configuration                         | I/O sensitive | This option allows the system power and performance profile to be set to favor compute intensive workload or I/O sensitive workload.| \\n| Dynamic L1                                     | Disable | This option applies only to the package-level setting to allow dynamically entering the lower power link state L1.|\\n| Set Fan Profile                                |Performance | This option allows the fan profile to be set to Performance, Balanced, or Quiet.|\\n| Cooling Configuration - Fan Speed Offset       | Medium| This option allows the fan speed offset to be set to Low, Medium, or High.|\\n\\n\\n### OS Installation\\n\\nFor this example, we will assume the SLE Micro RT 5.4 operating system has been already installed on the baremetal host used to deploy the edge clusters. To verify the `Real Time Kernel` version used, you can use the following command (pay attention to the `-rt` suffix):\\n\\n```shell\\nuname -r\\n5.14.21-150400.15.11-rt\\n```\\n\\n>If you want to download the SLE Micro RT 5.4 ISO, you can use the following link paying special attention to the `-RT` in the image name, once you\'ve logged in with your SUSE credentials: [Download SLE Micro RT from Suse Customer Center](https://www.suse.com/download/sle-micro/)\\n\\n> For more information about how to install the operating system SLE Micro RT, see the next [link](https://suse-edge.github.io/docs/product/atip/management-cluster#os-install)\\n\\n\\n### OS Configuration and Tuning\\n\\nOnce you have the Operating System installed, you can proceed with the Operating System configuration. For this article, we will configure the Operating System using the next steps:\\n\\n1. CPU Tuned Configuration\\n\\nThe first thing is to create a profile for the CPU cores we want to isolate. In this case, we will isolate the cores 1-30 and 33-62 to be used by FlexRan, keeping also some cores available for the system.\\n\\n```shell\\necho \\"export tuned_params\\" >> /etc/grub.d/00_tuned\\n\\necho \\"isolated_cores=1-30,33-62\\" >> /etc/tuned/cpu-partitioning-variables.conf\\n\\nsystemctl enable tuned; systemctl start tuned\\n\\ntuned-adm profile cpu-partitioning\\n```\\n\\n2. Grub Configuration\\n\\nWe need to modify some grub options to be able to do the CPU isolation as well as another important parameters for the CPU performance on this scenario.\\n\\nThe following options have to be customized:\\n\\n| parameter | value | description                                                                                          |\\n|-----------|-------|------------------------------------------------------------------------------------------------------|\\n| isolcpu| 1-30,33-62| Isolate the cores 1-30 and 33-62                                                                     |\\n| skew_tick| 1 | Allows the kernel to skew the timer interrupts across the isolated CPUs.                             |\\n| nohz| on | Allows the kernel to run the timer tick on a single CPU when the system is idle.                     |\\n| nohz_full| 1-30,33-62 | kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation. |\\n| rcu_nocbs| 1-30,33-62 | Allows the kernel to run the RCU callbacks on a single CPU when the system is idle.                  |\\n| kthread_cpus| 0,31,32,63 | Allows the kernel to run the kthreads on a single CPU when the system is idle.                       |\\n| irqaffinity| 0,31,32,63 | Allows the kernel to run the interrupts on a single CPU when the system is idle.                     |\\n|processor.max_cstate| 1 | Prevents the CPU from dropping into a sleep state when idle                                          |   \\n|intel_idle.max_cstate| 0 | Disables the intel_idle driver and allows acpi_idle to be used                                       |\\n| iommu       | pt         | Allows to use vfio for the dpdk interfaces                                                           |\\n| intel_iommu | on         | Enables to use vfio for VFs.                                                                         |\\n| hugepagesz | 1G    | Allows to set the size of huge pages to 1G                                                           |\\n| hugepages | 40    | Number of hugepages defined before                                                                   |\\n| default_hugepagesz| 1G | Default value to enable huge pages                                                                   |\\n\\n> For mor information about theses parameters, please refer to the next [link](https://suse-edge.github.io/docs/product/atip/features)\\n\\nWith the values showed above, we are isolating 60 cores, and we are using 4 cores for the OS.\\n\\nWe can modify the grub config as follows:\\n\\n```shell\\nvi /etc/default/grub\\n    GRUB_CMDLINE_LINUX=\\"intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpu=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll\\"\\n\\ntransactional-update grub.cfg\\n```\\n\\nTo validate that the parameters are applied after reboot, you could check:\\n\\n```shell\\ncat /proc/cmdline\\n```\\n\\n3. Compile the DPDK drivers\\n\\nIn order to use the `igb_uio` driver, which is necessary for the ACC100 acceleration card, we need to compile the DPDK drivers for the SLE Micro RT kernel.\\nThe process to build the `igb_uio` driver will be explained in this section, but if you don\'t want to deal with the compilation, you can also download the compiled driver for `SLE Micro RT 5.4` rpm files from [here](2023-07-29-flexran-images/dependencies.tar.gz).\\n\\nTo do that, we will use an auxiliary virtual machine (only to compile and generate the rpm files) with `SLE-RT 15 SP4 kernel` that you can download from [here](https://www.suse.com/download/sle-rt/).\\n\\nOnce you have available your virtual machine with the SLE 15 SP4 installed you need to verify `-rt` in the kernel to ensure we will compile for a real time kernel.\\n\\n```shell\\nuname -r\\n5.14.21-150400.15.53-rt\\n```\\n\\nThe first thing you have to do to compile the driver is to download the `dpdk` source code from the [DPDK repository](http://dpdk.org/git/dpdk-kmods).\\n```shell\\ngit clone http://dpdk.org/git/dpdk-kmods\\n```\\n\\nOnce you have the source code, you can compile the driver using the following commands:\\n```shell\\ncd dpdk-kmods/igb_uio\\nmake -C /lib/modules/`uname -r`/build M=`pwd` modules\\n```\\n\\nWith this command you will get the `igb_uio.ko` driver compiled. But in our case, we will create a rpm package to be able to install it in the SLE Micro RT kernel.\\n\\n- First create an x509.genkey setup to define the type of key we need:\\n```shell\\necho -e \\"[ req ] \\\\n\\\\\\ndefault_bits = 4096 \\\\n\\\\\\ndistinguished_name = req_distinguished_name \\\\n\\\\\\nprompt = no \\\\n\\\\\\nx509_extensions = myexts \\\\n\\\\\\n\\n[ req_distinguished_name ] \\\\n\\\\\\nCN = Modules \\\\n\\\\\\n\\\\n\\\\\\n[ myexts ] \\\\n\\\\\\nbasicConstraints=critical,CA:FALSE \\\\n\\\\\\nkeyUsage=digitalSignature \\\\n\\\\\\nsubjectKeyIdentifier=hash \\\\n\\\\\\nauthorityKeyIdentifier=keyid\\" > x509.genkey\\n```\\n\\n- Then create the certificates based on these:\\n```shell\\nopenssl req -new -nodes -utf8 -sha512 -days 36500 -batch -x509 -config x509.genkey -outform DER -out signing_key.x509 -keyout signing_key.priv\\n```\\n\\n> Note: The signing key instruction are only used for testing, please create \\"proper\\" certificates for production usage.\\n\\n- Install `build`, `kernel-rt` and `kernel-devel-rt`  to create the rpm package:\\n```shell\\nzypper in build kernel-rt kernel-devel-rt\\n``` \\n\\n- Modify the config file to adapt it for the RT kernel:\\n\\n```shell \\ncp /usr/lib/build/configs/sle15.4.conf /usr/lib/build/configs/sle15.4-rt.conf\\nsed -e \'s/kernel-default/kernel-rt/g\' -i /usr/lib/build/configs/sle15.4-rt.conf\\necho \\"Prefer: wicked\\" >> /usr/lib/build/configs/sle15.4-rt.conf\\n```\\n\\n- Prepare the spec file:\\n\\n```shell\\ncat << EOF >> igb_uio.spec\\n#\\n# spec file for package igb_uio kmp\\n#\\n# Copyright (c) 2023 SUSE LINUX GmbH, Nuernberg, Germany.\\n#\\n# All modifications and additions to the file contributed by third parties\\n# remain the property of their copyright owners, unless otherwise agreed\\n# upon. The license for this file, and modifications and additions to the\\n# file, is the same license as for the pristine package itself (unless the\\n# license for the pristine package is not an Open Source License, in which\\n# case the license is the MIT License). An \\"Open Source License\\" is a\\n# license that conforms to the Open Source Definition (Version 1.9)\\n# published by the Open Source Initiative.\\n\\n# Please submit bugfixes or comments via http://bugs.opensuse.org/\\n#\\n\\n\\n# norootforbuild\\n\\nName:           \\tigb_uio\\nVersion:                1.0\\nRelease:                0\\nSummary:                Kernel Module Package for igb_uio module\\nLicense:                GPL-2.0\\nGroup:          \\tSystem/Kernel\\nURL:                    https://www.suse.com\\n#Git-Clone:\\t\\thttp://dpdk.org/git/dpdk-kmods\\nSource0:                %{name}-%{version}.tar.gz\\n# Required to sign modules:  Include certificate named \u201csigning_key.x509\u201d\\n# Build structure should also include a private key named \u201csigning_key.priv\u201d\\n# Private key should not be listed as a source file\\nSource1:        signing_key.x509\\nBuildRequires:  %kernel_module_package_buildreqs\\nBuildRequires:\\tkernel-rt\\nBuildRequires:  kernel-rt-devel\\n#BuildRequires:  bash-sh\\n#BuildRequires:  libelf-devel\\n#BuildRequires:  systemd\\n#BuildRequires:  pam-config\\n#BuildRequires:  libffi7\\n#BuildRequires:  ghc-bootstrap\\nBuildRoot:      %{_tmppath}/%{name}-%{version}-build\\n\\n# Required to sign modules:  The -c option tells the macro to generate a\\n# suse-hello-ueficert subpackage that enrolls the certificate\\n%suse_kernel_module_package -c %_sourcedir/signing_key.x509\\n\\n%description\\nThis package contains the igb_uio.ko module.\\n\\n%prep\\n%setup\\n# Required to sign modules:  Copy the signing key to the build area\\ncp %_sourcedir/signing_key.* .\\nset -- *\\nmkdir source\\nmv \\"$@\\" source/\\nmkdir obj\\n\\n%build\\nfor flavor in %flavors_to_build; do\\n       rm -rf obj/$flavor\\n       cp -r source obj/$flavor\\n       make -C %{kernel_source $flavor} modules M=$PWD/obj/$flavor\\ndone\\n\\n%install\\nexport INSTALL_MOD_PATH=$RPM_BUILD_ROOT\\nexport INSTALL_MOD_DIR=updates\\nfor flavor in %flavors_to_build; do\\n       make -C %{kernel_source $flavor} modules_install M=$PWD/obj/$flavor\\n       # Required to sign modules:  Invoke kernel-sign-file to sign each module\\n       for x in $(find $INSTALL_MOD_PATH/lib/modules/*-$flavor/ -name \'*.ko\'); do\\n               /usr/lib/rpm/pesign/kernel-sign-file -i pkcs7 sha256 $PWD/obj/$flavor/signing_key.priv $PWD/obj/$flavor/signing_key.x509 $x\\n       done\\ndone\\n\\n%changelog\\n* Fri Jun 9 2023 Rhys Oxenham <rhys.oxenham@suse.com> - 1.0\\n- Initial spec file as base\\nEOF\\n\\ncat << EOF >> Kbuild\\nccflags-y := $(MODULE_CFLAGS)\\nobj-m := igb_uio.o\\nEOF\\n\\n\\n```\\n\\n- Create the rpm packages:\\n\\n```shell\\nbuild --dist /usr/lib/build/configs/sle15.4-rt.conf\\n``` \\n\\n- After creating the rpm files, the packages will be located here: `/var/tmp/build-root/home/abuild/rpmbuild/RPMS/x86_64`\\n\\n\\n4. Install the dependencies.\\n\\n```shell\\ntransactional-update shell\\n\\ncat > /etc/zypp/repos.d/flexran-dependencies.repo << EOF\\n[home_amorgante_branches_home_dpitchumani]\\nname=Branch project for package DPDK-22.11 (15.4)\\ntype=rpm-md\\nbaseurl=https://download.opensuse.org/repositories/home:/amorgante:/branches:/home:/dpitchumani/15.4/\\ngpgcheck=1\\ngpgkey=https://download.opensuse.org/repositories/home:/amorgante:/branches:/home:/dpitchumani/15.4/repodata/repomd.xml.key\\nenabled=1\\n\\n[home_amorgante]\\nname=home:amorgante (15.4)\\ntype=rpm-md\\nbaseurl=https://download.opensuse.org/repositories/home:/amorgante/15.4/\\ngpgcheck=1\\ngpgkey=https://download.opensuse.org/repositories/home:/amorgante/15.4/repodata/repomd.xml.key\\nenabled=1\\nEOF\\n```\\n\\nNow, we can install the dependencies (*.rpm files) with the igb_uio driver compiled previously (or downloaded from [here](2023-07-29-flexran-images/dependencies.tar.gz)):\\n```shell\\n\\n```shell\\nsuseconnect -p PackageHub/15.4/x86_64\\nzypper in *.rpm\\nzypper in dpdk dpdk-tools pf-bb-config pciutils \\n\\nexit\\n```\\n\\n5. CPU Performance\\n\\nFurther improve the deterministic and power efficiency:\\n\\n`cpupower frequency-set -g performance`\\n\\nSet cpu core frequency to 2.6Ghz which is the maximum allowed in our case (based on the hardware):\\n\\n`cpupower frequency-set -u 2500000`\\n\\n`cpupower frequency-set -d 2500000`\\n\\nSet cpu uncore to fixed \u2013 maximum allowed. Disable c6 and c1e in order to disable the powersaving features in your system (only if enabled):\\n\\n`cpupower idle-set -d 3`\\n\\n`cpupower idle-set -d 2`\\n\\n> In case you\'ve got the following message `Idlestate 3 not available on CPU x` you can ignore it, because that\'s means that the idle state is already disabled.\\n\\n\\n6. Check the CPU performance\\n\\nYou should see the driver intel_cpufreq and the governor performance with a frequency range between 2.5 and 2.6Ghz:\\n```shell\\ncpupower frequency-info\\n...\\nanalyzing CPU 0:\\n  driver: intel_cpufreq\\n  CPUs which run at the same hardware frequency: 0\\n  CPUs which need to have their frequency coordinated by software: 0\\n  maximum transition latency: 20.0 us\\n  hardware limits: 800 MHz - 3.50 GHz\\n  available cpufreq governors: ondemand performance schedutil\\n  current policy: frequency should be within 800 MHz and 3.50 GHz.\\n                  The governor \\"performance\\" may decide which speed to use\\n                  within this range.\\n  current CPU frequency: Unable to call hardware\\n  current CPU frequency: 2.60 GHz (asserted by call to kernel)\\n  boost state support:\\n    Supported: yes\\n    Active: yes\\n...\\n```\\n\\n### RKE2 Cluster Installation\\n\\nThe RKE2 installation could be done creating a new cluster from the Rancher UI or importing an existing RKE2 cluster to Rancher.\\nIn our case, for brevity we will install a new RKE2 Cluster from scratch importing it after that directly into Rancher.\\n\\n> If you want to install the RKE2 cluster from the Rancher UI you can follow [this document](https://suse-edge.github.io/docs/product/atip/edge-site#edge-site-definition)\\n\\n1. Run the RKE2 installer:\\n\\nIn this scenario we use the `v1.25.9+rke2r1` version as:\\n```\\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\\"v1.25.9+rke2r1\\" sh -\\n```\\n\\n2. Create the `/etc/rancher/rke2/config.yaml` file (and the rancher/rke2 directory) with the following content to enable `Multus + Calico` CNI plugins:\\n\\n```yaml\\ncni:\\n  - multus\\n  - calico\\n```\\n\\n3. Start rke2-server service:\\n```shell\\nsystemctl daemon-reload && systemctl enable --now rke2-server\\n```\\n\\n4. Check the installation\\n\\nMake sure the calico and multus pods are running:\\n```shell\\n$ kubectl get pods -A\\nNAMESPACE         NAME                                                    READY   STATUS      RESTARTS   AGE\\ncalico-system     calico-kube-controllers-687bc88ddf-6dp4r                1/1     Running     0          3m13s\\ncalico-system     calico-node-jkhx9                                       1/1     Running     0          3m13s\\ncalico-system     calico-typha-869bd9756d-ft4bs                           1/1     Running     0          3m13s\\nkube-system       cloud-controller-manager-xr11-2                         1/1     Running     0          3m49s\\nkube-system       etcd-xr11-2                                             1/1     Running     0          3m47s\\nkube-system       helm-install-rke2-calico-crd-q2cp2                      0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-calico-nv4rn                          0/1     Completed   1          3m36s\\nkube-system       helm-install-rke2-coredns-55k9x                         0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-ingress-nginx-fvmp4                   0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-metrics-server-d2dhz                  0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-multus-mm59z                          0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-snapshot-controller-crd-vbcjb         0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-snapshot-controller-jw6pk             0/1     Completed   0          3m36s\\nkube-system       helm-install-rke2-snapshot-validation-webhook-w5sj2     0/1     Completed   0          3m36s\\nkube-system       kube-apiserver-xr11-2                                   1/1     Running     0          3m53s\\nkube-system       kube-controller-manager-xr11-2                          1/1     Running     0          3m51s\\nkube-system       kube-proxy-xr11-2                                       1/1     Running     0          3m48s\\nkube-system       kube-scheduler-xr11-2                                   1/1     Running     0          3m51s\\nkube-system       rke2-coredns-rke2-coredns-6b9548f79f-bc54n              1/1     Running     0          3m26s\\nkube-system       rke2-coredns-rke2-coredns-autoscaler-57647bc7cf-bfggl   1/1     Running     0          3m26s\\nkube-system       rke2-ingress-nginx-controller-6vsgf                     1/1     Running     0          2m18s\\nkube-system       rke2-metrics-server-7d58bbc9c6-qjkvr                    1/1     Running     0          2m33s\\nkube-system       rke2-multus-ds-4zsqr                                    1/1     Running     0          3m26s\\nkube-system       rke2-snapshot-controller-7b5b4f946c-rhtxn               1/1     Running     0          2m32s\\nkube-system       rke2-snapshot-validation-webhook-7748dbf6ff-cfmpm       1/1     Running     0          2m1s\\ntigera-operator   tigera-operator-7bd6b54cb8-2jm92                        1/1     Running     0          3m23s\\n```\\n\\n\\n### ACC100 Configuration\\n\\nThe ACC100 accelerator card is a PCIe card that provides hardware acceleration for the Forward Error Correction (FEC) algorithm. This card is used by the FlexRan library to improve the performance of the DU/CU components.\\n\\n1. Load the `igb_uio` kernel module\\n\\n> During the OS configuration section we have installed the `igb` driver, `dpdk` as well as the `pf-bb-config` tool [requirements](#os-configuration-and-tunning)\\n\\nWe will start configuring the Accelerator Card by loading the `ibg_uio` and the `vfio-pci` module.\\n\\n```shell\\nmodprobe igb_uio\\nmodprobe vfio-pci\\n```\\n\\n2. Get the interface ACC100 PCI address:\\n\\n```shell\\n/sbin/lspci | grep -i acc\\n8a:00.0 Processing accelerators: Intel Corporation Device 0d5c\\n```\\n\\n\\n3. Bind the Physical Function (PF) with the `igb_uio` driver:\\n\\n\\n```shell\\ndpdk-devbind.py -b igb_uio 0000:8a:00.0\\n```\\n\\n4. Create 2 Virtual Functions (vfs) from the PF and bind them with `vfio-pci` driver:\\n\\n```shell\\necho 2 > /sys/bus/pci/devices/0000:8a:00.0/max_vfs\\ndpdk-devbind.py -b vfio-pci 0000:8b:00.0\\n```\\n\\n5. Configure acc100 using the `pf-bb-config` tool:\\nThe Physical Function (PF) Baseband Device (BBDEV) Configuration Application (`pf_bb_config`) provides a means to configure a baseband device at the host level. The program accesses the configuration space and sets various parameters through memory-mapped I/O (MMIO) reads and writes.\\nThe parameters are parsed from a given configuration file (with .cfg extensions) that is specific to a particular baseband device, although they follow same format.\\n\\n> For more information about the pf-bb-config tool, please refer to the [documentation](https://github.com/intel/pf-bb-config)\\n\\n```shell\\npf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg\\n\\nTue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL\\nTue Jun  6 10:49:20 2023:INFO:Configuration in VF mode\\nTue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92\\nTue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020\\nTue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete\\nTue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!\\n```\\n\\n6. Check the new VFs created are available and ready to be used by the FlexRan library:\\n\\n```shell\\ndpdk-devbind.py -s\\n...\\nBaseband devices using DPDK-compatible driver\\n=============================================\\n0000:8a:00.0 \'Device 0d5c\' drv=igb_uio unused=vfio-pci\\n0000:8b:00.0 \'Device 0d5d\' drv=vfio-pci unused=igb_uio\\n\\nOther Baseband devices\\n======================\\n0000:8b:00.1 \'Device 0d5d\' unused=igb_uio,vfio-pci\\n...\\n```\\n\\n\\n### DPDK Configuration\\n\\nThe Data Plane Development Kit (DPDK) is a set of data plane libraries and network interface controller drivers for fast packet processing. It is designed to run on any processors. In our case, we will use the DPDK libraries to accelerate the performance of the FlexRan library.\\nLet\'s start to create some VFs to be available for the FlexRan workloads:\\n\\n1. Create the VF PCI addresses in the node:\\n\\nIn this section we will create 4 VFs for each PF (2 PFs in total for the dual port E810 100G interface) binding to the vfio driver, and then, we will assign a MAC address to each VF. \\nThe MAC address is used by the FlexRan library to identify the VFs. This is not a mandatory step, but then, you will need to modify the FlexRan Docker entrypoint script in order to adapt the MAC addresses to the VFs created. We will talk more about the docker entrypoint script in the next section.\\n\\n```shell\\necho 4 > /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs\\nip link set p2p1 vf 0 mac 00:11:22:33:00:00\\nip link set p2p1 vf 1 mac 00:11:22:33:00:10\\nip link set p2p1 vf 2 mac 00:11:22:33:00:20\\nip link set p2p1 vf 3 mac 00:11:22:33:00:30\\necho 4 > /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs\\nip link set p2p2 vf 0 mac 00:11:22:33:00:01\\nip link set p2p2 vf 1 mac 00:11:22:33:00:11\\nip link set p2p2 vf 2 mac 00:11:22:33:00:21\\nip link set p2p2 vf 3 mac 00:11:22:33:00:31\\ndpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3\\n```\\n\\n2. Review the configuration:\\n\\n```shell\\ndpdk-devbind.py -s\\n\\nNetwork devices using DPDK-compatible driver\\n============================================\\n0000:51:01.0 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:01.1 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:01.2 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:01.3 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:01.0 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:11.1 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:21.2 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n0000:51:31.3 \'Ethernet Adaptive Virtual Function 1889\' drv=vfio-pci unused=iavf,igb_uio\\n\\nNetwork devices using kernel driver\\n===================================\\n0000:19:00.0 \'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751\' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*\\n0000:19:00.1 \'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751\' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci\\n0000:19:00.2 \'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751\' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci\\n0000:19:00.3 \'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751\' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci\\n0000:51:00.0 \'Ethernet Controller E810-C for QSFP 1592\' if=eth13 drv=ice unused=igb_uio,vfio-pci\\n0000:51:00.1 \'Ethernet Controller E810-C for QSFP 1592\' if=rename8 drv=ice unused=igb_uio,vfio-pci\\n\\nBaseband devices using DPDK-compatible driver\\n=============================================\\n0000:8a:00.0 \'Device 0d5c\' drv=igb_uio unused=vfio-pci\\n0000:8b:00.0 \'Device 0d5d\' drv=vfio-pci unused=igb_uio\\n\\nOther Baseband devices\\n======================\\n0000:8b:00.1 \'Device 0d5d\' unused=igb_uio,vfio-pci\\n\\n```\\n\\n### SRIOV Configuration\\n\\nThe Single Root I/O Virtualization `SR-IOV` is a specification that allows a PCIe device to appear to be multiple separate physical PCIe devices.\\nThe SR-IOV network device plugin is Kubernetes device plugin for discovering and advertising networking resources such as:\\n\\n- SR-IOV virtual functions `VFs`\\n- PCI physical functions `PFs`\\n- Auxiliary network devices, in particular Subfunctions `SFs`\\n\\nTo deploy workloads with SR-IOV VF, Auxiliary network devices or PCI PF, this plugin needs to work together with the following two CNI components:\\n\\n- Any CNI meta plugin supporting Device Plugin based network provisioning. In our case will be `Multus`\\n- A CNI capable of consuming the network device allocated to the Pod\\n\\n> For more information about the SR-IOV CNI, please refer to the [documentation](https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin/tree/master)\\n\\n1. Prepare the config map for the device plugin:\\n\\nYou could get the information to fill the config map from the `lspci` command. In our case we will have the next 3 types of devices:\\n- FEC acceleartor card VF: `0d5d`. \\nThis is the first VF created on the ACC100 card and should match with the first VF created on the previous section.\\n\\n- ODU interface: `1889`, `8086` and `p2p1` as a filter. \\nThis is the first port of the E810 interface and should match with the 4 first VFs created on the previous section.\\n\\n- ORU interface: `1889`, `8086` and `p2p2` as a filter.\\nThis is the second port of the E810 interface and should match with the 4 last VFs created on the previous section.\\n\\n\\n\\n```yaml\\ncat <<EOF | k apply -f -\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: sriovdp-config\\n  namespace: kube-system\\ndata:\\n  config.json: |\\n    {\\n        \\"resourceList\\": [\\n            {\\n                \\"resourceName\\": \\"intel_fec_5g\\",\\n                \\"devicetype\\": \\"accelerator\\",\\n                \\"selectors\\": {\\n                    \\"vendors\\": [\\"8086\\"],\\n                    \\"devices\\": [\\"0d5d\\"]\\n                }\\n            },\\n            {\\n                \\"resourceName\\": \\"intel_sriov_odu\\",\\n                \\"selectors\\": {\\n                    \\"vendors\\": [\\"8086\\"],\\n                    \\"devices\\": [\\"1889\\"],\\n                    \\"drivers\\": [\\"vfio-pci\\"],\\n                    \\"pfNames\\": [\\"p2p1\\"]\\n                }\\n            },\\n            {\\n                \\"resourceName\\": \\"intel_sriov_oru\\",\\n                \\"selectors\\": {\\n                    \\"vendors\\": [\\"8086\\"],\\n                    \\"devices\\": [\\"1889\\"],\\n                    \\"drivers\\": [\\"vfio-pci\\"],\\n                    \\"pfNames\\": [\\"p2p2\\"]\\n                }\\n            }\\n        ]\\n    }\\nEOF\\n``` \\n\\n2. Prepare the daemonset for the device plugin\\n\\nNo changes are needed in the daemonset, so you can use the same upstream daemonset as is for the FlexRAN deployment.\\n> For more information about the daemonset, please refer to the [documentation](https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin/blob/master/deployments/sriovdp-daemonset.yaml)\\n\\n```yaml\\ncat <<EOF | k apply -f -\\n---\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: sriov-device-plugin\\n  namespace: kube-system\\n\\n---\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: kube-sriov-device-plugin-amd64\\n  namespace: kube-system\\n  labels:\\n    tier: node\\n    app: sriovdp\\nspec:\\n  selector:\\n    matchLabels:\\n      name: sriov-device-plugin\\n  template:\\n    metadata:\\n      labels:\\n        name: sriov-device-plugin\\n        tier: node\\n        app: sriovdp\\n    spec:\\n      hostNetwork: true\\n      nodeSelector:\\n        kubernetes.io/arch: amd64\\n      tolerations:\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      serviceAccountName: sriov-device-plugin\\n      containers:\\n      - name: kube-sriovdp\\n        image: ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin:latest-amd64\\n        imagePullPolicy: IfNotPresent\\n        args:\\n        - --log-dir=sriovdp\\n        - --log-level=10\\n        securityContext:\\n          privileged: true\\n        resources:\\n          requests:\\n            cpu: \\"250m\\"\\n            memory: \\"40Mi\\"\\n          limits:\\n            cpu: 1\\n            memory: \\"200Mi\\"\\n        volumeMounts:\\n        - name: devicesock\\n          mountPath: /var/lib/kubelet/\\n          readOnly: false\\n        - name: log\\n          mountPath: /var/log\\n        - name: config-volume\\n          mountPath: /etc/pcidp\\n        - name: device-info\\n          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp\\n      volumes:\\n        - name: devicesock\\n          hostPath:\\n            path: /var/lib/kubelet/\\n        - name: log\\n          hostPath:\\n            path: /var/log\\n        - name: device-info\\n          hostPath:\\n            path: /var/run/k8s.cni.cncf.io/devinfo/dp\\n            type: DirectoryOrCreate\\n        - name: config-volume\\n          configMap:\\n            name: sriovdp-config\\n            items:\\n            - key: config.json\\n              path: config.json\\nEOF\\n```\\n\\nAfter deploying the daemonset on the RKE2 edge cluster, you should see the pods running:\\n\\n```shell\\n$ kubectl get pods -n kube-system | grep sriov\\nkube-system       kube-sriov-device-plugin-amd64-twjfl                    1/1     Running   0          2m\\n```\\n\\n3. Check the interfaces discovered and available in the node for the FlexRan workload:\\n\\n```shell\\n$ kubectl get $(kubectl get nodes -oname) -o jsonpath=\'{.status.allocatable}\' | jq\\n{\\n  \\"cpu\\": \\"64\\",\\n  \\"ephemeral-storage\\": \\"256196109726\\",\\n  \\"hugepages-1Gi\\": \\"40Gi\\",\\n  \\"hugepages-2Mi\\": \\"0\\",\\n  \\"intel.com/intel_fec_5g\\": \\"1\\",\\n  \\"intel.com/intel_sriov_odu\\": \\"4\\",\\n  \\"intel.com/intel_sriov_oru\\": \\"4\\",\\n  \\"memory\\": \\"221396384Ki\\",\\n  \\"pods\\": \\"110\\"\\n}\\n``` \\n\\nAs you can see in the output above, we have 2 types of resources available for the FlexRan workload:\\n\\n- The FEC will be `intel.com/intel_fec_5g` and the value will be 1 because we bind just only 1 of 2.\\n\\n- The VFs will be `intel.com/intel_sriov_odu` or `intel.com/intel_sriov_oru` and the value will be 4 because we bind 4 VFs for each PF. \\n\\nBasically, FlexRan will request some resources available in the host to be used as a VF for the tests we\'re going to run.\\n\\n> Important Note: If you don\'t get the resources available here, does not make sense continue with the flexran demo tests. Please, review the previous steps to ensure you have the VFs created and the SRIOV CNI plugin working properly.\\n\\n\\n\\n\\n\\n## FlexRan tests\\n\\n### References\\n\\nFor this article, we will use the next references to deploy the Intel FlexRan reference implementation on top of the ATIP edge cluster:\\n\\n- We will use the pre-defined containers from Intel: [FlexRan pre-defined containers](https://hub.docker.com/r/intel/flexran_vdu)\\n- You will also need to download the FlexRan-22.07 tarball from Intel to run the tests and mount the downloaded `tests` folder into the pre-defined containers because it\'s not included into the pre-defined containers. \\n> In order to download those files, it is required to have access to the Intel website (you can ask your Intel representative). \\n  - FlexRAN-22.07-L1.tar.gz_part00\\n  - FlexRAN-22.07-L1.tar.gz_part01\\n- Container Entrypoint script (Just in case you need to change anything else like the MAC addresses for the VFs)\\n\\n### Prepare the files downloaded from Intel\\n\\nOnce you have the tarball files downloaded from Intel, you can join the files and extract the content:\\n\\n```shell\\nmkdir flexran; \\ncp FlexRAN-22.07-L1.tar.gz_part* flexran; \\ncd flexran\\ncat FlexRAN-22.07-L1.tar.gz_part* | tar -xzvf -\\n```\\n\\nNow, we need to execute the `extract.sh` script to get the `tests` folder available to be mounted into the containers. Also, we will copy the docker entrypoint script to be modified if needed.\\n\\n> During this process manual intervention is required to accept the license agreement\\n\\n```shell\\n./extract.sh\\nmkdir /home/tmp_flexran\\ncp -R tests/ /home/tmp_flexran/\\ncp  build/docker/docker_entry.sh /home/tmp_flexran/\\n```\\n\\nThe `/home/tmp_flexran will be the folder to be mounted into the containers on the next section.\\n\\nBefore deploying the FlexRan containers, let\'s review the next steps:\\n\\n- Interfaces to be used in the flexran pods yaml files, should be referenced by their resource names. You could get those as: `kubectl get nodes -o json | jq \'.items[].status.allocatable\'` as explained in the previous [section](#sriov-configuration)\\n- The container entrypoint script contains the tests files customization for this specific environment. It will modify some parameters such as MAC addresses, `VF` information and the `dpdk` info into the `tests`. It will be explained in the next section.\\n- Tests should be mounted in `/home/tmp_flexran/tests` and exposed in `/home/flexran/tests`\\n\\n### Container entrypoint\\n\\nThe FlexRan containers run a script when the container is started.\\nThe script will modify the configuration files of the FlexRan tests applications to adapt the tests to our environment.\\n\\nThe modifications done in this file will be:\\n\\n- `PCIDEVICE_INTEL_COM_INTEL_FEC_5G=$(env|grep PCIDEVICE_INTEL_COM_INTEL_FEC_5G= |awk -F \'=\' \'{print $2}\')`  in order to select the right PCI device for the FEC ACC100 card used for the 5G acceleration.\\n\\n- `export INTEL_COM_INTEL_CPULIST=$(cat /sys/fs/cgroup/cpuset/cpuset.cpus)` to get the CPU list of the host machine.\\n\\nAlso, we need to change the CPU cores as well as the MAC addresses into the RU section because there isn\'t any substitution in the entrypoint script. We need to change the next lines:\\n\\n```shell\\n    sed -i \\"s/ioCore=2/ioCore=62/g\\" config_file_o_ru.dat\\n    sed -i \\"s/duMac0=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac0=00:11:22:33:00:00/g\\" config_file_o_ru.dat\\n    sed -i \\"s/duMac1=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac1=00:11:22:33:00:10/g\\" config_file_o_ru.dat\\n    sed -i \\"s/ruMac0=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac0=00:11:22:33:00:01/g\\" config_file_o_ru.dat\\n    sed -i \\"s/ruMac1=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac1=00:11:22:33:00:11/g\\" config_file_o_ru.dat\\n    \\n    sed -i \\"s/duMac2=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac2=00:11:22:33:00:20/g\\" config_file_o_ru.dat\\n    sed -i \\"s/duMac3=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac3=00:11:22:33:00:30/g\\" config_file_o_ru.dat\\n    sed -i \\"s/ruMac2=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac2=00:11:22:33:00:21/g\\" config_file_o_ru.dat\\n    sed -i \\"s/ruMac3=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac3=00:11:22:33:00:31/g\\" config_file_o_ru.dat\\n```\\nThen, we need to remove (or comment with the hashtag `#` sign) the tests not covered by the flexran container version we are using (the test files are not present in the FlexRan tests suite used so the script will fail):\\n\\n- flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub3_mu1_20mhz_4x4/gnb/\\n- flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub6_mu1_100mhz_4x4/gnb/\\n- flexran/bin/nr5g/gnb/l1/orancfg/mmwave_mu3_100mhz_2x2/gnb/\\n- flexran/bin/nr5g/gnb/l1/orancfg/mmwave_mu3_100mhz_2x2/gnb/\\n- flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub3_mu1_20mhz_4x4/oru/\\n- flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub6_mu1_100mhz_4x4/oru/\\n- flexran/bin/nr5g/gnb/l1/orancfg/mmwave_mu3_100mhz_2x2/oru/\\n\\nFor instance, you will have the next block commented (or removed):\\n\\n```shell\\n#cd /home/flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub3_mu1_20mhz_4x4/gnb/\\n#sed -i \\"s/<systemThread>2, 0, 0<\\\\/systemThread>/<systemThread>$systemthread, 0, 0<\\\\/systemThread>/g\\" phycfg_xran.xml\\n#sed -i \\"s/<timerThread>0, 96, 0<\\\\/timerThread>/<timerThread>$timerThread, 96, 0<\\\\/timerThread>/g\\" phycfg_xran.xml\\n#sed -i \\"s/<FpgaDriverCpuInfo>3, 96, 0<\\\\/FpgaDriverCpuInfo>/<FpgaDriverCpuInfo>$FpgaDriverCpuInfo, 96, 0<\\\\/FpgaDriverCpuInfo>/g\\" phycfg_xran.xml\\n#sed -i \\"s/<FrontHaulCpuInfo>3, 96, 0<\\\\/FrontHaulCpuInfo>/<FrontHaulCpuInfo>$FrontHaulCpuInfo, 96, 0<\\\\/FrontHaulCpuInfo>/g\\" phycfg_xran.xml\\n#sed -i \\"s/<radioDpdkMaster>2, 99, 0<\\\\/radioDpdkMaster>/<radioDpdkMaster>$radioDpdkMaster, 99, 0<\\\\/radioDpdkMaster>/g\\" phycfg_xran.xml\\n```\\n\\nAn example of docker entrypoint script is available [here](./2023-07-29-flexran-images/flexran-dockerentrypoint.sh)\\n\\n### FlexRan Timer Mode\\n\\nFlexRAN Timer Mode use case does not use fronthaul. RF IQ samples are read from files and write to files. The test pod is configured with two containers. One container is for `L1APP`, and another one is for `TestMAC`. \\nSmaller storage is built in pod as Kubernetes resources, but the storage is not enough for the test case. The test case requires 20GB storage for tests files, so it will be mounted from the host using the `tests` volume mount path.\\nThe test case is a setup for one peak cell and two average cells. 8 cores with HT enabled are pinned for `L1APP`, the average core utilization is between 45-65%. The below figure shows the threading model for this test case.\\n\\n![img.png](2023-07-29-flexran-images/timer3.png)\\n\\nTo deploy the Test Timer Mode, we will use the next yaml file:\\n\\n```yaml\\ncat <<EOF | kubectl apply -f - \\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  labels:\\n    app: flexran-dockerimage_release\\n  name: flexran-dockerimage-release\\nspec:\\n  containers:\\n  - securityContext:\\n      privileged: false\\n      capabilities:\\n        add:\\n          - IPC_LOCK\\n          - SYS_NICE\\n    command: [ \\"/bin/bash\\", \\"-c\\", \\"--\\" ]\\n    args: [\\" cp flexran/build/docker/docker_entry.sh .; sh docker_entry.sh -m timer ; cd /home/flexran/bin/nr5g/gnb/l1/; ./l1.sh -e ; top\\"]\\n    tty: true\\n    stdin: true\\n    env:\\n    - name: LD_LIBRARY_PATH\\n      value: /opt/oneapi/lib/intel64\\n    image: intel/flexran_vdu:v22.07\\n    name: flexran-l1app\\n    resources:\\n      requests:\\n        memory: \\"32Gi\\"\\n        intel.com/intel_fec_5g: \'1\'\\n        hugepages-1Gi: 16Gi\\n      limits:\\n        memory: \\"32Gi\\"\\n        intel.com/intel_fec_5g: \'1\'\\n        hugepages-1Gi: 16Gi\\n    volumeMounts:\\n    - name: hugepage\\n      mountPath: /hugepages\\n    - name: varrun\\n      mountPath: /var/run/dpdk\\n      readOnly: false\\n    - name: tests\\n      mountPath: /home/flexran/tests\\n      readOnly: false\\n    - name: proc\\n      mountPath: /proc\\n      readOnly: false\\n  - securityContext:\\n      privileged: false\\n      capabilities:\\n        add:\\n          - IPC_LOCK\\n          - SYS_NICE\\n    command: [ \\"/bin/bash\\", \\"-c\\", \\"--\\" ]\\n    args: [\\"sleep 10; sh docker_entry.sh -m timer ; cd /home/flexran/bin/nr5g/gnb/testmac/; ./l2.sh --testfile=icelake-sp/icxsp.cfg; top\\"]\\n    tty: true\\n    stdin: true\\n    env:\\n    - name: LD_LIBRARY_PATH\\n      value: /opt/oneapi/lib/intel64\\n    image: intel/flexran_vdu:v22.07\\n    name: flexran-testmac\\n    resources:\\n      requests:\\n        memory: \\"12Gi\\"\\n        hugepages-1Gi: 8Gi\\n      limits:\\n        memory: \\"12Gi\\"\\n        hugepages-1Gi: 8Gi\\n    volumeMounts:\\n    - name: hugepage\\n      mountPath: /hugepages\\n    - name: varrun\\n      mountPath: /var/run/dpdk\\n      readOnly: false\\n    - name: tests\\n      mountPath: /home/flexran/tests\\n      readOnly: false\\n    - name: proc\\n      mountPath: /proc\\n      readOnly: false  \\n  volumes:\\n  - name: hugepage\\n    emptyDir:\\n      medium: HugePages\\n  - name: varrun\\n    emptyDir: {}\\n  - name: tests\\n    hostPath:\\n      path: \\"/home/tmp_flexran/tests\\"\\n  - name: proc\\n    hostPath:\\n      path: /proc    \\nEOF\\n```\\n\\nThe next resources are required:\\n\\n- `intel.com/intel_fec_5g: \'1\'` to request the FEC ACC100 resource.\\n- `hugepages-1Gi: 16Gi` to request the hugepages resource for the first container.\\n- `hugepages-1Gi: 8Gi` to request the hugepages resource for the second container.\\n- `/home/tmp_flexran/tests` folder mounted to `/home/flexran/tests` in order to have the tests available for the containers.\\n- `image: intel/flexran_vdu:v22.07` to use the pre-defined container from Intel pre-defined containers.\\n\\nOnce the pod is deployed, you can check the status of the pod:\\n\\n```shell\\nkubectl get pods\\nNAME                        READY   STATUS    RESTARTS   AGE\\nflexran-dockerimage-release   2/2     Running   0          2m\\n```\\n\\nThis will launch automatically the Timer Mode Tests which contains 83 tests for the FlexRan library. It could take up to 3 hours to complete the tests. The status of the tests can be found in the container logs:\\n\\n```shell\\nkubectl logs flexran-dockerimage-release flexran-l1app\\n```\\n\\nAfter a successful tests execution the following output should be shown:\\n\\n```shell\\nAll Tests Completed, Total run 83 Tests, PASS 83 Tests, and FAIL 0 Tests\\n----------------------------------------------------------------------------\\nmem_mgr_display_size:\\n    Num Memory Alloc:               11\\n    Total Memory Size:         264,018\\n----------------------------------------------------------------------------\\n```\\n\\nUsing `htop` we could see the CPU usage of the containers as well as the pinned cores for this workload:\\n\\n![img_1.png](2023-07-29-flexran-images/clitimer.png)\\n\\nUsing the Rancher UI, you can check the status of the pods and the logs of the containers.\\nAlso, you could get the next metrics using prometheus and grafana to check the CPU usage , CPU cores isolated used, and the Memory usage of the containers:\\n\\n![img_6.png](2023-07-29-flexran-images/graftimer.png)\\n\\n\\n### FlexRan Xran Mode\\n\\nFlexRAN XRAN Mode uses two Intel\xae Ethernet 100G 2P E810 Adapter on the two PCIe x16 slots. The two NIC cards are connected by fiber directly for fronthaul eCPRI C-Plane and U-Plane traffic. \\nSR-IOV is enabled on the two E810 NIC cards. ACC100 accelerator cards and RF IQ samples over Ethernet VFs use SR-IOV resources. The O-DU test pod is configured with two containers. One container is for `L1APP`, and another one is for `TestMAC`. The O-RU test pod is configured with one container. \\nSmaller storage is built in pod as Kubernetes resources, but the larger storage is mapped from the host.\\nThe test case is a setup for six average cells with O-RAN lower layer split option 7-2x. 2 cores with HT enabled are pinned for `L1APP`, the average core utilization is 65%. 2 cores are pinned for O-RU fronthaul, 2 cores are pinned for L1 fronthaul. The below figure shows the threading model for this test case.\\n\\n![img.png](2023-07-29-flexran-images/xran1.png)\\n\\nTo deploy the XRAN Mode, we will use the next yaml file:\\n\\n```yaml\\ncat <<EOF | kubectl apply -f -\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app: flexran-vdu\\n  name: flexran-vdu\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: flexran-vdu\\n  template:\\n    metadata:\\n      labels:\\n        app: flexran-vdu\\n    spec:\\n      containers:\\n      - securityContext:\\n          privileged: true\\n          capabilities:\\n            add:\\n              - IPC_LOCK\\n              - SYS_NICE\\n        command: [ \\"/bin/bash\\", \\"-c\\", \\"--\\" ]\\n        args: [ \\"cp flexran/build/docker/docker_entry.sh . ; sh docker_entry.sh -m xran ; top\\"]\\n        tty: true\\n        stdin: true\\n        env:\\n         - name: LD_LIBRARY_PATH\\n           value: /opt/oneapi/lib/intel64\\n        image: intel/flexran_vdu:v22.07\\n        name: flexran-vdu\\n        resources:\\n          requests:\\n            memory: \\"24Gi\\"\\n            hugepages-1Gi: 20Gi\\n            intel.com/intel_fec_5g: \'1\'\\n            intel.com/intel_sriov_odu: \'4\'\\n          limits:\\n            memory: \\"24Gi\\"\\n            hugepages-1Gi: 20Gi\\n            intel.com/intel_fec_5g: \'1\'\\n            intel.com/intel_sriov_odu: \'4\'\\n        volumeMounts:\\n        - name: hugepage\\n          mountPath: /hugepages\\n        - name: varrun\\n          mountPath: /var/run/dpdk\\n          readOnly: false\\n        - name: tests\\n          mountPath: /home/flexran/tests\\n          readOnly: false\\n        - name: proc\\n          mountPath: /proc\\n          readOnly: false\\n      volumes:\\n      - name: hugepage\\n        emptyDir:\\n          medium: HugePages\\n      - name: varrun\\n        emptyDir: {}\\n      - name: tests\\n        hostPath:\\n          path: \\"/home/tmp_flexran/tests\\"\\n      - name: proc\\n        hostPath:\\n          path: /proc\\n---\\n\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app: flexran-vru\\n  name: flexran-vru\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: flexran-vru\\n  template:\\n    metadata:\\n      labels:\\n        app: flexran-vru\\n    spec:\\n      containers:\\n      - securityContext:\\n          privileged: true\\n          capabilities:\\n            add:\\n              - IPC_LOCK\\n              - SYS_NICE\\n        command: [ \\"/bin/bash\\", \\"-c\\", \\"--\\" ]\\n        args: [\\"cp flexran/build/docker/docker_entry.sh . ; sh docker_entry.sh -m xran ; top\\"]\\n        tty: true\\n        stdin: true\\n        env:\\n         - name: LD_LIBRARY_PATH\\n           value: /opt/oneapi/lib/intel64\\n        image: intel/flexran_vdu:v22.07\\n        name: flexran-oru\\n        resources:\\n          requests:\\n            memory: \\"24Gi\\"\\n            hugepages-1Gi: 16Gi\\n            intel.com/intel_sriov_oru: \'4\'\\n          limits:\\n            memory: \\"24Gi\\"\\n            hugepages-1Gi: 16Gi\\n            intel.com/intel_sriov_oru: \'4\'\\n        volumeMounts:\\n        - name: hugepage\\n          mountPath: /hugepages\\n        - name: varrun\\n          mountPath: /var/run/dpdk\\n          readOnly: false\\n        - name: tests\\n          mountPath: /home/flexran/tests\\n          readOnly: false\\n        - name: proc\\n          mountPath: /proc\\n          readOnly: false\\n      volumes:\\n      - name: hugepage\\n        emptyDir:\\n          medium: HugePages\\n      - name: varrun\\n        emptyDir: {}\\n      - name: tests\\n        hostPath:\\n          path: \\"/home/tmp_flexran/tests\\"\\n      - name: proc\\n        hostPath:\\n          path: /proc\\n\\nEOF  \\n```\\n\\nThe next resources are required:\\n\\n- `intel.com/intel_fec_5g: \'1\'` to request the FEC ACC100 resource.\\n- `intel.com/intel_sriov_odu: \'4\'` to request the 4 VFs for the ODU interface.\\n- `intel.com/intel_sriov_oru: \'4\'` to request the 4 VFs for the ORU interface.\\n- `hugepages-1Gi: 20Gi` to request the hugepages resource for the first container.\\n- `hugepages-1Gi: 16Gi` to request the hugepages resource for the second container.\\n- `/home/tmp_flexran/tests` folder mounted to `/home/flexran/tests` in order to have the tests available for the containers.\\n\\nOnce the pods are deployed, you need to launch the tests manually. You can do that using the next commands:\\n\\n1. Open a terminal in the `flexran-vdu` pod and run the following commands:\\n\\n```shell\\nkubectl exec -ti flexran-vdu-66c9bf5765-ftptg /bin/bash\\n\\ncd flexran/bin/nr5g/gnb/l1/orancfg/sub6_mu1_100mhz_4x4/gnb/\\n./l1.sh -oru\\n```\\n\\n2. Open a new terminal in the `flexran-vdu` pod and run the following commands:\\n\\n```shell\\nkubectl exec -ti flexran-vdu-66c9bf5765-ftptg /bin/bash\\n\\ncd flexran/bin/nr5g/gnb/testmac\\n./l2.sh --testfile=../l1/orancfg/sub6_mu1_100mhz_4x4/gnb/testmac_clxsp_mu1_100mhz_hton_oru.cfg\\n```\\n\\n\\n3. Open a third terminal to run the `flexran-vru` pod and run the following commands:\\n\\n```shell\\nkubectl exec -ti flexran-vru-66casd2e5765-23resd /bin/bash\\n\\ncd flexran/bin/nr5g/gnb/l1/orancfg/sub6_mu1_100mhz_4x4/oru/\\nchmod +x run_o_ru.sh; taskset -c 20 ./run_o_ru.sh\\n```\\n\\nUsing `htop` we could see the CPU usage of the containers as well as the pinned cores for this workload:\\n\\n![img_2.png](2023-07-29-flexran-images/xran2.png)\\n\\nUsing the Rancher UI, you can check the status of the pods and the logs of the containers.\\nAlso, you could get the next metrics using prometheus and grafana to check the CPU usage , CPU cores isolated used, and the Memory usage of the containers:\\n\\n![img_3.png](2023-07-29-flexran-images/xran3.png)\\n\\n\\n## Conclusion\\n\\nBuilding, testing, and deploying a properly configured Intel\xae FlexRAN implementation can show the benefits of VNFs and vRAN with Intel\xae Xeon\xae Scalable Processors and Intel\xae Advanced Vector Extensions.\\nSUSE provides all the components for an open-source, enterprise-grade, software-defined stack for cloud-native orchestration and management. SUSE Linux Enterprise Micro Real Time, Rancher Kubernetes Engine v2 (RKE2) and Rancher Management were used and illustrated as key ingredients to simplify the deployment of Intel\xae FlexRAN."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/2023-05-28-welcome-blog.md","title":"Welcome to SUSE Edge","description":"Thank you for visiting our SUSE Engineering Page.","date":"2023-05-28T00:00:00.000Z","formattedDate":"May 28, 2023","tags":[{"label":"edge","permalink":"/blog/tags/edge"},{"label":"team","permalink":"/blog/tags/team"},{"label":"announcement","permalink":"/blog/tags/announcement"}],"readingTime":0.095,"hasTruncateMarker":false,"authors":[{"name":"Jimmy Martin","title":"Field Product Manager","url":"https://github.com/jimmyrmartin","image_url":"https://github.com/jimmyrmartin.png","imageURL":"https://github.com/jimmyrmartin.png"}],"frontMatter":{"slug":"welcome","title":"Welcome to SUSE Edge","authors":{"name":"Jimmy Martin","title":"Field Product Manager","url":"https://github.com/jimmyrmartin","image_url":"https://github.com/jimmyrmartin.png","imageURL":"https://github.com/jimmyrmartin.png"},"tags":["edge","team","announcement"]},"unlisted":false,"prevItem":{"title":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","permalink":"/blog/Flexran"}},"content":"Thank you for visiting our SUSE Engineering Page.\\n\\nThere are many things we aim to accomplish with this website."}]}')}}]);