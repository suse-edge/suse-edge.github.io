<?xml version="1.0"?>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude">
<info>
<title>SUSE Edge Documentation</title>
<date>2025-09-26</date>
<!-- https://tdg.docbook.org/tdg/5.2/info -->

<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="suse-edge-documentation">
<title>SUSE Edge 3.4 Documentation</title>
<para>Welcome to the SUSE Edge documentation. You will find the high level architectural overview, quick start guides, validated designs, guidance on using components, third-party integrations, and best practices for managing your edge computing infrastructure and workloads.</para>
<section xml:id="id-what-is-suse-edge">
<title>What is SUSE Edge?</title>
<para>SUSE Edge is a purpose-built, tightly integrated, and comprehensively validated end-to-end solution for addressing the unique challenges of the deployment of infrastructure and cloud-native applications at the edge. Its driving focus is to provide an opinionated, yet highly flexible, highly scalable, and secure platform that spans initial deployment image building, node provisioning and onboarding, application deployment, observability, and complete lifecycle operations. The platform is built on best-of-breed open source software from the ground up, consistent with both our 30-year+ history in delivering secure, stable, and certified SUSE Linux platforms and our experience in providing highly scalable and feature-rich Kubernetes management with our Rancher portfolio. SUSE Edge builds on-top of these capabilities to deliver functionality that can address a wide number of market segments, including retail, medical, transportation, logistics, telecommunications, smart manufacturing, and Industrial IoT.</para>
</section>
<section xml:id="id-design-philosophy">
<title>Design Philosophy</title>
<para>The solution is designed with the notion that there is no "one-size-fits-all" edge platform due to customers’ widely varying requirements and expectations. Edge deployments push us to solve, and continually evolve, some of the most challenging problems, including massive scalability, restricted network availability, physical space constraints, new security threats and attack vectors, variations in hardware architecture and system resources, the requirement to deploy and interface with legacy infrastructure and applications, and customer solutions that have extended lifespans. Since many of these challenges are different from traditional ways of thinking, e.g. deployment of infrastructure and applications within data centers or in the public cloud, we have to look into the design in much more granular detail, and rethinking many common assumptions.</para>
<para>For example, we find value in minimalism, modularity, and ease of operations. Minimalism is important for edge environments since the more complex a system is, the more likely it is to break. When looking at hundreds of locations, up to hundreds of thousands, complex systems will break in complex ways. Modularity in our solution allows for more user choice while removing unneeded complexity in the deployed platform. We also need to balance these with the ease of operations. Humans may make mistakes when repeating a process thousands of times, so the platform should make sure any potential mistakes are recoverable, eliminating the need for on-site technician visits, but also strive for consistency and standardization.</para>
</section>
<section xml:id="id-high-level-architecture">
<title>High Level Architecture</title>
<para>The high level system architecture of SUSE Edge is broken into two core categories, namely "management" and "downstream" clusters. The management cluster is responsible for remote management of one or more downstream clusters, although it’s recognized that in certain circumstances, downstream clusters need to operate without remote management, e.g. in situations where an edge site has no external connectivity and needs to operate independently. In SUSE Edge, the technical components that are utilized for the operation of both the management and downstream clusters are largely common, although likely differentiate in both the system specifications and the applications that reside on-top, i.e. the management cluster would run applications that enable systems management and lifecycle operations, whereas the downstream clusters fulfil the requirements for serving user applications.</para>
<section xml:id="id-components-used-in-suse-edge">
<title>Components used in SUSE Edge</title>
<para>SUSE Edge is comprised of both existing SUSE and Rancher components along with additional features and components built by the Edge team to enable us to address the constraints and intricacies required in edge computing. The components used within both the management and downstream clusters are explained below, with a simplified high-level architecture diagram, noting that this isn’t an exhaustive list:</para>
<section xml:id="id-management-cluster">
<title>Management Cluster</title>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suse-edge-management-cluster.svg" width="100%"/>
</imageobject>
<textobject><phrase>suse edge management cluster</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management</emphasis>: This is the centralized part of SUSE Edge that is used to manage the provisioning and lifecycle of connected downstream clusters. The management cluster typically includes the following components:</para>
<itemizedlist>
<listitem>
<para>Multi-cluster management with Rancher Prime (<xref linkend="components-rancher"/>), enabling a common dashboard for downstream cluster onboarding and ongoing lifecycle management of infrastructure and applications, also providing comprehensive tenant isolation and <literal>IDP</literal> (Identity Provider) integrations, a large marketplace of third-party integrations and extensions, and a vendor-neutral API.</para>
</listitem>
<listitem>
<para>Linux systems management with SUSE Multi-Linux Manager, enabling automated Linux patch and configuration management of the underlying Linux operating system (*SUSE Linux Micro (<xref linkend="components-slmicro"/>)) that runs on the downstream clusters. Note that while this component is containerized, it currently needs to run on a separate system to the rest of the management components, hence labelled as "Linux Management" in the diagram above.</para>
</listitem>
<listitem>
<para>A dedicated Lifecycle Management (<xref linkend="components-upgrade-controller"/>) controller that handles management cluster component upgrades to a given SUSE Edge release.</para>
</listitem>
<listitem>
<para>Remote system on-boarding into Rancher Prime with Elemental (<xref linkend="components-elemental"/>), enabling late binding of connected edge nodes to desired Kubernetes clusters and application deployment, e.g. via GitOps.</para>
</listitem>
<listitem>
<para>An Optional full bare-metal lifecycle and management support with Metal3 (<xref linkend="components-metal3"/>), MetalLB (<xref linkend="components-metallb"/>), and <literal>CAPI</literal> (Cluster API) infrastructure providers, enabling the full end-to-end provisioning of baremetal systems that have remote management capabilities.</para>
</listitem>
<listitem>
<para>An optional GitOps engine called Fleet (<xref linkend="components-fleet"/>) for managing the provisioning and lifecycle of downstream clusters and applications that reside on them.</para>
</listitem>
<listitem>
<para>Underpinning the management cluster itself is SUSE Linux Micro (<xref linkend="components-slmicro"/>) as the base operating system and RKE2 (<xref linkend="components-rke2"/>) as the Kubernetes distribution supporting the management cluster applications.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-downstream-clusters">
<title>Downstream Clusters</title>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suse-edge-downstream-cluster.svg" width="100%"/>
</imageobject>
<textobject><phrase>suse edge downstream cluster</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Downstream</emphasis>: This is the distributed part of SUSE Edge that is used to run the user workloads at the Edge, i.e. the software that is running at the edge location itself, and is typically comprised of the following components:</para>
<itemizedlist>
<listitem>
<para>A choice of Kubernetes distributions, with secure and lightweight distributions like K3s (<xref linkend="components-k3s"/>) and RKE2 (<xref linkend="components-rke2"/>) (<literal>RKE2</literal> is hardened, certified and optimized for usage in government and regulated industries).</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>) to enable security features like image vulnerability scanning, deep packet inspection, and real-time threat and vulnerability protection.</para>
</listitem>
<listitem>
<para>Software block storage with SUSE Storage (<xref linkend="components-suse-storage"/>) to enable lightweight persistent, resilient, and scalable block-storage.</para>
</listitem>
<listitem>
<para>A lightweight, container-optimized, hardened Linux operating system with SUSE Linux Micro (<xref linkend="components-slmicro"/>), providing an immutable and highly resilient OS for running containers and virtual machines at the edge. SUSE Linux Micro is available for both AArch64 and AMD64/Intel 64 architectures, and it also supports <literal>Real-Time Kernel</literal> for latency sensitive applications (e.g. telco use-cases).</para>
</listitem>
<listitem>
<para>For connected clusters (i.e. those that do have connectivity to the management cluster) two agents are deployed, namely Rancher System Agent for managing the connectivity to Rancher Prime, and venv-salt-minion for taking instructions from SUSE Multi-Linux Manager for applying Linux software updates. These agents are not required for management of disconnected clusters.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-connectivity">
<title>Connectivity</title>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suse-edge-connected-architecture.svg" width="100%"/>
</imageobject>
<textobject><phrase>suse edge connected architecture</phrase></textobject>
</mediaobject>
</informalfigure>
<para>The above image provides a high-level architectural overview for <emphasis role="strong">connected</emphasis> downstream clusters and their attachment to the management cluster. The management cluster can be deployed on a wide variety of underlying infrastructure platforms, in both on-premises and cloud capacities, depending on networking availability between the downstream clusters and the target management cluster. The only requirement for this to function are API and callback URL’s to be accessible over the network that connects downstream cluster nodes to the management infrastructure.</para>
<para>It’s important to recognize that there are distinct mechanisms in which this connectivity is established relative to the mechanism of downstream cluster deployment. The details of this are explained in much more depth in the next section, but to set a baseline understanding, there are three primary mechanisms for connected downstream clusters to be established as a "managed" cluster:</para>
<orderedlist numeration="arabic">
<listitem>
<para>The downstream clusters are deployed in a "disconnected" capacity at first (e.g. via Edge Image Builder (<xref linkend="components-eib"/>)), and are then imported into the management cluster if/when connectivity allows.</para>
</listitem>
<listitem>
<para>The downstream clusters are configured to use the built-in onboarding mechanism (e.g. via Elemental (<xref linkend="components-elemental"/>)), and they automatically register into the management cluster at first-boot, allowing for late-binding of the cluster configuration.</para>
</listitem>
<listitem>
<para>The downstream clusters have been provisioned with the baremetal management capabilities (CAPI + Metal<superscript>3</superscript>), and they’re automatically imported into the management cluster once the cluster has been deployed and configured (via the Rancher Turtles operator).</para>
</listitem>
</orderedlist>
<note>
<para>It’s recommended that multiple management clusters are implemented to accommodate the scale of large deployments, optimize for bandwidth and latency concerns in geographically dispersed environments, and to minimize the disruption in the event of an outage or management cluster upgrade. You can find the current management cluster scalability limits and system requirements <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements">here</link>.</para>
</note>
</section>
</section>
<section xml:id="id-common-edge-deployment-patterns">
<title>Common Edge Deployment Patterns</title>
<para>Due to the varying set of operating environments and lifecycle requirements, we’ve implemented support for a number of distinct deployment patterns that loosely align to the market segments and use-cases that SUSE Edge operates in. We have documented a quickstart guide for each of these deployment patterns to help you get familiar with the SUSE Edge platform based around your needs. The three deployment patterns that we support today are described below, with a link to the respective quickstart page.</para>
<section xml:id="id-directed-network-provisioning">
<title>Directed network provisioning</title>
<para>Directed network provisioning is where you know the details of the hardware you wish to deploy to and have direct access to the out-of-band management interface to orchestrate and automate the entire provisioning process. In this scenario, our  customers expect a solution to be able to provision edge sites fully automated from a centralized location, going much further than the creation of a boot image by minimizing the manual operations at the edge location; simply rack, power, and attach the required networks to the physical hardware, and the automation process powers up the machine via the out-of-band management (e.g. via the Redfish API) and handles the provisioning, onboarding, and deployment of infrastructure without user intervention. The key for this to work is that the systems are known to the administrators; they know which hardware is in which location, and that deployment is expected to be handled centrally.</para>
<para>This solution is the most robust since you are directly interacting with the hardware’s management interface, are dealing with known hardware, and have fewer constraints on network availability. Functionality wise, this solution extensively uses Cluster API and Metal<superscript>3</superscript> for automated provisioning from bare-metal, through operating system, Kubernetes, and layered applications, and provides the ability to link into the rest of the common lifecycle management capabilities of SUSE Edge post-deployment. The quickstart for this solution can be found in <xref linkend="quickstart-metal3"/>.</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>"Phone Home" network provisioning</title>
<para>Sometimes you are operating in an environment where the central management cluster cannot manage the hardware directly (for example, your remote network is behind a firewall or there is no out-of-band management interface; common in "PC" type hardware often found at the edge). In this scenario, we provide tooling to remotely provision clusters and their workloads with no need to know where hardware is being shipped when it is bootstrapped. This is what most people think of when they think about edge computing; it’s the thousands or tens of thousands of somewhat unknown systems booting up at edge locations and securely phoning home, validating who they are, and receiving their instructions on what they’re supposed to do. Our requirements here expect provisioning and lifecycle management with very little user-intervention other than either pre-imaging the machine at the factory, or simply attaching a boot image, e.g. via USB, and switching the system on. The primary challenges in this space are addressing scale, consistency, security, and lifecycle of these devices in the wild.</para>
<para>This solution provides a great deal of flexibility and consistency in the way that systems are provisioned and on-boarded, regardless of their location, system type or specification, or when they’re powered on for the first time. SUSE Edge enables full flexibility and customization of the system via Edge Image Builder, and leverages the registration capabilities Rancher’s Elemental offering for node on-boarding and Kubernetes provisioning, along with SUSE Multi-Linux Manager for operating system patching. The quick start for this solution can be found in <xref linkend="quickstart-elemental"/>.</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>Image-based provisioning</title>
<para>For customers that need to operate in standalone, air-gapped, or network limited environments, SUSE Edge provides a solution that enables customers to generate fully customized installation media that contains all of the required deployment artifacts to enable both single-node and multi-node highly-available Kubernetes clusters at the edge, including any workloads or additional layered components required, all without any network connectivity to the outside world, and without the intervention of a centralized management platform. The user-experience follows closely to the "phone home" solution in that installation media is provided to the target systems, but the solution will "bootstrap in-place". In this scenario, it’s possible to attach the resulting clusters into Rancher for ongoing management (i.e. going from a "disconnected" to "connected" mode of operation without major reconfiguration or redeployment), or can continue to operate in isolation. Note that in both cases the same consistent mechanism for automating lifecycle operations can be applied.</para>
<para>Furthermore, this solution can be used to quickly create management clusters that may host the centralized infrastructure that supports both the "directed network provisioning" and "phone home network provisioning" models as it can be the quickest and most simple way to provision all types of Edge infrastructure. This solution heavily utilizes the capabilities of SUSE Edge Image Builder to create fully customized and unattended installation media; the quickstart can be found in <xref linkend="quickstart-eib"/>.</para>
</section>
</section>
<section xml:id="id-suse-edge-stack-validation">
<title>SUSE Edge Stack Validation</title>
<para>All SUSE Edge releases comprise of tightly integrated and thoroughly validated components that are versioned as one. As part of the continuous integration and stack validation efforts that not only test the integration between components but ensure that the system performs as expected under forced failure scenarios, the SUSE Edge team publishes all of the test runs and the results to the public. The results along with all input parameters can be found at <link xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link>.</para>
</section>
<section xml:id="id-full-component-list">
<title>Full Component List</title>
<para>The full list of components, along with a link to a high-level description of each and how it’s used in SUSE Edge can be found below:</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Rancher Dashboard Extensions (<xref linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>Rancher Turtles (<xref linkend="components-rancher-turtles"/>)</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux Manager</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>SUSE Storage (<xref linkend="components-suse-storage"/>)</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref linkend="components-system-upgrade-controller"/>)</para>
</listitem>
<listitem>
<para>Upgrade Controller (<xref linkend="components-upgrade-controller"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>Quick Starts</title>
<partintro>
<para>Quick Starts here</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>BMC automated deployments with Metal<superscript>3</superscript></title>
<para>Metal<superscript>3</superscript> is a <link xl:href="https://metal3.io/">CNCF project</link> which provides bare-metal infrastructure
management capabilities for Kubernetes.</para>
<para>Metal<superscript>3</superscript> provides Kubernetes-native resources to manage the lifecycle of bare-metal servers
which support management via out-of-band protocols such as <link xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>It also has mature support for <link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link> which enables management
of infrastructure resources across multiple infrastructure providers via broadly adopted vendor-neutral APIs.</para>
<section xml:id="id-why-use-this-method">
<title>Why use this method</title>
<para>This method is useful for scenarios where the target hardware supports out-of-band management, and a fully automated
infrastructure management flow is desired.</para>
<para>A management cluster is configured to provide declarative APIs that enable inventory and state management of downstream
cluster bare-metal servers, including automated inspection, cleaning and provisioning/deprovisioning.</para>
</section>
<section xml:id="id-high-level-architecture-2">
<title>High-level architecture</title>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="quickstart-metal3-architecture.svg" width="100%"/>
</imageobject>
<textobject><phrase>quickstart metal3 architecture</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>Prerequisites</title>
<para>There are some specific constraints related to the downstream cluster server hardware and networking:</para>
<itemizedlist>
<listitem>
<para>Management cluster</para>
<itemizedlist>
<listitem>
<para>Must have network connectivity to the target server management/BMC API</para>
</listitem>
<listitem>
<para>Must have network connectivity to the target server control plane network</para>
</listitem>
<listitem>
<para>For multi-node management clusters, an additional reserved IP address is required</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Hosts to be controlled</para>
<itemizedlist>
<listitem>
<para>Must support out-of-band management via Redfish, iDRAC or iLO interfaces</para>
</listitem>
<listitem>
<para>Must support deployment via virtual media (PXE is not currently supported)</para>
</listitem>
<listitem>
<para>Must have network connectivity to the management cluster for access to the Metal<superscript>3</superscript> provisioning APIs</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Some tools are required, these can be installed either on the management cluster, or on a host which can access it.</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>, <link xl:href="https://helm.sh">Helm</link> and <link xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para>A container runtime such as <link xl:href="https://podman.io">Podman</link> or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link></para>
</listitem>
</itemizedlist>
<para>The <literal>SL-Micro.x86_64-6.1-Base-GM.raw</literal> OS image file must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE Customer Center</link> or the <link xl:href="https://www.suse.com/download/sle-micro/">SUSE Download page</link>.</para>
</section>
<section xml:id="id-deployment">
<title>Deployment</title>
<section xml:id="id-setup-management-cluster">
<title>Setup Management Cluster</title>
<para>The basic steps to install a management cluster and use Metal<superscript>3</superscript> are:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Install an RKE2 management cluster</para>
</listitem>
<listitem>
<para>Install Rancher</para>
</listitem>
<listitem>
<para>Install a storage provider (optional)</para>
</listitem>
<listitem>
<para>Install the Metal<superscript>3</superscript> dependencies</para>
</listitem>
<listitem>
<para>Install CAPI dependencies via Rancher Turtles</para>
</listitem>
<listitem>
<para>Build a SLEMicro OS image for downstream cluster hosts</para>
</listitem>
<listitem>
<para>Register BareMetalHost CRs to define the bare-metal inventory</para>
</listitem>
<listitem>
<para>Create a downstream cluster by defining CAPI resources</para>
</listitem>
</orderedlist>
<para>This guide assumes an existing RKE2 cluster and Rancher (including cert-manager) has been installed, for example by using Edge Image Builder (<xref linkend="components-eib"/>).</para>
<tip>
<para>The steps here can also be fully automated as described in the Management Cluster Documentation (<xref linkend="atip-management-cluster"/>).</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Installing Metal<superscript>3</superscript> dependencies</title>
<para>If not already installed as part of the Rancher installation, cert-manager must be installed and running.</para>
<para>A persistent storage provider must be installed. SUSE Storage is recommended but <literal>local-path-provisioner</literal> can also be used for
dev/PoC environments. The instructions below assume a StorageClass has been
<link xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">marked as default</link>,
otherwise additional configuration for the Metal<superscript>3</superscript> chart is required.</para>
<para>An additional IP is required, which is managed by <link xl:href="https://metallb.universe.tf/">MetalLB</link> to provide a
consistent endpoint for the Metal<superscript>3</superscript> management services.
This IP must be part of the control plane subnet and reserved for static configuration (not part of any DHCP pool).</para>
<tip>
<para>If the management cluster is a single node, the requirement for an additional floating IP managed via MetalLB can be avoided, see <xref linkend="id-single-node-configuration"/></para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>First, we install MetalLB:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>Then we define an <literal>IPAddressPool</literal> and <literal>L2Advertisement</literal> using the reserved IP, defined as <literal>STATIC_IRONIC_IP</literal> below:</para>
<screen language="bash" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>Now Metal<superscript>3</superscript> can be installed:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/charts/metal3 \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="$STATIC_IRONIC_IP"</screen>
</listitem>
<listitem>
<para>It can take around two minutes for the init container to run on this deployment, so ensure the pods are all running before proceeding:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para>Do not proceed to the following steps until all pods in the <literal>metal3-system</literal> namespace are running.</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Installing cluster API dependencies</title>
<para>Cluster API dependencies are managed via the Rancher Turtles Helm chart:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/charts/rancher-turtles \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</screen>
<para>After some time, the controller pods should be running in the <literal>capi-system</literal>, <literal>capm3-system</literal>, <literal>rke2-bootstrap-system</literal> and <literal>rke2-control-plane-system</literal> namespaces.</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>Prepare downstream cluster image</title>
<para>Kiwi (<xref linkend="guides-kiwi-builder-images"/>) and Edge Image Builder (<xref linkend="components-eib"/>) are used to prepare a modified SLEMicro base image which is provisioned on downstream cluster hosts.</para>
<para>In this guide, we cover the minimal configuration necessary to deploy the downstream cluster.</para>
<section xml:id="id-image-configuration">
<title>Image configuration</title>
<note>
<para>Please follow <xref linkend="guides-kiwi-builder-images"/> first to build a fresh image as the first step required to create clusters.</para>
</note>
<para>When running Edge Image Builder, a directory is mounted from the host, so it is necessary to create a directory structure to store the configuration files used to define the target image.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> is the image definition file, see <xref linkend="quickstart-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The base image when downloaded is <literal>xz</literal> compressed, which must be uncompressed with <literal>unxz</literal> and copied/moved under the <literal>base-images</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>network</literal> folder is optional, see <xref linkend="metal3-add-network-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The custom/scripts directory contains scripts to be run on first-boot; currently a <literal>01-fix-growfs.sh</literal> script is required to resize the OS root partition on deployment</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>Downstream cluster image definition file</title>
<para>The <literal>downstream-cluster-config.yaml</literal> file is the main configuration file for the downstream cluster image. The following is a minimal example for deployment via Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
  sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Where <literal>$SCC_REGISTRATION_CODE</literal> is the registration code copied from <link xl:href="https://scc.suse.com/">SUSE Customer Center</link>, and the package list contains <literal>jq</literal> w
hich is required.</para>
<para><literal>$ROOT_PASSWORD</literal> is the encrypted password for the root user, which can be useful for test/debugging.  It can be generated with the <literal>openssl passwd -6 PASSWORD</literal> command</para>
<para>For the production environments, it is recommended to use the SSH keys that can be added to the users block replacing the <literal>$USERKEY1</literal> with the real SSH keys.</para>
<note>
<para><literal>net.ifnames=1</literal> enables <link xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable Network Interface Naming</link></para>
<para>This matches the default configuration for the Metal<superscript>3</superscript> chart, but the setting must match the configured chart <literal>predictableNicNames</literal> value.</para>
<para>Also note that <literal>ignition.platform.id=openstack</literal> is mandatory - without this argument SUSE Linux Micro configuration via ignition will fail in the Metal<superscript>3</superscript> automated flow.</para>
<para>The <literal>time</literal> section is optional but it is highly recommended to be configured to avoid potential issues with certificates and clock skew. The values provided in this example are for illustrative purposes only. Please adjust them to fit your specific requirements.</para>
</note>
</section>
<section xml:id="growfs-script">
<title>Growfs script</title>
<para>Currently, a custom script (<literal>custom/scripts/01-fix-growfs.sh</literal>) is required to grow the file system to match the disk size on first-boot after provisioning. The <literal>01-fix-growfs.sh</literal> script contains the following information:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>Add your own custom scripts to be executed during the provisioning process using the same approach.
For more information, see <xref linkend="quickstart-eib"/>.</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>Image creation</title>
<para>Once the directory structure is prepared following the previous sections, run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>This creates the output image file named <literal>SLE-Micro-eib-output.raw</literal>, based on the definition described above.</para>
<para>The output image must then be made available via a webserver, either the media-server container enabled via the Metal3 chart (<xref linkend="metal3-media-server"/>)
or some other locally accessible server.  In the examples below, we refer to this server as <literal>imagecache.local:8080</literal></para>
<note>
<para>When deploying EIB images to downstream clusters, it is required also to include the sha256 sum of the image on the <literal>Metal3MachineTemplate</literal> object.
It can be generated as:</para>
<screen language="shell" linenumbering="unnumbered">sha256sum &lt;image_file&gt; &gt; &lt;image_file&gt;.sha256
# On this example:
sha256sum SLE-Micro-eib-output.raw &gt; SLE-Micro-eib-output.raw.sha256</screen>
</note>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>Adding BareMetalHost inventory</title>
<para>Registering bare-metal servers for automated deployment requires creating two resources: a Secret storing
BMC access credentials and a Metal<superscript>3</superscript> BareMetalHost resource defining the BMC connection and other details:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>Note the following:</para>
<itemizedlist>
<listitem>
<para>The Secret username/password must be base64 encoded. Note this should not include any trailing newlines (for example, use <literal>echo -n</literal>, not just <literal>echo</literal>!)</para>
</listitem>
<listitem>
<para>The <literal>cluster-role</literal> label may be set now or later on cluster creation. In the example below, we expect <literal>control-plane</literal> or <literal>worker</literal></para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal> must be a valid MAC that matches the control plane NIC of the host</para>
</listitem>
<listitem>
<para>The <literal>bmc</literal> address is the connection to the BMC management API, the following are supported:</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;IP ADDRESS&gt;/redfish/v1/Systems/&lt;SYSTEM ID&gt;</literal>: Redfish virtual media, for example, SuperMicro</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;IP ADDRESS&gt;/redfish/v1/Systems/System.Embedded.1</literal>: Dell iDRAC</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>See the <link xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">Upstream API docs</link> for more details on the BareMetalHost API</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>Configuring Static IPs</title>
<para>The BareMetalHost example above assumes DHCP provides the controlplane network configuration, but for scenarios where manual configuration
is needed such as static IPs it is possible to provide additional configuration, as described below.</para>
<section xml:id="metal3-add-network-eib">
<title>Additional script for static network configuration</title>
<para>When creating the base image with Edge Image Builder, in the <literal>network</literal> folder, create the following <literal>configure-network.sh</literal> file.</para>
<para>This consumes configuration drive data on first-boot, and configures the host networking using the <link xl:href="https://github.com/suse-edge/nm-configurator">NM Configurator tool</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>Additional secret with host network configuration</title>
<para>An additional secret containing data in the <link xl:href="https://nmstate.io/">nmstate</link> format supported by NM Configurator (<xref linkend="components-nmc"/>) can be defined for each host.</para>
<para>The secret is then referenced in the <literal>BareMetalHost</literal> resource via the <literal>preprovisioningNetworkDataName</literal> spec field.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>In some circumstances the MAC address may be omitted. See <xref linkend="networking-unified"/> for additional details.</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>BareMetalHost preparation</title>
<para>After creating the BareMetalHost resource and associated secrets as described above, a host preparation workflow is triggered:</para>
<itemizedlist>
<listitem>
<para>A ramdisk image is booted by virtualmedia attachment to the target host BMC</para>
</listitem>
<listitem>
<para>The ramdisk inspects hardware details, and prepares the host for provisioning (for example by cleaning disks of previous data)</para>
</listitem>
<listitem>
<para>On completion of this process, hardware details in the BareMetalHost <literal>status.hardware</literal> field are updated and can be verified</para>
</listitem>
</itemizedlist>
<para>This process can take several minutes, but when completed you should see the BareMetalHost state become <literal>available</literal>:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>Creating downstream clusters</title>
<para>We now create Cluster API resources which define the downstream cluster, and Machine resources which will cause the BareMetalHost resources to
be provisioned, then bootstrapped to form an RKE2 cluster.</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>Control plane deployment</title>
<para>To deploy the controlplane we define a yaml manifest similar to the one below, which contains the following resources:</para>
<itemizedlist>
<listitem>
<para>Cluster resource defines the cluster name, networks, and type of controlplane/infrastructure provider (in this case RKE2/Metal3)</para>
</listitem>
<listitem>
<para>Metal3Cluster defines the controlplane endpoint (host IP for single-node, LoadBalancer endpoint for multi-node, this example assumes single-node)</para>
</listitem>
<listitem>
<para>RKE2ControlPlane defines the RKE2 version and any additional configuration needed during cluster bootstrapping</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate defines the OS Image to be applied to the BareMetalHost resources, and the hostSelector defines which BareMetalHosts to consume</para>
</listitem>
<listitem>
<para>Metal3DataTemplate defines additional metaData to be passed to the BareMetalHost (note networkData is not currently supported in the Edge solution)</para>
</listitem>
</itemizedlist>
<note>
<para>For simplicity this example assumes a single-node control plane where the BareMetalHost is configured with an IP of <literal>192.168.125.200</literal>. For more advanced multi-node examples, please see <xref linkend="atip-automated-provisioning"/>.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Once adapted to your environment, you can apply the example via <literal>kubectl</literal> and then monitor the cluster status via <literal>clusterctl</literal>.</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>Worker/Compute deployment</title>
<para>Similar to the control plane deployment, we define a YAML manifest which contains the following resources:</para>
<itemizedlist>
<listitem>
<para>MachineDeployment defines the number of replicas (hosts) and the bootstrap/infrastructure provider (in this case RKE2/Metal3)</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplate describes the RKE2 version and first-boot configuration for agent host bootstrapping</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate defines the OS Image to be applied to the BareMetalHost resources, and the host selector defines which BareMetalHosts to consume</para>
</listitem>
<listitem>
<para>Metal3DataTemplate defines additional metadata to be passed to the BareMetalHost (note that <literal>networkData</literal> is not currently supported)</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.33.3+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.33.3+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>When the example above has been copied and adapted to suit your environment, it can be applied via <literal>kubectl</literal> then the cluster status can be monitored with <literal>clusterctl</literal></para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait for the worker nodes to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>Cluster deprovisioning</title>
<para>The downstream cluster may be deprovisioned by deleting the resources applied in the creation steps above:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>This triggers deprovisioning of the BareMetalHost resources, which may take several minutes, after which they should be in available state again:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>The upstream <link xl:href="https://github.com/metal3-io/ip-address-manager">IP Address Management controller</link> is currently not supported, because it’s not yet compatible with our choice of network configuration tooling and first-boot toolchain in SLEMicro.</para>
</listitem>
<listitem>
<para>Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are not currently supported.</para>
</listitem>
<listitem>
<para>Only deployment via redfish-virtualmedia is currently supported.</para>
</listitem>
<listitem>
<para>It is possible to observe a network device name misalignment between the ironic python agent (IPA) and the target operating system (SL Micro 6.0/6.1), especially when trying to configure predictable names for the devices.</para>
</listitem>
</itemizedlist>
<para>This happens because the kernel of the ironic python agent (IPA) is not currently aligned with the kernel of the target operating system (SL Micro 6.0/6.1), therefore there’s a misalignment in the network drivers that allows the IPA to discover network devices in a different naming pattern than SL Micro expects.</para>
<para>There are two different approaches to be used as a workaround in the meantime:
* Create two different secrets with the network configuration, one to be used with the IPA using the device names as IPA will discover and use it as <literal>preprovisioningNetworkDataName</literal> on the <literal>BareMetalHost</literal> definition and another secret with the device names as SL Micro will discover and use it as <literal>networkData.name</literal> on the <literal>BareMetalHost</literal> definition.
* Use the UUIDs to reference other interfaces on the generated nmconnection files instead.
More details in the <link xl:href="..tips/metal3.adoc">tips and tricks</link> section.</para>
</section>
<section xml:id="id-planned-changes">
<title>Planned changes</title>
<itemizedlist>
<listitem>
<para>Enable support of the IPAM resources and configuration via networkData fields</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>Additional resources</title>
<para>The SUSE Telco Cloud Documentation (<xref linkend="atip"/>) has examples of more advanced usage of Metal<superscript>3</superscript> for telco use-cases.</para>
<section xml:id="id-single-node-configuration">
<title>Single-node configuration</title>
<para>For test/PoC environments where the management cluster is a single node, it is possible to avoid the requirement for an additional floating IP managed via MetalLB.</para>
<para>In this mode, the endpoint for the management cluster APIs is the IP of the management cluster, therefore it should be reserved when using DHCP
or statically configured to ensure the management cluster IP does not change - referred to as <literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal> below.</para>
<para>To enable this scenario, the Metal<superscript>3</superscript> chart values required are as follows:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="disabling-tls-for-virtualmedia-iso-attachment">
<title>Disabling TLS for virtualmedia ISO attachment</title>
<para>Some server vendors verify the SSL connection when attaching virtual-media ISO images to the BMC, which can cause a problem because the generated
certificates for the Metal<superscript>3</superscript> deployment are self-signed, to work around this issue it’s possible to disable TLS only for the virtualmedia disk attachment
with Metal<superscript>3</superscript> chart values as follows:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>An alternative solution is to configure the BMCs with the CA cert - in this case you can read the certificates from the cluster using <literal>kubectl</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>The certificate can then be configured on the server BMC console, although the process for that is vendor specific (and not possible for all
vendors, in which case the <literal>enable_vmedia_tls</literal> flag may be required).</para>
</section>
<section xml:id="id-storage-configuration">
<title>Storage configuration</title>
<para>For test/PoC environments where the management cluster is a single node, no persistent storage is required, but for production use-cases it
is recommended to install SUSE Storage (Longhorn) on the management cluster so that images related to Metal<superscript>3</superscript> can be persisted during a pod
restart/reschedule.</para>
<para>To enable this persistent storage, the Metal<superscript>3</superscript> chart values required are as follows:</para>
<screen language="yaml" linenumbering="unnumbered">metal3-ironic:
  persistence:
    ironic:
      size: "5Gi"</screen>
<para>The SUSE Telco Cloud Management Cluster Documentation (<xref linkend="atip-management-cluster"/>) has more details on how to configure a management cluster
with persistent storage.</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Remote host onboarding with Elemental</title>
<para>This section documents the "phone home network provisioning" solution as part of SUSE Edge, where we use Elemental to assist with node onboarding. Elemental is a software stack enabling remote host registration and centralized full cloud-native OS management with Kubernetes. In the SUSE Edge stack we use the registration feature of Elemental to enable remote host onboarding into Rancher so that hosts can be integrated into a centralized management platform and from there, deploy and manage Kubernetes clusters along with layered components, applications, and their lifecycle, all from a common place.</para>
<para>This approach can be useful in scenarios where the devices that you want to control are not on the same network as the management cluster or do not have a out-of-band management controller onboard to allow more direct control, and where you’re booting many different "unknown" systems at the edge, and need to securely onboard and manage them at scale. This is a common scenario for use cases in retail, industrial IoT, or other spaces where you have little control over the network your devices are being installed in.</para>
<section xml:id="id-high-level-architecture-3">
<title>High-level architecture</title>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="quickstart-elemental-architecture.svg" width="100%"/>
</imageobject>
<textobject><phrase>quickstart elemental architecture</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>Resources needed</title>
<para>The following describes the minimum system and environmental requirements to run through this quickstart:</para>
<itemizedlist>
<listitem>
<para>A host for the centralized management cluster (the one hosting Rancher and Elemental):</para>
<itemizedlist>
<listitem>
<para>Minimum 8 GB RAM and 20 GB disk space for development or testing (see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements#hardware-requirements">here</link> for production use)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>A target node to be provisioned, i.e. the edge device (a virtual machine can be used for demoing or testing purposes)</para>
<itemizedlist>
<listitem>
<para>Minimum 4GB RAM, 2 CPU cores, and 20 GB disk</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>A resolvable host name for the management cluster or a static IP address to use with a service like sslip.io</para>
</listitem>
<listitem>
<para>A host to build the installation media via Edge Image Builder</para>
<itemizedlist>
<listitem>
<para>Running SLES 15 SP6, openSUSE Leap 15.6, or another compatible operating system that supports Podman.</para>
</listitem>
<listitem>
<para>With <link xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>, <link xl:href="https://podman.io">Podman</link>, and <link xl:href="https://helm.sh">Helm</link> installed</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>A USB flash drive to boot from (if using physical hardware)</para>
</listitem>
<listitem>
<para>A downloaded copy of the latest SUSE Linux Micro 6.1 SelfInstall ISO image found <link xl:href="https://www.suse.com/download/sle-micro/">here</link>.</para>
</listitem>
</itemizedlist>
<note>
<para>Existing data found on target machines will be overwritten as part of the process, please make sure you backup any data on any USB storage devices and disks attached to target deployment nodes.</para>
</note>
<para>This guide is created using a Digital Ocean droplet to host the upstream cluster and an Intel NUC as the downstream device. For building the installation media, SUSE Linux Enterprise Server is used.</para>
</section>
<section xml:id="build-bootstrap-cluster">
<title>Build bootstrap cluster</title>
<para>Start by creating a cluster capable of hosting Rancher and Elemental. This cluster needs to be routable from the network that the downstream nodes are connected to.</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Create Kubernetes cluster</title>
<para>If you are using a hyperscaler (such as Azure, AWS or Google Cloud), the easiest way to set up a cluster is using their built-in tools. For the sake of conciseness in this guide, we do not detail the process of each of these options.</para>
<para>If you are installing onto bare-metal or another hosting service where you need to also provide the Kubernetes distribution itself, we recommend using <link xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>.</para>
</section>
<section xml:id="id-set-up-dns">
<title>Set up DNS</title>
<para>Before continuing, you need to set up access to your cluster. As with the setup of the cluster itself, how you configure DNS will be different depending on where it is being hosted.</para>
<tip>
<para>If you do not want to handle setting up DNS records (for example, this is just an ephemeral test server), you can use a service like <link xl:href="https://sslip.io">sslip.io</link> instead. With this service, you can resolve any IP address with <literal>&lt;address&gt;.sslip.io</literal>.</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Install Rancher</title>
<para>To install Rancher, you need to get access to the Kubernetes API of the cluster you just created. This looks different depending on what distribution of Kubernetes is being used.</para>
<para>For RKE2, the kubeconfig file will have been written to <literal>/etc/rancher/rke2/rke2.yaml</literal>.
Save this file as <literal>~/.kube/config</literal> on your local system.
You may need to edit the file to include the correct externally routable IP address or host name.</para>
<para>Install Rancher easily with the commands from the <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Rancher Documentation</link>:</para>
<para>Install <link xl:href="https://cert-manager.io">cert-manager</link>:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true</screen>
<para>Then install Rancher itself:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt; \
  --version 2.12.1</screen>
<note>
<para>If this is intended to be a production system, please use cert-manager to configure a real certificate (such as one from Let’s Encrypt).</para>
</note>
<para>Browse to the host name you set up and log in to Rancher with the <literal>bootstrapPassword</literal> you used. You will be guided through a short setup process.</para>
</section>
<section xml:id="install-elemental">
<title>Install Elemental</title>
<para>With Rancher installed, you can now install the Elemental operator and required CRD’s. The Helm chart for Elemental is published as an OCI artifact so the installation is a little simpler than other charts.
It can be installed from either the same shell you used to install Rancher or in the browser from within Rancher’s shell.</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.7.3

helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.7.3</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>(Optionally) Install the Elemental UI extension</title>
<orderedlist numeration="arabic">
<listitem>
<para>To use the Elemental UI, log in to your Rancher instance, click the three-line menu in the upper left:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="installing-elemental-extension-1.png" width="85%"/>
</imageobject>
<textobject><phrase>Installing Elemental extension 1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>From the "Available" tab on this page, click "Install" on the Elemental card:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="installing-elemental-extension-2.png" width="85%"/>
</imageobject>
<textobject><phrase>Installing Elemental extension 2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Confirm that you want to install the extension:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="installing-elemental-extension-3.png" width="100%"/>
</imageobject>
<textobject><phrase>Installing Elemental extension 3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>After it installs, you will be prompted to reload the page.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="installing-elemental-extension-4.png" width="100%"/>
</imageobject>
<textobject><phrase>Installing Elemental extension 4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Once you reload, you can access the Elemental extension through the "OS Management" global app.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="accessing-elemental-extension.png" width="100%"/>
</imageobject>
<textobject><phrase>Accessing Elemental extension</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configure-elemental">
<title>Configure Elemental</title>
<para>For simplicity, we recommend setting the variable <literal>$ELEM</literal> to the full path of where you want the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>To allow machines to register to Elemental, we need to create a <literal>MachineRegistration</literal> object in the <literal>fleet-default</literal> namespace.</para>
<para>Let us create a basic version of this object:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>The <literal>cat</literal> command escapes each <literal>$</literal> with a backslash (<literal>\</literal>) so that Bash does not template them. Remove the backslashes if copying manually.</para>
</note>
<para>Once the object is created, find and note the endpoint that gets assigned:</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>Alternatively, this can also be done from the UI.</para>
<variablelist>
<varlistentry>
<term>UI Extension</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>From the OS Management extension, click "Create Registration Endpoint":</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="click-create-registration.png" width="100%"/>
</imageobject>
<textobject><phrase>Click Create Registration</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Give this configuration a name.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="create-registration-name.png" width="100%"/>
</imageobject>
<textobject><phrase>Add Name</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>You can ignore the Cloud Configuration field as the data here is overridden by the following steps with Edge Image Builder.</para>
</note>
</listitem>
<listitem>
<para>Next, scroll down and click "Add Label" for each label you want to be on the resource that gets created when a machine registers. This is useful for distinguishing machines.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="create-registration-labels.png" width="100%"/>
</imageobject>
<textobject><phrase>Add Labels</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Click "Create" to save the configuration.</para>
</listitem>
<listitem>
<para>Once the registration is created, you should see the Registration URL listed and can click "Copy" to copy the address:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="get-registration-url.png" width="100%"/>
</imageobject>
<textobject><phrase>Copy URL</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>If you clicked away from that screen, you can click "Registration Endpoints" in the left menu, then click the name of the endpoint you just created.</para>
</tip>
<para>This URL is used in the next step.</para>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="build-installation-media">
<title>Build the image</title>
<para>While the current version of Elemental has a way to build its own installation media, in SUSE Edge 3.4 we do this with Kiwi and Edge Image Builder instead, so the resulting system is built with <link xl:href="https://www.suse.com/products/micro/">SUSE Linux Micro</link> as the base Operating System.</para>
<tip>
<para>For more details on Kiwi, please follow Kiwi Image Builder process (<xref linkend="guides-kiwi-builder-images"/>) to build fresh images first and for Edge Image Builder, check out the Edge Image Builder Getting Started Guide (<xref linkend="quickstart-eib"/>) and also the Component Documentation (<xref linkend="components-eib"/>).</para>
</tip>
<para>From a Linux system with Podman installed, create the directories and place the base image being built by Kiwi:</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/{micro-base-image-iso} $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.3
image:
    imageType: iso
    arch: x86_64
    baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: XXX
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para>The <literal>time</literal> section is optional but it is highly recommended to be configured to avoid potential issues with certificates and clock skew. The values provided in this example are for illustrative purposes only. Please adjust them to fit your specific requirements.</para>
</listitem>
<listitem>
<para>The unencoded password is <literal>eib</literal>.</para>
</listitem>
<listitem>
<para>The <literal>sccRegistrationCode</literal> is needed to download and install the necessary RPMs from the official sources (alternatively, the <literal>elemental-register</literal> and <literal>elemental-system-agent</literal> RPMs can be manually side-loaded instead)</para>
</listitem>
<listitem>
<para>The <literal>cat</literal> command escapes each <literal>$</literal> with a backslash (<literal>\</literal>) so that Bash does not template them. Remove the backslashes if copying manually.</para>
</listitem>
<listitem>
<para>The installation device will be wiped during the installation.</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file eib-config.yaml</screen>
<para>If you are booting a physical device, we need to burn the image to a USB flash drive. This can be done with:</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>Boot the downstream nodes</title>
<para>Now that we have created the installation media, we can boot our downstream nodes with it.</para>
<para>For each of the systems that you want to control with Elemental, add the installation media and boot the device. After installation, it will reboot and register itself.</para>
<para>If you are using the UI extension, you should see your node appear in the "Inventory of Machines."</para>
<note>
<para>Do not remove the installation medium until you’ve seen the login prompt; during first-boot files are still accessed on the USB stick.</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>Create downstream clusters</title>
<para>There are two objects we need to create when provisioning a new cluster using Elemental.</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>The first is the <literal>MachineInventorySelectorTemplate</literal>. This object allows us to specify a mapping between clusters and the machines in the inventory.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Create a selector which will match any machine in the inventory with a label:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>Apply the resource to the cluster:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>Obtain the name of the machine and add the matching label:</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>Create a simple single-node K3s cluster resource and apply it to the cluster:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.33.3+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI Extension</term>
<listitem>
<para>The UI extension allows for a few shortcuts to be taken. Note that managing multiple locations may involve too much manual work.</para>
<orderedlist numeration="arabic">
<listitem>
<para>As before, open the left three-line menu and select "OS Management." This brings you back to the main screen for managing your Elemental systems.</para>
</listitem>
<listitem>
<para>On the left sidebar, click "Inventory of Machines." This opens the inventory of machines that have registered.</para>
</listitem>
<listitem>
<para>To create a cluster from these machines, select the systems you want, click the "Actions" drop-down list, then "Create Elemental Cluster." This opens the Cluster Creation dialog while also creating a MachineSelectorTemplate to use in the background.</para>
</listitem>
<listitem>
<para>On this screen, configure the cluster you want to be built. For this quick start, K3s v1.30.5+k3s1 is selected and the rest of the options are left as is.</para>
<tip>
<para>You may need to scroll down to see more options.</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>After creating these objects, you should see a new Kubernetes cluster spin up using the new node you just installed with.</para>
</section>
<section xml:id="id-node-reset-optional">
<title>Node Reset (Optional)</title>
<para>SUSE Rancher Elemental supports the ability to perform a "node reset" which can optionally trigger when either a whole cluster is deleted from Rancher, a single node is deleted from a cluster, or a node is manually deleted from the machine inventory. This is useful when you want to reset and clean-up any orphaned resources and want to automatically bring the cleaned node back into the machine inventory so it can be reused. This is not enabled by default, and thus any system that is removed, will not be cleaned up (i.e. data will not be removed, and any Kubernetes cluster resources will continue to operate on the downstream clusters) and it will require manual intervention to wipe data and re-register the machine to Rancher via Elemental.</para>
<para>If you wish for this functionality to be enabled by default, you need to make sure that your <literal>MachineRegistration</literal> explicitly enables this by adding <literal>config.elemental.reset.enabled: true</literal>, for example:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>Then, all systems registered with this <literal>MachineRegistration</literal> will automatically receive the <literal>elemental.cattle.io/resettable: 'true'</literal> annotation in their configuration. If you wish to do this manually on individual nodes, e.g. because you’ve got an existing <literal>MachineInventory</literal> that doesn’t have this annotation, or you have already deployed nodes, you can modify the <literal>MachineInventory</literal> and add the <literal>resettable</literal> configuration, for example:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>In SUSE Edge 3.1, the Elemental Operator puts down a marker on the operating system that will trigger the cleanup process automatically; it will stop all Kubernetes services, remove all persistent data, uninstall all Kubernetes services, cleanup any remaining Kubernetes/Rancher directories, and force a re-registration to Rancher via the original Elemental <literal>MachineRegistration</literal> configuration. This happens automatically, there is no need for any manual intervention. The script that gets called can be found in <literal>/opt/edge/elemental_node_cleanup.sh</literal> and is triggered via <literal>systemd.path</literal> upon the placement of the marker, so its execution is immediate.</para>
<warning>
<para>Using the <literal>resettable</literal> functionality assumes that the desired behavior when removing a node/cluster from Rancher is to wipe data and force a re-registration. Data loss is guaranteed in this situation, so only use this if you’re sure that you want automatic reset to be performed.</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>Next steps</title>
<para>Here are some recommended resources to research after using this guide:</para>
<itemizedlist>
<listitem>
<para>End-to-end automation in <xref linkend="components-fleet"/></para>
</listitem>
<listitem>
<para>Additional network configuration options in <xref linkend="components-nmc"/></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Standalone clusters with Edge Image Builder</title>
<para>Edge Image Builder (EIB) is a tool that streamlines the process of generating Customized, Ready-to-Boot (CRB) disk images for bootstrapping machines, even in fully air-gapped scenarios. EIB is used to create deployment images for use in all three of the SUSE Edge deployment footprints, as it’s flexible enough to offer the smallest customizations, e.g. adding a user or setting the timezone, through offering a comprehensively configured image that sets up, for example, complex networking configurations, deploys multi-node Kubernetes clusters, deploys customer workloads, and registers to the centralized management platform via Rancher/Elemental and SUSE Multi-Linux Manager. EIB runs as in a container image, making it incredibly portable across platforms and ensuring that all of the required dependencies are self-contained, having a very minimal impact on the installed packages of the system that’s being used to operate the tool.</para>
<note>
<para>For multi-node scenarios, EIB automatically deploys MetalLB and Endpoint Copier Operator in order for hosts provisioned using the same built image to automatically join a Kubernetes cluster.</para>
</note>
<para>For more information, read the Edge Image Builder Introduction (<xref linkend="components-eib"/>).</para>
<warning>
<para>Edge Image Builder 1.3.0 supports customizing SUSE Linux Micro 6.1 images.
Older versions, such as SUSE Linux Enterprise Micro 5.5, or 6.0 are not supported.</para>
</warning>
<section xml:id="id-prerequisites-2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>An AMD64/Intel 64 build host machine (physical or virtual) running SLES 15 SP6.</para>
</listitem>
<listitem>
<para>The Podman container engine</para>
</listitem>
<listitem>
<para>A SUSE Linux Micro 6.1 SelfInstall ISO image created using the Kiwi Builder procedure (<xref linkend="guides-kiwi-builder-images"/>)</para>
</listitem>
</itemizedlist>
<note>
<para>For non-production purposes, openSUSE Leap 15.6, or openSUSE Tumbleweed may be used as a build host machine.
Other operating systems may function, so long as a compatible container runtime is available.</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>Getting the EIB Image</title>
<para>The EIB container image is publicly available and can be downloaded from the SUSE Edge registry by running the following command on your image build host:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>Creating the image configuration directory</title>
<para>As EIB runs within a container, we need to mount a configuration directory from the host, enabling you to specify your desired configuration, and during the build process EIB has access to any required input files and supporting artifacts. This directory must follow a specific structure. Let’s create it, assuming that this directory will exist in your home directory, and called "eib":</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>In the previous step we created a "base-images" directory that will host the SUSE Linux Micro 6.1 input image, let’s ensure that the image is copied over to the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>During the EIB run, the original base image is <emphasis role="strong">not</emphasis> modified; a new and customized version is created with the desired configuration in the root of the EIB config directory.</para>
</note>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>Creating the image definition file</title>
<para>The definition file describes the majority of configurable options that the Edge Image Builder supports, a full example of options can be found <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/pkg/image/testdata/full-valid-example.yaml">here</link>, and we would recommend that you take a look at the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">upstream building images guide</link> for more comprehensive examples than the one we’re going to run through below. Let’s start with a very basic definition file for our OS image:</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>This definition specifies that we’re generating an output image for an AMD64/Intel 64 based system. The image that will be used as the base for further modification is an <literal>iso</literal> image named <literal>slemicro.iso</literal>,
expected to be located at <literal>$CONFIG_DIR/base-images/slemicro.iso</literal>. It also outlines that after EIB finishes modifying the image, the output image will be named <literal>eib-image.iso</literal>, and by default will reside in <literal>$CONFIG_DIR</literal>.</para>
<para>Now our directory structure should look like:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>In the following sections we’ll walk through a few examples of common operations:</para>
<section xml:id="id-configuring-os-users">
<title>Configuring OS Users</title>
<para>EIB allows you to preconfigure users with login information, such as passwords or SSH keys, including setting a fixed root password. As part of this example we’re going to fix the root password, and the first step is to use <literal>OpenSSL</literal> to create a one-way encrypted password:</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>This will output something similar to:</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>We can then add a section in the definition file called <literal>operatingSystem</literal> with a <literal>users</literal> array inside it. The resulting file should look like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>It’s also possible to add additional users, create the home directories, set user-id’s, add ssh-key authentication, and modify group information. Please refer to the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">upstream building images guide</link> for further examples.</para>
</note>
</section>
<section xml:id="configuring-os-time">
<title>Configuring OS time</title>
<para>The <literal>time</literal> section is optional but it is highly recommended to be configured to avoid potential issues with certificates and clock skew. EIB will configure chronyd and <literal>/etc/localtime</literal> depending on the parameters here.</para>
<screen language="console" linenumbering="unnumbered">operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2</screen>
<itemizedlist>
<listitem>
<para>The <literal>timezone</literal> specifies the timezone in the format of "Region/Locality" (e.g. "Europe/London"). The full list may be found by running <literal>timedatectl list-timezones</literal> on a Linux system.</para>
</listitem>
<listitem>
<para>ntp - Defines attributes related to configuring NTP (using chronyd):</para>
</listitem>
<listitem>
<para>forceWait - Requests that chronyd attempts to synchronize timesources before starting other services, with a 180s timeout.</para>
</listitem>
<listitem>
<para>pools - Specifies a list of pools that chronyd will use as data sources (using <literal>iburst</literal> to improve the time taken for initial synchronization).</para>
</listitem>
<listitem>
<para>servers - Specifies a list of servers that chronyd will use as data sources (using <literal>iburst</literal> to improve the time taken for initial synchronization).</para>
</listitem>
</itemizedlist>
<note>
<para>The values provided in this example are for illustrative purposes only. Please adjust them to fit your specific requirements.</para>
</note>
</section>
<section xml:id="adding-certificates">
<title>Adding certificates</title>
<para>Certificate files with the extension ".pem" or ".crt" stored in the <literal>certificates</literal> directory will be installed in the node system-wide certificate store:</para>
<screen language="console" linenumbering="unnumbered">.
├── definition.yaml
└── certificates
    ├── my-ca.pem
    └── my-ca.crt</screen>
<para>See the <link xl:href="https://documentation.suse.com/smart/security/html/tls-certificates/index.html#tls-adding-new-certificates">"Securing Communication with TLS Certificate" guide</link> for more information.</para>
</section>
<section xml:id="eib-configuring-rpm-packages">
<title>Configuring RPM packages</title>
<para>One of the major features of EIB is to provide a mechanism to add additional software packages to the image, so when the installation completes the system is able to leverage the installed packages right away. EIB permits users to specify the following:</para>
<itemizedlist>
<listitem>
<para>Packages by their name within a list in the image definition</para>
</listitem>
<listitem>
<para>Network repositories to search for these packages in</para>
</listitem>
<listitem>
<para>SUSE Customer Center (SCC) credentials to search official SUSE repositories for the listed packages</para>
</listitem>
<listitem>
<para>Via an <literal>$CONFIG_DIR/rpms</literal> directory, side-load custom RPM’s that don’t exist in network repositories</para>
</listitem>
<listitem>
<para>Via the same directory (<literal>$CONFIG_DIR/rpms/gpg-keys</literal>), GPG-keys to enable validation of third party packages</para>
</listitem>
</itemizedlist>
<para>EIB will then run through a package resolution process at image build time, taking the base image as the input, and attempts to pull and install all supplied packages, either specified via the list or provided locally. EIB downloads all of the packages, including any dependencies into a repository that exists within the output image and instructs the system to install these during the first boot process. Doing this process during the image build guarantees that the packages will successfully install during first-boot on the desired platform, e.g. the node at the edge. This is also advantageous in environments where you want to bake the additional packages into the image rather than pull them over the network when in operation, e.g. for air-gapped or restricted network environments.</para>
<para>As a simple example to demonstrate this, we are going to install the <literal>nvidia-container-toolkit</literal> RPM package found in the third party vendor-supported NVIDIA repository:</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>The resulting definition file looks like the following:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>The above is a simple example, but for completeness, download the NVIDIA package signing key before running the image generation:</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; $CONFIG_DIR/rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>Adding in additional RPM’s via this method is meant for the addition of supported third party components or user-supplied (and maintained) packages; this mechanism should not be used to add packages that would not usually be supported on SUSE Linux Micro. If this mechanism is used to add components from openSUSE repositories (which are not supported), including from newer releases or service packs, you may end up with an unsupported configuration, especially when dependency resolution results in core parts of the operating system being replaced, even though the resulting system may appear to function as expected. If you’re unsure, contact your SUSE representative for assistance in determining the supportability of your desired configuration.</para>
</warning>
<note>
<para>A more comprehensive guide with additional examples can be found in the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">upstream installing packages guide</link>.</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Configuring Kubernetes cluster and user workloads</title>
<para>Another feature of EIB is the ability to use it to automate the deployment of both single-node and multi-node highly-available Kubernetes clusters that "bootstrap in place" (i.e. don’t require any form of centralized management infrastructure to coordinate). The primary driver behind this approach is for air-gapped deployments, or network restricted environments, but it also serves as a way of quickly bootstrapping standalone clusters, even if full and unrestricted network access is available.</para>
<para>This method enables not only the deployment of the customized operating system, but also the ability to specify Kubernetes configuration, any additional layered components via Helm charts, and any user workloads via supplied Kubernetes manifests. However, the design principle behind using this method is that we default to assuming that the user is wanting to air-gap. Therefore, any items specified in the image definition will be pulled into the image, which includes user-supplied workloads. EIB ensures that any discovered images that are required by definitions are copied locally and are served by the embedded image registry in the resulting deployed system.</para>
<para>In this next example, we’re going to take our existing image definition and will specify a Kubernetes configuration (in this example it doesn’t list the systems and their roles, so we default to assuming single-node), which will instruct EIB to provision a single-node RKE2 Kubernetes cluster. To show the automation of both the deployment of both user-supplied workloads (via manifest) and layered components (via Helm), we are going to install KubeVirt via the SUSE Edge Helm chart, as well as NGINX via a Kubernetes manifest. The additional configuration we need to append to the existing image definition is as follows:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<para>The resulting full definition file should now look like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.33.3+k3s1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<note>
<para>Further examples of options such as multi-node deployments, custom networking, and Helm chart options/values can be found in the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">upstream documentation</link>.</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>Configuring the network</title>
<para>In the last example in this quickstart, let’s configure the network that will be brought up when a system is provisioned with the image generated by EIB. It’s important to understand that unless a network configuration is supplied, the default model is that DHCP will be used on all interfaces discovered at boot time. However, this is not always a desirable configuration, especially if DHCP is not available and you need to provide static configurations, or you need to set up more complex networking constructs, e.g. bonds, LACP, and VLAN’s, or need to override certain parameters, e.g. hostnames, DNS servers, and routes.</para>
<para>EIB provides the ability to provide either per-node configurations (where the system in question is uniquely identified by its MAC address), or an override for supplying an identical configuration to each machine, which is more useful when the system MAC addresses aren’t known. An additional tool is used by EIB called Network Manager Configurator, or <literal>nmc</literal> for short, which is a tool built by the SUSE Edge team to allow custom networking configurations to be applied based on the <link xl:href="https://nmstate.io/">nmstate.io</link> declarative network schema, and at boot time will identify the node it’s booting on and will apply the desired network configuration prior to any services coming up.</para>
<para>We’ll now apply a static network configuration for a system with a single interface by describing the desired network state in a node-specific file (based on the desired hostname) in the required <literal>network</literal> directory:</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>The above example is set up for the default <literal>192.168.122.0/24</literal> subnet assuming that testing is being executed on a virtual machine, please adapt to suit your environment, not forgetting the MAC address. As the same image can be used to provision multiple nodes, networking configured by EIB (via <literal>nmc</literal>) is dependent on it being able to uniquely identify the node by its MAC address, and hence during boot <literal>nmc</literal> will apply the correct networking configuration to each machine. This means that you’ll need to know the MAC addresses of the systems you want to install onto. Alternatively, the default behavior is to rely on DHCP, but you can utilize the <literal>configure-network.sh</literal> hook to apply a common configuration to all nodes - see the networking guide (<xref linkend="components-nmc"/>) for further details.</para>
</warning>
<para>The resulting file structure should look like:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>The network configuration we just created will be parsed and the necessary NetworkManager connection files will be automatically generated and inserted into the new installation image that EIB will create. These files will be applied during the provisioning of the host, resulting in a complete network configuration.</para>
<note>
<para>Please refer to the Edge Networking component (<xref linkend="components-nmc"/>) for a more comprehensive explanation of the above configuration and examples of this feature.</para>
</note>
</section>
</section>
<section xml:id="eib-how-to-build-image">
<title>Building the image</title>
<para>Now that we’ve got a base image and an image definition for EIB to consume, let’s go ahead and build the image. For this, we simply use <literal>podman</literal> to call the EIB container with the "build" command, specifying the definition file:</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>The output of the command should be similar to:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Downloading file: dl-manifest-1.yaml 100% (498/498 B, 9.5 MB/s)
Pulling selected Helm charts... 100% (1/1, 43 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (3/3, 10 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (657/657 MB, 48 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (368/368 MB, 48 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (35/35 MB, 50 MB/s)
Downloading file: sha256sum-amd64.txt 100% (4.3/4.3 kB, 6.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>The built ISO image is stored at <literal>$CONFIG_DIR/eib-image.iso</literal>:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>Each build creates a time-stamped folder in <literal>$CONFIG_DIR/_build/</literal> that includes the logs of the build, the artifacts used during the build,
and the <literal>combustion</literal> and <literal>artefacts</literal> directories which contain all the scripts and artifacts that are added to the CRB image.</para>
<para>The contents of this directory should look like:</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:&lt;version&gt;-registry.tar.zst
│   │   │   ├── rancher_kubectl:&lt;version&gt;-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.6_virt-operator:&lt;version&gt;-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   ├── nvidia-container-toolkit-&lt;version&gt;.rpm
│   │   │       │   ├── nvidia-container-toolkit-base-&lt;version&gt;.rpm
│   │   │       │   ├── libnvidia-container1-&lt;version&gt;.rpm
│   │   │       │   └── libnvidia-container-tools-&lt;version&gt;.rpm
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt
│   │       └── kubevirt-0.4.0.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>If the build fails, <literal>eib-build.log</literal> is the first log that contains information. From there, it will direct you to the component that failed for debugging.</para>
<para>At this point, you should have a ready-to-use image that will:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Deploy SUSE Linux Micro 6.1</para>
</listitem>
<listitem>
<para>Configure the root password</para>
</listitem>
<listitem>
<para>Install the <literal>nvidia-container-toolkit</literal> package</para>
</listitem>
<listitem>
<para>Configure an embedded container registry to serve content locally</para>
</listitem>
<listitem>
<para>Install single-node RKE2</para>
</listitem>
<listitem>
<para>Configure static networking</para>
</listitem>
<listitem>
<para>Install KubeVirt</para>
</listitem>
<listitem>
<para>Deploy a user-supplied manifest</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>Debugging the image build process</title>
<para>If the image build process fails, refer to the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/debugging.md">upstream debugging guide</link>.</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>Testing your newly built image</title>
<para>For instructions on how to test the newly built CRB image, refer to the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/testing-guide.md">upstream image testing guide</link>.</para>
</section>
</chapter>
<chapter xml:id="quickstart-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Multi-Linux Manager is included in SUSE Edge to provide automation and control for keeping SUSE Linux Micro as the underlying operating system consistently up-to-date on all nodes of your edge deployment. It can also be used to manage Kubernetes and applications deployed on Kubernetes on your edge nodes.</para>
<para>This quickstart guide is intended to get you up to speed with SUSE Multi-Linux Manager as quickly as possible, with the goal of providing operating system updates to your edge nodes. The quickstart guide doesn’t discuss topics like sizing your storage, creating and managing additional software channels for staging purposes, or managing users, system groups, and organizations for larger deployments. For production use, we strongly recommend to get familiar with the comprehensive <link xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUSE Multi-Linux Manager Documentation</link>.</para>
<para>The following steps are required to prepare SUSE Edge for using SUSE Multi-Linux Manager effectively:</para>
<itemizedlist>
<listitem>
<para>Deploy and configure SUSE Multi-Linux Manager Server.</para>
</listitem>
<listitem>
<para>Sync the SUSE Linux Micro package repositories.</para>
</listitem>
<listitem>
<para>Create system groups.</para>
</listitem>
<listitem>
<para>Create activation keys.</para>
</listitem>
<listitem>
<para>Use Edge Image Builder to prepare installation media for SUSE Multi-Linux Manager registration</para>
</listitem>
</itemizedlist>
<section xml:id="id-deploy-suse-multi-linux-manager-server">
<title>Deploy SUSE Multi-Linux Manager Server</title>
<para>If you already have an instance of SUSE Multi-Linux Manager 5.0.5 running, you can skip this step.</para>
<para>You can run SUSE Multi-Linux Manager Server on a dedicated physical server, as a virtual machine on your own hardware, or in the cloud. Pre-configured virtual machine images for SUSE Multi-Linux Server are provided for supported public clouds.</para>
<para>In this quick start we’re using the "qcow2" image <literal>SUSE-Manager-Server.x86_64-5.0.4-Qcow-5.0-2025-04.qcow2</literal> for AMD64/Intel 64 that you can find at <link xl:href="https://www.suse.com/download/suse-manager/">https://www.suse.com/download/suse-manager/</link> or in the SUSE Customer Center. This image will work as a virtual machine on hypervisors like KVM. Please always check for the newest version of the image and use it for new installations.</para>
<para>You can also install SUSE Multi-Linux Manager Server on any of the other supported hardware architectures. In that case pick the image that matches your hardware architecture.</para>
<para>Once you have downloaded the image, create a virtual machine that meets at least the following minimal hardware specifications:</para>
<itemizedlist>
<listitem>
<para>16 GB RAM</para>
</listitem>
<listitem>
<para>4 physical or virtual cores</para>
</listitem>
<listitem>
<para>an additional block device that has at least 100 GB</para>
</listitem>
</itemizedlist>
<para>With the qcow2 image, there is no need to install the operating system. You can directly attach the image as your root partition.</para>
<para>You need to set up the network so that your edge nodes can later access SUSE Multi-Linux Manager Server with a hostname that contains the fully qualified domain name ("FQDN")!</para>
<para>When you boot SUSE Multi-Linux Manager for the first time you need to perform some initial configuration:</para>
<itemizedlist>
<listitem>
<para>Select your keyboard layout</para>
</listitem>
<listitem>
<para>Accept the license agreement</para>
</listitem>
<listitem>
<para>Select your time zone</para>
</listitem>
<listitem>
<para>Enter the root password for the operating system</para>
</listitem>
</itemizedlist>
<para>The next steps need to be done as the "root" user:</para>
<para>For the next step you need the registration code for the SUSE Multi-Linux Manager Extension that you can find in the SUSE Customer Center. The same code can be used for both registering SUSE Linux Micro and SUSE Multi-Linux Manager:</para>
<para>Register SUSE Linux Micro:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -r &lt;REGCODE&gt; -e &lt;your_email&gt;</screen>
<para>Register SUSE Multi-Linux Manager:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -p SUSE-Manager-Server/5.0/x86_64 -r &lt;REGCODE&gt;</screen>
<para>The product string depends on your hardware architecture! For example, if you are using SUSE Multi-Linux Manager on a 64-bit Arm system, the string is "SUSE-Manager-Server/5.0/aarch64".</para>
<para>Reboot</para>
<para>Update the system:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update</screen>
<para>Unless there were no changes, reboot to apply the updates.</para>
<para>SUSE Multi-Linux Manager is provided via a container that is managed by Podman. The <literal>mgradm</literal> command handles the setup and configuration for you.</para>
<warning>
<para>It is very important that your SUSE Multi-Linux Manager Server has the hostname configured with a fully qualified domain name ("FQDN") that the edge nodes you want to manage can properly resolve in your network!</para>
</warning>
<para>Before you install and configure the SUSE Multi-Linux Manager Server container, you need to prepare the additional block device that you have previously added. For that, you need to know the name the virtual machine has given to the device. For example, if the block device is <literal>/dev/vdb</literal>, you can configure it to be used for SUSE Multi-Linux Manager using the following command:</para>
<screen language="shell" linenumbering="unnumbered">mgr-storage-server /dev/vdb</screen>
<para>Deploy SUSE Multi-Linux Manager:</para>
<screen language="shell" linenumbering="unnumbered">mgradm install podman &lt;FQDN&gt;</screen>
<para>Provide the password for the CA certificate. This password should be different from your login passwords. You usually don’t need to enter it later, but you should note it down.</para>
<para>Provide the password for the "admin" user. This is the initial user for logging into SUSE Multi-Linux Manager. You can create additional users with full or restricted rights later.</para>
</section>
<section xml:id="id-configure-suse-multi-linux-manager">
<title>Configure SUSE Multi-Linux Manager</title>
<para>Once the deployment has finished, you can log into the SUSE Multi-Linux Manager web UI using the host name you provided earlier. The initial user is "admin". Use the password you provided in the previous step.</para>
<para>For the next step you need your Organization Credentials that you can find on the 2nd sub-tab of the "Users" tab of your organization in SUSE Customer Center. With those credentials, SUSE Multi-Linux Manager can synchronize all the products that you have subscriptions for.</para>
<para>Select <literal>Admin &gt; Setup Wizard</literal>.</para>
<para>On the <literal>Organization Credentials</literal> tab create a new credential with your <literal>Username</literal> and <literal>Password</literal> that you found in the SUSE Customer Center.</para>
<para>Go to the next tab <literal>SUSE Products</literal>. You need to wait until the first data synchronization with SUSE Customer Center has finished.</para>
<para>Once the list is populated, you use the filter to only show "Micro 6.1".
Check the box for SUSE Linux Micro 6.1 for the hardware architecture your edge nodes will run on (<literal>x86_64</literal> or <literal>aarch64</literal>).</para>
<para>Click <literal>Add Products</literal>. This will add the main package repository ("channel") for SUSE Linux Micro and automatically add the channel for the SUSE Manager client tools as a sub-channel.</para>
<para>Depending on your Internet connection, the first synchronization will take a while. You can already start with the next steps:</para>
<para>Under <literal>Systems &gt; System Groups</literal>, create at least one group that your systems will automatically join when they are onboarded. Groups are an important way of categorizing systems, so you can apply configuration or actions to a whole set of systems at once. They are conceptionally similar to labels in Kubernetes.</para>
<para>Click <literal>+ Create Group</literal></para>
<para>Provide a short name, e.g., "Edge Nodes", and long description.</para>
<para>Under <literal>Systems &gt; Activation Keys</literal>, create at least one activation key. Activation keys can be thought of as a configuration profile that is automatically applied to systems when they are onboarded to SUSE Multi-Linux Manager. If you want certain edge nodes to be added to different groups or use different configuration, you can create separate activation keys for them and use them later in Edge Image Builder to create customized installation media.</para>
<para>A typical advanced use case for activation keys would be to assign your test clusters to the software channels with the latest updates and your production clusters to software channels that only get those latest updates once you’ve tested them in the test cluster.</para>
<para>Click <literal>+ Create Key</literal></para>
<para>Choose a short description, e.g., "Edge Nodes".
Provide a unique name that identifies the key, e.g., "edge-x86_64" for your edge nodes with AMD64/Intel 64 hardware architecture.
A number prefix is automatically added to the key. For the default organization, the number is always "1". If you create additional organizations in SUSE Multi-Linux Manager and create keys for them, that number may differ.</para>
<para>If you haven’t created any cloned software channels, you can keep the setting for the Base Channel to "SUSE Manager Default". This will automatically assign the correct SUSE update repository for your edge nodes.</para>
<para>As "Child Channel", select the "include recommended" slider for the hardware architecture your activation key is used for. This will add the "SUSE-Manager-Tools-For-SL-Micro-6.1" channel.</para>
<para>On the "Groups" tab, add the group you’ve created before. All nodes that are onboarded using this activation key will automatically be added to that group.</para>
</section>
<section xml:id="id-create-a-customized-installation-image-with-edge-image-builder">
<title>Create a customized installation image with Edge Image Builder</title>
<para>To use Edge Image Builder, you only need an environment where you can start a Linux-based container with podman.</para>
<para>For a minimal lab setup, we can actually use the same virtual machine SUSE Multi-Linux Manager Server is running on. Please make sure that you have enough disk space in the virtual machine! This is not a recommended setup for production use. See <xref linkend="id-prerequisites-2"/> for host operating systems we have tested Edge Image Builder with.</para>
<para>Log into your SUSE Multi-Linux Manager Server host as root.</para>
<para>Pull the Edge Image Builder container:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
<para>Create the directory <literal>/opt/eib</literal> and a sub-directory <literal>base-images</literal>:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p /opt/eib/base-images</screen>
<para>In this quickstart we’re using the "self-install" flavor of the SUSE Linux Micro image. That image can later be written to a physical USB thumb drive that you can use to install on physical servers. If your server has the option of remote attachment of installation ISOs via a BMC (Baseboard Management Controller), you can also use that approach. Finally that image can also be used with most virtualization tools.</para>
<para>If you either want to preload the image directly to a physical node or directly start it from a VM, you can also use the "raw" image flavor.</para>
<para>You can find those images in the SUSE Customer Center or on <link xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro/</link></para>
<para>Download or copy the image <literal>SL-Micro.x86_64-6.1-Default-SelfInstall-GM.install.iso</literal> to the <literal>base-images</literal> directoy and name it "slemicro.iso".</para>
<para>Building AArch64 images on an Arm-based build host is a technology preview in SUSE Edge 3.4. It will most likely work, but isn’t supported yet. If you want to try it out, you need to be running Podman on a 64-bit Arm machine, and you need to replace "x86_64" in all the examples and code snippets with "aarch64".</para>
<para>In <literal>/opt/eib</literal>, create a file called <literal>iso-definition.yaml</literal>. This is your build definition for Edge Image Builder.</para>
<para>Here is a simple example that installs SL Micro 6.1, sets a root password and the keymap, starts the Cockpit graphical UI and registers your node to SUSE Multi-Linux Manager:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
  - username: root
    createHomeDir: true
    encryptedPassword: $6$aaBTHyqDRUMY1HAp$pmBY7.qLtoVlCGj32XR/Ogei4cngc3f4OX7fwBD/gw7HWyuNBOKYbBWnJ4pvrYwH2WUtJLKMbinVtBhMDHQIY0
  keymap: de
  systemd:
    enable:
      - cockpit.socket
  packages:
    noGPGCheck: true
  suma:
    host: ${fully qualified hostname of your SUSE Multi-Linux Manager Server}
    activationKey: 1-edge-x86_64</screen>
<para>Edge Image Builder can also configure the network, automatically install Kubernetes on the node, and even deploy applications via Helm charts. See <xref linkend="quickstart-eib"/> for more comprehensive examples.</para>
<para>For <literal>baseImage</literal>, specify the actual name of the ISO in the <literal>base-images</literal> directory that you want to use.</para>
<para>In this example, the root password would be "root". See <xref linkend="id-configuring-os-users"/> for creating password hashes for the secure password you want to use.</para>
<para>Set the keymap to the actual keyboard layout you want the system to have after installation.</para>
<note>
<para>We use the option <literal>noGPGCheck: true</literal> because we aren’t going to provide a GPG key to check RPM packages. A comprehensive guide with a more secure setup that we recommend for production use can be found in the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">upstream installing packages guide</link>.</para>
</note>
<para>As mentioned several times, your SUSE Multi-Linux Manager host requires a fully qualified hostname that can be resolved in the network your edge nodes will boot into.</para>
<para>The value for <literal>activationKey</literal> needs to match the key you created in SUSE Multi-Linux Manager.</para>
<para>To build an installation image that automatically registers your edge nodes to SUSE Multi-Linux Manager after installation, you also need to prepare two artifacts:</para>
<itemizedlist>
<listitem>
<para>the Salt minion package that installs the management agent for SUSE Multi-Linux Manager</para>
</listitem>
<listitem>
<para>the CA certificate of your SUSE Multi-Linux Manager server</para>
</listitem>
</itemizedlist>
<section xml:id="id-download-the-venv-salt-minion-package">
<title>Download the venv-salt-minion package</title>
<para>In <literal>/opt/eib</literal>, create a subdirectory <literal>rpms</literal>.</para>
<para>Download the package <literal>venv-salt-minion</literal> from your SUSE Multi-Linux Manager server into that directory. You can either get it via the web UI by finding the package under <literal>Software &gt; Channel List</literal> and downloading it from the SUSE-Manager-Tools …​ channel or download it from the SUSE Multi-Linux Manager "bootstrap repo" with a tool like curl:</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/repositories/slmicro/6/1/bootstrap/x86_64/venv-salt-minion-3006.0-8.1.x86_64.rpm</screen>
<para>The actual package name may differ if a newer release has already been released. If there are multiple packages to choose from, always pick the latest.</para>
<para>To work around an issue documented in the <link xl:href="https://www.suse.com/releasenotes/x86_64/multi-linux-manager/5.1/index.html#_bootstrapping_sl_micro_6_1_clients">release notes</link> for SUSE Multi-Linux Manager, you also need to put the latest version of the build key package into the <literal>rpms</literal> directory (<literal>suse-build-key-12.0-slfo.1.1_3.1.noarch.rpm</literal> at the time this documentation was created). You can find it in the <literal>Software</literal> section of SUSE Multi-Linux Manager via the <literal>Packages</literal> tab of the Pool channel of SL Micro. There is a <literal>Download</literal> button in the <literal>Details</literal> view.</para>
</section>
</section>
<section xml:id="id-download-the-suse-multi-linux-manager-ca-certificate">
<title>Download the SUSE Multi-Linux Manager CA certificate</title>
<para>In <literal>/opt/eib</literal>, create a subdirectory <literal>certificates</literal></para>
<para>Download the CA certificate from SUSE Multi-Linux Manager into that directory:</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/RHN-ORG-TRUSTED-SSL-CERT</screen>
<warning>
<para>You have to rename the certificate to <literal>RHN-ORG-TRUSTED-SSL-CERT.crt</literal>. Edge Image Builder will then make sure that the certificate is installed and activated on the edge node during installation.</para>
</warning>
<para>Now you can run Edge Image Builder:</para>
<screen language="bash" linenumbering="unnumbered">cd /opt/eib
podman run --rm -it --privileged -v /opt/eib:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>If you have used a different name for your YAML definition file or want to use a different version of Edge Image Builder, you need to adapt the command accordingly.</para>
<para>After the build is finished, you’ll find the installation ISO in the <literal>/opt/eib</literal> directory as <literal>eib-image.iso</literal>.</para>
<para>That image can now be used to deploy nodes that will try to register with SUSE Multi-Linux Manager.</para>
<para>After the node has fully installed, you will see its key listed as <literal>pending</literal> in the <literal>Salt/Keys</literal> section of SUSE Multi-Linux Manager. Once you have accepted the key, the node will automatically be onboarded to SUSE Multi-Liux Manager and show up in the <literal>Systems</literal> list after that process is finished. It will have the system group(s) assigned that you provided in the activation key.</para>
<para>You should then schedule a reboot before applying any additional configuration.</para>
<para>Note that accepting the key can be fully automated using whitelists as described <link xl:href="https://docs.saltproject.io/en/latest/topics/tutorials/autoaccept_grains.html">here</link>.</para>
</section>
</chapter>
</part>
<part xml:id="id-components">
<title>Components</title>
<partintro>
<para>List of components for Edge</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para>See Rancher documentation at <link xl:href="https://ranchermanager.docs.rancher.com/v2.12">https://ranchermanager.docs.rancher.com/v2.12</link>.</para>
<blockquote>
<para>Rancher is a powerful open-source Kubernetes management platform that streamlines the deployment, operations and monitoring of Kubernetes clusters across multiple environments. Whether you manage clusters on premises, in the cloud, or at the edge, Rancher provides a unified and centralized platform for all your Kubernetes needs.</para>
</blockquote>
<section xml:id="id-key-features-of-rancher">
<title>Key Features of Rancher</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Multi-cluster management:</emphasis> Rancher’s intuitive interface lets you manage Kubernetes clusters from anywhere—public clouds, private data centers and edge locations.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Security and compliance:</emphasis> Rancher enforces security policies, role-based access control (RBAC), and compliance standards across your Kubernetes landscape.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Simplified cluster operations:</emphasis> Rancher automates cluster provisioning, upgrades and troubleshooting, simplifying Kubernetes operations for teams of all sizes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Centralized application catalog:</emphasis> The Rancher application catalog offers a diverse range of Helm charts and Kubernetes Operators, making it easy to deploy and manage containerized applications.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Continuous delivery:</emphasis> Rancher supports GitOps and CI/CD pipelines, enabling automated and streamlined application delivery processes.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>Rancher’s use in SUSE Edge</title>
<para>Rancher provides several core functionalities to the SUSE Edge stack:</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Centralized Kubernetes management</title>
<para>In typical edge deployments with numerous distributed clusters, Rancher acts as a central control plane for managing these Kubernetes clusters. It offers a unified interface for provisioning, upgrading, monitoring, and troubleshooting, simplifying operations, and ensuring consistency.</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>Simplified cluster deployment</title>
<para>Rancher streamlines Kubernetes cluster creation on the lightweight SUSE Linux Micro operating system, easing the rollout of edge infrastructure with robust Kubernetes capabilities.</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>Application deployment and management</title>
<para>The integrated Rancher application catalog can simplify deploying and managing containerized applications across SUSE Edge clusters, enabling seamless edge workload deployment.</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>Security and policy enforcement</title>
<para>Rancher provides policy-based governance tools, role-based access control (RBAC), and integration with external authentication providers. This helps SUSE Edge deployments maintain security and compliance, critical in distributed environments.</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>Best practices</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>Rancher includes Fleet as a built-in component to allow manage cluster configurations and application deployments with code stored in git.</para>
</section>
<section xml:id="id-observability">
<title>Observability</title>
<para>Rancher includes built-in monitoring and logging tools like Prometheus and Grafana for comprehensive insights into your cluster health and performance.</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize base SUSE Linux Micro OS images.
Follow <xref linkend="rancher-install"/> for an air-gapped installation of Rancher on top of Kubernetes clusters provisioned by EIB.</para>
</section>
<section xml:id="id-additional-resources-2">
<title>Additional Resources</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancher Documentation</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher Community</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Helm Charts</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Kubernetes Operators</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Rancher Dashboard Extensions</title>
<para>Extensions allow users, developers, partners, and customers to extend and enhance the Rancher UI. SUSE Edge provides KubeVirt and Akri dashboard extensions.</para>
<para>See <literal><link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions">Rancher documentation</link></literal> for general information about Rancher Dashboard Extensions.</para>
<section xml:id="id-installation">
<title>Installation</title>
<para>All of the SUSE Edge 3.4 components, including dashboard extensions, are distributed as OCI artifacts. To install SUSE Edge Extensions you can use Rancher Dashboard UI, Helm or Fleet:</para>
<section xml:id="id-installing-with-rancher-dashboard-ui">
<title>Installing with Rancher Dashboard UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>Click <emphasis role="strong">Extensions</emphasis> in the <emphasis role="strong">Configuration</emphasis> section of the navigation sidebar.</para>
</listitem>
<listitem>
<para>On the Extensions page, click the three dot menu at the top right and select <emphasis role="strong">Manage Repositories</emphasis>.</para>
<para>Each extension is distributed via its own OCI artifact. They are available from the SUSE Edge Helm charts repository.</para>
</listitem>
<listitem>
<para>On the <emphasis role="strong">Repositories page</emphasis>, click <literal>Create</literal>.</para>
</listitem>
<listitem>
<para>In the form, specify the repository name and URL, and click <literal>Create</literal>.</para>
<para>SUSE Edge Helm charts repository URL:
<literal>oci://registry.suse.com/edge/charts</literal></para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="dashboard-extensions-create-oci-repository.png" width="100%"/>
</imageobject>
<textobject><phrase>dashboard extensions create oci repository</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>You can see that the extension repository is added to the list and is in <literal>Active</literal> state.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="dashboard-extensions-repositories-list.png" width="100%"/>
</imageobject>
<textobject><phrase>dashboard extensions repositories list</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Navigate back to the <emphasis role="strong">Extensions</emphasis> in the <emphasis role="strong">Configuration</emphasis> section of the navigation sidebar.</para>
<para>In the <emphasis role="strong">Available</emphasis> tab you can see the extensions available for installation.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="dashboard-extensions-available-extensions.png" width="100%"/>
</imageobject>
<textobject><phrase>dashboard extensions available extensions</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>On the extension card click <literal>Install</literal> and confirm the installation.</para>
<para>Once the extension is installed Rancher UI prompts to reload the page as described in the <literal><link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions#installing-extensions">Installing Extensions Rancher documentation page</link></literal>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-helm">
<title>Installing with Helm</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension --version 304.0.2+up1.3.2 --namespace cattle-ui-plugin-system</screen>
<note>
<para>The extensions need to be installed in <literal>cattle-ui-plugin-system</literal> namespace.</para>
</note>
<note>
<para>After an extension is installed, Rancher Dashboard UI needs to be reloaded.</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Installing with Fleet</title>
<para>Installing Dashboard Extensions with Fleet requires defining a <literal>gitRepo</literal> resource which points to a Git repository with custom <literal>fleet.yaml</literal> bundle configuration file(s).</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension
  version: "304.0.2+up1.3.2"</screen>
<note>
<para>The <literal>releaseName</literal> property is required and needs to match the extension name to get the extension correctly installed.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>For more information, see <xref linkend="components-fleet"/> and the <literal><link xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal> repository.</para>
<para>Once the Extensions are installed they are listed in <emphasis role="strong">Extensions</emphasis> section under <emphasis role="strong">Installed</emphasis> tabs. Since they are not installed via Apps/Marketplace, they are marked with <literal>Third-Party</literal> label.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="installed-dashboard-extensions.png" width="100%"/>
</imageobject>
<textobject><phrase>installed dashboard extensions</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>KubeVirt Dashboard Extension</title>
<para>KubeVirt Extension provides basic virtual machine management for Rancher dashboard UI. Its capabilities are described in <xref linkend="kubevirt-dashboard-extension-usage"/>.</para>
</section>
</chapter>
<chapter xml:id="components-rancher-turtles">
<title>Rancher Turtles</title>
<para>See Rancher Turtles documentation at <link xl:href="https://documentation.suse.com/cloudnative/cluster-api/">https://documentation.suse.com/cloudnative/cluster-api/</link></para>
<blockquote>
<para>Rancher Turtles is a Kubernetes Operator that provides integration between Rancher Manager and Cluster API (CAPI) with the aim of bringing full CAPI support to Rancher</para>
</blockquote>
<section xml:id="id-key-features-of-rancher-turtles">
<title>Key Features of Rancher Turtles</title>
<itemizedlist>
<listitem>
<para>Automatically import CAPI clusters into Rancher, by installing the Rancher Cluster Agent in CAPI provisioned clusters.</para>
</listitem>
<listitem>
<para>Install and configure CAPI controller dependencies via the <link xl:href="https://cluster-api-operator.sigs.k8s.io/">CAPI Operator</link>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-rancher-turtles-use-in-suse-edge">
<title>Rancher Turtles use in SUSE Edge</title>
<para>The SUSE Edge stack provides a helm wrapper chart which installs Rancher Turtles with a specific configuration that enables:</para>
<itemizedlist>
<listitem>
<para>Core CAPI controller components</para>
</listitem>
<listitem>
<para>RKE2 Control Plane and Bootstrap provider components</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>) infrastructure provider components</para>
</listitem>
</itemizedlist>
<para>Only the default providers installed via the wrapper chart are supported - alternative Control Plane, Bootstrap and Infrastructure providers are not currently supported as part of the SUSE Edge stack.</para>
</section>
<section xml:id="id-installing-rancher-turtles">
<title>Installing Rancher Turtles</title>
<para>Rancher Turtles may be installed by following the Metal3 Quickstart (<xref linkend="quickstart-metal3"/>) guide, or the Management Cluster (<xref linkend="atip-management-cluster"/>) documentation.</para>
</section>
<section xml:id="id-additional-resources-3">
<title>Additional Resources</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancher Documentation</link></para>
</listitem>
<listitem>
<para><link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API Book</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link xl:href="https://fleet.rancher.io">Fleet</link> is a container management and deployment engine designed to offer users more control on the local cluster and constant monitoring through GitOps. Fleet focuses not only on the ability to scale, but it also gives users a high degree of control and visibility to monitor exactly what is installed on the cluster.</para>
<para>Fleet can manage deployments from Git of raw Kubernetes YAML, Helm charts, Kustomize, or any combination of the three. Regardless of the source, all resources are dynamically turned into Helm charts, and Helm is used as the engine to deploy all resources in the cluster. As a result, users can enjoy a high degree of control, consistency and auditability of their clusters.</para>
<para>For information about how Fleet works, see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/architecture">Fleet Architecture</link>.</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Installing Fleet with Helm</title>
<para>Fleet comes built-in to Rancher, but it can be also <link xl:href="https://fleet.rancher.io/installation">installed</link> as a standalone application on any Kubernetes cluster using Helm.</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>Using Fleet with Rancher</title>
<para>Rancher uses Fleet to deploy applications across managed clusters. Continuous delivery with Fleet introduces GitOps at scale, designed to manage applications running on large numbers of clusters.</para>
<para>Fleet shines as an integrated part of Rancher. Clusters managed with Rancher automatically get the Fleet agent deployed as part of the installation/import process and the cluster is immediately available to be managed by Fleet.</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Accessing Fleet in the Rancher UI</title>
<para>Fleet comes preinstalled in Rancher and is managed by the <emphasis role="strong">Continuous Delivery</emphasis> option in the Rancher UI.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-dashboard.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Continuous Delivery section consists of following items:</para>
<section xml:id="id-dashboard">
<title>Dashboard</title>
<para>An overview page of all GitOps repositories across all workspaces. Only the workspaces with repositories are displayed.</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos</title>
<para>A list of GitOps repositories in the selected workspace. Select the active workspace using the dropdown list at the top of the page.</para>
</section>
<section xml:id="id-clusters">
<title>Clusters</title>
<para>A list of managed clusters. By default, all Rancher-managed clusters are added to the <literal>fleet-default</literal> workspace. <literal>fleet-local</literal> workspace includes the local (management) cluster. From here, it is possible to <literal>Pause</literal> or <literal>Force update</literal> the clusters or move the cluster into another workspace. Editing the cluster allows to update labels and annotations used for grouping the clusters.</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups</title>
<para>This section allows custom grouping of the clusters within the workspace using selectors.</para>
</section>
<section xml:id="id-advanced">
<title>Advanced</title>
<para>The "Advanced" section allows to manage workspaces and other related Fleet resources.</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Example of installing KubeVirt with Rancher and Fleet using Rancher dashboard</title>
<orderedlist numeration="arabic">
<listitem>
<para>Create a Git repository containing the <literal>fleet.yaml</literal> file:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/charts/kubevirt"
  version: "304.0.1+up0.6.0"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>In the Rancher dashboard, navigate to <emphasis role="strong">☰ &gt; Continuous Delivery &gt; Git Repos</emphasis> and click <literal>Add Repository</literal>.</para>
</listitem>
<listitem>
<para>The Repository creation wizard guides through creation of the Git repo. Provide <emphasis role="strong">Name</emphasis>, <emphasis role="strong">Repository URL</emphasis> (referencing the Git repository created in the previous step) and select the appropriate branch or revision. In the case of a more complex repository, specify <emphasis role="strong">Paths</emphasis> to use multiple directories in a single repository.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-create-repo1.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet create repo1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Click <literal>Next</literal>.</para>
</listitem>
<listitem>
<para>In the next step, you can define where the workloads will get deployed. Cluster selection offers several basic options: you can select no clusters, all clusters, or directly choose a specific managed cluster or cluster group (if defined). The "Advanced" option allows to directly edit the selectors via YAML.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-create-repo2.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet create repo2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Click <literal>Create</literal>. The repository gets created. From now on, the workloads are installed and kept in sync on the clusters matching the repository definition.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>Debugging and troubleshooting</title>
<para>The "Advanced" navigation section provides overviews of lower-level Fleet resources. <link xl:href="https://fleet.rancher.io/ref-bundle-stages">A bundle</link> is an internal resource used for the orchestration of resources from Git. When a Git repo is scanned, it produces one or more bundles.</para>
<para>To find bundles relevant to a specific repository, go to the Git repo detail page and click the <literal>Bundles</literal> tab.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-repo-bundles.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet repo bundles</phrase></textobject>
</mediaobject>
</informalfigure>
<para>For each cluster, the bundle is applied to a BundleDeployment resource that is created. To view BundleDeployment details, click the <literal>Graph</literal> button in the upper right of the Git repo detail page.
A graph of <emphasis role="strong">Repo &gt; Bundles &gt; BundleDeployments</emphasis> is loaded. Click the BundleDeployment in the graph to see its details and click the <literal>Id</literal> to view the BundleDeployment YAML.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-repo-graph.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet repo graph</phrase></textobject>
</mediaobject>
</informalfigure>
<para>For additional information on Fleet troubleshooting tips, refer <link xl:href="https://fleet.rancher.io/troubleshooting">here</link>.</para>
</section>
<section xml:id="id-fleet-examples">
<title>Fleet examples</title>
<para>The Edge team maintains a <link xl:href="https://github.com/suse-edge/fleet-examples">repository</link> with examples of installing Edge projects with Fleet.</para>
<para>The Fleet project includes a <link xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link> repository that covers all use cases for <link xl:href="https://fleet.rancher.io/gitrepo-content">Git repository structure</link>.</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SUSE Linux Micro</title>
<para>See <link xl:href="https://documentation.suse.com/sle-micro/6.1/">SUSE Linux Micro official documentation</link></para>
<blockquote>
<para>SUSE Linux Micro is a lightweight and secure operating system for the edge. It merges the enterprise-hardened components of SUSE Linux Enterprise with the features that developers want in a modern, immutable operating system. As a result, you get a reliable infrastructure platform with best-in-class compliance that is also simple to use.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-suse-linux-micro">
<title>How does SUSE Edge use SUSE Linux Micro?</title>
<para>We use SUSE Linux Micro as the base operating system for our platform stack. This provides us with a secure, stable and minimal base for building upon.</para>
<para>SUSE Linux Micro is unique in its use of file system (Btrfs) snapshots to allow for easy rollbacks in case something goes wrong with an upgrade. This allows for secure remote upgrades for the entire platform even without physical access in case of issues.</para>
</section>
<section xml:id="id-best-practices-2">
<title>Best practices</title>
<section xml:id="id-installation-media">
<title>Installation media</title>
<para>SUSE Edge uses the Edge Image Builder (<xref linkend="components-eib"/>) to preconfigure the SUSE Linux Micro self-install installation image.</para>
</section>
<section xml:id="id-local-administration">
<title>Local administration</title>
<para>SUSE Linux Micro comes with Cockpit to allow the local management of the host through a Web application.</para>
<para>This service is disabled by default but can be started by enabling the systemd service <literal>cockpit.socket</literal>.</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>There is no desktop environment available in SUSE Linux Micro at the moment but a containerized solution is in development.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link xl:href="https://metal3.io/">Metal<superscript>3</superscript></link> is a CNCF project which provides bare-metal infrastructure
management capabilities for Kubernetes.</para>
<para>Metal<superscript>3</superscript> provides Kubernetes-native resources to manage the lifecycle of bare-metal servers
which support management via out-of-band protocols such as <link xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>It also has mature support for <link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link> which enables management
of infrastructure resources across multiple infrastructure providers via broadly adopted vendor-neutral APIs.</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>How does SUSE Edge use Metal<superscript>3</superscript>?</title>
<para>This method is useful for scenarios where the target hardware supports out-of-band management, and a fully automated
infrastructure management flow is desired.</para>
<para>This method provides declarative APIs that enable inventory and state management of bare-metal servers, including
automated inspection, cleaning and provisioning/deprovisioning.</para>
</section>
<section xml:id="id-known-issues-3">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>The upstream <link xl:href="https://github.com/metal3-io/ip-address-manager">IP Address Management controller</link> is currently not supported, because it is not yet compatible with our choice of network configuration tooling.</para>
</listitem>
<listitem>
<para>Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are not supported.</para>
</listitem>
<listitem>
<para>Only deployment via redfish-virtualmedia is currently supported.</para>
</listitem>
<listitem>
<para>It is possible to observe a network device name misalignment between the ironic python agent (IPA) and the target operating system (SL Micro 6.0/6.1), especially when trying to configure predictable names for the devices.</para>
</listitem>
</itemizedlist>
<para>This happens because the kernel of the ironic python agent (IPA) is not currently aligned with the kernel of the target operating system (SL Micro 6.0/6.1), therefore there’s a misalignment in the network drivers that allows the IPA to discover network devices in a different naming pattern than SL Micro expects.</para>
<para>There are two different approaches to be used as a workaround in the meantime:
* Create two different secrets with the network configuration, one to be used with the IPA using the device names as IPA will discover and use it as <literal>preprovisioningNetworkDataName</literal> on the <literal>BareMetalHost</literal> definition and another secret with the device names as SL Micro will discover and use it as <literal>networkData.name</literal> on the <literal>BareMetalHost</literal> definition.
* Use the UUIDs to reference other interfaces on the generated nmconnection files instead.
More details in the <link xl:href="..tips/metal3.adoc">tips and tricks</link> section.</para>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para>See the <link xl:href="https://github.com/suse-edge/edge-image-builder">Official Repository</link>.</para>
<para>Edge Image Builder (EIB) is a tool that streamlines the generation of Customized, Ready-to-Boot (CRB) disk images for bootstrapping machines. These images enable the end-to-end deployment of the entire SUSE software stack with a single image.</para>
<para>Whilst EIB can create CRB images for all provisioning scenarios, EIB demonstrates a tremendous value in air-gapped deployments with limited or completely isolated networks.</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>How does SUSE Edge use Edge Image Builder?</title>
<para>SUSE Edge uses EIB for the simplified and quick configuration of customized SUSE Linux Micro images for a variety of scenarios. These scenarios include the bootstrapping of virtual and bare-metal machines with:</para>
<itemizedlist>
<listitem>
<para>Fully air-gapped deployments of K3s/RKE2 Kubernetes (single &amp; multi-node)</para>
</listitem>
<listitem>
<para>Fully air-gapped Helm chart and Kubernetes manifest deployments</para>
</listitem>
<listitem>
<para>Registration to Rancher via Elemental API</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>Customized networking (for example, static IP, host name, VLAN’s, bonding, etc.)</para>
</listitem>
<listitem>
<para>Customized operating system configurations (for example, users, groups, passwords, SSH keys, proxies, NTP, custom SSL certificates, etc.)</para>
</listitem>
<listitem>
<para>Air-gapped installation of host-level and side-loaded RPM packages (including dependency resolution)</para>
</listitem>
<listitem>
<para>Registration to SUSE Multi-Linux Manager for OS management</para>
</listitem>
<listitem>
<para>Embedded container images</para>
</listitem>
<listitem>
<para>Kernel command-line arguments</para>
</listitem>
<listitem>
<para>Systemd units to be enabled/disabled at boot time</para>
</listitem>
<listitem>
<para>Custom scripts and files for any manual tasks</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>Getting started</title>
<para>Comprehensive documentation for the usage and testing of Edge Image Builder can be found <link xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.3/docs">here</link>.</para>
<para>Additionally, see <xref linkend="quickstart-eib"/> covering a basic deployment scenario.</para>
<para>Once you are familiar with this tool, please find some more useful information on our EIB Tips and Tricks section (<xref linkend="tips-and-tricks"/>) page.</para>
</section>
<section xml:id="id-known-issues-4">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>EIB air-gaps Helm charts through templating the Helm charts and parsing all the images within the template. If a Helm chart does not keep all of its images within the template and instead side-loads the images, EIB will not be able to air-gap those images automatically. The solution to this is to manually add any undetected images to the <literal>embeddedArtifactRegistry</literal> section of the definition file.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Edge Networking</title>
<para>This section describes the approach to network configuration in the SUSE Edge solution.
We will show how to configure NetworkManager on SUSE Linux Micro in a declarative manner, and explain how the related tools are integrated.</para>
<section xml:id="id-overview-of-networkmanager">
<title>Overview of NetworkManager</title>
<para>NetworkManager is a tool that manages the primary network connection and other connection interfaces.</para>
<para>NetworkManager stores network configurations as connection files that contain the desired state.
These connections are stored as files in the <literal>/etc/NetworkManager/system-connections/</literal> directory.</para>
<para>Details about NetworkManager can be found in the <link xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-network-configuration/index.html">SUSE Linux Micro documentation</link>.</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>Overview of nmstate</title>
<para>nmstate is a widely adopted library (with an accompanying CLI tool) which offers a declarative API for network configurations via a predefined schema.</para>
<para>Details about nmstate can be found in the <link xl:href="https://nmstate.io/">upstream documentation</link>.</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>Enter: NetworkManager Configurator (nmc)</title>
<para>The network customization options available in SUSE Edge are achieved via a CLI tool called NetworkManager Configurator or <emphasis>nmc</emphasis> for short.
It is leveraging the functionality provided by the nmstate library and, as such, it is fully capable of configuring static IP addresses, DNS servers, VLANs, bonding, bridges, etc.
This tool allows us to generate network configurations from predefined desired states and to apply those across many different nodes in an automated fashion.</para>
<para>Details about the NetworkManager Configurator (nmc) can be found in the <link xl:href="https://github.com/suse-edge/nm-configurator">upstream repository</link>.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>How does SUSE Edge use NetworkManager Configurator?</title>
<para>SUSE Edge utilizes <emphasis>nmc</emphasis> for the network customizations in the various different provisioning models:</para>
<itemizedlist>
<listitem>
<para>Custom network configurations in the Directed Network Provisioning scenarios (<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>Declarative static configurations in the Image Based Provisioning scenarios (<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Configuring with Edge Image Builder</title>
<para>Edge Image Builder (EIB) is a tool which enables configuring multiple hosts with a single OS image.
In this section we’ll show how you can use a declarative approach to describe the desired network states, how those are converted to the respective NetworkManager connections, and are then applied during the provisioning process.</para>
<section xml:id="id-prerequisites-3">
<title>Prerequisites</title>
<para>If you’re following this guide, it’s assumed that you’ve got the following already available:</para>
<itemizedlist>
<listitem>
<para>An AMD64/Intel 64 physical host (or virtual machine) running SLES 15 SP6 or openSUSE Leap 15.6</para>
</listitem>
<listitem>
<para>An available container runtime (e.g. Podman)</para>
</listitem>
<listitem>
<para>A copy of the SUSE Linux Micro 6.1 RAW image found <link xl:href="https://www.suse.com/download/sle-micro/">here</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Getting the Edge Image Builder container image</title>
<para>The EIB container image is publicly available and can be downloaded from the SUSE Edge registry by running:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>Creating the image configuration directory</title>
<para>Let’s start with creating the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>We will now ensure that the downloaded base image copy is moved over to the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SL-Micro.x86_64-6.1-Base-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIB is never going to modify the base image input. It will create a new image with its modifications.</para>
</note>
</blockquote>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>Creating the image definition file</title>
<para>The definition file describes the majority of configurable options that the Edge Image Builder supports.</para>
<para>Let’s start with a very basic definition file for our OS image:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.3
image:
  arch: x86_64
  imageType: raw
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para>The <literal>image</literal> section is required, and it specifies the input image, its architecture and type, as well as what the output image will be called.
The <literal>operatingSystem</literal> section is optional, and contains configuration to enable login on the provisioned systems with the <literal>root/eib</literal> username/password.</para>
<blockquote>
<note>
<para>Feel free to use your own encrypted password by running <literal>openssl passwd -6 &lt;password&gt;</literal>.</para>
</note>
</blockquote>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>Defining the network configurations</title>
<para>The desired network configurations are not part of the image definition file that we just created.
We’ll now populate those under the special <literal>network/</literal> directory. Let’s create it:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>As previously mentioned, the NetworkManager Configurator (<emphasis>nmc</emphasis>) tool expects an input in the form of predefined schema.
You can find how to set up a wide variety of different networking options in the <link xl:href="https://nmstate.io/examples.html">upstream NMState examples documentation</link>.</para>
<para>This guide will explain how to configure the networking on three different nodes:</para>
<itemizedlist>
<listitem>
<para>A node which uses two Ethernet interfaces</para>
</listitem>
<listitem>
<para>A node which uses network bonding</para>
</listitem>
<listitem>
<para>A node which uses a network bridge</para>
</listitem>
</itemizedlist>
<warning>
<para>Using completely different network setups is not recommended in production builds, especially if configuring Kubernetes clusters.
Networking configurations should generally be homogeneous amongst nodes or at least amongst roles within a given cluster.
This guide is including various different options only to serve as an example reference.</para>
</warning>
<blockquote>
<note>
<para>The following assumes a default <literal>libvirt</literal> network with an IP address range <literal>192.168.122.1/24</literal>.
Adjust accordingly if this differs in your environment.</para>
</note>
</blockquote>
<para>Let’s create the desired states for the first node which we will call <literal>node1.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>In this example we define a desired state of two Ethernet interfaces (eth0 and eth3), their requested IP addresses, routing, and DNS resolution.</para>
<warning>
<para>You must ensure that the MAC addresses of all Ethernet interfaces are listed.
Those are used during the provisioning process as the identifiers of the nodes and serve to determine which configurations should be applied.
This is how we are able to configure multiple nodes using a single ISO or RAW image.</para>
</warning>
<para>Next up is the second node which we will call <literal>node2.suse.com</literal> and which will use network bonding:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>In this example we define a desired state of two Ethernet interfaces (eth0 and eth1) which are not enabling IP addressing,
as well as a bond with a round-robin policy and its respective address which is going to be used to forward the network traffic.</para>
<para>Lastly, we’ll create the third and final desired state file which will be utilizing a network bridge and which we’ll call <literal>node3.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<blockquote>
<note>
<para>The names of the files under the <literal>network/</literal> directory are intentional.
They correspond to the hostnames which will be set during the provisioning process.</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>Building the OS image</title>
<para>Now that all the necessary configurations are in place, we can build the image by simply running:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>The snippet above tells us that the <literal>Network</literal> component has successfully been configured, and we can proceed with provisioning our edge nodes.</para>
<blockquote>
<note>
<para>A log file (<literal>network-config.log</literal>) and the respective NetworkManager connection files can be inspected
in the resulting <literal>_build</literal> directory under a timestamped directory for the image run.</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>Provisioning the edge nodes</title>
<para>Let’s copy the resulting RAW image:</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>You will notice that we copied the built image four times but only specified the network configurations for three nodes.
This is because we also want to showcase what will happen if we provision a node which does not match any of the desired configurations.</para>
<blockquote>
<note>
<para>This guide will use virtualization for the node provisioning examples. Ensure the necessary extensions are enabled
in the BIOS (see <link xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">here</link> for details).</para>
</note>
</blockquote>
<para>We will be using <literal>virt-install</literal> to create virtual machines using the copied raw disks.
Each virtual machine will be using 10 GB of RAM and 6 vCPUs.</para>
<section xml:id="id-provisioning-the-first-node">
<title>Provisioning the first node</title>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>It is important that we create the network interfaces with the same MAC addresses as the ones in the desired state we described above.</para>
</note>
</blockquote>
<para>Once the operation is complete, we will see something similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Micro 6.0 (x86_64) - Kernel 6.4.0-18-default (tty1).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>We’re now able to log in with the <literal>root:eib</literal> credentials pair.
We’re also able to SSH into the host if we prefer that over the <literal>virsh console</literal> we’re presented with here.</para>
<para>Once logged in, let’s confirm that all the settings are in place.</para>
<para>Verify that the hostname is properly set:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>Verify that the routing is properly configured:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>Verify that Internet connection is available:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>Verify that exactly two Ethernet interfaces are configured and only one of those is active:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>You’ll notice that the second interface is <literal>eth1</literal> instead of the predefined <literal>eth3</literal> in our desired networking state.
This is the case because the NetworkManager Configurator (<emphasis>nmc</emphasis>) is able to detect that the OS has given a different name for the NIC with MAC address <literal>34:8a:b1:4b:16:e2</literal> and it adjusts its settings accordingly.</para>
<para>Verify this has indeed happened by inspecting the Combustion phase of the provisioning:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>We will now provision the rest of the nodes, but we will only show the differences in the final configuration.
Feel free to apply any or all of the above checks for all nodes you are about to provision.</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>Provisioning the second node</title>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>Once the virtual machine is up and running, we can confirm that this node is using bonded interfaces:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>Confirm that the routing is using the bond:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>Ensure that the static connection files are properly utilized:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>Provisioning the third node</title>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>Once the virtual machine is up and running, we can confirm that this node is using a network bridge:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>Confirm that the routing is using the bridge:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>Ensure that the static connection files are properly utilized:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>Provisioning the fourth node</title>
<para>Lastly, we will provision a node which will not match any of the predefined configurations by a MAC address.
In these cases, we will default to DHCP to configure the network interfaces.</para>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>Once the virtual machine is up and running, we can confirm that this node is using a random IP address for its network interface:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>Verify that nmc failed to apply static configurations for this node:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>Verify that the Ethernet interface was configured via DHCP:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="networking-unified">
<title>Unified node configurations</title>
<para>There are occasions where relying on known MAC addresses is not an option. In these cases we can opt for the so-called <emphasis>unified configuration</emphasis>
which allows us to specify settings in an <literal>_all.yaml</literal> file which will then be applied across all provisioned nodes.</para>
<para>We will build and provision an edge node using different configuration structure. Follow all steps starting from <xref linkend="image-config-dir-creation"/> up until <xref linkend="default-network-definition"/>.</para>
<para>In this example we define a desired state of two Ethernet interfaces (eth0 and eth1) - one using DHCP, and one assigned a static IP address.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>Once the image is successfully built, let’s create a virtual machine using it:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>The provisioning process might take a few minutes. Once it’s finished, log in to the system with the provided credentials.</para>
<para>Verify that the routing is properly configured:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>Verify that Internet connection is available:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Verify that the Ethernet interfaces are configured and active:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>Custom network configurations</title>
<para>We have already covered the default network configuration for Edge Image Builder which relies on the NetworkManager Configurator.
However, there is also the option to modify it via a custom script. Whilst this option is very flexible and is also not MAC address dependant,
its limitation stems from the fact that using it is much less convenient when bootstrapping multiple nodes with a single image.</para>
<blockquote>
<note>
<para>It is recommended to use the default network configuration via files describing the desired network states under the <literal>/network</literal> directory.
Only opt for custom scripting when that behaviour is not applicable to your use case.</para>
</note>
</blockquote>
<para>We will build and provision an edge node using different configuration structure. Follow all steps starting from <xref linkend="image-config-dir-creation"/> up until <xref linkend="default-network-definition"/>.</para>
<para>In this example, we will create a custom script which applies static configuration for the <literal>eth0</literal> interface on all provisioned nodes,
as well as removing and disabling the automatically created wired connections by NetworkManager. This is beneficial in situations where you want to make sure that every node in your cluster has an identical networking configuration, and as such you do not need to be concerned with the MAC address of each node prior to image creation.</para>
<para>Let’s start by storing the connection file in the <literal>/custom/files</literal> directory:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>Now that the static configuration is created, we will also create our custom network script:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>The nmc binary will still be included by default, so it can also be used in the <literal>configure-network.sh</literal> script if necessary.</para>
</note>
</blockquote>
<warning>
<para>The custom script must always be provided under <literal>/network/configure-network.sh</literal> in the configuration directory. If present, all other files will be ignored.
It is NOT possible to configure a network by working with both static configurations in YAML format and a custom script simultaneously.</para>
</warning>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>Once the image is successfully built, let’s create a virtual machine using it:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>The provisioning process might take a few minutes. Once it’s finished, log in to the system with the provided credentials.</para>
<para>Verify that the routing is properly configured:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>Verify that Internet connection is available:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>Verify that an Ethernet interface is statically configured using our connection file and is active:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elemental is a software stack enabling centralized and full cloud-native OS management with Kubernetes. The Elemental stack consists of a number of components that either reside on Rancher itself, or on the edge nodes. The core components are:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> - The core operator that resides on Rancher and handles registration requests from clients.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis> - The client that runs on the edge nodes allowing registration via the <literal>elemental-operator</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis> - An agent that resides on the edge nodes; its configuration is fed from <literal>elemental-register</literal> and it receives a <literal>plan</literal> for configuring the <literal>rancher-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis> - Once the edge node has fully registered, this takes over from <literal>elemental-system-agent</literal> and waits for further <literal>plans</literal> from Rancher Manager (e.g. for Kubernetes installation).</para>
</listitem>
</itemizedlist>
<para>See <link xl:href="https://elemental.docs.rancher.com/">Elemental upstream documentation</link> for full information about Elemental and its relationship to Rancher.</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>How does SUSE Edge use Elemental?</title>
<para>We use portions of Elemental for managing remote devices where Metal<superscript>3</superscript> is not an option (for example, there is no BMC, or the device is behind a NAT gateway). This tooling allows for an operator to bootstrap their devices in a lab before knowing when or where they will be shipped to. Namely, we leverage the <literal>elemental-register</literal> and <literal>elemental-system-agent</literal> components to enable the onboarding of SUSE Linux Micro hosts to Rancher for "phone home" network provisioning use-cases. When using Edge Image Builder (EIB) to create deployment images, the automatic registration through Rancher via Elemental can be achieved by specifying the registration configuration in the configuration directory for EIB.</para>
<note>
<para>In SUSE Edge 3.4 we do <emphasis role="strong">not</emphasis> leverage the operating system management aspects of Elemental, and therefore it’s not possible to manage your operating system patching via Rancher. Instead of using the Elemental tools to build deployment images, SUSE Edge uses the Edge Image Builder tooling, which consumes the registration configuration.</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>Best practices</title>
<section xml:id="id-installation-media-2">
<title>Installation media</title>
<para>The SUSE Edge recommended way of building deployments image that can leverage Elemental for registration to Rancher in the "phone home network provisioning" deployment footprint is to follow the instructions detailed in the remote host onboarding with Elemental (<xref linkend="quickstart-elemental"/>) quickstart.</para>
</section>
<section xml:id="id-labels">
<title>Labels</title>
<para>Elemental tracks its inventory with the <literal>MachineInventory</literal> CRD and provides a way to select inventory, e.g. for selecting machines to deploy Kubernetes clusters to, based on labels. This provides a way for users to predefine most (if not all) of their infrastructure needs prior to hardware even being purchased. Also, since nodes can add/remove labels on their respective inventory object (by re-running <literal>elemental-register</literal> with the additional flag <literal>--label "FOO=BAR"</literal>), we can write scripts that will discover and let Rancher know where a node is booted.</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>The Elemental UI does not currently know how to build installation media or update non-"Elemental Teal" operating systems. This should be addressed in future releases.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link xl:href="https://k3s.io/">K3s</link> is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances.</para>
<para>It is packaged as a single and small binary, so installations and updates are fast and easy.</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>How does SUSE Edge use K3s</title>
<para>K3s can be used as the Kubernetes distribution backing the SUSE Edge stack.
It is meant to be installed on a SUSE Linux Micro operating system.</para>
<para>Using K3s as the SUSE Edge stack Kubernetes distribution is only recommended when etcd as a backend does not fit your constraints. If etcd as a backend is possible, it is better to use RKE2 (<xref linkend="components-rke2"/>).</para>
</section>
<section xml:id="id-best-practices-4">
<title>Best practices</title>
<section xml:id="id-installation-2">
<title>Installation</title>
<para>The recommended way of installing K3s as part of the SUSE Edge stack is by using Edge Image Builder (EIB). See its documentation (<xref linkend="components-eib"/>) for more details on how to configure it to deploy K3s.</para>
<para>It automatically supports HA setup, as well as Elemental setup.</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>Fleet for GitOps workflow</title>
<para>The SUSE Edge stack uses Fleet as its preferred GitOps tool.
For more information around its installation and use, refer to the Fleet section (<xref linkend="components-fleet"/>) in this documentation.</para>
</section>
<section xml:id="id-storage-management">
<title>Storage management</title>
<para>K3s comes with local-path storage preconfigured, which is suitable for single-node clusters.
For clusters spanning over multiple nodes, we recommend using SUSE Storage (<xref linkend="components-suse-storage"/>).</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>Load balancing and HA</title>
<para>If you installed K3s using EIB, this part is already covered by the EIB documentation in the HA section.</para>
<para>Otherwise, you need to install and configure MetalLB as per our MetalLB documentation (<xref linkend="guides-metallb-k3s"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para>See <link xl:href="https://docs.rke2.io/">RKE2 official documentation</link>.</para>
<para>RKE2 is a fully conformant Kubernetes distribution that focuses on security and compliance by:</para>
<itemizedlist>
<listitem>
<para>Providing defaults and configuration options that allow clusters to pass the CIS Kubernetes Benchmark v1.6 or v1.23 with minimal operator intervention</para>
</listitem>
<listitem>
<para>Enabling FIPS 140-2 compliance</para>
</listitem>
<listitem>
<para>Regularly scanning components for CVEs using <link xl:href="https://trivy.dev">trivy</link> in the RKE2 build pipeline</para>
</listitem>
</itemizedlist>
<para>RKE2 launches control plane components as static pods, managed by kubelet. The embedded container runtime is containerd.</para>
<para>Note: RKE2 is also known as RKE Government in order to convey another use case and sector it currently targets.</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2 vs K3s</title>
<para>K3s is a fully compliant and lightweight Kubernetes distribution focused on Edge, IoT, ARM - optimized for ease of use and resource constrained environments.</para>
<para>RKE2 combines the best of both worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s.</para>
<para>From K3s, it inherits the usability, ease of operation and deployment model.</para>
<para>From RKE1, it inherits close alignment with upstream Kubernetes. In places, K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>How does SUSE Edge use RKE2?</title>
<para>RKE2 is a fundamental piece of the SUSE Edge stack. It sits on top of
SUSE Linux Micro (<xref linkend="components-slmicro"/>), providing a standard Kubernetes interface required to deploy Edge workloads.</para>
</section>
<section xml:id="id-best-practices-5">
<title>Best practices</title>
<section xml:id="id-installation-3">
<title>Installation</title>
<para>The recommended way of installing RKE2 as part of the SUSE Edge stack is by using Edge Image Builder (EIB). See the EIB documentation (<xref linkend="components-eib"/>) for more details on how to configure it to deploy RKE2.</para>
<para>EIB is flexible enough to support any parameter required by RKE2, such as specifying the RKE2 version, the <link xl:href="https://docs.rke2.io/reference/server_config">servers</link> or the <link xl:href="https://docs.rke2.io/reference/linux_agent_config">agents</link> configuration, covering all the Edge use cases.</para>
<para>For other use cases involving Metal<superscript>3</superscript>, RKE2 is also being used and installed. In those particular cases, the <link xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">Cluster API Provider RKE2</link> automatically deploys RKE2 on clusters being provisioned with Metal<superscript>3</superscript> using the Edge Stack.</para>
<para>In those cases, the RKE2 configuration must be applied on the different CRDs involved. An example of how to provide a different CNI using the <literal>RKE2ControlPlane</literal> CRD looks like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>For more information about the Metal<superscript>3</superscript> use cases, see <xref linkend="components-metal3"/>.</para>
</section>
<section xml:id="id-high-availability">
<title>High availability</title>
<para>For HA deployments, EIB automatically deploys and configures
MetalLB (<xref linkend="components-metallb"/>) and the Endpoint Copier Operator (<xref linkend="components-eco"/>) to expose the RKE2 API endpoint externally.</para>
</section>
<section xml:id="id-networking">
<title>Networking</title>
<para>SUSE Edge Stack supports <link xl:href="https://docs.cilium.io/en/stable/">Cilium</link>, <link xl:href="https://docs.tigera.io/calico/latest/about/">Calico</link>, with Cilium as its default CNI. <link xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link> meta-plugin can also be used when pods require multiple network interfaces. RKE2 standalone supports <link xl:href="https://docs.rke2.io/install/network_options">a wider range of CNI options</link>.</para>
</section>
<section xml:id="id-storage">
<title>Storage</title>
<para>RKE2 does not provide any kind of persistent storage class or operators. For clusters spanning over multiple nodes, it is recommended to use SUSE Storage (<xref linkend="components-suse-storage"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-suse-storage">
<title><link xl:href="https://www.suse.com/products/rancher/storage/">SUSE Storage</link></title>
<para>SUSE Storage is a lightweight, reliable, and user-friendly distributed block storage system designed for Kubernetes. It is a product based on Longhorn, an open-source project initially developed by Rancher Labs and currently incubated under the CNCF.</para>
<section xml:id="id-prerequisites-4">
<title>Prerequisites</title>
<para>If you are following this guide, it assumes that you have the following already available:</para>
<itemizedlist>
<listitem>
<para>At least one host with SUSE Linux Micro 6.1 installed; this can be physical or virtual</para>
</listitem>
<listitem>
<para>A Kubernetes cluster installed; either K3s or RKE2</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-suse-storage">
<title>Manual installation of SUSE Storage</title>
<section xml:id="id-installing-open-iscsi">
<title>Installing Open-iSCSI</title>
<para>A core requirement of deploying and using SUSE Storage is the installation of the <literal>open-iscsi</literal> package and the <literal>iscsid</literal> daemon running on all Kubernetes nodes.
This is necessary, since Longhorn relies on <literal>iscsiadm</literal> on the host to provide persistent volumes to Kubernetes.</para>
<para>Let’s install it:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>It is important to note that once the operation is completed, the package is only installed into a new snapshot as SUSE Linux Micro is an immutable operating system.
In order to load it and for the <literal>iscsid</literal> daemon to start running, we must reboot into that new snapshot that we just created.
Issue the reboot command when you are ready:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>For additional help installing open-iscsi, refer to the <link xl:href="https://longhorn.io/docs/1.9.1/deploy/install/#installing-open-iscsi">official Longhorn documentation</link>.</para>
</tip>
</section>
<section xml:id="id-installing-suse-storage">
<title>Installing SUSE Storage</title>
<para>There are several ways to install SUSE Storage on your Kubernetes clusters.
This guide will follow through the Helm installation, however feel free to follow the <link xl:href="https://longhorn.io/docs/1.9.1/deploy/install/">official documentation</link> if another approach is desired.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Add the Rancher Charts Helm repository:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>Fetch the latest charts from the repository:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para>Install SUSE Storage in the <literal>longhorn-system</literal> namespace:</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn-crd rancher-charts/longhorn-crd --namespace longhorn-system --create-namespace --version 107.0.0+up1.9.1
helm install longhorn rancher-charts/longhorn --namespace longhorn-system --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>Confirm that the deployment succeeded:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-suse-storage-volumes">
<title>Creating SUSE Storage volumes</title>
<para>SUSE Storage utilizes Kubernetes resources called <literal>StorageClass</literal> in order to automatically provision <literal>PersistentVolume</literal> objects for pods.
Think of <literal>StorageClass</literal> as a way for administrators to describe the <emphasis>classes</emphasis> or <emphasis>profiles</emphasis> of storage they offer.</para>
<para>Let’s create a <literal>StorageClass</literal> with some default options:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para>Now that we have our <literal>StorageClass</literal> in place, we need a <literal>PersistentVolumeClaim</literal> referencing it.
A <literal>PersistentVolumeClaim</literal> (PVC) is a request for storage by a user. PVCs consume <literal>PersistentVolume</literal> resources.
Claims can request specific sizes and access modes (e.g., they can be mounted once read/write or many times read-only).</para>
<para>Let’s create a <literal>PersistentVolumeClaim</literal>:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>That’s it! Once we have the <literal>PersistentVolumeClaim</literal> created, we can proceed with attaching it to a <literal>Pod</literal>.
When the <literal>Pod</literal> is deployed, Kubernetes creates the Longhorn volume and binds it to the <literal>Pod</literal> if storage is available.</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>The concept of storage in Kubernetes is a complex, but important topic. We briefly mentioned some of the most common Kubernetes resources,
however, we suggest to familiarize yourself with the <link xl:href="https://longhorn.io/docs/1.9.1/terminology/">terminology documentation</link> that Longhorn offers.</para>
</tip>
<para>In this example, the result should look something like this:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>Accessing the UI</title>
<para>If you installed Longhorn with kubectl or Helm, you need to set up an Ingress controller to
allow external traffic into the cluster. Authentication is not enabled by
default. If the Rancher catalog app was used, Rancher automatically created an Ingress controller with
access control (the rancher-proxy).</para>
<orderedlist numeration="arabic">
<listitem>
<para>Get the Longhorn’s external service IP address:</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para>Once you have retrieved the <literal>longhorn-frontend</literal> IP address, you can start using the UI by navigating to it in your browser.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize base SUSE Linux Micro OS images.
We are going to demonstrate how to do so for provisioning an RKE2 cluster with Longhorn on top of it.</para>
<para>Let’s create the definition file:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>Customizing any of the Helm chart values is possible via a separate file provided under <literal>helm.charts[].valuesFile</literal>.
Refer to the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md#kubernetes">upstream documentation</link> for details.</para>
</note>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>After the image is built, you can use it to install your OS on a physical or virtual host.
Once the provisioning is complete, you are able to log in to the system using the <literal>root:eib</literal> credentials pair.</para>
<para>Ensure that Longhorn has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>This installation will not work for completely air-gapped environments.
In those cases, please refer to <xref linkend="suse-storage-install"/>.</para>
</note>
</section>
</chapter>
<chapter xml:id="components-suse-security">
<title><link xl:href="https://www.suse.com/products/rancher/security/">SUSE Security</link></title>
<para>SUSE Security is a security solution for Kubernetes that provides L7 network security, runtime security, supply chain security, and compliance checks in a cohesive package.</para>
<para>SUSE Security is a product that is deployed as a platform of multiple containers, each communicating over various ports and interfaces. Under the hood, it uses NeuVector as its underlying container security component. The following containers make up the SUSE Security platform:</para>
<itemizedlist>
<listitem>
<para>Manager. A stateless container which presents the Web-based console. Typically, only
one is needed and this can run anywhere. Failure of the Manager does not affect any of
the operations of the controller or enforcer. However, certain notifications (events) and
recent connection data are cached in memory by the Manager so viewing of these would
be affected.</para>
</listitem>
<listitem>
<para>Controller. The ‘control plane’ for SUSE Security must be deployed in an HA
configuration, so configuration is not lost in a node failure. These can run anywhere,
although customers often choose to place these on ‘management’, master or
infra nodes because of their criticality.</para>
</listitem>
<listitem>
<para>Enforcer. This container is deployed as a DaemonSet so one Enforcer is on every node to
be protected. Typically deploys to every worker node but scheduling can be enabled for
master and infra nodes to deploy there as well. Note: If the Enforcer is not on a cluster node
and connections come from a pod on that node, SUSE Security labels them as ‘unmanaged’ workloads.</para>
</listitem>
<listitem>
<para>Scanner. Performs the vulnerability scanning using the built-in CVE database, as
directed by the Controller. Multiple scanners can be deployed to increase scanning
capacity. Scanners can run anywhere but are often run on the nodes where the controllers
run. See below for sizing considerations of scanner nodes. A scanner can also be invoked
independently when used for build-phase scanning, for example, within a pipeline that triggers a scan, retrieves the results, and stops the scanner. The scanner contains the latest CVE database so
should be updated daily.</para>
</listitem>
<listitem>
<para>Updater. The updater triggers an update of the scanner through a Kubernetes cron job
when an update of the CVE database is desired. Please be sure to configure this for your
environment.</para>
</listitem>
</itemizedlist>
<para>A more in-depth SUSE Security onboarding and best practices documentation can be found <link xl:href="https://open-docs.neuvector.com/">here</link>.</para>
<section xml:id="id-how-does-suse-edge-use-suse-security">
<title>How does SUSE Edge use SUSE Security?</title>
<para>SUSE Edge provides a leaner configuration of SUSE Security as a starting point for edge deployments.</para>
</section>
<section xml:id="id-important-notes">
<title>Important notes</title>
<itemizedlist>
<listitem>
<para>The <literal>Scanner</literal> container must have enough memory to pull the
image to be scanned into memory and expand it. To scan images exceeding 1 GB, increase the scanner’s memory to slightly above the largest expected image size.</para>
</listitem>
<listitem>
<para>High network connections expected in Protect mode. The <literal>Enforcer</literal> requires CPU and
memory when in Protect (inline firewall blocking) mode to hold and inspect connections
and possible payload (DLP). Increasing memory and dedicating a CPU core to the
<literal>Enforcer</literal> can ensure adequate packet filtering capacity.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize base SUSE Linux Micro OS images.
Follow <xref linkend="suse-security-install"/> for an air-gapped installation of SUSE Security on top of Kubernetes clusters provisioned by EIB.</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para>See <link xl:href="https://metallb.universe.tf/">MetalLB official documentation</link>.</para>
<blockquote>
<para>MetalLB is a load-balancer implementation for bare-metal Kubernetes clusters, using standard routing protocols.</para>
<para>In bare-metal environments, setting up network load balancers is notably more complex than in cloud environments. Unlike the straightforward API calls in cloud setups, bare-metal requires either dedicated network appliances or a combination of load balancers and Virtual IP (VIP) configurations to manage High Availability (HA) or address the potential Single Point of Failure (SPOF) inherent in a single node load balancer. These configurations are not easily automated, posing challenges in Kubernetes deployments where components dynamically scale up and down.</para>
<para>MetalLB addresses these challenges by harnessing the Kubernetes model to create LoadBalancer type services as if they were operating in a cloud environment, even on bare-metal setups.</para>
<para>There are two different approaches, via <link xl:href="https://metallb.universe.tf/concepts/layer2/">L2 mode</link> (using ARP <emphasis>tricks</emphasis>) or via <link xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>. Mainly L2 does not need any special network gear but BGP is in general better. It depends on the use cases.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>How does SUSE Edge use MetalLB?</title>
<para>SUSE Edge uses MetalLB in three key ways:</para>
<itemizedlist>
<listitem>
<para>As a Load Balancer Solution: MetalLB serves as the Load Balancer solution for bare-metal machines.</para>
</listitem>
<listitem>
<para>For an HA K3s/RKE2 Setup: MetalLB allows for load balancing the Kubernetes API using a Virtual IP address.</para>
</listitem>
<listitem>
<para>As an L3 BGP solution where MetalLB advertises routes to the service IPs to
nearby routers.</para>
</listitem>
</itemizedlist>
<note>
<para>In order to be able to expose the API, the Endpoint Copier Operator (<xref linkend="components-eco"/>) is used to keep in sync the K8s API endpoints from the <literal>kubernetes</literal> service to a <literal>kubernetes-vip</literal> LoadBalancer service.</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>Best practices</title>
<para>Installation of MetalLB in L2 mode is described in <xref linkend="guides-metallb-k3s"/> and
for L3 mode in <xref linkend="guides-metallb-k3s-l3"/>.</para>
<para>A guide on installing MetalLB in front of the <literal>kube-api-server</literal> to achieve high-availability topology can be found in <xref linkend="guides-metallb-kubernetes"/>.</para>
</section>
<section xml:id="id-known-issues-6">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>K3s comes with its Load Balancer solution called <literal>Klipper</literal>. To use MetalLB, <literal>Klipper</literal> must be disabled. This can be done by starting the K3s server with the <literal>--disable servicelb</literal> option, as described in the <link xl:href="https://docs.k3s.io/networking">K3s documentation</link>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eco">
<title>Endpoint Copier Operator</title>
<para><link xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint Copier Operator</link> is a Kubernetes operator whose purpose is to create a copy of a Kubernetes Service and Endpoint and to keep them synced.</para>
<section xml:id="id-how-does-suse-edge-use-endpoint-copier-operator">
<title>How does SUSE Edge use Endpoint Copier Operator?</title>
<para>At SUSE Edge, the Endpoint Copier Operator plays a crucial role in achieving High Availability (HA) setup for K3s/RKE2 clusters. This is accomplished by creating a <literal>kubernetes-vip</literal> service of type <literal>LoadBalancer</literal>, ensuring its Endpoint remains in constant synchronization with the kubernetes Endpoint. MetalLB (<xref linkend="components-metallb"/>) is leveraged to manage the <literal>kubernetes-vip</literal> service, as the exposed IP address is used from other nodes to join the cluster.</para>
</section>
<section xml:id="id-best-practices-7">
<title>Best Practices</title>
<para>Comprehensive documentation for using the Endpoint Copier Operator can be found <link xl:href="https://github.com/suse-edge/endpoint-copier-operator/blob/main/README.md">here</link>.</para>
<para>Additionally, refer to our guide (<xref linkend="guides-metallb-k3s"/>)  on achieving a K3s/RKE2 HA setup using the Endpoint Copier Operator and MetalLB.</para>
</section>
<section xml:id="id-known-issues-7">
<title>Known issues</title>
<para>Presently, the Endpoint Copier Operator is limited to working with only one Service/Endpoint. Enhancements to support multiple Services/Endpoints are planned for the future.</para>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>This section describes how you can use Edge Virtualization to run virtual machines on your edge nodes. Edge Virtualization is designed for lightweight virtualization use-cases, where it is expected that a common workflow for the deployment and management of both virtualized and containerized applications will be utilized.</para>
<para>SUSE Edge Virtualization supports two methods of running virtual machines:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Deploying the virtual machines manually via libvirt+qemu-kvm at the host level (where Kubernetes is not involved)</para>
</listitem>
<listitem>
<para>Deploying the KubeVirt operator for Kubernetes-based management of virtual machines</para>
</listitem>
</orderedlist>
<para>Both options are valid, but only the second one is covered below. If you want to use the standard out-of-the box virtualization mechanisms provided by SUSE Linux Micro, a comprehensive guide can be found <link xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/chap-virtualization-introduction.html">here</link>, and whilst it was primarily written for SUSE Linux Enterprise Server, the concepts are almost identical.</para>
<para>This guide initially explains how to deploy the additional virtualization components onto a system that has already been pre-deployed, but follows with a section that describes how to embed this configuration in the initial deployment via Edge Image Builder. If you do not want to run through the basics and set things up manually, skip right ahead to that section.</para>
<section xml:id="id-kubevirt-overview">
<title>KubeVirt overview</title>
<para>KubeVirt allows for managing Virtual Machines with Kubernetes alongside the rest of your containerized workloads. It does this by running the user space portion of the Linux virtualization stack in a container. This minimizes the requirements on the host system, allowing for easier setup and management.</para>
<informalexample>
<para>Details about KubeVirt’s architecture can be found in <link xl:href="https://kubevirt.io/user-guide/architecture/">the upstream documentation.</link></para>
</informalexample>
</section>
<section xml:id="id-prerequisites-5">
<title>Prerequisites</title>
<para>If you are following this guide, we assume you have the following already available:</para>
<itemizedlist>
<listitem>
<para>At least one physical host with SUSE Linux Micro 6.1 installed, and with virtualization extensions enabled in the BIOS (see <link xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">here</link> for details).</para>
</listitem>
<listitem>
<para>Across your nodes, a K3s/RKE2 Kubernetes cluster already deployed and with an appropriate <literal>kubeconfig</literal> that enables superuser access to the cluster.</para>
</listitem>
<listitem>
<para>Access to the root user — these instructions assume you are the root user, and <emphasis>not</emphasis> escalating your privileges via <literal>sudo</literal>.</para>
</listitem>
<listitem>
<para>You have <link xl:href="https://helm.sh/docs/intro/install/">Helm</link> available locally with an adequate network connection to be able to push configurations to your Kubernetes cluster and download the required images.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Manual installation of Edge Virtualization</title>
<para>This guide will not walk you through the deployment of Kubernetes, but it assumes that you have either installed the SUSE Edge-appropriate version of <link xl:href="https://k3s.io/">K3s</link> or <link xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> and that you have your kubeconfig configured accordingly so that standard <literal>kubectl</literal> commands can be executed as the superuser. We assume your node forms a single-node cluster, although there are no significant differences expected for multi-node deployments.</para>
<para>SUSE Edge Virtualization is deployed via three separate Helm charts, specifically:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>: The core virtualization components, that is, Kubernetes CRDs, operators and other components required for enabling Kubernetes to deploy and manage virtual machines.</para>
</listitem>
<listitem>
<para><emphasis role="strong">KubeVirt Dashboard Extension</emphasis>: An optional Rancher UI extension that allows basic virtual machine management, for example, starting/stopping of virtual machines as well as accessing the console.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>: An additional component that enables persistent-storage integration for KubeVirt, providing capabilities for virtual machines to use existing Kubernetes storage back-ends for data, but also allowing users to import or clone data volumes for virtual machines.</para>
</listitem>
</itemizedlist>
<para>Each of these Helm charts is versioned according to the SUSE Edge release you are currently using. For production/supported usage, employ the artifacts that can be found in the SUSE Registry.</para>
<para>First, ensure that your <literal>kubectl</literal> access is working:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.30.5+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1</screen>
<para>Now you can proceed to install the <emphasis role="strong">KubeVirt</emphasis> and <emphasis role="strong">Containerized Data Importer (CDI)</emphasis> Helm charts:</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/charts/kubevirt --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/charts/cdi --namespace cdi-system --create-namespace</screen>
<para>In a few minutes, you should have all KubeVirt and CDI components deployed. You can validate this by checking all the deployed resources in the <literal>kubevirt-system</literal> and <literal>cdi-system</literal> namespace.</para>
<para>Verify KubeVirt resources:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>Verify CDI resources:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para>To verify that the <literal>VirtualMachine</literal> custom resource definitions (CRDs) are deployed, you can validate with:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para>This should print out the definition of the <literal>VirtualMachine</literal> object, which should print as follows:</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>Deploying virtual machines</title>
<para>Now that KubeVirt and CDI are deployed, let us define a simple virtual machine based on <link xl:href="https://get.opensuse.org/tumbleweed/">openSUSE Tumbleweed</link>. This virtual machine has the most simple of configurations, using standard "pod networking" for a networking configuration identical to any other pod. It also employs non-persistent storage, ensuring the storage is ephemeral, just like in any container that does not have a <link xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>This should print that a <literal>VirtualMachine</literal> was created:</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>This <literal>VirtualMachine</literal> definition is minimal, specifying little about the configuration. It simply outlines that it is a machine type "<link xl:href="https://wiki.qemu.org/Features/Q35">q35</link>" with 2 GB of memory that uses a disk image based on an ephemeral <literal><link xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal> (that is, a disk image that is stored in a container image from a remote image repository), and specifies a base64 encoded cloudInit disk, which we only use for user creation and password enforcement at boot time (use <literal>base64 -d</literal> to decode it).</para>
<blockquote>
<note>
<para>This virtual machine image is only for testing. The image is not officially supported and is only meant as a documentation example.</para>
</note>
</blockquote>
<para>This machine takes a few minutes to boot as it needs to download the openSUSE Tumbleweed disk image, but once it has done so, you can view further details about the virtual machine by checking the virtual machine information:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>This should print the node that the virtual machine was started on, and the IP address of the virtual machine. Remember, since it uses pod networking, the reported IP address will be just like any other pod, and routable as such:</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>When running these commands on the Kubernetes cluster nodes themselves, with a CNI that routes traffic directly to pods (for example, Cilium), you should be able to <literal>ssh</literal> directly to the machine itself. Substitute the following IP address with the one that was assigned to your virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>Once you are in this virtual machine, you can play around, but remember that it is limited in terms of resources, and only has 1 GB disk space. When you are finished, <literal>Ctrl-D</literal> or <literal>exit</literal> to disconnect from the SSH session.</para>
<para>The virtual machine process is still wrapped in a standard Kubernetes pod. The <literal>VirtualMachine</literal> CRD is a representation of the desired virtual machine, but the process in which the virtual machine is actually started is via the <literal><link xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal> pod, a standard Kubernetes pod, just like any other application. For every virtual machine started, you can see there is a <literal>virt-launcher</literal> pod:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>This should then show the one <literal>virt-launcher</literal> pod for the Tumbleweed machine that we have defined:</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>If we take a look into this <literal>virt-launcher</literal> pod, you see it is executing <literal>libvirt</literal> and <literal>qemu-kvm</literal> processes. We can enter the pod itself and have a look under the covers, noting that you need to adapt the following command for your pod name:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Once you are in the pod, try running <literal>virsh</literal> commands along with looking at the processes. You will see the <literal>qemu-system-x86_64</literal> binary running, along with certain processes for monitoring the virtual machine. You will also see the location of the disk image and how the networking is plugged (as a tap device):</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>Finally, let us delete this virtual machine to clean up:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>Using virtctl</title>
<para>Along with the standard Kubernetes CLI tooling, that is, <literal>kubectl</literal>, KubeVirt comes with an accompanying CLI utility that allows you to interface with your cluster in a way that bridges some gaps between the virtualization world and the world that Kubernetes was designed for. For example, the <literal>virtctl</literal> tool provides the capability of managing the lifecycle of virtual machines (starting, stopping, restarting, etc.), providing access to the virtual consoles, uploading virtual machine images, as well as interfacing with Kubernetes constructs such as services, without using the API or CRDs directly.</para>
<para>Let us download the latest stable version of the <literal>virtctl</literal> tool:</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.5.2
$ wget https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64</screen>
<para>If you are using a different architecture or a non-Linux machine, you can find other releases <link xl:href="https://github.com/kubevirt/kubevirt/releases">here</link>. You need to make this executable before proceeding, and it may be useful to move it to a location within your <literal>$PATH</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-$VERSION-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>You can then use the <literal>virtctl</literal> command-line tool to create virtual machines. Let us replicate our previous virtual machine, noting that we are piping the output directly into <literal>kubectl apply</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>This should then show the virtual machine running (it should start a lot quicker this time given that the container image will be cached):</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>Now we can use <literal>virtctl</literal> to connect directly to the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para>There are plenty of other commands that can be used by <literal>virtctl</literal>. For example, <literal>virtctl console</literal> can give you access to the serial console if networking is not working, and you can use <literal>virtctl  guestosinfo</literal> to get comprehensive OS information, subject to the guest having the <literal>qemu-guest-agent</literal> installed and running.</para>
<para>Finally, let us pause and resume the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para>You find that the <literal>VirtualMachine</literal> object shows as <emphasis role="strong">Paused</emphasis> and the <literal>VirtualMachineInstance</literal> object shows as <emphasis role="strong">Running</emphasis> but <emphasis role="strong">READY=False</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>You also find that you can no longer connect to the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>Let us resume the virtual machine and try again:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>Now we should be able to re-establish a connection:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>Finally, let us remove the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>Simple ingress networking</title>
<para>In this section, we show how you can expose virtual machines as standard Kubernetes services and make them available via the Kubernetes ingress service, for example, <link xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINX with RKE2</link> or <link xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">Traefik with K3s</link>. This document assumes that these components are already configured appropriately and that you have an appropriate DNS pointer, for example, via a wild card, to point at your Kubernetes server nodes or your ingress virtual IP for proper ingress resolution.</para>
<blockquote>
<note>
<para>In SUSE Edge 3.1+, if you are using K3s in a multi-server node configuration, you might have needed to configure a MetalLB-based VIP for Ingress; this is not required for RKE2.</para>
</note>
</blockquote>
<para>In the example environment, another openSUSE Tumbleweed virtual machine is deployed, cloud-init is used to install NGINX as a simple Web server at boot time, and a simple message is configured to be returned to verify that it works as expected when a call is made. To see how this is done, simply <literal>base64 -d</literal> the cloud-init section in the output below.</para>
<para>Let us create this virtual machine now:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>When this virtual machine has successfully started, we can use the <literal>virtctl</literal> command to expose the <literal>VirtualMachineInstance</literal> with an external port of <literal>8080</literal> and a target port of <literal>80</literal> (where NGINX listens by default). We use the <literal>virtctl</literal> command here as it understands the mapping between the virtual machine object and the pod. This creates a new service for us:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>We will then have an appropriate service automatically created:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>Next, if you then use <literal>kubectl create ingress</literal>, we can create an ingress object that points to this service. Adapt the URL (known as the "host" in the <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link> object) here to match your DNS configuration and ensure that you point it to port <literal>8080</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>With DNS being configured correctly, you should be able to curl the URL immediately:</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>Let us clean up by removing this virtual machine and its service and ingress resources:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Using the Rancher UI extension</title>
<para>SUSE Edge Virtualization provides a UI extension for Rancher Manager, enabling basic virtual machine management using the Rancher dashboard UI.</para>
<section xml:id="id-installation-4">
<title>Installation</title>
<para>See Rancher Dashboard Extensions (<xref linkend="components-rancher-dashboard-extensions"/>) for installation guidance.</para>
</section>
<section xml:id="kubevirt-dashboard-extension-usage">
<title>Using KubeVirt Rancher Dashboard Extension</title>
<para>The extension introduces a new <emphasis role="strong">KubeVirt</emphasis> section to the Cluster Explorer. This section is added to any managed cluster which has KubeVirt installed.</para>
<para>The extension allows you to directly interact with KubeVirt Virtual Machine resources to manage Virtual Machines lifecycle.</para>
<section xml:id="id-creating-a-virtual-machine">
<title>Creating a virtual machine</title>
<orderedlist numeration="arabic">
<listitem>
<para>Navigate to <emphasis role="strong">Cluster Explorer</emphasis> clicking KubeVirt-enabled managed cluster in the left navigation.</para>
</listitem>
<listitem>
<para>Navigate to <emphasis role="strong">KubeVirt &gt; Virtual Machines</emphasis> page and click <literal>Create from YAML</literal> in the upper right of the screen.</para>
</listitem>
<listitem>
<para>Fill in or paste a virtual machine definition and press <literal>Create</literal>. Use virtual machine definition from Deploying Virtual Machines section as an inspiration.</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="virtual-machines-page.png" width=""/>
</imageobject>
<textobject><phrase>virtual machines page</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-virtual-machine-actions">
<title>Virtual Machine Actions</title>
<para>You can use the action menu accessed from the <emphasis role="strong">⋮</emphasis> drop-down list to the right of each virtual machine to perform start, stop, pause or soft reboot actions. Alternatively you can also use group actions at the top of the list by selecting virtual machines to perform the action on.</para>
<para>Performing the actions may have an effect on Virtual Machine Run Strategy. <link xl:href="https://kubevirt.io/user-guide/compute/run_strategies/#virtctl">See the table in KubeVirt documentation</link> for more details.</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>Accessing virtual machine console</title>
<para>The "Virtual machines" list provides a <literal>Console</literal> drop-down list that allows to connect to the machine using <emphasis role="strong">VNC or Serial Console</emphasis>. This action is only available to running machines.</para>
<para>In some cases, it takes a short while before the console is accessible on a freshly started virtual machine.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="vnc-console-ui.png" width=""/>
</imageobject>
<textobject><phrase>vnc console ui</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize base SUSE Linux Micro OS images.
Follow <xref linkend="kubevirt-install"/> for an air-gapped installation of both KubeVirt and CDI on top of Kubernetes clusters provisioned by EIB.</para>
</section>
</chapter>
<chapter xml:id="components-system-upgrade-controller">
<title>System Upgrade Controller</title>
<para>See the <link xl:href="https://github.com/rancher/system-upgrade-controller">System Upgrade Controller documentation</link>.</para>
<blockquote>
<para>The System Upgrade Controller (SUC) aims to provide a general-purpose, Kubernetes-native upgrade controller (for nodes). It introduces a new CRD, the Plan, for defining any and all of your upgrade policies/requirements. A Plan is an outstanding intent to mutate nodes in your cluster.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-system-upgrade-controller">
<title>How does SUSE Edge use System Upgrade Controller?</title>
<para>SUSE Edge uses <literal>SUC</literal> to facilitate various "Day 2" operations related to OS and Kubernetes version upgrades on management and downstream clusters.</para>
<para>"Day 2" operations are defined through <literal>SUC Plans</literal>. Based on these plans, <literal>SUC</literal> deploys workloads on each node to execute the respective "Day 2" operation.</para>
<para><literal>SUC</literal> is also used within the <xref linkend="components-upgrade-controller"/>. To learn more about the key differences between SUC and the Upgrade Controller, see <xref linkend="components-upgrade-controller-uc-vs-suc"/>.</para>
</section>
<section xml:id="components-system-upgrade-controller-install">
<title>Installing the System Upgrade Controller</title>
<important>
<para>Starting with Rancher <link xl:href="https://github.com/rancher/rancher/releases/tag/v2.10.0">v2.10.0</link>, the <literal>System Upgrade Controller</literal> is installed automatically.</para>
<para>Follow the steps below <emphasis role="strong">only</emphasis> if your environment is <emphasis role="strong">not</emphasis> managed by Rancher, or if your Rancher version is lesser than <literal>v2.10.0</literal>.</para>
</important>
<para>We recommend that you install SUC through Fleet (<xref linkend="components-fleet"/>) located in the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository.</para>
<note>
<para>The resources offered by the <literal>suse-edge/fleet-examples</literal> repository <emphasis role="strong">must</emphasis> always be used from a valid <link xl:href="https://github.com/suse-edge/fleet-examples/releases">fleet-examples release</link>. To determine which release you need to use, refer to the Release Notes (<xref linkend="release-notes"/>).</para>
</note>
<para>If you are unable to use Fleet for the installation of SUC, you can install it through Rancher’s Helm chart repository, or incorporate the Rancher’s Helm chart in your own third-party GitOps workflow.</para>
<para>This section covers:</para>
<itemizedlist>
<listitem>
<para>Fleet installation (<xref linkend="components-system-upgrade-controller-fleet"/>)</para>
</listitem>
<listitem>
<para>Helm installation (<xref linkend="components-system-upgrade-controller-helm"/>)</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet">
<title>System Upgrade Controller Fleet installation</title>
<para>Using Fleet, there are two possible resources that can be used to deploy SUC:</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> resource - for use cases where an external/local Git server is available. For installation instructions, see System Upgrade Controller installation - GitRepo (<xref linkend="components-system-upgrade-controller-fleet-gitrepo"/>).</para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resource - for air-gapped use cases that do not support a local Git server option. For installation instructions, see System Upgrade Controller installation - Bundle (<xref linkend="components-system-upgrade-controller-fleet-bundle"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet-gitrepo">
<title>System Upgrade Controller installation - GitRepo</title>
<note>
<para>This process can also be done through the Rancher UI, if such is available. For more information, see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing Fleet in the Rancher UI</link>.</para>
</note>
<para>In your <emphasis role="strong">management</emphasis> cluster:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Determine on which clusters you want to deploy SUC. This is done by deploying a SUC <literal>GitRepo</literal> resource in the correct Fleet workspace on your <emphasis role="strong">management</emphasis> cluster. By default, Fleet has two workspaces:</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - for resources that need to be deployed on the <emphasis role="strong">management</emphasis> cluster.</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - for resources that need to be deployed on <emphasis role="strong">downstream</emphasis> clusters.</para>
<para>For more information on Fleet workspaces, see the <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">upstream</link> documentation.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Deploy the <literal>GitRepo</literal> resource:</para>
<itemizedlist>
<listitem>
<para>To deploy SUC on your management cluster:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-local -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF</screen>
</listitem>
<listitem>
<para>To deploy SUC on your downstream clusters:</para>
<note>
<para>Before deploying the resource below, you <emphasis role="strong">must</emphasis> provide a valid <literal>targets</literal> configuration, so that Fleet knows on which downstream clusters to deploy your resource. For information on how to map to downstream clusters, see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Validate that the <literal>GitRepo</literal> resource is deployed:</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get gitrepo system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.4.0   1/1</screen>
</listitem>
<listitem>
<para>Validate the System Upgrade Controller deployment:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-fleet-bundle">
<title>System Upgrade Controller installation - Bundle</title>
<para>This section illustrates how to build and deploy a <literal>Bundle</literal> resource from a standard Fleet configuration using the <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<orderedlist numeration="arabic">
<listitem>
<para>On a machine with network access download the <literal>fleet-cli</literal>:</para>
<note>
<para>Make sure that the version of the fleet-cli you download matches the version of Fleet that has been deployed on your cluster.</para>
</note>
<itemizedlist>
<listitem>
<para>For Mac users there is a <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link> Homebrew Formulae.</para>
</listitem>
<listitem>
<para>For Linux and Windows users the binaries are present as <emphasis role="strong">assets</emphasis> to each Fleet <link xl:href="https://github.com/rancher/fleet/releases">release</link>.</para>
<itemizedlist>
<listitem>
<para>Linux AMD:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Make <literal>fleet-cli</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x fleet-cli</screen>
</listitem>
<listitem>
<para>Clone the <literal>suse-edge/fleet-examples</literal> <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> that you wish to use:</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Navigate to the SUC fleet, located in the <literal>fleet-examples</literal> repo:</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller</screen>
</listitem>
<listitem>
<para>Determine on which clusters you want to deploy SUC. This is done by deploying the SUC Bundle in the correct Fleet workspace inside your management cluster. By default, Fleet has two workspaces:</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - for resources that need to be deployed on the <emphasis role="strong">management</emphasis> cluster.</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - for resources that need to be deployed on <emphasis role="strong">downstream</emphasis> clusters.</para>
<para>For more information on Fleet workspaces, see the <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">upstream</link> documentation.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>If you intend to deploy SUC only on downstream clusters, create a <literal>targets.yaml</literal> file that matches the specific clusters:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>For information on how to map to downstream clusters, see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link></para>
</listitem>
<listitem>
<para>Proceed to building the Bundle:</para>
<note>
<para>Make sure you did <emphasis role="strong">not</emphasis> download the fleet-cli in the <literal>fleet-examples/fleets/day2/system-upgrade-controller</literal> directory, otherwise it will be packaged with the Bundle, which is not advised.</para>
</note>
<itemizedlist>
<listitem>
<para>To deploy SUC on your management cluster, execute:</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>To deploy SUC on your downstream clusters, execute:</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
<para>For more information about this process, see <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert a Helm Chart into a Bundle</link>.</para>
<para>For more information about the <literal>fleet-cli apply</literal> command, see <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet apply</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Transfer the <literal>system-upgrade-controller-bundle.yaml</literal> bundle to your management cluster machine:</para>
<screen language="bash" linenumbering="unnumbered">scp system-upgrade-controller-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para>On your management cluster, deploy the <literal>system-upgrade-controller-bundle.yaml</literal> Bundle:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>On your management cluster, validate that the Bundle is deployed:</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get bundle system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1</screen>
</listitem>
<listitem>
<para>Based on the Fleet workspace that you deployed your Bundle to, navigate to the cluster and validate the SUC deployment:</para>
<note>
<para>SUC is always deployed in the <emphasis role="strong">cattle-system</emphasis> namespace.</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-helm">
<title>System Upgrade Controller Helm installation</title>
<orderedlist numeration="arabic">
<listitem>
<para>Add the Rancher chart repository:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>Deploy the SUC chart:</para>
<screen language="bash" linenumbering="unnumbered">helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 107.0.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace</screen>
<para>This will install SUC version 0.16.0 which is needed by the Edge 3.4 platform.</para>
</listitem>
<listitem>
<para>Validate the SUC deployment:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans">
<title>Monitoring System Upgrade Controller Plans</title>
<para>SUC Plans can be viewed in the following ways:</para>
<itemizedlist>
<listitem>
<para>Through the Rancher UI (<xref linkend="components-system-upgrade-controller-monitor-plans-rancher"/>).</para>
</listitem>
<listitem>
<para>Through manual monitoring (<xref linkend="components-system-upgrade-controller-monitor-plans-manual"/>) inside of the cluster.</para>
</listitem>
</itemizedlist>
<important>
<para>Pods deployed for SUC Plans are kept alive <emphasis role="strong">15</emphasis> minutes after a successful execution. After that they are removed by the corresponding Job that created them. To have access to the Pod’s logs after this time period, you should enable logging for your cluster. For information on how to do this in Rancher, see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/logging">Rancher Integration with Logging Services</link>.</para>
</important>
<section xml:id="components-system-upgrade-controller-monitor-plans-rancher">
<title>Monitoring System Upgrade Controller Plans - Rancher UI</title>
<para>To check Pod logs for the specific SUC plan:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ → &lt;your-cluster-name&gt;</emphasis></para>
</listitem>
<listitem>
<para>Select Workloads → Pods</para>
</listitem>
<listitem>
<para>Select the <literal>Only User Namespaces</literal> drop down menu and add the <literal>cattle-system</literal> namespace</para>
</listitem>
<listitem>
<para>In the Pod filter bar, write the name for your SUC Plan Pod. The name will be in the following template format: <literal>apply-&lt;plan_name&gt;-on-&lt;node_name&gt;</literal></para>
<note>
<para>There may be both <literal>Completed</literal> and <literal>Unknown</literal> Pods for a specific SUC Plan. This is expected and happens due to the nature of some of the upgrades.</para>
</note>
</listitem>
<listitem>
<para>Select the pod that you want to review the logs of and navigate to <emphasis role="strong">⋮ → View Logs</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans-manual">
<title>Monitoring System Upgrade Controller Plans - Manual</title>
<note>
<para>The below steps assume that <literal>kubectl</literal> has been configured to connect to the cluster where the <emphasis role="strong">SUC Plans</emphasis> have been deployed to.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>List deployed <emphasis role="strong">SUC</emphasis> Plans:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get plans -n cattle-system</screen>
</listitem>
<listitem>
<para>Get Pod for <emphasis role="strong">SUC</emphasis> Plan:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l upgrade.cattle.io/plan=&lt;plan_name&gt; -n cattle-system</screen>
<note>
<para>There may be both <literal>Completed</literal> and <literal>Unknown</literal> Pods for a specific SUC Plan. This is expected and happens due to the nature of some of the upgrades.</para>
</note>
</listitem>
<listitem>
<para>Get logs for the Pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n cattle-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="components-upgrade-controller">
<title>Upgrade Controller</title>
<para>A Kubernetes controller capable of performing upgrades over the following SUSE Edge platform components:</para>
<itemizedlist>
<listitem>
<para>Operating System (SUSE Linux Micro)</para>
</listitem>
<listitem>
<para>Kubernetes (K3s &amp; RKE2)</para>
</listitem>
<listitem>
<para>Additional components (Rancher, Elemental, SUSE Security, etc.)</para>
</listitem>
</itemizedlist>
<para>The <link xl:href="https://github.com/suse-edge/upgrade-controller">Upgrade Controller</link> streamlines the upgrade process for the components mentioned above by encapsulating their complexities within a single <literal>user-facing</literal> resource that serves as a <emphasis role="strong">trigger</emphasis> for the upgrade. Users only need to configure this resource and the <literal>Upgrade Controller</literal> takes care of the rest.</para>
<note>
<para>The <literal>Upgrade Controller</literal> currently supports SUSE Edge platform upgrades only for <emphasis role="strong">non air-gapped management</emphasis> clusters. Refer to the <xref linkend="components-upgrade-controller-known-issues"/> section for more information.</para>
</note>
<section xml:id="id-how-does-suse-edge-use-upgrade-controller">
<title>How does SUSE Edge use Upgrade Controller?</title>
<para>The <emphasis role="strong">Upgrade Controller</emphasis> is essential in automating the (formerly manual) "Day 2" operations required to upgrade management clusters from one SUSE Edge release version to the next.</para>
<para>To achieve this automation, the Upgrade Controller utilizes tools such as the System Upgrade Controller (<xref linkend="components-system-upgrade-controller"/>) and the <link xl:href="https://github.com/k3s-io/helm-controller/">Helm Controller</link>.</para>
<para>For further details on how the Upgrade Controller works, see <xref linkend="components-upgrade-controller-how"/>.</para>
<para>For known limitations that the Upgrade Controller has, see <xref linkend="components-upgrade-controller-known-issues"/>.</para>
<para>For information on the difference between the Upgrade Controller and the System Upgrade Controller, see <xref linkend="components-upgrade-controller-uc-vs-suc"/>.</para>
</section>
<section xml:id="components-upgrade-controller-uc-vs-suc">
<title>Upgrade Controller vs System Upgrade Controller</title>
<para>The System Upgrade Controller (SUC) (<xref linkend="components-system-upgrade-controller"/>) is a general-purpose tool that propagates upgrade instructions to specific Kubernetes nodes.</para>
<para>While it supports some "Day 2" operations for the SUSE Edge platform, it <emphasis role="strong">does not</emphasis> cover all of them. Moreover, even for supported operations, users have to manually configure, maintain, and deploy multiple <literal>SUC Plans</literal> — an error-prone process that can lead to unexpected issues.</para>
<para>This led to the need for a tool that <emphasis role="strong">automates</emphasis> and <emphasis role="strong">abstracts</emphasis> the complexity of managing various "Day 2" operations for the SUSE Edge platform. Thus, the <literal>Upgrade Controller</literal> was developed. It simplifies the upgrade process by introducing a single <literal>user-facing resource</literal> that drives the upgrade. Users only need to manage this resource, while the <literal>Upgrade Controller</literal> takes care of the rest.</para>
</section>
<section xml:id="components-upgrade-controller-installation">
<title>Installing the Upgrade Controller</title>
<section xml:id="id-prerequisites-6">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://helm.sh/docs/intro/install/">Helm</link></para>
</listitem>
<listitem>
<para><link xl:href="https://cert-manager.io/v1.15-docs/installation/helm/#installing-with-helm">cert-manager</link></para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref linkend="components-system-upgrade-controller-install"/>)</para>
</listitem>
<listitem>
<para>A Kubernetes cluster; either K3s or RKE2</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-steps">
<title>Steps</title>
<orderedlist numeration="arabic">
<listitem>
<para>Install the Upgrade Controller Helm chart on your management cluster:</para>
<screen language="bash" linenumbering="unnumbered">helm install upgrade-controller oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1 --create-namespace --namespace upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Validate the Upgrade Controller deployment:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Validate the Upgrade Controller pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Validate the Upgrade Controller pod logs:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n upgrade-controller-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-how">
<title>How does the Upgrade Controller work?</title>
<para>In order to perform an Edge release upgrade, the Upgrade Controller introduces two new Kubernetes <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link>:</para>
<itemizedlist>
<listitem>
<para>UpgradePlan (<xref linkend="components-upgrade-controller-extensions-upgrade-plan"/>) - created by the user; holds configurations regarding an Edge release upgrade.</para>
</listitem>
<listitem>
<para>ReleaseManifest (<xref linkend="components-upgrade-controller-extensions-release-manifest"/>) - created by the Upgrade Controller; holds component versions specific to a particular Edge release version. <emphasis role="strong">This file must not be edited by users.</emphasis></para>
</listitem>
</itemizedlist>
<para>The Upgrade Controller proceeds to create a <literal>ReleaseManifest</literal> resource that holds the component data for the Edge release version specified by the user under the <literal>releaseVersion</literal> property in the <literal>UpgradePlan</literal> resource.</para>
<para>Using the component data from the <literal>ReleaseManifest</literal>, the Upgrade Controller proceeds to upgrade the Edge release components in the following order:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Operating System (OS) (<xref linkend="components-upgrade-controller-how-os"/>).</para>
</listitem>
<listitem>
<para>Kubernetes (<xref linkend="components-upgrade-controller-how-k8s"/>).</para>
</listitem>
<listitem>
<para>Additional components (<xref linkend="components-upgrade-controller-how-additional"/>).</para>
</listitem>
</orderedlist>
<note>
<para>During the upgrade process, the Upgrade Controller continually outputs upgrade information to the created <literal>UpgradePlan</literal>. For more information on how to track the upgrade process, see Tracking the upgrade process (<xref linkend="components-upgrade-controller-how-track"/>).</para>
</note>
<section xml:id="components-upgrade-controller-how-os">
<title>Operating System upgrade</title>
<para>To upgrade the operating system, the Upgrade Controller creates SUC (<xref linkend="components-system-upgrade-controller"/>) Plans that have the following naming template:</para>
<itemizedlist>
<listitem>
<para>For SUC Plans related to control plane node OS upgrades - <literal>control-plane-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal>.</para>
</listitem>
<listitem>
<para>For SUC Plans related to worker node OS upgrades - <literal>workers-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal>.</para>
</listitem>
</itemizedlist>
<para>Based on these plans, SUC proceeds to create workloads on each node of the cluster that perform the actual OS upgrade.</para>
<para>Depending on the <literal>ReleaseManifest</literal>, the OS upgrade may include:</para>
<itemizedlist>
<listitem>
<para>Package only updates - for use-cases where the OS version does not change between Edge releases.</para>
</listitem>
<listitem>
<para>Full OS migration - for use-cases where the OS version changes between Edge releases.</para>
</listitem>
</itemizedlist>
<para>The upgrade is executed <emphasis role="strong">one</emphasis> node at a time starting with the control plane nodes first. Only if the control-plane node upgrade finishes will the worker nodes begin to be upgraded.</para>
<note>
<para>The Upgrade Controller configures the OS SUC Plans to do perform a <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link> of the cluster nodes if the cluster has more than <emphasis role="strong">one</emphasis> node of the specified type.</para>
<para>For clusters where the control plane nodes are <emphasis role="strong">greater than</emphasis> one and there is <emphasis role="strong">only one</emphasis> worker node, a drain will be performed only for the control plane nodes and vice versa.</para>
<para>For information on how to disable node drains altogether, see the UpgradePlan (<xref linkend="components-upgrade-controller-extensions-upgrade-plan"/>) section.</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-k8s">
<title>Kubernetes upgrade</title>
<para>To upgrade the Kubernetes distribution of a cluster, the Upgrade Controller creates SUC (<xref linkend="components-system-upgrade-controller"/>) Plans that have the following naming template:</para>
<itemizedlist>
<listitem>
<para>For SUC Plans related to control plane node Kubernetes upgrades - <literal>control-plane-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>.</para>
</listitem>
<listitem>
<para>For SUC Plans related to worker node Kubernetes upgrades - <literal>workers-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>.</para>
</listitem>
</itemizedlist>
<para>Based on these plans, SUC proceeds to create workloads on each node of the cluster that perform the actual Kubernetes upgrade.</para>
<para>The Kubernetes upgrade will happen <emphasis role="strong">one</emphasis> node at a time starting with the control plane nodes first. Only if the control plane node upgrade finishes will the worker nodes begin to be upgraded.</para>
<note>
<para>The Upgrade Controller configures the Kubernetes SUC Plans to perform a <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link> of the cluster nodes if the cluster has more than <emphasis role="strong">one</emphasis> node of the specified type.</para>
<para>For clusters where the control plane nodes are <emphasis role="strong">greater than</emphasis> one and there is <emphasis role="strong">only one</emphasis> worker node, a drain will be performed only for the control plane nodes and vice versa.</para>
<para>For information on how to disable node drains altogether, see <xref linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-additional">
<title>Additional components upgrades</title>
<para>Currently, all additional components are installed via Helm charts. For a full list of the components for a specific release, refer to the Release Notes (<xref linkend="release-notes"/>).</para>
<para>For Helm charts deployed through EIB (<xref linkend="components-eib"/>), the Upgrade Controller updates the existing <link xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart CR</link> of each component.</para>
<para>For Helm charts deployed outside of EIB, the Upgrade Controller creates a <literal>HelmChart</literal> resource for each component.</para>
<para>After the creation/update of the <literal>HelmChart</literal> resource, the Upgrade Controller relies on the <link xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link> to pick up this change and proceed with the actual component upgrade.</para>
<para>Charts will be upgraded sequentially based on their order in the <literal>ReleaseManifest</literal>. Additional values can also be passed through the <literal>UpgradePlan</literal>. If a chart’s version remains unchanged in the new SUSE Edge release, it will not be upgraded. For more information about this, refer to <xref linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</section>
</section>
<section xml:id="components-upgrade-controller-extensions">
<title>Kubernetes API extensions</title>
<para>Extensions to the Kubernetes API introduced by the Upgrade Controller.</para>
<section xml:id="components-upgrade-controller-extensions-upgrade-plan">
<title>UpgradePlan</title>
<para>The Upgrade Controller introduces a new Kubernetes <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource</link> called an <literal>UpgradePlan</literal>.</para>
<para>The <literal>UpgradePlan</literal> serves as an instruction mechanism for the Upgrade Controller and it supports the following configurations:</para>
<itemizedlist>
<listitem>
<para><literal>releaseVersion</literal> - Edge release version to which the cluster should be upgraded to. The release version must follow <link xl:href="https://semver.org">semantic</link> versioning and should be retrieved from the Release Notes (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para><literal>disableDrain</literal> - <emphasis role="strong">Optional</emphasis>; instructs the Upgrade Controller on whether to disable node <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drains</link>. Useful for when you have workloads with <link xl:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Disruption Budgets</link>.</para>
<itemizedlist>
<listitem>
<para>Example for control plane node drain disablement:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true</screen>
</listitem>
<listitem>
<para>Example for control plane and worker node drain disablement:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true
    worker: true</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>helm</literal> - <emphasis role="strong">Optional</emphasis>; specifies additional values for components installed via Helm.</para>
<warning>
<para>It is only advised to use this field for values that are critical for upgrades. Standard chart value updates should be performed after the respective charts have been upgraded to the next version.</para>
</warning>
<itemizedlist>
<listitem>
<para>Example:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  helm:
  - chart: foo
    values:
      bar: baz</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-upgrade-controller-extensions-release-manifest">
<title>ReleaseManifest</title>
<para>The Upgrade Controller introduces a new Kubernetes <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource</link> called a <literal>ReleaseManifest</literal>.</para>
<para>The <literal>ReleaseManifest</literal> resource is created by the Upgrade Controller and holds component data for <emphasis role="strong">one</emphasis> specific Edge release version. This means that each Edge release version upgrade will be represented by a different <literal>ReleaseManifest</literal> resource.</para>
<warning>
<para>The Release Manifest should always be created by the Upgrade Controller.</para>
<para>It is not advisable to manually create or edit the <literal>ReleaseManifest</literal> resources. Users that decide to do so should do this <emphasis role="strong">at their own risk</emphasis>.</para>
</warning>
<para>Component data that the Release Manifest ships include, but is not limited to:</para>
<itemizedlist>
<listitem>
<para>Operating System data - version, supported architectures, additional upgrade data, etc.</para>
</listitem>
<listitem>
<para>Kubernetes distribution data - <link xl:href="https://docs.rke2.io">RKE2</link>/<link xl:href="https://k3s.io">K3s</link> supported versions</para>
</listitem>
<listitem>
<para>Additional components data - SUSE Helm chart data (location, version, name, etc.)</para>
</listitem>
</itemizedlist>
<para>For an example of how a Release Manifest can look, refer to the <link xl:href="https://github.com/suse-edge/upgrade-controller/blob/main/config/samples/lifecycle_v1alpha1_releasemanifest.yaml">upstream</link> documentation. <emphasis>Please note that this is just an example and it is not intended to be created as a valid <literal>ReleaseManifest</literal> resource.</emphasis></para>
</section>
</section>
<section xml:id="components-upgrade-controller-how-track">
<title>Tracking the upgrade process</title>
<para>This section serves as means to track and debug the upgrade process that the Upgrade Controller initiates once the user creates an <literal>UpgradePlan</literal> resource.</para>
<section xml:id="components-upgrade-controller-how-track-general">
<title>General</title>
<para>General information about the state of the upgrade process can be viewed in the Upgrade Plan’s status conditions.</para>
<para>The Upgrade Plan resource’s status can be viewed in the following way:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get upgradeplan &lt;upgradeplan_name&gt; -n upgrade-controller-system -o yaml</screen>
<formalpara>
<title>Running Upgrade Plan example:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Control plane nodes are being upgraded
    reason: InProgress
    status: "False"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Kubernetes upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Rancher upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Longhorn upgrade is not yet started
    reason: Pending
    status: Unknown
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: MetalLB upgrade is not yet started
    reason: Pending
    status: Unknown
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: CDI upgrade is not yet started
    reason: Pending
    status: Unknown
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: KubeVirt upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: NeuVector upgrade is not yet started
    reason: Pending
    status: Unknown
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: EndpointCopierOperator upgrade is not yet started
    reason: Pending
    status: Unknown
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Elemental upgrade is not yet started
    reason: Pending
    status: Unknown
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: SRIOV upgrade is not yet started
    reason: Pending
    status: Unknown
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Metal3 upgrade is not yet started
    reason: Pending
    status: Unknown
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: RancherTurtles upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherTurtlesUpgraded
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
<para>Here you can view every component that the Upgrade Controller will try to schedule an upgrade for. Each condition follows the below template:</para>
<itemizedlist>
<listitem>
<para><literal>lastTransitionTime</literal> - the last time that this component condition has transitioned from one status to another.</para>
</listitem>
<listitem>
<para><literal>message</literal> - message that indicates the current upgrade state of the specific component condition.</para>
</listitem>
<listitem>
<para><literal>reason</literal> - the current upgrade state of the specific component condition. Possible <literal>reasons</literal> include:</para>
<itemizedlist>
<listitem>
<para><literal>Succeeded</literal> - upgrade of the specific component is successful.</para>
</listitem>
<listitem>
<para><literal>Failed</literal> - upgrade of the specific component has failed.</para>
</listitem>
<listitem>
<para><literal>InProgress</literal> - upgrade of the specific component is currently in progress.</para>
</listitem>
<listitem>
<para><literal>Pending</literal> - upgrade of the specific component is not yet scheduled.</para>
</listitem>
<listitem>
<para><literal>Skipped</literal> - specific component is not found on the cluster, so its upgrade will be skipped.</para>
</listitem>
<listitem>
<para><literal>Error</literal> - specific component has encountered a transient error.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>status</literal> - status of the current condition <literal>type</literal>, one of <literal>True</literal>, <literal>False</literal>, <literal>Unknown</literal>.</para>
</listitem>
<listitem>
<para><literal>type</literal> - indicator for the currently upgraded component.</para>
</listitem>
</itemizedlist>
<para>The Upgrade Controller creates SUC Plans for component conditions of type <literal>OSUpgraded</literal> and <literal>KubernetesUpgraded</literal>. To further track the SUC Plans created for these components, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<para>All other component condition types can be further tracked by viewing the resources created for them by the <link xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>. For more information, see
<xref linkend="components-upgrade-controller-how-track-helm"/>.</para>
<para>An Upgrade Plan scheduled by the Upgrade Controller can be marked as <literal>successful</literal> once:</para>
<orderedlist numeration="arabic">
<listitem>
<para>There are no <literal>Pending</literal> or <literal>InProgress</literal> component conditions.</para>
</listitem>
<listitem>
<para>The <literal>lastSuccessfulReleaseVersion</literal> property points to the <literal>releaseVersion</literal> that is specified in the Upgrade Plan’s configuration. <emphasis>This property is added to the Upgrade Plan’s status by the Upgrade Controller once the upgrade process is successful.</emphasis></para>
</listitem>
</orderedlist>
<formalpara>
<title>Successful <literal>UpgradePlan</literal> example:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:48Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:59Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart rancher upgrade succeeded
    reason: Succeeded
    status: "True"
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart longhorn is not installed
    reason: Skipped
    status: "False"
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Specified version of chart metallb is already installed
    reason: Skipped
    status: "False"
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart cdi is not installed
    reason: Skipped
    status: "False"
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart kubevirt is not installed
    reason: Skipped
    status: "False"
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart neuvector-crd is not installed
    reason: Skipped
    status: "False"
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Specified version of chart endpoint-copier-operator is already installed
    reason: Skipped
    status: "False"
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Chart elemental-operator upgrade succeeded
    reason: Succeeded
    status: "True"
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:27:15Z"
    message: Chart sriov-crd is not installed
    reason: Skipped
    status: "False"
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:27:19Z"
    message: Chart metal3 is not installed
    reason: Skipped
    status: "False"
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:27:27Z"
    message: Chart rancher-turtles is not installed
    reason: Skipped
    status: "False"
    type: RancherTurtlesUpgraded
  lastSuccessfulReleaseVersion: 3.4
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
</section>
<section xml:id="components-upgrade-controller-how-track-helm">
<title>Helm Controller</title>
<para>This section covers how to track resources created by the <link xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>.</para>
<note>
<para>The below steps assume that <literal>kubectl</literal> has been configured to connect to the cluster where the Upgrade Controller has been deployed to.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Locate the <literal>HelmChart</literal> resource for the specific component:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmcharts -n kube-system</screen>
</listitem>
<listitem>
<para>Using the name of the <literal>HelmChart</literal> resource, locate the upgrade Pod that was created by the <literal>helm-controller</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n kube-system

# Example for Rancher
kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n kube-system
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-tv9wn   0/1     Completed   0          16m</screen>
</listitem>
<listitem>
<para>View the logs of the component specific pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n kube-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-known-issues">
<title>Known Limitations</title>
<itemizedlist>
<listitem>
<para>Downstream cluster upgrades are not yet managed by the Upgrade Controller. For information on how to upgrade downstream clusters, refer to <xref linkend="day2-downstream-clusters"/>.</para>
</listitem>
<listitem>
<para>The Upgrade Controller expects any additional SUSE Edge Helm charts that are deployed through EIB (<xref linkend="components-eib"/>) to have their <link xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart CR</link> deployed in the <literal>kube-system</literal> namespace. To do this, configure the <literal>installationNamespace</literal> property in your EIB definition file. For more information, see the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">upstream</link> documentation.</para>
</listitem>
<listitem>
<para>Currently the Upgrade Controller has no way to determine the current running Edge release version on the management cluster. Ensure to provide an Edge release version that is greater than the currently running Edge release version on the cluster.</para>
</listitem>
<listitem>
<para>Currently the Upgrade Controller supports <emphasis role="strong">non air-gapped</emphasis> environment upgrades only. <emphasis role="strong">Air-gapped</emphasis> upgrades are not yet possible.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Multi-Linux Manager is included in SUSE Edge to provide automation and control for keeping SUSE Linux Micro as the underlying operating system consistently up-to-date on all nodes of your edge deployment.</para>
<para>For more information please refer to the <xref linkend="quickstart-suma"/> and the <link xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUSE Multi-Linux Manager Documentation</link>.</para>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>How-To Guides</title>
<partintro>
<para>How-to guides and best practices</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>MetalLB on K3s (using Layer 2 Mode)</title>
<para>MetalLB is a load-balancer implementation for bare-metal Kubernetes clusters, using standard routing protocols.</para>
<para>In this guide, we demonstrate how to deploy MetalLB in layer 2 (L2) mode.</para>
<section xml:id="id-why-use-metallb">
<title>Why use MetalLB</title>
<para>MetalLB is a compelling choice for load balancing in bare-metal Kubernetes clusters for several reasons:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Native Integration with Kubernetes: MetalLB seamlessly integrates with Kubernetes, making it easy to deploy and manage using familiar Kubernetes tools and practices.</para>
</listitem>
<listitem>
<para>Bare-Metal Compatibility: Unlike cloud-based load balancers, MetalLB is designed specifically for on-premises deployments where traditional load balancers might not be available or feasible.</para>
</listitem>
<listitem>
<para>Supports Multiple Protocols: MetalLB supports both Layer 2 and BGP (Border Gateway Protocol) modes, providing flexibility for different network architectures and requirements.</para>
</listitem>
<listitem>
<para>High Availability: By distributing load-balancing responsibilities across multiple nodes, MetalLB ensures high availability and reliability for your services.</para>
</listitem>
<listitem>
<para>Scalability: MetalLB can handle large-scale deployments, scaling alongside your Kubernetes cluster to meet increasing demand.</para>
</listitem>
</orderedlist>
<para>In layer 2 mode, one node assumes the responsibility of advertising a service to the local network. From the network’s perspective, it simply looks like that machine has multiple IP addresses assigned to its network interface.</para>
<para>The major advantage of the layer 2 mode is its universality: it works on any Ethernet network, with no special hardware required, not even fancy routers.</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>MetalLB on K3s (using L2)</title>
<para>In this quick start, L2 mode will be used.
This means we do not need any special network equipment but three free IPs within
the network range.</para>
</section>
<section xml:id="id-prerequisites-7">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>A K3s cluster where MetalLB is going to be deployed.</para>
</listitem>
</itemizedlist>
<warning>
<para>K3S comes with its own service load balancer named Klipper. You <link xl:href="https://metallb.universe.tf/configuration/k3s/">need to disable it to run MetalLB</link>. To disable Klipper, K3s needs to be installed using the <literal>--disable=servicelb</literal> flag.</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>Three free IP adressess within the network range. In this example <literal>192.168.122.10-192.168.122.12</literal></para>
</listitem>
</itemizedlist>
<important>
<para>You must make sure these IP addresses are unassigned.
In a DHCP environment these addresses must not be part of the DHCP pool to avoid dual assignments.</para>
</important>
</section>
<section xml:id="id-deployment-2">
<title>Deployment</title>
<para>We will be using the MetalLB Helm chart published as part of the SUSE Edge solution:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>Configuration</title>
<para>At this point, the installation is completed. Now it is time to <link xl:href="https://metallb.universe.tf/configuration/">configure</link> using our example values:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>Now, it is ready to be used. You can customize many things for L2 mode, such as:</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">IPv6 And Dual Stack Services</link></para>
</listitem>
<listitem>
<para><link xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">Control automatic address allocation</link></para>
</listitem>
<listitem>
<para><link xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">Reduce the scope of address allocation to specific namespaces and services</link></para>
</listitem>
<listitem>
<para><link xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">Limiting the set of nodes where the service can be announced from</link></para>
</listitem>
<listitem>
<para><link xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">Specify network interfaces that LB IP can be announced from</link></para>
</listitem>
</itemizedlist>
<para>And a lot more for <link xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>.</para>
<section xml:id="traefik-and-metallb">
<title>Traefik and MetalLB</title>
<para>Traefik is deployed by default with K3s (<link xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">it can be disabled</link> with <literal>--disable=traefik</literal>) and it is by default exposed as <literal>LoadBalancer</literal> (to be used with Klipper). However, as Klipper needs to be disabled, Traefik service for ingress is still a <literal>LoadBalancer</literal> type. So at the moment of deploying MetalLB, the first IP will be assigned automatically to Traefik Ingress.</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>This will be applied later (<xref linkend="ingress-with-metallb"/>) in the process.</para>
</section>
</section>
<section xml:id="id-usage">
<title>Usage</title>
<para>Let us create an example deployment:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>And finally, the service:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>Let us see it in action:</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<section xml:id="ingress-with-metallb">
<title>Ingress with MetalLB</title>
<para>As Traefik is already serving as an ingress controller, we can expose any HTTP/HTTPS traffic via an <literal>Ingress</literal> object such as:</para>
<screen language="bash" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>And then:</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>Verify that MetalLB works correctly:</para>
<screen language="bash" linenumbering="unnumbered">% arping hellok3s.${IP}.sslip.io

ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>In the example above, the traffic flows as follows:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal> is resolved to the actual IP.</para>
</listitem>
<listitem>
<para>Then the traffic is handled by the <literal>metallb-speaker</literal> pod.</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal> redirects the traffic to the <literal>traefik</literal> controller.</para>
</listitem>
<listitem>
<para>Finally, Traefik forwards the request to the <literal>hello-kubernetes</literal> service.</para>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="guides-metallb-k3s-l3">
<title>MetalLB on K3s (using Layer 3 Mode)</title>
<para>MetalLB is a load-balancer implementation for bare-metal Kubernetes clusters, using standard routing protocols.</para>
<para>In this guide, we demonstrate how to deploy MetalLB in layer 3 (L3) BGP mode.</para>
<section xml:id="id-why-use-metallb-2">
<title>Why use MetalLB</title>
<para>MetalLB is a compelling choice for load balancing in bare-metal Kubernetes clusters for several reasons:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Native Integration with Kubernetes: MetalLB seamlessly integrates with Kubernetes, making it easy to deploy and manage using familiar Kubernetes tools and practices.</para>
</listitem>
<listitem>
<para>Bare-Metal Compatibility: Unlike cloud-based load balancers, MetalLB is designed specifically for on-premises deployments where traditional load balancers might not be available or feasible.</para>
</listitem>
<listitem>
<para>Supports Multiple Protocols: MetalLB supports both Layer 2 and Layer 3 BGP (Border Gateway Protocol) modes, providing flexibility for different network architectures and requirements.</para>
</listitem>
<listitem>
<para>High Availability: By distributing load-balancing responsibilities across multiple nodes, MetalLB ensures high availability and reliability for your services.</para>
</listitem>
<listitem>
<para>Scalability: MetalLB can handle large-scale deployments, scaling alongside your Kubernetes cluster to meet increasing demand.</para>
</listitem>
</orderedlist>
<para>In layer 2 mode, one node assumes the responsibility of advertising a service to the local network. From the network’s perspective, it simply looks like that machine has multiple IP addresses assigned to its network interface.</para>
<para>The major advantage of the layer 2 mode is its universality: it works on any Ethernet network, with no special hardware required, not even fancy routers.</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l3">
<title>MetalLB on K3s (using L3)</title>
<para>In this quick start, L3 mode is used.
This means that we need to have neighboring router(s) with BGP capabilities within
the network range.</para>
</section>
<section xml:id="id-prerequisites-8">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>A K3s cluster where MetalLB is going to be deployed.</para>
</listitem>
<listitem>
<para>Router(s) on the network that support the BGP protocol.</para>
</listitem>
<listitem>
<para>A free IP address within the network range for the service. In this example
<literal>192.168.10.100</literal></para>
</listitem>
</itemizedlist>
<important>
<para>You must make sure this IP address is unassigned.
In a DHCP environment this address must not be part of the DHCP pool to avoid dual assignments.</para>
</important>
</section>
<section xml:id="id-configuration-to-advertise-service-ip-addresses">
<title>Configuration to Advertise Service IP Addresses</title>
<para>Out of the box BGP advertises a Service IP address to all the peers that are
configured. These peers, which are usually routers, will receive a route for
each Service IP address with a 32 bit network mask. In this examlple we will use
an FRR based router and is on the same network as our cluster. We will then use
MetalLB’s BGP capability to advertise a service to that FRR based router.</para>
</section>
<section xml:id="id-deployment-3">
<title>Deployment</title>
<para>We will be using the MetalLB Helm chart published as part of the SUSE Edge solution:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration-2">
<title>Configuration</title>
<orderedlist numeration="arabic">
<listitem>
<para>At this point, the installation is complete. Create an <literal>IPAddressPool</literal>:</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: bgp-pool
  namespace: metallb-system
  labels:
    app: httpd
spec:
  addresses:
  - 192.168.10.100/32
  autoAssign: true
  avoidBuggyIPs: false
  serviceAllocation:
    namespaces:
    - metallb-system
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - key: serviceType
        operator: In
        values:
        - httpd
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>Configure a <literal>BGPPeer</literal>.</para>
</listitem>
</orderedlist>
<note>
<para>The FRR router has ASN 1000 while our <literal>BGPPeer</literal> will have 1001. We can also see that the FRR Router has an IP
address that is 192.168.3.140.</para>
</note>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: mypeertest
spec:
  peerAddress: 192.168.3.140
  peerASN: 1000
  myASN: 1001
  routerID: 4.4.4.4
EOF</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>Create the BGPAdvertisement (L3):</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-test
  namespace: metallb-system
spec:
  ipAddressPools:
  - bgp-pool
EOF</screen>
</section>
<section xml:id="id-usage-2">
<title>Usage</title>
<orderedlist numeration="arabic">
<listitem>
<para>Create an example application with a service. In this case, IP address from the <literal>IPAddressPool</literal> is <literal>192.168.10.100</literal> for that service.</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment
  namespace: metallb-system
  labels:
    app: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      pod-label: httpd
  template:
    metadata:
      labels:
        pod-label: httpd
    spec:
      containers:
      - name: httpdcontainer
        image: image: docker.io/library/httpd:2.4
        ports:
          - containerPort: 80
            protocol: TCP
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: http-service
  namespace: metallb-system
  labels:
    serviceType: httpd
spec:
  selector:
    pod-label: httpd
  type: LoadBalancer
  ports:
  - protocol: TCP
    port: 8080
    name: 8080-tcp
    targetPort: 80
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>To verify, log onto the FRR Router to can see the routes created from the BGP advertisement.</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered">42178089cba5# show ip bgp all

For address family: IPv4 Unicast
BGP table version is 3, local router ID is 2.2.2.2, vrf id 0
Default local pref 100, local AS 1000
Status codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
* i172.16.0.0/24    1.1.1.1                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
* i172.17.0.0/24    3.3.3.3                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
*= 192.168.10.100/32
                    192.168.3.162                          0 1001 i
*=                  192.168.3.163                          0 1001 i
*&gt;                  192.168.3.161                          0 1001 i

Displayed  3 routes and 7 total paths
kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>If this router is the default gateway for your network, you can run the <literal>curl</literal> command from a box on that network to verify that they can reach the httpd sample app</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered"># curl http://192.168.10.100:8080
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
#</screen>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>MetalLB in front of the Kubernetes API server</title>
<para>This guide demonstrates using a MetalLB service to expose the RKE2/K3s API externally on an HA cluster with three control-plane nodes.
To achieve this, a Kubernetes Service of type <literal>LoadBalancer</literal> will be manually created. Then an <literal>EndpointSlices</literal> object will be automatically created which keeps the IPs of all control plane nodes available in the cluster.
For the EndpointSlices to be continuously synchronized with the events occurring in the cluster (adding/removing a node or a node goes offline), the Endpoint Copier Operator (<xref linkend="components-eco"/>) will be deployed. The operator monitors the events happening in the default <literal>kubernetes</literal> EndpointSlices and updates the managed one automatically to keep them in sync.
Since the managed Service is of type <literal>LoadBalancer</literal>, MetalLB assigns it a static <literal>ExternalIP</literal>. This <literal>ExternalIP</literal> will be used to communicate with the API Server.</para>
<section xml:id="id-prerequisites-9">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>Three hosts to deploy RKE2/K3s on top.</para>
<itemizedlist>
<listitem>
<para>Ensure the hosts have different host names.</para>
</listitem>
<listitem>
<para>For testing, these could be virtual machines</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>At least 2 available IPs in the network (one for the Traefik/Nginx and one for the managed service).</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-rke2k3s">
<title>Installing RKE2/K3s</title>
<note>
<para>If you do not want to use a fresh cluster but want to use an existing one, skip this step and proceed to the next one.</para>
</note>
<para>First, a free IP in the network must be reserved that will be used later for <literal>ExternalIP</literal> of the managed Service.</para>
<para>SSH to the first host and install the wanted distribution in cluster mode.</para>
<para>For RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC="server \
 --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" sh -

systemctl enable rke2-server.service
systemctl start rke2-server.service

# Fetch the cluster token:
RKE2_TOKEN=$(tr -d '\n' &lt; /var/lib/rancher/rke2/server/node-token)</screen>
<para>For K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>Make sure that <literal>--disable=servicelb</literal> flag is provided in the <literal>k3s server</literal> command.</para>
</note>
<important>
<para>From now on, the commands should be run on the local machine.</para>
</important>
<para>To access the API server from outside, the IP of the RKE2/K3s VM will be used.</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
export KUBE_DISTRIBUTION=&lt;k3s/rke2&gt;

scp ${NODE_IP}:/etc/rancher/${KUBE_DISTRIBUTION}/${KUBE_DISTRIBUTION}.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-cluster">
<title>Configuring an existing cluster</title>
<note>
<para>This step is valid only if you intend to use an existing RKE2/K3s cluster.</para>
</note>
<para>To use an existing cluster the <literal>tls-san</literal> flags should be modified. Additionally, the <literal>servicelb</literal> LB should be disabled for K3s.</para>
<para>To change the flags for RKE2 or K3s servers, you need to modify either the <literal>/etc/systemd/system/rke2.service</literal> or <literal>/etc/systemd/system/k3s.service</literal> file on all the VMs in the cluster, depending on the distribution.</para>
<para>The flags should be inserted in the <literal>ExecStart</literal>. For example:</para>
<para>For RKE2:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/rke2 \
    server \
        '--write-kubeconfig-mode=644' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>For K3s:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>Then the following commands should be executed to load the new configurations:</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart ${KUBE_DISTRIBUTION}</screen>
</section>
<section xml:id="id-installing-metallb">
<title>Installing MetalLB</title>
<para>To deploy <literal>MetalLB</literal>, the MetalLB on K3s (<xref linkend="guides-metallb-k3s"/>) guide can be used.</para>
<para><emphasis role="strong">NOTE:</emphasis> Ensure that the <literal>VIP_SERVICE_IP</literal> IP address does not overlap with the existing <literal>IPAddressPools</literal> in the cluster.</para>
<para>Create a separate <literal>IpAddressPool</literal> and <literal>L2Advertisement</literal> that will be used only for the managed Service.</para>
<para><emphasis role="strong">NOTE:</emphasis> The IPAddressPool below will be assigned to a Service of type <literal>LoadBalancer</literal> in the <literal>default</literal> Namespace. If multiple <literal>LoadBalancer</literal> services exist there, additional <link xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">ServiceSelectors</link> may be configured to match this VIP service explicitly.</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: kubernetes-vip-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Installing the Endpoint Copier Operator</title>
<screen language="bash" linenumbering="unnumbered">helm install \
endpoint-copier-operator oci://registry.suse.com/edge/charts/endpoint-copier-operator \
--namespace endpoint-copier-operator \
--create-namespace</screen>
<para>The command above will deploy the <literal>endpoint-copier-operator</literal> operator Deployment with two replicas. One will be the leader and the other will take over the leader role if needed.</para>
<para>Now, the <literal>kubernetes-vip</literal> Service should be deployed, which will be reconciled by the operator and an EndpointSlices with the configured ports and IP will be created.</para>
<para>For RKE2:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  type: LoadBalancer
EOF</screen>
<para>For K3s:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer
EOF</screen>
<para>Verify that the <literal>kubernetes-vip</literal> Service has the correct IP address:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para>Ensure that the <literal>kubernetes-vip-*</literal> and <literal>kubernetes</literal> EndpointSlices resources in the <literal>default</literal> namespace point to the same IPs.</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpointslices | grep kubernetes</screen>
<para>If everything is correct, the last thing left is to use the <literal>VIP_SERVICE_IP</literal> in our <literal>Kubeconfig</literal>.</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>From now on, all the <literal>kubectl</literal> will go through the <literal>kubernetes-vip</literal> service.</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>Adding control-plane nodes</title>
<para>To monitor the entire process, two more terminal tabs can be opened.</para>
<para>First terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>Second terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpointslices</screen>
<para>Now execute the commands below on the second and third nodes.</para>
<para>For RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
systemctl enable rke2-server.service


mkdir -p /etc/rancher/rke2/
cat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml
server: https://${VIP_SERVICE_IP}:9345
token: ${RKE2_TOKEN}
EOF

systemctl start rke2-server.service</screen>
<para>For K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Air-gapped deployments with Edge Image Builder</title>
<section xml:id="id-intro">
<title>Intro</title>
<para>This guide will show how to deploy several of the SUSE Edge components completely air-gapped on SUSE Linux Micro 6.1 utilizing Edge Image Builder(EIB) (<xref linkend="components-eib"/>). With this, you’ll be able to boot into a customized, ready to boot (CRB) image created by EIB and have the specified components deployed on either a RKE2 or K3s cluster without an Internet connection or any manual steps. This configuration is highly desirable for customers that want to pre-bake all artifacts required for deployment into their OS image, so they are immediately available on boot.</para>
<para>We will cover an air-gapped installation of:</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-security"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-storage"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIB will parse and pre-download all images referenced in the provided Helm charts and Kubernetes manifests. However, some of those may be attempting to pull container images and create Kubernetes resources based on those at runtime. In these cases we have to manually specify the necessary images in the definition file if we want to set up a completely air-gapped environment.</para>
</warning>
</section>
<section xml:id="id-prerequisites-10">
<title>Prerequisites</title>
<para>If you’re following this guide, it’s assumed that you are already familiar with EIB (<xref linkend="components-eib"/>). If not, please follow the quick start guide (<xref linkend="quickstart-eib"/>) to better understand the concepts shown in practice below.</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Libvirt Network Configuration</title>
<note>
<para>To demo the air-gapped deployment, this guide will be done using a simulated air-gapped <literal>libvirt</literal> network and the following configuration will be tailored to that. For your own deployments, you may have to modify the <literal>host1.local.yaml</literal> configuration that will be introduced in the next step.</para>
</note>
<para>If you would like to use the same <literal>libvirt</literal> network configuration, follow along. If not, skip to <xref linkend="config-dir-creation"/>.</para>
<para>Let’s create an isolated network configuration with an IP address range <literal>192.168.100.2/24</literal> for DHCP:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>Now, the only thing left is to create the network and start it:</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>Base Directory Configuration</title>
<para>The base directory configuration is the same across all different components, so we will set it up here.</para>
<para>We will first create the necessary subdirectories:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>Make sure to add whichever base image you plan to use into the <literal>base-images</literal> directory. This guide will focus on the Self Install ISO found <link xl:href="https://www.suse.com/download/sle-micro/">here</link>.</para>
<para>Let’s copy the downloaded image:</para>
<screen language="shell" linenumbering="unnumbered">cp SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIB is never going to modify the base image input.</para>
</note>
<para>Let’s create a file containing the desired network configuration:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>This configuration ensures the following are present on the provisioned systems (using the specified MAC address):</para>
<itemizedlist>
<listitem>
<para>an Ethernet interface with a static IP address</para>
</listitem>
<listitem>
<para>routing</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>hostname (<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>The resulting file structure should now look like:</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>Base Definition File</title>
<para>Edge Image Builder is using <emphasis>definition files</emphasis> to modify the SUSE Linux Micro images. These files contain the majority of configurable options.
Many of these options will be repeated across the different component sections, so we will list and explain those here.</para>
<tip>
<para>Full list of customization options in the definition file can be found in the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#image-definition-file">upstream documentation</link></para>
</tip>
<para>We will take a look at the following fields which will be present in all definition files:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para>The <literal>image</literal> section is required, and it specifies the input image, its architecture and type, as well as what the output image will be called.</para>
<para>The <literal>operatingSystem</literal> section is optional, and contains configuration to enable login on the provisioned systems with the <literal>root/eib</literal> username/password.</para>
<para>The <literal>kubernetes</literal> section is optional, and it defines the Kubernetes type and version. We are going to use the RKE2 distribution. Use <literal>kubernetes.version: v1.33.3+k3s1</literal> if K3s is desired instead. Unless explicitly configured via the <literal>kubernetes.nodes</literal> field, all clusters we bootstrap in this guide will be single-node ones.</para>
<para>The <literal>embeddedArtifactRegistry</literal> section will include all images which are only referenced and pulled at runtime for the specific component.</para>
</section>
<section xml:id="rancher-install">
<title>Rancher Installation</title>
<note>
<para>The Rancher (<xref linkend="components-rancher"/>) deployment that will be demonstrated will be highly slimmed down for demonstration purposes. For your actual deployments, additional artifacts may be necessary depending on your configuration.</para>
</note>
<para>The <link xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">Rancher 2.12.1</link> release assets contain a <literal>rancher-images.txt</literal> file which lists all the images required for an air-gapped installation.</para>
<para>There are over 600 container images in total which means that the resulting CRB image would be roughly 30GB. For our Rancher installation, we will strip down that list to the smallest working configuration. From there, you can add back any images you may need for your deployments.</para>
<para>We will create the definition file and include the stripped down image list:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.18.2
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v8.0.0
    - name: registry.rancher.com/rancher/compliance-operator:v1.1.0
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.23-build20250612
    - name: registry.rancher.com/rancher/hardened-calico:v3.30.2-build20250711
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.26.0-build20250611
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.21-k3s1-build20250612
    - name: registry.rancher.com/rancher/hardened-flannel:v0.27.1-build20250710
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.33.3-rke2r1-build20250716
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-multus-dynamic-networks-controller:v0.3.7-build20250711
    - name: registry.rancher.com/rancher/hardened-multus-thick:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.9.1-build20250704
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.13
    - name: registry.rancher.com/rancher/kubectl:v1.33.1
    - name: registry.rancher.com/rancher/kuberlr-kubectl:v5.0.0
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.31
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher131
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.10.2
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.rancher.com/rancher/prom-prometheus:v3.2.1
    - name: registry.rancher.com/rancher/prometheus-federator:v4.1.0
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.5-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.5-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.12.1
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v7.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/remotedialer-proxy:v0.5.0
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.33.1-0.20250516163953-99d91538b132-build20250612
    - name: registry.rancher.com/rancher/rke2-runtime:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/scc-operator:v0.1.1
    - name: registry.rancher.com/rancher/security-scan:v0.7.1
    - name: registry.rancher.com/rancher/shell:v0.5.0
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.13-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.rancher.com/rancher/ui-plugin-catalog:4.0.3
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.1
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.2
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.3
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.6.0</screen>
<para>As compared to the full list of 600+ container images, this slimmed down version only contains ~60 which makes the new CRB image only about 7GB.</para>
<para>We also need to create a Helm values file for Rancher:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para>Setting the <literal>systemDefaultRegistry</literal> to <literal>registry.rancher.com</literal> allows Rancher to automatically look for images in the embedded artifact registry started within the CRB image at boot. Omitting this field may result in failure to find the container images on the node.</para>
</warning>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Downloading file: dl-manifest-1.yaml 100% |██████████████████████████████████████████████████████████████████████████████| (583/583 kB, 12 MB/s)
Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████| (56/56, 8 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% |███████████████████████████████████████████████████████████| (644/644 MB, 29 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% |█████████████████████████████████████████████████████████| (400/400 MB, 29 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% |███████████████████████████████████████████████████████████████████████████| (36/36 MB, 30 MB/s)
Downloading file: sha256sum-amd64.txt 100% |█████████████████████████████████████████████████████████████████████████████| (4.3/4.3 kB, 29 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Once a node using the built image is provisioned, we can verify the Rancher installation:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cattle-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/helm-operation-6l6ld                        0/2     Completed   0          107s
pod/helm-operation-8tk2v                        0/2     Completed   0          2m2s
pod/helm-operation-blnrr                        0/2     Completed   0          2m49s
pod/helm-operation-hdcmt                        0/2     Completed   0          3m19s
pod/helm-operation-m74c7                        0/2     Completed   0          97s
pod/helm-operation-qzzr4                        0/2     Completed   0          2m30s
pod/helm-operation-s9jh5                        0/2     Completed   0          3m
pod/helm-operation-tq7ts                        0/2     Completed   0          2m41s
pod/rancher-99d599967-ftjkk                     1/1     Running     0          4m15s
pod/rancher-webhook-79798674c5-6w28t            1/1     Running     0          2m27s
pod/system-upgrade-controller-56696956b-trq5c   1/1     Running     0          104s

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/rancher           ClusterIP   10.43.255.80   &lt;none&gt;        80/TCP,443/TCP   4m15s
service/rancher-webhook   ClusterIP   10.43.7.238    &lt;none&gt;        443/TCP          2m27s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rancher                     1/1     1            1           4m15s
deployment.apps/rancher-webhook             1/1     1            1           2m27s
deployment.apps/system-upgrade-controller   1/1     1            1           104s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/rancher-99d599967                     1         1         1       4m15s
replicaset.apps/rancher-webhook-79798674c5            1         1         1       2m27s
replicaset.apps/system-upgrade-controller-56696956b   1         1         1       104s</screen>
<para>And when we go to <literal>https://192.168.100.50.sslip.io</literal> and log in with the <literal>adminadminadmin</literal> password that we set earlier, we are greeted with the Rancher dashboard:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="air-gapped-rancher.png" width="100%"/>
</imageobject>
<textobject><phrase>air gapped rancher</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="suse-security-install">
<title>SUSE Security Installation</title>
<para>Unlike the Rancher installation, the SUSE Security installation does not require any special handling in EIB. EIB will automatically air-gap every image required by its underlying component NeuVector.</para>
<para>We will create the definition file:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>We will also create a Helm values file for NeuVector:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 4 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████| (5/5, 13 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Once a node using the built image is provisioned, we can verify the SUSE Security installation:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/neuvector-cert-upgrader-job-bxbnz           0/1     Completed   0          3m39s
pod/neuvector-controller-pod-7d854bfdc7-nhxjf   1/1     Running     0          3m44s
pod/neuvector-enforcer-pod-ct8jm                1/1     Running     0          3m44s

NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.234.241   &lt;none&gt;        443/TCP                         3m44s
service/neuvector-svc-controller          ClusterIP   None            &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   3m44s
service/neuvector-svc-crd-webhook         ClusterIP   10.43.50.190    &lt;none&gt;        443/TCP                         3m44s

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          3m44s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           3m44s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-7d854bfdc7   1         1         1       3m44s

NAME                                        SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-cert-upgrader-pod   0 0 1 1 *   &lt;none&gt;     True      0        &lt;none&gt;          3m44s
cronjob.batch/neuvector-updater-pod         0 0 * * *   &lt;none&gt;     False     0        &lt;none&gt;          3m44s

NAME                                    STATUS     COMPLETIONS   DURATION   AGE
job.batch/neuvector-cert-upgrader-job   Complete   1/1           7s         3m39s</screen>
</section>
<section xml:id="suse-storage-install">
<title>SUSE Storage Installation</title>
<para>The <link xl:href="https://longhorn.io/docs/1.9.1/deploy/install/airgap/">official documentation</link> for Longhorn contains a
<literal>longhorn-images.txt</literal> file which lists all the images required for an air-gapped installation.
We will be including their mirrored counterparts from the Rancher container registry in our definition file.
Let’s create it:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: [reg-code]
    packageList:
      - open-iscsi
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 107.0.0+up1.9.1
      - name: longhorn-crd
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        version: 107.0.0+up1.9.1
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.52
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1</screen>
<note>
<para>You will notice that the definition file lists the <literal>open-iscsi</literal> package. This is necessary since Longhorn
relies on a <literal>iscsiadm</literal> daemon running on the different nodes to provide persistent volumes to Kubernetes.</para>
</note>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 20956 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Once a node using the built image is provisioned, we can verify the Longhorn installation:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/csi-attacher-787fd9c6c8-sf42d                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-tb82p                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-zhc6s                       1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-b2v9s                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-hwllt                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-mlrpk                    1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-t54zk                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-vdt5d                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-x9kh4                        1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-r62gr                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-vrwjn                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-z65nb                    1/1     Running   0          2m28s
pod/engine-image-ei-4623b511-9vhkb                      1/1     Running   0          3m13s
pod/instance-manager-6f95fd57d4a4cd0459e469d75a300552   1/1     Running   0          2m43s
pod/longhorn-csi-plugin-gx98x                           3/3     Running   0          2m28s
pod/longhorn-driver-deployer-55f9c88499-fbm6q           1/1     Running   0          3m28s
pod/longhorn-manager-dpdp7                              2/2     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-gg5hq                        1/1     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-s49jc                        1/1     Running   0          3m28s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.77.89    &lt;none&gt;        9502/TCP   3m28s
service/longhorn-backend              ClusterIP   10.43.56.17    &lt;none&gt;        9500/TCP   3m28s
service/longhorn-conversion-webhook   ClusterIP   10.43.54.73    &lt;none&gt;        9501/TCP   3m28s
service/longhorn-frontend             ClusterIP   10.43.22.82    &lt;none&gt;        80/TCP     3m28s
service/longhorn-recovery-backend     ClusterIP   10.43.45.143   &lt;none&gt;        9503/TCP   3m28s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-4623b511   1         1         1       1            1           &lt;none&gt;          3m13s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          2m28s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          3m28s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           2m28s
deployment.apps/csi-provisioner            3/3     3            3           2m28s
deployment.apps/csi-resizer                3/3     3            3           2m28s
deployment.apps/csi-snapshotter            3/3     3            3           2m28s
deployment.apps/longhorn-driver-deployer   1/1     1            1           3m28s
deployment.apps/longhorn-ui                2/2     2            2           3m28s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-787fd9c6c8               3         3         3       2m28s
replicaset.apps/csi-provisioner-74486b95c6            3         3         3       2m28s
replicaset.apps/csi-resizer-859d4557fd                3         3         3       2m28s
replicaset.apps/csi-snapshotter-6f69c6c8cc            3         3         3       2m28s
replicaset.apps/longhorn-driver-deployer-55f9c88499   1         1         1       3m28s
replicaset.apps/longhorn-ui-59c85fcf94                2         2         2       3m28s</screen>
</section>
<section xml:id="kubevirt-install">
<title>KubeVirt and CDI Installation</title>
<para>The Helm charts for both KubeVirt and CDI are only installing their respective operators.
It is up to the operators to deploy the rest of the systems which means we will have to include all
necessary container images in our definition file. Let’s create it:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: kubevirt
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/virt-api:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-controller:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-handler:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2-150700.3.5.2</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 48 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 4 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Once a node using the built image is provisioned, we can verify the installation of both KubeVirt and CDI.</para>
<para>Verify KubeVirt:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                  READY   STATUS    RESTARTS   AGE
pod/virt-api-59cb997648-mmt67         1/1     Running   0          2m34s
pod/virt-controller-69786b785-7cc96   1/1     Running   0          2m8s
pod/virt-controller-69786b785-wq2dz   1/1     Running   0          2m8s
pod/virt-handler-2l4dm                1/1     Running   0          2m8s
pod/virt-operator-7c444cff46-nps4l    1/1     Running   0          3m1s
pod/virt-operator-7c444cff46-r25xq    1/1     Running   0          3m1s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.167.109   &lt;none&gt;        443/TCP   2m36s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m36s
service/virt-api                      ClusterIP   10.43.18.202    &lt;none&gt;        443/TCP   2m36s
service/virt-exportproxy              ClusterIP   10.43.142.188   &lt;none&gt;        443/TCP   2m36s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   2m8s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m34s
deployment.apps/virt-controller   2/2     2            2           2m8s
deployment.apps/virt-operator     2/2     2            2           3m1s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-59cb997648         1         1         1       2m34s
replicaset.apps/virt-controller-69786b785   2         2         2       2m8s
replicaset.apps/virt-operator-7c444cff46    2         2         2       3m1s

NAME                            AGE    PHASE
kubevirt.kubevirt.io/kubevirt   3m1s   Deployed</screen>
<para>Verify CDI:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-5598c9bf47-pqfxw     1/1     Running   0          3m44s
pod/cdi-deployment-7cbc5db7f8-g46z7    1/1     Running   0          3m44s
pod/cdi-operator-777c865745-2qcnj      1/1     Running   0          3m48s
pod/cdi-uploadproxy-646f4cd7f7-fzkv7   1/1     Running   0          3m44s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.2.224    &lt;none&gt;        443/TCP    3m44s
service/cdi-prometheus-metrics   ClusterIP   10.43.237.13   &lt;none&gt;        8080/TCP   3m44s
service/cdi-uploadproxy          ClusterIP   10.43.114.91   &lt;none&gt;        443/TCP    3m44s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m44s
deployment.apps/cdi-deployment    1/1     1            1           3m44s
deployment.apps/cdi-operator      1/1     1            1           3m48s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m44s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-5598c9bf47     1         1         1       3m44s
replicaset.apps/cdi-deployment-7cbc5db7f8    1         1         1       3m44s
replicaset.apps/cdi-operator-777c865745      1         1         1       3m48s
replicaset.apps/cdi-uploadproxy-646f4cd7f7   1         1         1       3m44s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>Troubleshooting</title>
<para>If you run into any issues while building the images or are looking to further test and debug the process, please refer to the <link xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">upstream documentation</link>.</para>
</section>
</chapter>
<chapter xml:id="guides-kiwi-builder-images">
<title>Building Updated SUSE Linux Micro Images with Kiwi</title>
<para>This section explains how to generate updated SUSE Linux Micro images to be used with Edge Image Builder, with Cluster API (CAPI) + Metal<superscript>3</superscript>, or to write the disk image directly to a block device. This process is useful in situations where the latest patches are required to be included in the initial system boot images (to minimise patch transfer post-installation), or for scenarios where CAPI is used, where it’s preferred to reinstall the operating system with a new image rather than upgrading the hosts in place.</para>
<para>This process makes use of <link xl:href="https://osinside.github.io/kiwi/">Kiwi</link> to run the image build. SUSE Edge ships with a containerized version that simplifies the overall process with a helper utility baked in, allowing to specify the target <emphasis role="strong">profile</emphasis> required. The profile defines the type of output image that is required, with the common ones listed below:</para>
<itemizedlist>
<listitem>
<para>"<emphasis role="strong">Base</emphasis>" - A SUSE Linux Micro disk image with a reduced package set (it includes podman).</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-SelfInstall</emphasis>" - A SelfInstall image based on the "Base" above.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-RT</emphasis>" - Same as "Base" above but using a real-time (rt) kernel instead.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-RT-SelfInstall</emphasis>" - A SelfInstall image based on the "Base-RT" above</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Default</emphasis>" - A SUSE Linux Micro disk image based on the "Base" above but with a few more tools, including the virtualization stack, Cockpit and salt-minion.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Default-SelfInstall</emphasis>" - A SelfInstall image based on the "Default" above</para>
</listitem>
</itemizedlist>
<para>See <link xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-deployment-images/index.html#alp-images-installer-type">SUSE Linux Micro 6.1</link> documentation for more details.</para>
<para>This process works for both AMD64/Intel 64 and AArch64 architectures, although not all image profiles are available for both architectures, e.g. in SUSE Edge 3.4, where SUSE Linux Micro 6.1 is used, a profile with a real-time kernel (i.e. "Base-RT" or "Base-RT-SelfInstall") is not currently available for AArch64.</para>
<note>
<para>It is required to use a build host with the same architecture of the images being built. In other words, to build an AArch64 image, it is required to use an AArch64 build host, and vice-versa for AMD64/Intel 64 - cross-builds are not supported at this time.</para>
</note>
<section xml:id="id-prerequisites-11">
<title>Prerequisites</title>
<para>Kiwi image builder requires the following:</para>
<itemizedlist>
<listitem>
<para>A SUSE Linux Micro 6.1 host ("build system") with the same architecture of the image being built.</para>
</listitem>
<listitem>
<para>The build system needs to be already registered via <literal>SUSEConnect</literal> (the registration is used to pull the latest packages from the SUSE repositories)</para>
</listitem>
<listitem>
<para>An internet connection that can be used to pull the required packages. If connected via proxy, the build host needs to be pre-configured.</para>
</listitem>
<listitem>
<para>SELinux needs to be disabled on the build host (as SELinux labelling takes place in the container and it can conflict with the host policy)</para>
</listitem>
<listitem>
<para>At least 10GB free disk space to accommodate the container image, the build root, and the resulting output image(s)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started-2">
<title>Getting Started</title>
<para>Due to certain limitations, it is currently required to disable SELinux. Connect to the SUSE Linux Micro 6.1 image build host and ensure SELinux is disabled:</para>
<screen language="console" linenumbering="unnumbered"># setenforce 0</screen>
<para>Create an output directory to be shared with the Kiwi build container to save the resulting images:</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/output</screen>
<para>Pull the latest Kiwi builder image from the SUSE Registry:</para>
<screen language="console" linenumbering="unnumbered"># podman pull registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
(...)</screen>
</section>
<section xml:id="id-building-the-default-image">
<title>Building the Default Image</title>
<para>This is the default behavior of the Kiwi image container if no arguments are provided during the container image run. The following command runs <literal>podman</literal> with two directories mapped to the container:</para>
<itemizedlist>
<listitem>
<para>The <literal>/etc/zypp/repos.d</literal> SUSE Linux Micro package repository directory from the underlying host.</para>
</listitem>
<listitem>
<para>The output <literal>~/output</literal> directory created above.</para>
</listitem>
</itemizedlist>
<para>The Kiwi image container requires to run the <literal>build-image</literal> helper script as:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<note>
<para>It’s expected that if you’re running this script for the first time that it will <emphasis role="strong">fail</emphasis> shortly after starting with "<emphasis role="strong">ERROR: Early loop device test failed, please retry the container run.</emphasis>", this is a symptom of loop devices being created on the underlying host system that are not immediately visible inside of the container image. Simply re-run the command again and it should proceed without issue.</para>
</note>
<para>After a few minutes the images can be found in the local output directory:</para>
<screen language="console" linenumbering="unnumbered">(...)
INFO: Image build successful, generated images are available in the 'output' directory.

# ls -1 output/
SLE-Micro.x86_64-6.1.changes
SLE-Micro.x86_64-6.1.packages
SLE-Micro.x86_64-6.1.raw
SLE-Micro.x86_64-6.1.verified
build
kiwi.result
kiwi.result.json</screen>
</section>
<section xml:id="id-building-images-with-other-profiles">
<title>Building images with other profiles</title>
<para>In order to build different image profiles, the "<emphasis role="strong">-p</emphasis>" command option in the Kiwi container image helper script is used. For example, to build the "<emphasis role="strong">Default-SelfInstall</emphasis>" ISO image:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall
(...)</screen>
<note>
<para>To avoid data loss, Kiwi will refuse to run if there are images in the <literal>output</literal> directory. It is required to remove the contents of the output directory before proceeding with <literal>rm -f output/*</literal>.</para>
</note>
<para>Alternatively, to build a SelfInstall ISO image with the RealTime kernel ("<emphasis role="strong">kernel-rt</emphasis>"):</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Base-RT-SelfInstall
(...)</screen>
</section>
<section xml:id="id-building-images-with-large-sector-sizes">
<title>Building images with large sector sizes</title>
<para>Some hardware requires an image with a large sector size, i.e. <emphasis role="strong">4096 bytes</emphasis> rather than the standard 512 bytes. The containerized Kiwi builder supports the ability to generate images with large block size by specifying the "<emphasis role="strong">-b</emphasis>" parameter. For example, to build a "<emphasis role="strong">Default-SelfInstall</emphasis>" image with a large sector size:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall -b
(...)</screen>
</section>
<section xml:id="id-using-a-custom-kiwi-image-definition-file">
<title>Using a custom Kiwi image definition file</title>
<para>For advanced use-cases a custom Kiwi image definition file (<literal>SL-Micro.kiwi</literal>) can be used along with any necessary post-build scripts. This requires overriding the default definitions pre-packaged by the SUSE Edge team.</para>
<para>Create a new directory and map it into the container image where the helper script is looking (<literal>/micro-sdk/defs</literal>):</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/mydefs/
# cp /path/to/SL-Micro.kiwi ~/mydefs/
# cp /path/to/config.sh ~/mydefs/
# podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output -v ~/mydefs/:/micro-sdk/defs/ \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<warning>
<para>This is only required for advanced use-cases and may cause supportability issues. Please contact your SUSE representative for further advice and guidance.</para>
</warning>
<para>To get the default Kiwi image definition files included in the container, the following commands can be used:</para>
<screen language="console" linenumbering="unnumbered">$ podman create --name kiwi-builder registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi .
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi.4096 .
$ podman rm kiwi-builder
$ ls ./SL-Micro.*
(...)</screen>
</section>
</chapter>
<chapter xml:id="guides-clusterclass-example">
<title>Using clusterclass to deploy downstream clusters</title>
<section xml:id="id-introduction">
<title>Introduction</title>
<para>Provisioning Kubernetes clusters is a complex task that demands deep expertise in configuring cluster components. As configurations grow more intricate,
or as the demands of different providers introduce numerous provider-specific resource definitions, cluster creation can feel daunting.
Thankfully, Kubernetes Cluster API (CAPI) offers a more elegant, declarative approach that is further enhanced by ClusterClass.
This feature introduces a template-driven model, allowing you to define a reusable cluster class that encapsulates complexity and promotes consistency.</para>
</section>
<section xml:id="id-what-is-clusterclass">
<title>What is ClusterClass?</title>
<para>The CAPI project introduced the ClusterClass feature as a paradigm shift in Kubernetes cluster lifecycle management through the adoption of a template-based methodology for cluster instantiation. Instead of defining resources independently for every cluster, users define a ClusterClass, which serves as a comprehensive and reusable blueprint. This abstract representation encapsulates the desired state and configuration of a Kubernetes cluster, enabling the rapid and consistent creation of multiple clusters that adhere to the defined specifications.
This abstraction reduces the configuration burden, resulting in more manageable deployment manifests.  This means that the core components of a workload cluster are defined at the class level allowing users to use these templates as Kubernetes cluster flavors that can be reused one/many times for cluster provisioning.
The implementation of ClusterClass yields several key advantages that address the inherent challenges of traditional CAPI management at scale:</para>
<itemizedlist>
<listitem>
<para>Substantial Reduction in Complexity and YAML Verbosity</para>
</listitem>
<listitem>
<para>Optimized Maintenance and Update Processes</para>
</listitem>
<listitem>
<para>Enhanced Consistency and Standardization Across Deployments</para>
</listitem>
<listitem>
<para>Improved Scalability and Automation Capabilities</para>
</listitem>
<listitem>
<para>Declarative Management and Robust Version Control</para>
</listitem>
</itemizedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="clusterclass.png" width="100%"/>
</imageobject>
<textobject><phrase>clusterclass</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-example-of-current-capi-provisioning-file">
<title>Example of current CAPI provisioning file</title>
<para>The deployment of a Kubernetes cluster leveraging the Cluster API (CAPI) and the RKE2 provider requires definition of several custom resources.
These resources define the desired state of the cluster and its underlying infrastructure, enabling CAPI to orchestrate the provisioning and management lifecycle.
The code snippet below illustrates the resource types that must be configured:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Cluster</emphasis>: This resource encapsulates high-level configurations, including the network topology that will govern inter-node communication and service discovery. Furthermore, it establishes essential linkages to the control plane specification and the designated infrastructure provider resource, thereby informing CAPI about the desired cluster architecture and the underlying infrastructure upon which it will be provisioned.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3Cluster</emphasis>: This resource defines infrastructure-level attributes unique to Metal3, for example the external endpoint through which the Kubernetes API server will be accessible.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlane</emphasis>: The RKE2ControlPlane resource defines the characteristics and behavior of the cluster’s control plane nodes. Within this specification, parameters such as the desired number of control plane replicas (crucial for ensuring high availability and fault tolerance), the specific Kubernetes distribution version (aligned with the chosen RKE2 release), and the strategy for rolling out updates to the control plane components are configured. Additionally, this resource dictates the Container Network Interface (CNI) to be employed within the cluster and facilitates the injection of agent-specific configurations, often leveraging Ignition for seamless and automated provisioning of the RKE2 agents on the control plane nodes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3MachineTemplate</emphasis>: This resource acts as a blueprint for the creation of the individual compute instances that will form the worker nodes of the Kubernetes cluster defining the image to be used.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3DataTemplate</emphasis>: Complementing the Metal3MachineTemplate, the Metal3DataTemplate resource enables additional metadata to be specified for the newly provisioned machine instances.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: emea-spa-cluster-3
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: emea-spa-cluster-3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  controlPlaneEndpoint:
    host: 192.168.122.203
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: emea-spa-cluster-3
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  registrationMethod: "control-plane-endpoint"
  registrationAddress: 192.168.122.203
  serverConfig:
    cni: cilium
    cniMultusEnable: true
    tlsSan:
      - 192.168.122.203
      - https://192.168.122.203.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - 192.168.122.203/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          node: group-3
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
</section>
<section xml:id="id-transforming-the-capi-provisioning-file-to-clusterclass">
<title>Transforming the CAPI provisioning file to ClusterClass</title>
<section xml:id="id-clusterclass-definition">
<title>ClusterClass definition</title>
<para>The following code defines a ClusterClass resource, a declarative template for consistently deploying a specific type of Kubernetes cluster. This specification includes common infrastructure and control plane configurations, enabling efficient provisioning and uniform lifecycle management across a cluster fleet.
There are some variables in the following clusterclass example, that will be replaced during the cluster instatiation process using the real values.
The following variables are used in the example:</para>
<itemizedlist>
<listitem>
<para><literal>controlPlaneMachineTemplate</literal>: This is the name to define the ControlPlane Machine Template reference to be used</para>
</listitem>
<listitem>
<para><literal>controlPlaneEndpointHost</literal>: This is the host name or IP address of the control plane endpoint</para>
</listitem>
<listitem>
<para><literal>tlsSan</literal>: This is the TLS Subject Alternative Name for the control plane endpoint</para>
</listitem>
</itemizedlist>
<para>The clusterclass definition file is defined based on the 3 following resources:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ClusterClass</emphasis>: This resource encapsulates the entire cluster class definition, including the control plane and infrastructure templates. Moreover, it include the list of variables that will be replaced during the instantiation process.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlaneTemplate</emphasis>: This resource defines the control plane template, specifying the desired configuration for the control plane nodes. It includes parameters such as the number of replicas, the Kubernetes version, and the CNI to be used. Also, some paremeters will be replaced with the right values during the instantiation process.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3ClusterTemplate</emphasis>: This resource defines the infrastructure template, specifying the desired configuration for the underlying infrastructure. It includes parameters such as the control plane endpoint and the noCloudProvider flag. Also, some paremeters will be replaced with the right values during the instantiation process.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlaneTemplate
metadata:
  name: example-controlplane-type2
  namespace: emea-spa
spec:
  template:
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: example-controlplane    # This will be replaced by the patch applied in each cluster instances
        namespace: emea-spa
      replicas: 1
      version: v1.33.3+rke2r1
      rolloutStrategy:
        type: "RollingUpdate"
        rollingUpdate:
          maxSurge: 1
      registrationMethod: "control-plane-endpoint"
      registrationAddress: "default"  # This will be replaced by the patch applied in each cluster instances
      serverConfig:
        cni: cilium
        cniMultusEnable: true
        tlsSan:
          - "default"  # This will be replaced by the patch applied in each cluster instances
      agentConfig:
        format: ignition
        additionalUserData:
          config: |
            default
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3ClusterTemplate
metadata:
  name: example-cluster-template-type2
  namespace: emea-spa
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: "default"  # This will be replaced by the patch applied in each cluster instances
        port: 6443
      noCloudProvider: true
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: example-clusterclass-type2
  namespace: emea-spa
spec:
  variables:
    - name: controlPlaneMachineTemplate
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: controlPlaneEndpointHost
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: tlsSan
      required: true
      schema:
        openAPIV3Schema:
          type: array
          items:
            type: string
  infrastructure:
    ref:
      kind: Metal3ClusterTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: example-cluster-template-type2
  controlPlane:
    ref:
      kind: RKE2ControlPlaneTemplate
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      name: example-controlplane-type2
  patches:
    - name: setControlPlaneMachineTemplate
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/infrastructureRef/name"
              valueFrom:
                variable: controlPlaneMachineTemplate
    - name: setControlPlaneEndpoint
      definitions:
        - selector:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: Metal3ClusterTemplate
            matchResources:
              infrastructureCluster: true  # Added to select InfraCluster
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/controlPlaneEndpoint/host"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setRegistrationAddress
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/registrationAddress"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setTlsSan
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/serverConfig/tlsSan"
              valueFrom:
                variable: tlsSan
    - name: updateAdditionalUserData
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/agentConfig/additionalUserData"
              valueFrom:
                template: |
                  config: |
                    variant: fcos
                    version: 1.4.0
                    storage:
                      files:
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: endpoint-copier-operator
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                                targetNamespace: endpoint-copier-operator
                                version: 304.0.1+up0.3.0
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: metallb
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/metallb
                                targetNamespace: metallb-system
                                version: 304.0.0+up0.14.9
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: metallb.io/v1beta1
                              kind: IPAddressPool
                              metadata:
                                name: kubernetes-vip-ip-pool
                                namespace: metallb-system
                              spec:
                                addresses:
                                  - {{ .controlPlaneEndpointHost }}/32
                                serviceAllocation:
                                  priority: 100
                                  namespaces:
                                    - default
                                  serviceSelectors:
                                    - matchExpressions:
                                      - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                              ---
                              apiVersion: metallb.io/v1beta1
                              kind: L2Advertisement
                              metadata:
                                name: ip-pool-l2-adv
                                namespace: metallb-system
                              spec:
                                ipAddressPools:
                                  - kubernetes-vip-ip-pool
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: v1
                              kind: Service
                              metadata:
                                name: kubernetes-vip
                                namespace: default
                                labels:
                                  serviceType: kubernetes-vip
                              spec:
                                ports:
                                - name: rke2-api
                                  port: 9345
                                  protocol: TCP
                                  targetPort: 9345
                                - name: k8s-api
                                  port: 6443
                                  protocol: TCP
                                  targetPort: 6443
                                type: LoadBalancer
                    systemd:
                      units:
                        - name: rke2-preinstall.service
                          enabled: true
                          contents: |
                            [Unit]
                            Description=rke2-preinstall
                            Wants=network-online.target
                            Before=rke2-install.service
                            ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                            [Service]
                            Type=oneshot
                            User=root
                            ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                            ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                            ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                            ExecStartPost=/bin/sh -c "umount /mnt"
                            [Install]
                            WantedBy=multi-user.target</screen>
</section>
<section xml:id="id-cluster-instance-definition">
<title>Cluster instance definition</title>
<para>Within the context of ClusterClass, a cluster instance refers to a specific, running instantiation of a cluster that has been created based on a defined ClusterClass.
It represents a concrete deployment with its unique configurations, resources, and operational state, directly derived from the blueprint specified in the ClusterClass.
This includes the specific set of machines, networking configurations, and associated Kubernetes components that are actively running.
Understanding the cluster instance is crucial for managing the lifecycle, performing upgrades, executing scaling operations, and conducting monitoring of a particular deployed cluster that was provisioned using the ClusterClass framework.</para>
<para>To define a cluster instance we need to define the following resources:</para>
<itemizedlist>
<listitem>
<para>Cluster</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate</para>
</listitem>
<listitem>
<para>Metal3DataTemplate</para>
</listitem>
</itemizedlist>
<para>The variables defined previously in the template (clusterclass definition file) will be replaced with the final values for this instantiation of cluster:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  topology:
    class: example-clusterclass-type2  # Correct way to reference ClusterClass
    version: v1.33.3+rke2r1
    controlPlane:
      replicas: 1
    variables:                         # Variables to be replaced for this cluster instance
      - name: controlPlaneMachineTemplate
        value: emea-spa-cluster-3-machinetemplate
      - name: controlPlaneEndpointHost
        value: 192.168.122.203
      - name: tlsSan
        value:
          - 192.168.122.203
          - https://192.168.122.203.sslip.io
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3-machinetemplate
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          cluster-type: type2
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine</screen>
<para>This approach allows for a more streamlined process, deploying a cluster with only 3 resources once you have defined the clusterclass.</para>
</section>
</section>
</chapter>
</part>
<part xml:id="tips-and-tricks">
<title>Tips and Tricks</title>
<partintro>
<para>Tips and tricks for Edge components</para>
</partintro>
<chapter xml:id="tips-edge-image-builder">
<title>Edge Image Builder</title>
<section xml:id="id-common">
<title>Common</title>
<itemizedlist>
<listitem>
<para>If you are in a non-Linux environment and following these instructions to build an image, then you are likely running <literal>Podman</literal> via a virtual machine. By default, this virtual machine will be configured to have a small amount of system resources allocated to it and can cause instability for <literal>Edge Image Builder</literal> during resource intensive operations, such as the RPM resolution process. You will need to adjust the resources of the podman machine, either by using Podman Desktop (settings cogwheel → podman machine edit icon) or directly  via the <literal>podman-machine-set</literal> <link xl:href="https://docs.podman.io/en/stable/markdown/podman-machine-set.1.html">command</link></para>
</listitem>
<listitem>
<para>At this point in time, the <literal>Edge Image Builder</literal> is not able to build images in a cross architecture setup, i.e. you have to run it on:</para>
<itemizedlist>
<listitem>
<para>AArch64 systems (such as Apple Silicon) to build SL Micro <literal>aarch64</literal> images</para>
</listitem>
<listitem>
<para>AMD64/Intel 64 systems to build SL Micro <literal>x86_64</literal> images.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-kubernetes">
<title>Kubernetes</title>
<itemizedlist>
<listitem>
<para>Creating multi node Kubernetes clusters requires adjusting the <literal>kubernetes</literal> section in the definition file to:</para>
<itemizedlist>
<listitem>
<para>list all server and agent nodes under <literal>kubernetes.nodes</literal></para>
</listitem>
<listitem>
<para>set a virtual IP address that would be used for all non-initializer nodes to join the cluster under <literal>kubernetes.network.apiVIP</literal></para>
</listitem>
<listitem>
<para>optionally, set an API host to specify a domain address for accessing the cluster under <literal>kubernetes.network.apiHost</literal>
To learn more about this configuration, please refer to the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">Kubernetes section docs</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Edge Image Builder</literal> relies on the hostnames of the different nodes to determine their Kubernetes type (<literal>server</literal> or <literal>agent</literal>). While this configuration is managed in the definition file, for the general networking setup of the machines we can utilize either DHCP configuration as described in <xref linkend="components-nmc"/>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="tips-elemental">
<title>Elemental</title>
<section xml:id="id-common-2">
<title>Common</title>
<section xml:id="id-expose-rancher-service">
<title>Expose Rancher service</title>
<para>When using RKE2 or K3s we need to expose services (Rancher in this context) from the management cluster as they are not exposed by default.
In RKE2 there is an NGINX Ingress controller, whilst k3s is using Traefik.
The current workflow suggests using MetalLB for announcing a service (via L2 or BGP Advertisement) and the respective Ingress Controller
to create an Ingress via <literal>HelmChartConfig</literal> since creating a new Ingress object would override the existing setup.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Install Rancher Prime (via Helm) and configure the necessary values</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-192.168.64.101.sslip.io
replicas: 1
bootstrapPassword: Admin
global.cattle.psp.enabled: "false"</screen>
<tip>
<para>Follow the <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Rancher installation</link> documentation for more details.</para>
</tip>
</listitem>
<listitem>
<para>Create a LoadBalancer service to expose Rancher</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local
EOF</screen>
</listitem>
<listitem>
<para>Create an IP Address Pool for the service using the IP address we set up earlier in the Helm values</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.64.101/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}
EOF</screen>
</listitem>
<listitem>
<para>Create an L2 Advertisement for the IP address pool</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ingress-ippool
EOF</screen>
</listitem>
<listitem>
<para>Ensure Elemental is properly installed</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Install the Elemental Operator and Elemental UI on the management nodes</para>
</listitem>
<listitem>
<para>Add the Elemental configuration on the downstream node together with a registration code, as that will prompt Edge Image Builder to include the remote registration option for the machine.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<tip>
<para>Check <xref linkend="install-elemental"/> and <xref linkend="configure-elemental"/> for additional information and examples.</para>
</tip>
</section>
</section>
<section xml:id="id-hardware-specific">
<title>Hardware Specific</title>
<section xml:id="id-trusted-platform-module">
<title>Trusted Platform Module</title>
<para>It is necessary to properly handle the <link xl:href="https://elemental.docs.rancher.com/tpm/">Trusted Platform Module</link> (TPM) configuration.
Failing to do so will result in errors similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Nov 25 18:17:06 eled elemental-register[4038]: Error: registering machine: cannot generate authentication token: opening tpm for getting attestation data: TPM device not available</screen>
<para>This can be mitigated by one of the following approaches:</para>
<itemizedlist>
<listitem>
<para>Enable TPM in the Virtual Machine settings</para>
</listitem>
</itemizedlist>
<para><emphasis>Example with UTM on MacOS</emphasis></para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="tpm.png" width="100%"/>
</imageobject>
<textobject><phrase>TPM</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para>Emulate TPM by using negative value for the TPM seed in the <literal>MachineRegistration</literal> resource</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: true
        emulated-tpm-seed: -1</screen>
<itemizedlist>
<listitem>
<para>Disable TPM in the <literal>MachineRegistration</literal> resource</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: false</screen>
</section>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>Third-Party Integration</title>
<partintro>
<para>How to integrate third-party tools</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link xl:href="https://nats.io/">NATS</link> is a connective technology built for the ever-increasingly hyper-connected world. It is a single technology that enables applications to securely communicate across any combination of cloud vendors, on-premises, edge, Web and mobile devices. NATS consists of a family of open-source products that are tightly integrated but can be deployed easily and independently. NATS is being used globally by thousands of companies, spanning use cases including microservices, edge computing, mobile and IoT, and can be used to augment or replace traditional messaging.</para>
<section xml:id="id-architecture">
<title>Architecture</title>
<para>NATS is an infrastructure that allows data exchange between applications in the form of messages.</para>
<section xml:id="id-nats-client-applications">
<title>NATS client applications</title>
<para>NATS client libraries can be used to allow the applications to publish, subscribe, request and reply between different instances.
These applications are generally referred to as <literal>client applications</literal>.</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>NATS service infrastructure</title>
<para>The NATS services are provided by one or more NATS server processes that are configured to interconnect with each other and provide a NATS service infrastructure. The NATS service infrastructure can scale from a single NATS server process running on an end device to a public global super-cluster of many clusters spanning all major cloud providers and all regions of the world.</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>Simple messaging design</title>
<para>NATS makes it easy for applications to communicate by sending and receiving messages. These messages are addressed and identified by subject strings and do not depend on network location.
Data is encoded and framed as a message and sent by a publisher. The message is received, decoded and processed by one or more subscribers.</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATS has a built-in distributed persistence system called JetStream.
JetStream was created to solve the problems identified with streaming in technology today — complexity, fragility and a lack of scalability. JetStream also solves the problem with the coupling of the publisher and the subscriber (the subscribers need to be up and running to receive the message when it is published).
More information about NATS JetStream can be found <link xl:href="https://docs.nats.io/nats-concepts/jetstream">here</link>.</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>Installation</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>Installing NATS on top of K3s</title>
<para>NATS is built for multiple architectures so it can easily be installed on K3s. (<xref linkend="components-k3s"/>)</para>
<para>Let us create a values file to overwrite the default values of NATS.</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>Now let us install NATS via Helm:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>With the <literal>values.yaml</literal> file above, the following components will be in the <literal>nats</literal> namespace:</para>
<orderedlist numeration="arabic">
<listitem>
<para>HA version of NATS Statefulset containing three containers: NATS server + Config reloader and Metrics sidecars.</para>
</listitem>
<listitem>
<para>NATS box container, which comes with a set of <literal>NATS</literal> utilities that can be used to verify the setup.</para>
</listitem>
<listitem>
<para>JetStream also leverages its Key-Value back-end that comes with <literal>PVCs</literal> bounded to the pods.</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>Testing the setup</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>Create a subscription for the test subject:</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>Send a message to the test subject:</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>Cleaning up</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>NATS as a back-end for K3s</title>
<para>One component K3s leverages is <link xl:href="https://github.com/k3s-io/kine">KINE</link>, which is a shim enabling the replacement of etcd with alternate storage back-ends originally targeting relational databases.
As JetStream provides a Key Value API, this makes it possible to have NATS as a back-end for the K3s cluster.</para>
<para>There is an already merged PR which makes the built-in NATS in K3s straightforward, but the change is still <link xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">not included</link> in the K3s releases.</para>
<para>For this reason, the K3s binary should be built manually.</para>
<section xml:id="id-building-k3s">
<title>Building K3s</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>The following command adds <literal>nats</literal> in the build tags to enable the NATS built-in feature in K3s:</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>Replace &lt;node-ip&gt; with the actual IP of the node where the K3s will be started:</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>Locally building K3s requires the buildx Docker CLI plugin.
It can be <link xl:href="https://github.com/docker/buildx#manual-download">manually installed</link> if <literal>$ make local</literal> fails.</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>Installing NATS CLI</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>Running NATS as K3s back-end</title>
<para>Let us <literal>ssh</literal> on the node and run the K3s with the <literal>--datastore-endpoint</literal> flag pointing to <literal>nats</literal>.</para>
<note>
<para>The command below starts K3s as a foreground process, so the logs can be easily followed to see if there are any issues.
To not block the current terminal, a <literal>&amp;</literal> flag could be added before the command to start it as a background process.</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>For making the K3s server with the NATS back-end permanent on your <literal>slemicro</literal> VM, the script below can be run, which creates a <literal>systemd</literal> service with the needed configurations.</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>Troubleshooting</title>
<para>The following commands can be run on the node to verify that everything with the stream works properly:</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-suse-linux-micro">
<title>NVIDIA GPUs on SUSE Linux Micro</title>
<section xml:id="id-intro-2">
<title>Intro</title>
<para>This guide demonstrates how to implement host-level NVIDIA GPU support via the pre-built <link xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">open-source drivers</link> on SUSE Linux Micro 6.1. These are drivers that are baked into the operating system rather than dynamically loaded by NVIDIA’s <link xl:href="https://github.com/NVIDIA/gpu-operator">GPU Operator</link>. This configuration is highly desirable for customers that want to pre-bake all artifacts required for deployment into the image, and where the dynamic selection of the driver version, that is, the user selecting the version of the driver via Kubernetes, is not a requirement. This guide initially explains how to deploy the additional components onto a system that has already been pre-deployed, but follows with a section that describes how to embed this configuration into the initial deployment via Edge Image Builder. If you do not want to run through the basics and set things up manually, skip right ahead to that section.</para>
<para>It is important to call out that the support for these drivers is provided by both SUSE and NVIDIA in tight collaboration, where the driver is built and shipped by SUSE as part of the package repositories. However, if you have any concerns or questions about the combination in which you use the drivers, ask your SUSE or NVIDIA account managers for further assistance. If you plan to use <link xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</link> (NVAIE), ensure that you are using an <link xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">NVAIE certified GPU</link>, which <emphasis>may</emphasis> require the use of proprietary NVIDIA drivers. If you are unsure, speak with your NVIDIA representative.</para>
<para>Further information about NVIDIA GPU operator integration is <emphasis>not</emphasis> covered in this guide. While integrating the NVIDIA GPU Operator for Kubernetes is not covered here, you can still follow most of the steps in this guide to set up the underlying operating system and simply enable the GPU operator to use the <emphasis>pre-installed</emphasis> drivers via the <literal>driver.enabled=false</literal> flag in the NVIDIA GPU Operator Helm chart, where it will simply pick up the installed drivers on the host. More comprehensive instructions are available from NVIDIA <link xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">here</link>.</para>
</section>
<section xml:id="id-prerequisites-12">
<title>Prerequisites</title>
<para>If you are following this guide, it assumes that you have the following already available:</para>
<itemizedlist>
<listitem>
<para>At least one host with SUSE Linux Micro 6.1 installed; this can be physical or virtual.</para>
</listitem>
<listitem>
<para>Your hosts are attached to a subscription as this is required for package access — an evaluation is available <link xl:href="https://www.suse.com/download/sle-micro/">here</link>.</para>
</listitem>
<listitem>
<para>A <link xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">compatible NVIDIA GPU</link> installed (or <emphasis>fully</emphasis> passed through to the virtual machine in which SUSE Linux Micro is running).</para>
</listitem>
<listitem>
<para>Access to the root user — these instructions assume you are the root user, and <emphasis>not</emphasis> escalating your privileges via <literal>sudo</literal>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>Manual installation</title>
<para>In this section, you are going to install the NVIDIA drivers directly onto the SUSE Linux Micro operating system as the NVIDIA open-driver is now part of the core SUSE Linux Micro package repositories, which makes it as easy as installing the required RPM packages. There is no compilation or downloading of executable packages required. Below we walk through deploying the "G06" generation of driver, which supports the latest GPUs (see <link xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">here</link> for further information), so select an appropriate driver generation for the NVIDIA GPU that your system has. For modern GPUs, the "G06" driver is the most common choice.</para>
<para>Before we begin, it is important to recognize that besides the NVIDIA open-driver that SUSE ships as part of SUSE Linux Micro, you might also need additional NVIDIA components for your setup. These could include OpenGL libraries, CUDA toolkits, command-line utilities such as <literal>nvidia-smi</literal>, and container-integration components such as <literal>nvidia-container-toolkit</literal>. Many of these components are not shipped by SUSE as they are proprietary NVIDIA software, or it makes no sense for us to ship them instead of NVIDIA. Therefore, as part of the instructions, we are going to configure additional repositories that give us access to said components and walk through certain examples of how to use these tools, resulting in a fully functional system. It is important to distinguish between SUSE repositories and NVIDIA repositories, as occasionally there can be a mismatch between the package versions that NVIDIA makes available versus what SUSE has built. This usually arises when SUSE makes a new version of the open-driver available, and it takes a couple of days before the equivalent packages are made available in NVIDIA repositories to match.</para>
<para>We recommend that you ensure that the driver version that you are selecting is compatible with your GPU and meets any CUDA requirements that you may have by checking:</para>
<itemizedlist>
<listitem>
<para>The <link xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">CUDA release notes</link></para>
</listitem>
<listitem>
<para>The driver version that you plan on deploying has a matching version in the <link xl:href="https://download.nvidia.com/suse/sle15sp6/x86_64/">NVIDIA repository</link> and ensuring that you have equivalent package versions for the supporting components available</para>
</listitem>
</itemizedlist>
<tip>
<para>To find the NVIDIA open-driver versions, either run <literal>zypper se -s nvidia-open-driver</literal> on the target machine <emphasis>or</emphasis> search the SUSE Customer Center for the "nvidia-open-driver" in <link xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.1&amp;arch=x86_64">SUSE Linux Micro 6.1 for AMD64/Intel 64</link>.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="scc-packages-nvidia.png" width="100%"/>
</imageobject>
<textobject><phrase>SUSE Customer Centre</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>When you have confirmed that an equivalent version is available in the NVIDIA repos, you are ready to install the packages on the host operating system. For this, we need to open up a <literal>transactional-update</literal> session, which creates a new read/write snapshot of the underlying operating system so we can make changes to the immutable platform (for further instructions on <literal>transactional-update</literal>, see <link xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-transactional-updates/transactional-updates.html">here</link>):</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>When you are in your <literal>transactional-update</literal> shell, add an additional package repository from NVIDIA. This allows us to pull in additional utilities, for example, <literal>nvidia-smi</literal>:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-suse-main
zypper --gpg-auto-import-keys refresh</screen>
<para>You can then install the driver and <literal>nvidia-compute-utils</literal> for additional utilities. If you do not need the utilities, you can omit it, but for testing purposes, it is worth installing at this stage:</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>If the installation fails, this might indicate a dependency mismatch between the selected driver version and what NVIDIA ships in their repositories. Refer to the previous section to verify that your versions match. Attempt to install a different driver version. For example, if the NVIDIA repositories have an earlier version, you can try specifying <literal>nvidia-open-driver-G06-signed-kmp=550.54.14</literal> on your install command to specify a version that aligns.</para>
</note>
<para>Next, if you are <emphasis>not</emphasis> using a supported GPU (remembering that the list can be found <link xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">here</link>), you can see if the driver works by enabling support at the module level, but your mileage may vary — skip this step if you are using a <emphasis>supported</emphasis> GPU:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>Now that you have installed these packages, it is time to exit the <literal>transactional-update</literal> session:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>Make sure that you have exited the <literal>transactional-update</literal> session before proceeding.</para>
</note>
<para>Now that you have installed the drivers, it is time to reboot. As SUSE Linux Micro is an immutable operating system, it needs to reboot into the new snapshot that you created in a previous step. The drivers are only installed into this new snapshot, hence it is not possible to load the drivers without rebooting into this new snapshot, which happens automatically. Issue the reboot command when you are ready:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>Once the system has rebooted successfully, log back in and use the <literal>nvidia-smi</literal> tool to verify that the driver is loaded successfully and that it can both access and enumerate your GPUs:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>The output of this command should show you something similar to the following output, noting that in the example below, we have two GPUs:</para>
<screen language="shell" linenumbering="unnumbered">+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>This concludes the installation and verification process for the NVIDIA drivers on your SUSE Linux Micro system.</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>Further validation of the manual installation</title>
<para>At this stage, all we have been able to verify is that, at the host level, the NVIDIA device can be accessed and that the drivers are loading successfully. However, if we want to be sure that it is functioning, a simple test would be to validate that the GPU can take instructions from a user-space application, ideally via a container, and through the CUDA library, as that is typically what a real workload would use. For this, we can make a further modification to the host OS by installing the <literal>nvidia-container-toolkit</literal> (<link xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA Container Toolkit</link>). First, open another <literal>transactional-update</literal> shell, noting that we could have done this in a single transaction in the previous step, and see how to do this fully automated in a later section:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>Next, install the <literal>nvidia-container-toolkit</literal> package from the NVIDIA Container Toolkit repo:</para>
<itemizedlist>
<listitem>
<para>The <literal>nvidia-container-toolkit.repo</literal> below contains a stable (<literal>nvidia-container-toolkit</literal>) and an experimental (<literal>nvidia-container-toolkit-experimental</literal>) repository. The stable repository is recommended for production use. The experimental repository is disabled by default.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>When you are ready, you can exit the <literal>transactional-update</literal> shell:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>…​and reboot the machine into the new snapshot:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>As before, you need to ensure that you have exited the <literal>transactional-shell</literal> and rebooted the machine for your changes to be enacted.</para>
</note>
<para>With the machine rebooted, you can verify that the system can successfully enumerate the devices using the NVIDIA Container Toolkit. The output should be verbose, with INFO and WARN messages, but no ERROR messages:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>This ensures that any container started on the machine can employ NVIDIA GPU devices that have been discovered. When ready, you can then run a podman-based container. Doing this via <literal>podman</literal> gives us a good way of validating access to the NVIDIA device from within a container, which should give confidence for doing the same with Kubernetes at a later stage. Give <literal>podman</literal> access to the labeled NVIDIA devices that were taken care of by the previous command, based on <link xl:href="https://registry.suse.com/repositories/bci-bci-base-15sp6">SLE BCI</link>, and simply run the Bash command:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>You will now execute commands from within a temporary podman container. It does not have access to your underlying system and is ephemeral, so whatever we do here will not persist, and you should not be able to break anything on the underlying host. As we are now in a container, we can install the required CUDA libraries, again checking the correct CUDA version for your driver <link xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">here</link>, although the previous output of <literal>nvidia-smi</literal> should show the required CUDA version. In the example below, we are installing <emphasis>CUDA 12.3</emphasis> and pulling many examples, demos and development kits so you can fully validate the GPU:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-suse
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>Once this has been installed successfully, do not exit the container. We will run the <literal>deviceQuery</literal> CUDA example, which comprehensively validates GPU access via CUDA, and from within the container itself:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>If successful, you should see output that shows similar to the following, noting the <literal>Result = PASS</literal> message at the end of the command, and noting that in the output below, the system correctly identifies two GPUs, whereas your environment may only have one:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>From here, you can continue to run any other CUDA workload — use compilers and any other aspect of the CUDA ecosystem to run further tests. When done, you can exit from the container, noting that whatever you have installed in there is ephemeral (so will be lost!), and has not impacted the underlying operating system:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Implementation with Kubernetes</title>
<para>Now that we have proven the installation and use of the NVIDIA open-driver on SUSE Linux Micro, let us explore configuring Kubernetes on the same machine. This guide does not walk you through deploying Kubernetes, but it assumes that you have installed <link xl:href="https://k3s.io/">K3s</link> or <link xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> and that your kubeconfig is configured accordingly, so that standard <literal>kubectl</literal> commands can be executed as the superuser. We assume that your node forms a single-node cluster, although the core steps should be similar for multi-node clusters. First, ensure that your <literal>kubectl</literal> access is working:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.33.3+rke2r1</screen>
<para>What you should find is that your k3s/rke2 installation has detected the NVIDIA Container Toolkit on the host and auto-configured the NVIDIA runtime integration into <literal>containerd</literal> (the Container Runtime Interface that k3s/rke2 use). Confirm this by checking the containerd <literal>config.toml</literal> file:</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>This must show something akin to the following. The equivalent K3s location is <literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>:</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>If these entries are not present, the detection might have failed. This could be due to the machine or the Kubernetes services not being restarted. Add these manually as above, if required.</para>
</note>
<para>Next, we need to configure the NVIDIA <literal>RuntimeClass</literal> as an additional Kubernetes runtime to the default, ensuring that any user requests for pods that need access to the GPU can use the NVIDIA Container Toolkit to do so, via the <literal>nvidia-container-runtime</literal>, as configured in the <literal>containerd</literal> configuration:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>The next step is to configure the <link xl:href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA Device Plugin</link>, which configures Kubernetes to leverage the NVIDIA GPUs as resources within the cluster that can be used, working in combination with the NVIDIA Container Toolkit. This tool initially detects all capabilities on the underlying host, including GPUs, drivers and other capabilities (such as GL) and then allows you to request GPU resources and consume them as part of your applications.</para>
<para>First, you need to add and update the Helm repository for the NVIDIA Device Plugin:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>Now you can install the NVIDIA Device Plugin:</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>After a few minutes, you see a new pod running that will complete the detection on your available nodes and tag them with the number of GPUs that have been detected:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>Now you are ready to create an NVIDIA pod that attempts to use this GPU. Let us try with the CUDA Benchmark container:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>If all went well, you can look at the logs and see the benchmark information:</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
        -fullscreen       (run n-body simulation in fullscreen mode)
        -fp64             (use double precision floating point values for simulation)
        -hostmem          (stores simulation data in host memory)
        -benchmark        (run benchmark to measure performance)
        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
        -compare          (compares simulation results running once on the default GPU and once on the CPU)
        -cpu              (run n-body simulation on the CPU)
        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>Finally, if your applications require OpenGL, you can install the required NVIDIA OpenGL libraries at the host level, and the NVIDIA Device Plugin and NVIDIA Container Toolkit can make them available to containers. To do this, install the package as follows:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>You need to reboot to make this package available to your applications. The NVIDIA Device Plugin should automatically redetect this via the NVIDIA Container Toolkit.</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Bringing it together via Edge Image Builder</title>
<para>Okay, so you have demonstrated full functionality of your applications and GPUs on SUSE Linux Micro and you now want to use <xref linkend="components-eib"/> to provide it all together via a deployable/consumable ISO or RAW disk image. This guide does not explain how to use Edge Image Builder, but it provides the necessary configurations to build such image. Below you can find an example of an image definition, along with the necessary Kubernetes configuration files, to ensure that all the required components are deployed out of the box. Here is the directory structure of the Edge Image Builder directory for the example shown below:</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>Let us explore those files. First, here is a sample image definition for a single-node cluster running K3s that deploys the utilities and OpenGL packages, too (<literal>eib-config-iso.yaml</literal>):</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: [snip]
kubernetes:
  version: v1.33.3+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>This is just an example. You may need to customize it to fit your requirements and expectations. Additionally, if using SUSE Linux Micro, you need to provide your own <literal>sccRegistrationCode</literal> to resolve package dependencies and pull the NVIDIA drivers.</para>
</note>
<para>Besides this, we need to add additional components, so they get loaded by Kubernetes at boot time. The EIB directory needs a <literal>kubernetes</literal> directory first, with subdirectories for the configuration, Helm chart values and any additional manifests required:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>Let us now set up the (optional) Kubernetes configuration by choosing a CNI (which defaults to Cilium if unselected) and enabling SELinux:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>Now ensure that the NVIDIA RuntimeClass is created on the Kubernetes cluster:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>We use the built-in Helm Controller to deploy the NVIDIA Device Plugin through Kubernetes itself.
Let’s provide the runtime class in the values file for the chart:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>We need to grab the NVIDIA Container Toolkit RPM public key before proceeding:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>All the required artifacts, including Kubernetes binary, container images, Helm charts (and any referenced images), will be automatically air-gapped, meaning that the systems at deploy time should require no Internet connectivity by default. Now you need only to grab the SUSE Linux Micro ISO from the <link xl:href="https://www.suse.com/download/sle-micro/">SUSE Downloads Page</link> (and place it in the <literal>base-images</literal> directory), and you can call the Edge Image Builder tool to generate the ISO for you. To complete the example, here is the command that was used to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-config-iso.yaml</screen>
<para>For further instructions, please see the <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">documentation</link> for Edge Image Builder.</para>
</section>
<section xml:id="id-resolving-issues">
<title>Resolving issues</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smi does not find the GPU</title>
<para>Check the kernel messages using <literal>dmesg</literal>. If this indicates that it cannot allocate <literal>NvKMSKapDevice</literal>, apply the unsupported GPU workaround:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>NOTE</emphasis>: You will need to reload the kernel module, or reboot, if you change the kernel module configuration in the above step for it to take effect.</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="day-2-operations">
<title>Day 2 Operations</title>
<partintro>
<para>This section explains how administrators can handle different "Day Two" operation tasks both on the management and on the downstream clusters.</para>
</partintro>
<chapter xml:id="day2-migration">
<title>Edge 3.4 migration</title>
<para>This section explains how to migrate your <literal>management</literal> and <literal>downstream</literal> clusters from <literal>Edge 3.3</literal> to <literal>Edge 3.4.0</literal>.</para>
<important>
<para>Always perform cluster migrations from the <literal>latest Z-stream</literal> release of <literal>Edge 3.3</literal>.</para>
<para>Always migrate to the <literal>Edge 3.4.0</literal> release. For subsequent post-migration upgrades, refer to the management (<xref linkend="day2-mgmt-cluster"/>) and downstream (<xref linkend="day2-downstream-clusters"/>) cluster sections.</para>
</important>
<section xml:id="day2-migration-mgmt">
<title>Management Cluster</title>
<para>This section covers the following topics:</para>
<para><xref linkend="day2-migration-mgmt-prereq"/> - prerequisite steps to complete before starting the migration.</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller"/> - how to do a <literal>management</literal> cluster migration using the <xref linkend="components-upgrade-controller"/>.</para>
<para><xref linkend="day2-migration-mgmt-fleet"/> - how to do a <literal>management</literal> cluster migration using <xref linkend="components-fleet"/>.</para>
<section xml:id="day2-migration-mgmt-prereq">
<title>Prerequisites</title>
<section xml:id="id-upgrade-the-bare-metal-operator-crds">
<title>Upgrade the Bare Metal Operator CRDs</title>
<note>
<para>Applies only to clusters that require a <xref linkend="components-metal3"/> chart upgrade.</para>
</note>
<para>The <literal>Metal<superscript>3</superscript></literal> Helm chart includes the <link xl:href="https://book.metal3.io/bmo/introduction.html">Bare Metal Operator (BMO)</link> CRDs by leveraging Helm’s <link xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you">CRD</link> directory.</para>
<para>However, this approach has certain limitations, particularly the inability to upgrade CRDs in this directory using Helm. For more information, refer to the <link xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations">Helm documentation</link>.</para>
<para>As a result, before upgrading Metal<superscript>3</superscript> to an <literal>Edge 3.4.0</literal> compatible version, users must manually upgrade the underlying BMO CRDs.</para>
<para>On a machine with <literal>Helm</literal> installed and <literal>kubectl</literal> configured to point to your <literal>management</literal> cluster:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Manually apply the BMO CRDs:</para>
<screen language="bash" linenumbering="unnumbered">helm show crds oci://registry.suse.com/edge/charts/metal3 --version 304.0.16+up0.12.6 | kubectl apply -f -</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller">
<title>Upgrade Controller</title>
<important>
<para>The <literal>Upgrade Controller</literal> currently supports Edge release migrations only for <emphasis role="strong">non air-gapped management</emphasis> clusters.</para>
</important>
<para>The following topics are covered as part of this section:</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-prereq"/> - prerequisites specific to the <literal>Upgrade Controller</literal>.</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-migration"/> - steps for migrating a <literal>management</literal> cluster to a new Edge version using the <literal>Upgrade Controller</literal>.</para>
<section xml:id="day2-migration-mgmt-upgrade-controller-prereq">
<title>Prerequisites</title>
<section xml:id="id-edge-3-4-upgrade-controller">
<title>Edge 3.4 Upgrade Controller</title>
<para>Before using the <literal>Upgrade Controller</literal>, you must first ensure that it is running a version that is capable of migrating to the desired Edge release.</para>
<para>To do this:</para>
<orderedlist numeration="arabic">
<listitem>
<para>If you already have <literal>Upgrade Controller</literal> deployed from a previous Edge release, upgrade its chart:</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade upgrade-controller -n upgrade-controller-system oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1</screen>
</listitem>
<listitem>
<para>If you do <emphasis role="strong">not</emphasis> have <literal>Upgrade Controller</literal> deployed, follow <xref linkend="components-upgrade-controller-installation"/>.</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller-migration">
<title>Migration steps</title>
<para>Performing a <literal>management</literal> cluster migration with the <literal>Upgrade Controller</literal> is fundamentally similar to executing an upgrade.</para>
<para>The only difference is that your <literal>UpgradePlan</literal> <emphasis role="strong">must</emphasis> specify the <literal>3.4.0</literal> release version:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  # Change to the namespace of your Upgrade Controller
  namespace: CHANGE_ME
spec:
  releaseVersion: 3.4.0</screen>
<para>For information on how to use the above <literal>UpgradePlan</literal> to do a migration, refer to Upgrade Controller upgrade process (<xref linkend="management-day2-upgrade-controller"/>).</para>
</section>
</section>
<section xml:id="day2-migration-mgmt-fleet">
<title>Fleet</title>
<note>
<para>Whenever possible, use the <xref linkend="day2-migration-mgmt-upgrade-controller"/> for migration.</para>
<para>Refer to this section only for use cases not covered by the <literal>Upgrade Controller</literal>.</para>
</note>
<para>Performing a <literal>management</literal> cluster migration with <literal>Fleet</literal> is fundamentally similar to executing an upgrade.</para>
<para>The <emphasis role="strong">key</emphasis> differences being that:</para>
<orderedlist numeration="arabic">
<listitem>
<para>The fleets <emphasis role="strong">must be used</emphasis> from the <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link> release of the <literal>suse-edge/fleet-examples</literal> repository.</para>
</listitem>
<listitem>
<para>Charts scheduled for an upgrade <emphasis role="strong">must</emphasis> be upgraded to versions compatible with the <literal>Edge 3.4.0</literal> release. For a list of the <literal>Edge 3.4.0</literal> components, refer to <xref linkend="release-notes-3-4-0"/>.</para>
</listitem>
</orderedlist>
<important>
<para>To ensure a successful <literal>Edge 3.4.0</literal> migration, it is important that users comply with the points outlined above.</para>
</important>
<para>Considering the points above, users can follow the <literal>management</literal> cluster Fleet (<xref linkend="management-day2-fleet"/>) documentation for a comprehensive guide on the steps required to perform a migration.</para>
</section>
</section>
<section xml:id="day2-migration-downstream">
<title>Downstream Clusters</title>
<para><xref linkend="day2-migration-downstream-fleet"/> - how to do a <literal>downstream</literal> cluster migration using <xref linkend="components-fleet"/>.</para>
<section xml:id="day2-migration-downstream-fleet">
<title>Fleet</title>
<para>Performing a <literal>downstream</literal> cluster migration with <literal>Fleet</literal> is fundamentally similar to executing an upgrade.</para>
<para>The <emphasis role="strong">key</emphasis> differences being that:</para>
<orderedlist numeration="arabic">
<listitem>
<para>The fleets <emphasis role="strong">must be used</emphasis> from the <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link> release of the <literal>suse-edge/fleet-examples</literal> repository.</para>
</listitem>
<listitem>
<para>Charts scheduled for an upgrade <emphasis role="strong">must</emphasis> be upgraded to versions compatible with the <literal>Edge 3.4.0</literal> release. For a list of the <literal>Edge 3.4.0</literal> components, refer to <xref linkend="release-notes-3-4-0"/>.</para>
</listitem>
</orderedlist>
<important>
<para>To ensure a successful <literal>Edge 3.4.0</literal> migration, it is important that users comply with the points outlined above.</para>
</important>
<para>Considering the points above, users can follow the <literal>downstream</literal> cluster Fleet (<xref linkend="downstream-day2-fleet"/>) documentation for a comprehensive guide on the steps required to perform a migration.</para>
</section>
</section>
</chapter>
<chapter xml:id="day2-mgmt-cluster">
<title>Management Cluster</title>
<para>Currently, there are two ways to perform "Day 2" operations on your <literal>management</literal> cluster:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through <xref linkend="components-upgrade-controller"/> - <xref linkend="management-day2-upgrade-controller"/></para>
</listitem>
<listitem>
<para>Through <xref linkend="components-fleet"/> - <xref linkend="management-day2-fleet"/></para>
</listitem>
</orderedlist>
<section xml:id="management-day2-upgrade-controller">
<title>Upgrade Controller</title>
<important>
<para>The <literal>Upgrade Controller</literal> currently only supports <literal>Day 2</literal> operations for <emphasis role="strong">non air-gapped management</emphasis> clusters.</para>
</important>
<para>This section covers how to perform the various <literal>Day 2</literal> operations related to upgrading your <literal>management</literal> cluster from one Edge platform version to another.</para>
<para>The <literal>Day 2</literal> operations are automated by the Upgrade Controller (<xref linkend="components-upgrade-controller"/>) and include:</para>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro (<xref linkend="components-slmicro"/>) OS upgrade</para>
</listitem>
<listitem>
<para><xref linkend="components-rke2"/> or <xref linkend="components-k3s"/> Kubernetes upgrade</para>
</listitem>
<listitem>
<para>SUSE additional components (SUSE Rancher Prime, SUSE Security, etc.) upgrade</para>
</listitem>
</itemizedlist>
<section xml:id="id-prerequisites-13">
<title>Prerequisites</title>
<para>Before upgrading your <literal>management</literal> cluster, the following prerequisites must be met:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>SCC registered nodes</literal> - ensure your cluster nodes' OS are registered with a subscription key that supports the OS version specified in the Edge release (<xref linkend="release-notes"/>) you intend to upgrade to.</para>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal> - make sure that the <literal>Upgrade Controller</literal> has been deployed on your <literal>management</literal> cluster. For installation steps, refer to <xref linkend="components-upgrade-controller-installation"/>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade">
<title>Upgrade</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine the Edge release (<xref linkend="release-notes"/>) version that you wish to upgrade your <literal>management</literal> cluster to.</para>
</listitem>
<listitem>
<para>In the <literal>management</literal> cluster, deploy an <literal>UpgradePlan</literal> that specifies the desired <literal>release version</literal>. The <literal>UpgradePlan</literal> must be deployed in the namespace of the <literal>Upgrade Controller</literal>.</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n &lt;upgrade_controller_namespace&gt; -f - &lt;&lt;EOF
apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
spec:
  # Version retrieved from release notes
  releaseVersion: 3.X.Y
EOF</screen>
<note>
<para>There may be use-cases where you would want to make additional configurations over the <literal>UpgradePlan</literal>. For all possible configurations, refer to <xref linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</note>
</listitem>
<listitem>
<para>Deploying the <literal>UpgradePlan</literal> to the <literal>Upgrade Controller’s</literal> namespace will begin the <literal>upgrade process</literal>.</para>
<note>
<para>For more information on the actual <literal>upgrade process</literal>, refer to <xref linkend="components-upgrade-controller-how"/>.</para>
<para>For information on how to track the <literal>upgrade process</literal>, refer to <xref linkend="components-upgrade-controller-how-track"/>.</para>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet">
<title>Fleet</title>
<para>This section offers information on how to perform "Day 2" operations using the Fleet (<xref linkend="components-fleet"/>) component.</para>
<para>The following topics are covered as part of this section:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-components"/> - default components used for all "Day 2" operations.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-determine-use-case"/> - provides an overview of the Fleet custom resources that will be used and their suitability for different "Day 2" operations use-cases.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-upgrade-workflow"/> - provides a workflow guide for executing "Day 2" operations with Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade"/> - describes how to do OS upgrades using Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade"/> - describes how to do Kubernetes version upgrades using Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade"/> - describes how to do Helm chart upgrades using Fleet.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-components">
<title>Components</title>
<para>Below you can find a description of the default components that should be set up on your <literal>management</literal> cluster so that you can successfully perform "Day 2" operations using Fleet.</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<para><emphasis role="strong">Optional</emphasis>; Responsible for managing <literal>downstream clusters</literal> and deploying the <literal>System Upgrade Controller</literal> on your <literal>management cluster</literal>.</para>
<para>For more information, see <xref linkend="components-rancher"/>.</para>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System Upgrade Controller (SUC)</title>
<para><emphasis role="strong">System Upgrade Controller</emphasis> is responsible for executing tasks on specified nodes based on configuration data provided through a custom resource, called a <literal>Plan</literal>.</para>
<para><emphasis role="strong">SUC</emphasis> is actively utilized to upgrade the operating system and Kubernetes distribution.</para>
<para>For more information about the <emphasis role="strong">SUC</emphasis> component and how it fits in the Edge stack, see <xref linkend="components-system-upgrade-controller"/>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-determine-use-case">
<title>Determine your use-case</title>
<para>Fleet uses two types of <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link> to enable the management of Kubernetes and Helm resources.</para>
<para>Below you can find information about the purpose of these resources and the use-cases they are best suited for in the context of "Day 2" operations.</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para>A <literal>GitRepo</literal> is a Fleet (<xref linkend="components-fleet"/>) resource that represents a Git repository from which <literal>Fleet</literal> can create <literal>Bundles</literal>. Each <literal>Bundle</literal> is created based on configuration paths defined inside of the <literal>GitRepo</literal> resource. For more information, see the <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> documentation.</para>
<para>In the context of "Day 2" operations, <literal>GitRepo</literal> resources are normally used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> in <emphasis role="strong">non air-gapped</emphasis> environments that utilize a <emphasis>Fleet GitOps</emphasis> approach.</para>
<para>Alternatively, <literal>GitRepo</literal> resources can also be used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> on <emphasis role="strong">air-gapped</emphasis> environments, <emphasis role="strong">provided you mirror your repository setup through a local git server</emphasis>.</para>
</section>
<section xml:id="id-bundle">
<title>Bundle</title>
<para><literal>Bundles</literal> hold <emphasis role="strong">raw</emphasis> Kubernetes resources that will be deployed on the targeted cluster. Usually they are created from a <literal>GitRepo</literal> resource, but there are use-cases where they can be deployed manually. For more information refer to the <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> documentation.</para>
<para>In the context of "Day 2" operations, <literal>Bundle</literal> resources are normally used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> in <emphasis role="strong">air-gapped</emphasis> environments that do not use some form of <emphasis>local GitOps</emphasis> procedure (e.g. a <emphasis role="strong">local git server</emphasis>).</para>
<para>Alternatively, if your use-case does not allow for a <emphasis>GitOps</emphasis> workflow (e.g. using a Git repository), <literal>Bundle</literal> resources could also be used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> in <emphasis role="strong">non air-gapped</emphasis> environments.</para>
</section>
</section>
<section xml:id="management-day2-upgrade-workflow">
<title>Day 2 workflow</title>
<para>The following is a "Day 2" workflow that should be followed when upgrading a management cluster to a specific Edge release.</para>
<orderedlist numeration="arabic">
<listitem>
<para>OS upgrade (<xref linkend="management-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Kubernetes version upgrade (<xref linkend="management-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helm chart upgrade (<xref linkend="management-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade">
<title>OS upgrade</title>
<para>This section describes how to perform an operating system upgrade using <xref linkend="components-fleet"/> and the <xref linkend="components-system-upgrade-controller"/>.</para>
<para>The following topics are covered as part of this section:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-components"/> - additional components used by the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-overview"/> - overview of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-requirements"/> - requirements of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-plan-deployment"/> - information on how to deploy <literal>SUC plans</literal>, responsible for triggering the upgrade process.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-os-upgrade-components">
<title>Components</title>
<para>This section covers the custom components that the <literal>OS upgrade</literal> process uses over the default "Day 2" components (<xref linkend="management-day2-fleet-components"/>).</para>
<section xml:id="management-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>The OS upgrade on a specific node is handled by a <link xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>.</para>
<para>A different service is created depending on what type of upgrade the OS requires from one Edge version to another:</para>
<itemizedlist>
<listitem>
<para>For Edge versions that require the same OS version (e.g. <literal>6.0</literal>), the <literal>os-pkg-update.service</literal> will be created. It uses <link xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link> to perform a <link xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">normal package upgrade</link>.</para>
</listitem>
<listitem>
<para>For Edge versions that require an OS version migration (e.g <literal>6.0</literal> → <literal>6.1</literal>), the <literal>os-migration.service</literal> will be created. It uses <link xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link> to perform:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>A <link xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">normal package upgrade</link> which ensures that all packages are at up-to-date in order to mitigate any failures in the migration related to old package versions.</para>
</listitem>
<listitem>
<para>An OS migration by utilizing the <literal>zypper migration</literal> command.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>The services mentioned above are shipped on each node through a <literal>SUC plan</literal> which must be located on the management cluster that is in need of an OS upgrade.</para>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-overview">
<title>Overview</title>
<para>The upgrade of the operating system for management cluster nodes is done by utilizing <literal>Fleet</literal> and the <literal>System Upgrade Controller (SUC)</literal>.</para>
<para><emphasis role="strong">Fleet</emphasis> is used to deploy and manage <literal>SUC plans</literal> onto the desired cluster.</para>
<note>
<para><literal>SUC plans</literal> are <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link> that describe the steps that <literal>SUC</literal> needs to follow in order for a specific task to be executed on a set of nodes. For an example of how an <literal>SUC plan</literal> looks like, refer to the <link xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">upstream repository</link>.</para>
</note>
<para>The <literal>OS SUC plans</literal> are shipped to each cluster by deploying a <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> or <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resource to a specific Fleet <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">workspace</link>. Fleet retrieves the deployed <literal>GitRepo/Bundle</literal> and deploys its contents (the <literal>OS SUC plans</literal>) to the desired cluster(s).</para>
<note>
<para><literal>GitRepo/Bundle</literal> resources are always deployed on the <literal>management cluster</literal>. Whether to use a <literal>GitRepo</literal> or <literal>Bundle</literal> resource depends on your use-case, check <xref linkend="management-day2-fleet-determine-use-case"/> for more information.</para>
</note>
<para><literal>OS SUC plans</literal> describe the following workflow:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Always <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link> the nodes before OS upgrades.</para>
</listitem>
<listitem>
<para>Always upgrade <literal>control-plane</literal> nodes before <literal>worker</literal> nodes.</para>
</listitem>
<listitem>
<para>Always upgrade the cluster on a <emphasis role="strong">one</emphasis> node at a time basis.</para>
</listitem>
</orderedlist>
<para>Once the <literal>OS SUC plans</literal> are deployed, the workflow looks like this:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconciles the deployed <literal>OS SUC plans</literal> and creates a <literal>Kubernetes Job</literal> on <emphasis role="strong">each node</emphasis>.</para>
</listitem>
<listitem>
<para>The <literal>Kubernetes Job</literal> creates a systemd.service (<xref linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>) for either package upgrade, or OS migration.</para>
</listitem>
<listitem>
<para>The created <literal>systemd.service</literal> triggers the OS upgrade process on the specific node.</para>
<important>
<para>Once the OS upgrade process finishes, the corresponding node will be <literal>rebooted</literal> to apply the updates on the system.</para>
</important>
</listitem>
</orderedlist>
<para>Below you can find a diagram of the above description:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-day2-management-os-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2 management os upgrade</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-os-upgrade-requirements">
<title>Requirements</title>
<para><emphasis>General:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC registered machine</emphasis> - All management cluster nodes should be registered to <literal><link xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal> which is needed so that the respective <literal>systemd.service</literal> can successfully connect to the desired RPM repository.</para>
<important>
<para>For Edge releases that require an OS version migration (e.g. <literal>6.0</literal> → <literal>6.1</literal>), make sure that your SCC key supports the migration to the new version.</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">Make sure that SUC Plan tolerations match node tolerations</emphasis> - If your Kubernetes cluster nodes have custom <emphasis role="strong">taints</emphasis>, make sure to add <link xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</link> for those taints in the <emphasis role="strong">SUC Plans</emphasis>. By default, <emphasis role="strong">SUC Plans</emphasis> have tolerations only for <emphasis role="strong">control-plane</emphasis> nodes. Default tolerations include:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Any additional tolerations must be added under the <literal>.spec.tolerations</literal> section of each Plan. <emphasis role="strong">SUC Plans</emphasis> related to the OS upgrade can be found in the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository under <literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>. <emphasis role="strong">Make sure you use the Plans from a valid repository <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</emphasis></para>
<para>An example of defining custom tolerations for the <emphasis role="strong">control-plane</emphasis> SUC Plan would look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>Air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Mirror SUSE RPM repositories</emphasis> - OS RPM repositories should be locally mirrored so that the <literal>systemd.service</literal> can have access to them. This can be achieved by using either <link xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link> or <link xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment">
<title>OS upgrade - SUC plan deployment</title>
<important>
<para>For environments previously upgraded using this procedure, users should ensure that <emphasis role="strong">one</emphasis> of the following steps is completed:</para>
<itemizedlist>
<listitem>
<para><literal>Remove any previously deployed SUC Plans related to older Edge release versions from the management cluster</literal> - can be done by removing the desired cluster from the existing <literal>GitRepo/Bundle</literal> <link xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">target configuration</link>, or removing the <literal>GitRepo/Bundle</literal> resource altogether.</para>
</listitem>
<listitem>
<para><literal>Reuse the existing GitRepo/Bundle resource</literal> - can be done by pointing the resource’s revision to a new tag that holds the correct fleets for the desired <literal>suse-edge/fleet-examples</literal> <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>.</para>
</listitem>
</itemizedlist>
<para>This is done in order to avoid clashes between <literal>SUC Plans</literal> for older Edge release versions.</para>
<para>If users attempt to upgrade, while there are existing <literal>SUC Plans</literal> on the management cluster, they will see the following fleet error:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>As mentioned in <xref linkend="management-day2-fleet-os-upgrade-overview"/>, OS upgrades are done by shipping <literal>SUC plans</literal> to the desired cluster through one of the following ways:</para>
<itemizedlist>
<listitem>
<para>Fleet <literal>GitRepo</literal> resource - <xref linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>.</para>
</listitem>
<listitem>
<para>Fleet <literal>Bundle</literal> resource - <xref linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle"/>.</para>
</listitem>
</itemizedlist>
<para>To determine which resource you should use, refer to <xref linkend="management-day2-fleet-determine-use-case"/>.</para>
<para>For use-cases where you wish to deploy the <literal>OS SUC plans</literal> from a third-party GitOps tool, refer to <xref linkend="management-day2-fleet-os-upgrade-plan-deployment-third-party"/></para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>SUC plan deployment - GitRepo resource</title>
<para>A <emphasis role="strong">GitRepo</emphasis> resource, that ships the needed <literal>OS SUC plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the OS upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo creation - Rancher UI</title>
<para>To create a <literal>GitRepo</literal> resource through the Rancher UI, follow their official <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentation</link>.</para>
<para>The Edge team maintains a ready to use <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">fleet</link>. Depending on your environment this fleet could be used directly or as a template.</para>
<important>
<para>Always use this fleet from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>For use-cases where no custom changes need to be included to the <literal>SUC plans</literal> that the fleet ships, users can directly refer the <literal>os-upgrade</literal> fleet from the <literal>suse-edge/fleet-examples</literal> repository.</para>
<para>In cases where custom changes are needed (e.g. to add custom tolerations), users should refer the <literal>os-upgrade</literal> fleet from a separate repository, allowing them to add the changes to the SUC plans as required.</para>
<para>An example of how a <literal>GitRepo</literal> can be configured to use the fleet from the <literal>suse-edge/fleet-examples</literal> repository, can be viewed <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">here</link>.</para>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configuration:</para>
<itemizedlist>
<listitem>
<para>Remove the <literal>spec.targets</literal> section - only needed for downstream clusters.</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Point the namespace of the <literal>GitRepo</literal> to the <literal>fleet-local</literal> namespace - done in order to deploy the resource on the management cluster.</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resource your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under the <literal>fleet-local</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-local

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>SUC plan deployment - Bundle resource</title>
<para>A <emphasis role="strong">Bundle</emphasis> resource, that ships the needed <literal>OS SUC Plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the OS upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<para>The Edge team maintains a ready to use <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle</link> that can be used in the below steps.</para>
<important>
<para>Always use this bundle from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>To create a bundle through Rancher’s UI:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, click <emphasis role="strong">☰ → Continuous Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<note>
<para>There might be use-cases where you would need to include custom changes to the <literal>SUC plans</literal> that the bundle ships (e.g. to add custom tolerations). Make sure to include those changes in the bundle that will be generated by the below steps.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>By manually copying the <link xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle content</link> from <literal>suse-edge/fleet-examples</literal> to the <emphasis role="strong">Create from YAML</emphasis> page.</para>
</listitem>
<listitem>
<para>By cloning the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository from the desired <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag and selecting the <emphasis role="strong">Read from File</emphasis> option in the <emphasis role="strong">Create from YAML</emphasis> page. From there, navigate to the bundle location (<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>) and select the bundle file. This will auto-populate the <emphasis role="strong">Create from YAML</emphasis> page with the bundle content.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Edit the Bundle in the Rancher UI:</para>
<itemizedlist>
<listitem>
<para>Change the <emphasis role="strong">namespace</emphasis> of the <literal>Bundle</literal> to point to the <literal>fleet-local</literal> namespace.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the <literal>Bundle</literal> to point to your <literal>local</literal>(management) cluster:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>There are some use-cases where your <literal>local</literal> cluster could have a different name.</para>
<para>To retrieve your <literal>local</literal> cluster name, execute the command below:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> configuration:</para>
<itemizedlist>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the <literal>Bundle</literal> to point to your <literal>local</literal>(management) cluster:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>There are some use-cases where your <literal>local</literal> cluster could have a different name.</para>
<para>To retrieve your <literal>local</literal> cluster name, execute the command below:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">namespace</emphasis> of the <literal>Bundle</literal> to point to the <literal>fleet-local</literal> namespace.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resource to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under the <literal>fleet-local</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-local</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>SUC Plan deployment - third-party GitOps workflow</title>
<para>There might be use-cases where users would like to incorporate the <literal>OS SUC plans</literal> to their own third-party GitOps workflow (e.g. <literal>Flux</literal>).</para>
<para>To get the OS upgrade resources that you need, first determine the Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag of the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository that you would like to use.</para>
<para>After that, resources can be found at <literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>, where:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> is a SUC plan resource for <emphasis role="strong">control-plane</emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> is a SUC plan resource for <emphasis role="strong">worker</emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> is a Secret that contains the <literal>upgrade.sh</literal> script, which is responsible for creating the systemd.service (<xref linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>).</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> is a ConfigMap that holds configurations that are consumed by the <literal>upgrade.sh</literal> script.</para>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the <literal>System Upgrade Controller</literal> and should be deployed on each downstream cluster that you wish to upgrade. For SUC deployment information, see <xref linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the <emphasis role="strong">SUC Plans</emphasis> for OS upgrade, it can be beneficial to take a look at overview (<xref linkend="management-day2-fleet-os-upgrade-overview"/>).</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade">
<title>Kubernetes version upgrade</title>
<para>This section describes how to perform a Kubernetes upgrade using <xref linkend="components-fleet"/> and the <xref linkend="components-system-upgrade-controller"/>.</para>
<para>The following topics are covered as part of this section:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-components"/> - additional components used by the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-overview"/> - overview of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-requirements"/> - requirements of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment"/> - information on how to deploy <literal>SUC plans</literal>, responsible for triggering the upgrade process.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-k8s-upgrade-components">
<title>Components</title>
<para>This section covers the custom components that the <literal>K8s upgrade</literal> process uses over the default "Day 2" components (<xref linkend="management-day2-fleet-components"/>).</para>
<section xml:id="management-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>Container image responsible for upgrading the RKE2 version of a specific node.</para>
<para>Shipped through a Pod created by <emphasis role="strong">SUC</emphasis> based on a <emphasis role="strong">SUC Plan</emphasis>. The Plan should be located on each <emphasis role="strong">cluster</emphasis> that is in need of a RKE2 upgrade.</para>
<para>For more information regarding how the <literal>rke2-upgrade</literal> image performs the upgrade, see the <link xl:href="https://github.com/rancher/rke2-upgrade/tree/master">upstream</link> documentation.</para>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Container image responsible for upgrading the K3s version of a specific node.</para>
<para>Shipped through a Pod created by <emphasis role="strong">SUC</emphasis> based on a <emphasis role="strong">SUC Plan</emphasis>. The Plan should be located on each <emphasis role="strong">cluster</emphasis> that is in need of a K3s upgrade.</para>
<para>For more information regarding how the <literal>k3s-upgrade</literal> image performs the upgrade, see the <link xl:href="https://github.com/k3s-io/k3s-upgrade">upstream</link> documentation.</para>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-overview">
<title>Overview</title>
<para>The Kubernetes distribution upgrade for management cluster nodes is done by utilizing <literal>Fleet</literal> and the <literal>System Upgrade Controller (SUC)</literal>.</para>
<para><literal>Fleet</literal> is used to deploy and manage <literal>SUC plans</literal> onto the desired cluster.</para>
<note>
<para><literal>SUC plans</literal> are <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link> that describe the steps that <emphasis role="strong">SUC</emphasis> needs to follow in order for a specific task to be executed on a set of nodes. For an example of how an <literal>SUC plan</literal> looks like, refer to the <link xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">upstream repository</link>.</para>
</note>
<para>The <literal>K8s SUC plans</literal> are shipped on each cluster by deploying a <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> or <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resource to a specific Fleet <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">workspace</link>. Fleet retrieves the deployed <literal>GitRepo/Bundle</literal> and deploys its contents (the <literal>K8s SUC plans</literal>) to the desired cluster(s).</para>
<note>
<para><literal>GitRepo/Bundle</literal> resources are always deployed on the <literal>management cluster</literal>. Whether to use a <literal>GitRepo</literal> or <literal>Bundle</literal> resource depends on your use-case, check <xref linkend="management-day2-fleet-determine-use-case"/> for more information.</para>
</note>
<para><literal>K8s SUC plans</literal> describe the following workflow:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Always <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link> the nodes before K8s upgrades.</para>
</listitem>
<listitem>
<para>Always upgrade <literal>control-plane</literal> nodes before <literal>worker</literal> nodes.</para>
</listitem>
<listitem>
<para>Always upgrade the <literal>control-plane</literal> nodes <emphasis role="strong">one</emphasis> node at a time and the <literal>worker</literal> nodes <emphasis role="strong">two</emphasis> nodes at a time.</para>
</listitem>
</orderedlist>
<para>Once the <literal>K8s SUC plans</literal> are deployed, the workflow looks like this:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconciles the deployed <literal>K8s SUC plans</literal> and creates a <literal>Kubernetes Job</literal> on <emphasis role="strong">each node</emphasis>.</para>
</listitem>
<listitem>
<para>Depending on the Kubernetes distribution, the Job will create a Pod that runs either the rke2-upgrade (<xref linkend="management-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>) or the k3s-upgrade (<xref linkend="management-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>) container image.</para>
</listitem>
<listitem>
<para>The created Pod will go through the following workflow:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Replace the existing <literal>rke2/k3s</literal> binary on the node with the one from the <literal>rke2-upgrade/k3s-upgrade</literal> image.</para>
</listitem>
<listitem>
<para>Kill the running <literal>rke2/k3s</literal> process.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Killing the <literal>rke2/k3s</literal> process triggers a restart, launching a new process that runs the updated binary, resulting in an upgraded Kubernetes distribution version.</para>
</listitem>
</orderedlist>
<para>Below you can find a diagram of the above description:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-day2-management-k8s-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2 management k8s upgrade</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-requirements">
<title>Requirements</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Backup your Kubernetes distribution:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>For <emphasis role="strong">RKE2 clusters</emphasis>, see the <link xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2 Backup and Restore</link> documentation.</para>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s clusters</emphasis>, see the <link xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and Restore</link> documentation.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Make sure that SUC Plan tolerations match node tolerations</emphasis> - If your Kubernetes cluster nodes have custom <emphasis role="strong">taints</emphasis>, make sure to add <link xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</link> for those taints in the <emphasis role="strong">SUC Plans</emphasis>. By default <emphasis role="strong">SUC Plans</emphasis> have tolerations only for <emphasis role="strong">control-plane</emphasis> nodes. Default tolerations include:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Any additional tolerations must be added under the <literal>.spec.tolerations</literal> section of each Plan. <emphasis role="strong">SUC Plans</emphasis> related to the Kubernetes version upgrade can be found in the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository under:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> - <literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis>  - <literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Make sure you use the Plans from a valid repository <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</emphasis></para>
<para>An example of defining custom tolerations for the RKE2 <emphasis role="strong">control-plane</emphasis> SUC Plan, would look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment">
<title>K8s upgrade - SUC plan deployment</title>
<important>
<para>For environments previously upgraded using this procedure, users should ensure that <emphasis role="strong">one</emphasis> of the following steps is completed:</para>
<itemizedlist>
<listitem>
<para><literal>Remove any previously deployed SUC Plans related to older Edge release versions from the management cluster</literal> - can be done by removing the desired cluster from the existing <literal>GitRepo/Bundle</literal> <link xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">target configuration</link>, or removing the <literal>GitRepo/Bundle</literal> resource altogether.</para>
</listitem>
<listitem>
<para><literal>Reuse the existing GitRepo/Bundle resource</literal> - can be done by pointing the resource’s revision to a new tag that holds the correct fleets for the desired <literal>suse-edge/fleet-examples</literal> <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>.</para>
</listitem>
</itemizedlist>
<para>This is done in order to avoid clashes between <literal>SUC Plans</literal> for older Edge release versions.</para>
<para>If users attempt to upgrade, while there are existing <literal>SUC Plans</literal> on the management cluster, they will see the following fleet error:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>As mentioned in <xref linkend="management-day2-fleet-k8s-upgrade-overview"/>, Kubernetes upgrades are done by shipping <literal>SUC plans</literal> to the desired cluster through one of the following ways:</para>
<itemizedlist>
<listitem>
<para>Fleet GitRepo resource (<xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Fleet Bundle resource (<xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>To determine which resource you should use, refer to <xref linkend="management-day2-fleet-determine-use-case"/>.</para>
<para>For use-cases where you wish to deploy the <literal>K8s SUC plans</literal> from a third-party GitOps tool, refer to <xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-third-party"/></para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>SUC plan deployment - GitRepo resource</title>
<para>A <emphasis role="strong">GitRepo</emphasis> resource, that ships the needed <literal>K8s SUC plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the Kubernetes upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo creation - Rancher UI</title>
<para>To create a <literal>GitRepo</literal> resource through the Rancher UI, follow their official <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentation</link>.</para>
<para>The Edge team maintains ready to use fleets for both <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link> and <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link> Kubernetes distributions. Depending on your environment, this fleet could be used directly or as a template.</para>
<important>
<para>Always use these fleets from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>For use-cases where no custom changes need to be included to the <literal>SUC plans</literal> that these fleets ship, users can directly refer the fleets from the <literal>suse-edge/fleet-examples</literal> repository.</para>
<para>In cases where custom changes are needed (e.g. to add custom tolerations), users should refer the fleets from a separate repository, allowing them to add the changes to the SUC plans as required.</para>
<para>Configuration examples for a <literal>GitRepo</literal> resource using the fleets from <literal>suse-edge/fleet-examples</literal> repository:</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configuration:</para>
<itemizedlist>
<listitem>
<para>Remove the <literal>spec.targets</literal> section - only needed for downstream clusters.</para>
<itemizedlist>
<listitem>
<para>For RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>For K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Point the namespace of the <literal>GitRepo</literal> to the <literal>fleet-local</literal> namespace - done in order to deploy the resource on the management cluster.</para>
<itemizedlist>
<listitem>
<para>For RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>For K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resources to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under the <literal>fleet-local</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-local

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-local

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>SUC plan deployment - Bundle resource</title>
<para>A <emphasis role="strong">Bundle</emphasis> resource, that ships the needed <literal>Kubernetes upgrade SUC Plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the Kubernetes upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<para>The Edge team maintains ready to use bundles for both <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link> and <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link> Kubernetes distributions. Depending on your environment these bundles could be used directly or as a template.</para>
<important>
<para>Always use this bundle from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>To create a bundle through Rancher’s UI:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, click <emphasis role="strong">☰ → Continuous Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<note>
<para>There might be use-cases where you would need to include custom changes to the <literal>SUC plans</literal> that the bundle ships (e.g. to add custom tolerations). Make sure to include those changes in the bundle that will be generated by the below steps.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>By manually copying the bundle content for <link xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link> or <link xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link> from <literal>suse-edge/fleet-examples</literal> to the <emphasis role="strong">Create from YAML</emphasis> page.</para>
</listitem>
<listitem>
<para>By cloning the <link xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link> repository from the desired <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag and selecting the <emphasis role="strong">Read from File</emphasis> option in the <emphasis role="strong">Create from YAML</emphasis> page. From there, navigate to the bundle that you need (<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal> for RKE2 and <literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal> for K3s). This will auto-populate the <emphasis role="strong">Create from YAML</emphasis> page with the bundle content.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Edit the Bundle in the Rancher UI:</para>
<itemizedlist>
<listitem>
<para>Change the <emphasis role="strong">namespace</emphasis> of the <literal>Bundle</literal> to point to the <literal>fleet-local</literal> namespace.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the <literal>Bundle</literal> to point to your <literal>local</literal>(management) cluster:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>There are some use-cases where your <literal>local</literal> cluster could have a different name.</para>
<para>To retrieve your <literal>local</literal> cluster name, execute the command below:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resources:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> configuration:</para>
<itemizedlist>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the <literal>Bundle</literal> to point to your <literal>local</literal>(management) cluster:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>There are some use-cases where your <literal>local</literal> cluster could have a different name.</para>
<para>To retrieve your <literal>local</literal> cluster name, execute the command below:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">namespace</emphasis> of the <literal>Bundle</literal> to point to the <literal>fleet-local</literal> namespace.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resources to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under the <literal>fleet-local</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-local

# For K3s
kubectl get bundles k3s-upgrade -n fleet-local

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>SUC Plan deployment - third-party GitOps workflow</title>
<para>There might be use-cases where users would like to incorporate the <literal>Kubernetes upgrade SUC plans</literal> to their own third-party GitOps workflow (e.g. <literal>Flux</literal>).</para>
<para>To get the K8s upgrade resources that you need, first determine the Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag of the <link xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link> repository that you would like to use.</para>
<para>After that, the resources can be found at:</para>
<itemizedlist>
<listitem>
<para>For a RKE2 cluster upgrade:</para>
<itemizedlist>
<listitem>
<para>For <literal>control-plane</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>For <literal>worker</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>For a K3s cluster upgrade:</para>
<itemizedlist>
<listitem>
<para>For <literal>control-plane</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>For <literal>worker</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the <literal>System Upgrade Controller</literal> and should be deployed on each downstream cluster that you wish to upgrade. For SUC deployment information, see <xref linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the <emphasis role="strong">SUC Plans</emphasis> for Kubernetes version upgrade, it can be beneficial to take a look at the overview (<xref linkend="management-day2-fleet-k8s-upgrade-overview"/>) of the update procedure using <literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade">
<title>Helm chart upgrade</title>
<para>This section covers the following parts:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-air-gap"/> - holds information on how to ship Edge related OCI charts and images to your private registry.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure"/> - holds information on different Helm chart upgrade use-cases and their upgrade procedure.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-air-gap">
<title>Preparation for air-gapped environments</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet">
<title>Ensure you have access to your Helm chart Fleet</title>
<para>Depending on what your environment supports, you can take one of the following options:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Host your chart’s Fleet resources on a local Git server that is accessible by your <literal>management cluster</literal>.</para>
</listitem>
<listitem>
<para>Use Fleet’s CLI to <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">convert a Helm chart into a Bundle</link> that you can directly use and will not need to be hosted somewhere. Fleet’s CLI can be retrieved from their <link xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">release</link> page, for Mac users there is a <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link> Homebrew Formulae.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Find the required assets for your Edge release version</title>
<orderedlist numeration="arabic">
<listitem>
<para>Go to the "Day 2" <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> page and find the Edge release that you want to upgrade your chart to and click <emphasis role="strong">Assets</emphasis>.</para>
</listitem>
<listitem>
<para>From the <emphasis role="strong">"Assets"</emphasis> section, download the following files:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Release File</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Description</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Pulls the images specified in the <literal>edge-release-images.txt</literal> file and packages them inside of a '.tar.gz' archive.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Pulls the OCI chart images related to the specific Edge release and packages them inside of a '.tar.gz' archive.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Loads images from a '.tar.gz' archive, retags and pushes them to a private registry.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Takes a directory containing Edge OCI '.tgz' chart packages and loads them to a private registry.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contains a list of OCI chart images related to a specific Edge release.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contains a list of images related to a specific Edge release.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive">
<title>Create the Edge release images archive</title>
<para><emphasis>On a machine with internet access:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Make <literal>edge-save-images.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Generate the image archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>This will create a ready to load archive named <literal>edge-images.tar.gz</literal>.</para>
<note>
<para>If the <literal>-i|--images</literal> option is specified, the name of the archive may differ.</para>
</note>
</listitem>
<listitem>
<para>Copy this archive to your <emphasis role="strong">air-gapped</emphasis> machine:</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive">
<title>Create the Edge OCI chart images archive</title>
<para><emphasis>On a machine with internet access:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Make <literal>edge-save-oci-artefacts.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Generate the OCI chart image archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>This will create an archive named <literal>oci-artefacts.tar.gz</literal>.</para>
<note>
<para>If the <literal>-a|--archive</literal> option is specified, the name of the archive may differ.</para>
</note>
</listitem>
<listitem>
<para>Copy this archive to your <emphasis role="strong">air-gapped</emphasis> machine:</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine">
<title>Load Edge release images to your air-gapped machine</title>
<para><emphasis>On your air-gapped machine:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Log into your private registry (if required):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Make <literal>edge-load-images.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Execute the script, passing the previously <emphasis role="strong">copied</emphasis> <literal>edge-images.tar.gz</literal> archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>This will load all images from the <literal>edge-images.tar.gz</literal>, retag and push them to the registry specified under the <literal>--registry</literal> option.</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine">
<title>Load the Edge OCI chart images to your air-gapped machine</title>
<para><emphasis>On your air-gapped machine:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Log into your private registry (if required):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Make <literal>edge-load-oci-artefacts.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Untar the copied <literal>oci-artefacts.tar.gz</literal> archive:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>This will produce a directory with the naming template <literal>edge-release-oci-tgz-&lt;date&gt;</literal></para>
</listitem>
<listitem>
<para>Pass this directory to the <literal>edge-load-oci-artefacts.sh</literal> script to load the Edge OCI chart images to your private registry:</para>
<note>
<para>This script assumes the <literal>helm</literal> CLI has been pre-installed on your environment. For Helm installation instructions, see <link xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution">
<title>Configure your private registry in your Kubernetes distribution</title>
<para>For RKE2, see <link xl:href="https://docs.rke2.io/install/private_registry">Private Registry Configuration</link></para>
<para>For K3s, see <link xl:href="https://docs.k3s.io/installation/private-registry">Private Registry Configuration</link></para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure">
<title>Upgrade procedure</title>
<para>This section focuses on the following Helm upgrade procedure use-cases:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>Manually deployed Helm charts cannot be reliably upgraded. We suggest to redeploy the Helm chart using the <xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/> method.</para>
</important>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>I have a new cluster and would like to deploy and manage an Edge Helm chart</title>
<para>This section covers how to:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>Prepare the fleet resources for your chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Acquire the chart’s Fleet resources from the Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag that you wish to use.</para>
</listitem>
<listitem>
<para>Navigate to the Helm chart fleet (<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">If you intend to use a GitOps workflow</emphasis>, copy the chart Fleet directory to the Git repository from where you will do GitOps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Optionally</emphasis>, if the Helm chart requires configurations to its <emphasis role="strong">values</emphasis>, edit the <literal>.helm.values</literal> configuration inside the <literal>fleet.yaml</literal> file of the copied directory.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Optionally</emphasis>, there may be use-cases where you need to add additional resources to your chart’s fleet so that it can better fit your environment. For information on how to enhance your Fleet directory, see <link xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents</link>.</para>
</listitem>
</orderedlist>
<note>
<para>In some cases, the default timeout Fleet uses for Helm operations may be insufficient, resulting in the following error:</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>In such cases, add the <link xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link> property under the <literal>helm</literal> configuration of your <literal>fleet.yaml</literal> file.</para>
</note>
<para>An <emphasis role="strong">example</emphasis> for the <literal>longhorn</literal> helm chart would look like:</para>
<itemizedlist>
<listitem>
<para>User Git repository structure:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet.yaml</literal> content populated with user <literal>Longhorn</literal> data:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>These are just example values that are used to illustrate custom configurations over the <literal>longhorn</literal> chart. They should <emphasis role="strong">NOT</emphasis> be treated as deployment guidelines for the <literal>longhorn</literal> chart.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>Deploy the fleet for your chart</title>
<para>You can deploy the fleet for your chart by either using a GitRepo (<xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>) or Bundle (<xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>).</para>
<note>
<para>While deploying your Fleet, if you get a <literal>Modified</literal> message, make sure to add a corresponding <literal>comparePatches</literal> entry to the Fleet’s <literal>diff</literal> section. For more information, see <link xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore Modified GitRepos</link>.</para>
</note>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>Fleet’s <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> resource holds information on how to access your chart’s Fleet resources and to which clusters it needs to apply those resources.</para>
<para>The <literal>GitRepo</literal> resource can be deployed through the <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher UI</link>, or manually, by <link xl:href="https://fleet.rancher.io/tut-deployment">deploying</link> the resource to the <literal>management cluster</literal>.</para>
<para>Example <emphasis role="strong">Longhorn</emphasis> <literal>GitRepo</literal> resource for <emphasis role="strong">manual</emphasis> deployment:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-local
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url</screen>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>Bundle</title>
<para><link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resources hold the raw Kubernetes resources that need to be deployed by Fleet. Normally it is encouraged to use the <literal>GitRepo</literal> approach, but for use-cases where the environment is air-gapped and cannot support a local Git server, <literal>Bundles</literal> can help you in propagating your Helm chart Fleet to your target clusters.</para>
<para>A <literal>Bundle</literal> can be deployed either through the Rancher UI (<literal>Continuous Delivery → Advanced → Bundles → Create from YAML</literal>) or by manually deploying the <literal>Bundle</literal> resource in the correct Fleet namespace. For information about Fleet namespaces, see the upstream <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentation</link>.</para>
<para><literal>Bundles</literal> for Edge Helm charts can be created by utilizing Fleet’s <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert a Helm Chart into a Bundle</link> approach.</para>
<para>Below you can find an example on how to create a <literal>Bundle</literal> resource from the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link> and <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link> Helm chart fleet templates and manually deploy this bundle to your <literal>management cluster</literal>.</para>
<note>
<para>To illustrate the workflow, the below example uses the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> directory structure.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Navigate to the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link> Chart fleet template:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>Create a <literal>targets.yaml</literal> file that will instruct Fleet to which clusters it should deploy the Helm chart:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
<note>
<para>There are some use-cases where your local cluster could have a different name.</para>
<para>To retrieve your local cluster name, execute the command below:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Convert the <literal>Longhorn</literal> Helm chart Fleet to a Bundle resource using the <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<note>
<para>Fleet’s CLI can be retrieved from their <link xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">release</link> <emphasis role="strong">Assets</emphasis> page (<literal>fleet-linux-amd64</literal>).</para>
<para>For Mac users there is a <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link> Homebrew Formulae.</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>Navigate to the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link> Chart fleet template:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>Create a <literal>targets.yaml</literal> file that will instruct Fleet to which clusters it should deploy the Helm chart:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>Convert the <literal>Longhorn CRD</literal> Helm chart Fleet to a Bundle resource using the <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>Deploy the <literal>longhorn-bundle.yaml</literal> and <literal>longhorn-crd-bundle.yaml</literal> files to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>Following these steps will ensure that <literal>SUSE Storage</literal> is deployed on all of the specified management cluster.</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>Manage the deployed Helm chart</title>
<para>Once deployed with Fleet, for Helm chart upgrades, see <xref linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>I would like to upgrade a Fleet managed Helm chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine the version to which you need to upgrade your chart so that it is compatible with the desired Edge release. Helm chart version per Edge release can be viewed from the release notes (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>In your Fleet monitored Git repository, edit the Helm chart’s <literal>fleet.yaml</literal> file with the correct chart <emphasis role="strong">version</emphasis> and <emphasis role="strong">repository</emphasis> from the release notes (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>After committing and pushing the changes to your repository, this will trigger an upgrade of the desired Helm chart</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>I would like to upgrade a Helm chart deployed via EIB</title>
<para><xref linkend="components-eib"/> deploys Helm charts by creating a <literal>HelmChart</literal> resource and utilizing the <literal>helm-controller</literal> introduced by the <link xl:href="https://docs.rke2.io/helm">RKE2</link>/<link xl:href="https://docs.k3s.io/helm">K3s</link> Helm integration feature.</para>
<para>To ensure that a Helm chart deployed via <literal>EIB</literal> is successfully upgraded, users need to do an upgrade over the respective <literal>HelmChart</literal> resources.</para>
<para>Below you can find information on:</para>
<itemizedlist>
<listitem>
<para>The general overview (<xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>) of the upgrade process.</para>
</listitem>
<listitem>
<para>The necessary upgrade steps (<xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>).</para>
</listitem>
<listitem>
<para>An example (<xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>) showcasing a <link xl:href="https://longhorn.io">Longhorn</link> chart upgrade using the explained method.</para>
</listitem>
<listitem>
<para>How to use the upgrade process with a different GitOps tool (<xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>Overview</title>
<para>Helm charts that are deployed via <literal>EIB</literal> are upgraded through a <literal>fleet</literal> called <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>.</para>
<para>This <literal>fleet</literal> processes <emphasis role="strong">user-provided</emphasis> data to <emphasis role="strong">update</emphasis> a specific set of HelmChart resources.</para>
<para>Updating these resources triggers the <link xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>, which <emphasis role="strong">upgrades</emphasis> the Helm charts associated with the modified <literal>HelmChart</literal> resources.</para>
<para>The user is only expected to:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Locally <link xl:href="https://helm.sh/docs/helm/helm_pull/">pull</link> the archives for each Helm chart that needs to be upgraded.</para>
</listitem>
<listitem>
<para>Pass these archives to the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link> <literal>generate-chart-upgrade-data.sh</literal> script, which will include the data from these archives to the <literal>eib-charts-upgrader</literal> fleet.</para>
</listitem>
<listitem>
<para>Deploy the <literal>eib-charts-upgrader</literal> fleet to their <literal>management cluster</literal>. This is done through either a <literal>GitRepo</literal> or <literal>Bundle</literal> resource.</para>
</listitem>
</orderedlist>
<para>Once deployed, the <literal>eib-charts-upgrader</literal>, with the help of Fleet, will ship its resources to the desired management cluster.</para>
<para>These resources include:</para>
<orderedlist numeration="arabic">
<listitem>
<para>A set of <literal>Secrets</literal> holding the <emphasis role="strong">user-provided</emphasis> Helm chart data.</para>
</listitem>
<listitem>
<para>A <literal>Kubernetes Job</literal> which will deploy a <literal>Pod</literal> that will mount the previously mentioned <literal>Secrets</literal> and based on them <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">patch</link> the corresponding HelmChart resources.</para>
</listitem>
</orderedlist>
<para>As mentioned previously this will trigger the <literal>helm-controller</literal> which will perform the actual Helm chart upgrade.</para>
<para>Below you can find a diagram of the above description:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-day2-management-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2 management helm eib upgrade</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>Upgrade Steps</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clone the <literal>suse-edge/fleet-examples</literal> repository from the correct release <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>.</para>
</listitem>
<listitem>
<para>Create a directory in which you will store the pulled Helm chart archive(s).</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Inside of the newly created archive directory, <link xl:href="https://helm.sh/docs/helm/helm_pull/">pull</link> the archive(s) for the Helm chart(s) you wish to upgrade:</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>From <emphasis role="strong">Assets</emphasis> of the desired <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release tag</link>, download the <literal>generate-chart-upgrade-data.sh</literal> script.</para>
</listitem>
<listitem>
<para>Execute the <literal>generate-chart-upgrade-data.sh</literal> script:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>For each chart archive in the <literal>--archive-dir</literal> directory, the script generates a <literal>Kubernetes Secret YAML</literal> file containing the chart upgrade data and stores it in the <literal>base/secrets</literal> directory of the fleet specified by <literal>--fleet-path</literal>.</para>
<para>The <literal>generate-chart-upgrade-data.sh</literal> script also applies additional modifications to the fleet to ensure the generated <literal>Kubernetes Secret YAML</literal> files are correctly utilized by the workload deployed by the fleet.</para>
<important>
<para>Users should not make any changes over what the <literal>generate-chart-upgrade-data.sh</literal> script generates.</para>
</important>
</listitem>
</orderedlist>
<para>The steps below depend on the environment that you are running:</para>
<orderedlist numeration="arabic">
<listitem>
<para>For an environment that supports GitOps (e.g. is non air-gapped, or is air-gapped, but allows for local Git server support):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copy the <literal>fleets/day2/eib-charts-upgrader</literal> Fleet to the repository that you will use for GitOps.</para>
<note>
<para>Make sure that the Fleet includes the changes that have been made by the <literal>generate-chart-upgrade-data.sh</literal> script.</para>
</note>
</listitem>
<listitem>
<para>Configure a <literal>GitRepo</literal> resource that will be used to ship all the resources of the <literal>eib-charts-upgrader</literal> Fleet.</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>For <literal>GitRepo</literal> configuration and deployment through the Rancher UI, see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing Fleet in the Rancher UI</link>.</para>
</listitem>
<listitem>
<para>For <literal>GitRepo</literal> manual configuration and deployment, see <link xl:href="https://fleet.rancher.io/tut-deployment">Creating a Deployment</link>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>For an environment that does not support GitOps (e.g. is air-gapped and does not allow local Git server usage):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Download the <literal>fleet-cli</literal> binary from the <literal>rancher/fleet</literal> <link xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">release</link> page (<literal>fleet-linux-amd64</literal> for Linux). For Mac users, there is a Homebrew Formulae that can be used - <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Navigate to the <literal>eib-charts-upgrader</literal> Fleet:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Create a <literal>targets.yaml</literal> file that will instruct Fleet where to deploy your resources:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To map the local(management) cluster
- clusterName: local
EOF</screen>
<note>
<para>There are some use-cases where your <literal>local</literal> cluster could have a different name.</para>
<para>To retrieve your <literal>local</literal> cluster name, execute the command below:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Use the <literal>fleet-cli</literal> to convert the Fleet to a <literal>Bundle</literal> resource:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>This will create a Bundle (<literal>bundle.yaml</literal>) that will hold all the templated resource from the <literal>eib-charts-upgrader</literal> Fleet.</para>
<para>For more information regarding the <literal>fleet apply</literal> command, see <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet apply</link>.</para>
<para>For more information regarding converting Fleets to Bundles, see <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert a Helm Chart into a Bundle</link>.</para>
</listitem>
<listitem>
<para>Deploy the <literal>Bundle</literal>. This can be done in one of two ways:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Through Rancher’s UI - Navigate to <emphasis role="strong">Continuous Delivery → Advanced → Bundles → Create from YAML</emphasis> and either paste the <literal>bundle.yaml</literal> contents, or click the <literal>Read from File</literal> option and pass the file itself.</para>
</listitem>
<listitem>
<para>Manually - Deploy the <literal>bundle.yaml</literal> file manually inside of your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Executing these steps will result in a successfully deployed <literal>GitRepo/Bundle</literal> resource. The resource will be picked up by Fleet and its contents will be deployed onto the target clusters that the user has specified in the previous steps. For an overview of the process, refer to <xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>.</para>
<para>For information on how to track the upgrade process, you can refer to <xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>.</para>
<important>
<para>Once the chart upgrade has been successfully verified, remove the <literal>Bundle/GitRepo</literal> resource.</para>
<para>This will remove the no longer necessary upgrade resources from your <literal>management</literal> cluster, ensuring that no future version clashes might occur.</para>
</important>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>Example</title>
<note>
<para>The example below demonstrates how to upgrade a Helm chart deployed via <literal>EIB</literal> from one version to another on a <literal>management</literal> cluster. Note that the versions used in this example are <emphasis role="strong">not</emphasis> recommendations. For version recommendations specific to an Edge release, refer to the release notes (<xref linkend="release-notes"/>).</para>
</note>
<para><emphasis>Use-case:</emphasis></para>
<itemizedlist>
<listitem>
<para>A <literal>management</literal> cluster is running an older version of <link xl:href="https://longhorn.io">Longhorn</link>.</para>
</listitem>
<listitem>
<para>The cluster has been deployed through EIB, using the following image definition <emphasis>snippet</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal> needs to be upgraded to a version that is compatible with the Edge 3.4 release. Meaning it needs to be upgraded to <literal>107.0.0+up1.9.1</literal>.</para>
</listitem>
<listitem>
<para>It is assumed that the <literal>management cluster</literal> is <emphasis role="strong">air-gapped</emphasis>, without support for a local Git server and has a working Rancher setup.</para>
</listitem>
</itemizedlist>
<para>Follow the Upgrade Steps (<xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>):</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clone the <literal>suse-edge/fleet-example</literal> repository from the <literal>release-3.4.0</literal> tag.</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Create a directory where the <literal>Longhorn</literal> upgrade archive will be stored.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Pull the desired <literal>Longhorn</literal> chart archive version:</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>Outside of the <literal>archives</literal> directory, download the <literal>generate-chart-upgrade-data.sh</literal> script from the <literal>suse-edge/fleet-examples</literal> release <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>.</para>
</listitem>
<listitem>
<para>Directory setup should look similar to:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>Execute the <literal>generate-chart-upgrade-data.sh</literal> script:</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>The directory structure after the script execution should look similar to:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>The files changed in git should look like this:</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>Create a <literal>Bundle</literal> for the <literal>eib-charts-upgrader</literal> Fleet:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>First, navigate to the Fleet itself:</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Then create a <literal>targets.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>Then use the <literal>fleet-cli</literal> binary to convert the Fleet to a Bundle:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Deploy the Bundle through the Rancher UI:</para>
<figure>
<title>Deploy Bundle through Rancher UI</title>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_1.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 1</phrase></textobject>
</mediaobject>
</figure>
<para>From here, select <emphasis role="strong">Read from File</emphasis> and find the <literal>bundle.yaml</literal> file on your system.</para>
<para>This will auto-populate the <literal>Bundle</literal> inside of Rancher’s UI.</para>
<para>Select <emphasis role="strong">Create</emphasis>.</para>
</listitem>
<listitem>
<para>After a successful deployment, your Bundle would look similar to:</para>
<figure>
<title>Successfully deployed Bundle</title>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_2.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 2</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</orderedlist>
<para>After the successful deployment of the <literal>Bundle</literal>, to monitor the upgrade process:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Verify the logs of the <literal>Upgrade Pod</literal>:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_3_management.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 3 management</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Now verify the logs of the Pod created for the upgrade by the helm-controller:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>The Pod name will be with the following template - <literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>The Pod will be in the namespace where the <literal>HelmChart</literal> resource was deployed. In our case this is <literal>kube-system</literal>.</para>
<figure>
<title>Logs for successfully upgraded Longhorn chart</title>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_4_management.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 4 management</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Verify that the <literal>HelmChart</literal> version has been updated by navigating to Rancher’s <literal>HelmCharts</literal> section (<literal>More Resources → HelmCharts</literal>). Select the namespace where the chart was deployed, for this example it would be <literal>kube-system</literal>.</para>
</listitem>
<listitem>
<para>Finally check that the Longhorn Pods are running.</para>
</listitem>
</orderedlist>
<para>After making the above validations, it is safe to assume that the Longhorn Helm chart has been upgraded to the <literal>107.0.0+up1.9.1</literal> version.</para>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>Helm chart upgrade using a third-party GitOps tool</title>
<para>There might be use-cases where users would like to use this upgrade procedure with a GitOps workflow other than Fleet (e.g. <literal>Flux</literal>).</para>
<para>To produce the resources needed for the upgrade procedure, you can use the <literal>generate-chart-upgrade-data.sh</literal> script to populate the <literal>eib-charts-upgrader</literal> Fleet with the user provided data. For more information on how to do this, see <xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
<para>After you have the full setup, you can use <link xl:href="https://kustomize.io">kustomize</link> to generate a full working solution that you can deploy in your cluster:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>If you want to include the solution to your GitOps workflow, you can remove the <literal>fleet.yaml</literal> file and use what is left as a valid <literal>Kustomize</literal> setup. Just do not forget to first run the <literal>generate-chart-upgrade-data.sh</literal> script, so that it can populate the <literal>Kustomize</literal> setup with the data for the Helm charts that you wish to upgrade to.</para>
<para>To understand how this workflow is intended to be used, it can be beneficial to look at <xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/> and <xref linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="day2-downstream-clusters">
<title>Downstream clusters</title>
<important>
<para>The following steps do not apply to <literal>downstream</literal> clusters managed by SUSE Telco Cloud (<xref linkend="atip"/>). For guidance on upgrading these clusters, refer to <xref linkend="atip-lifecycle-downstream"/>.</para>
</important>
<para>This section covers the possible ways to perform "Day 2" operations for different parts of your <literal>downstream</literal> cluster.</para>
<section xml:id="downstream-day2-fleet">
<title>Fleet</title>
<para>This section offers information on how to perform "Day 2" operations using the Fleet (<xref linkend="components-fleet"/>) component.</para>
<para>The following topics are covered as part of this section:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-components"/> - default components used for all "Day 2" operations.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-determine-use-case"/> - provides an overview of the Fleet custom resources that will be used and their suitability for different "Day 2" operations use-cases.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-upgrade-workflow"/> - provides a workflow guide for executing "Day 2" operations with Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade"/> - describes how to do OS upgrades using Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade"/> - describes how to do Kubernetes version upgrades using Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade"/> - describes how to do Helm chart upgrades using Fleet.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-components">
<title>Components</title>
<para>Below you can find a description of the default components that should be set up on your <literal>downstream</literal> cluster so that you can successfully perform "Day 2" operations using Fleet.</para>
<section xml:id="id-system-upgrade-controller-suc-2">
<title>System Upgrade Controller (SUC)</title>
<note>
<para><emphasis role="strong">Must</emphasis> be deployed on each downstream cluster.</para>
</note>
<para><emphasis role="strong">System Upgrade Controller</emphasis> is responsible for executing tasks on specified nodes based on configuration data provided through a custom resource, called a <literal>Plan</literal>.</para>
<para><emphasis role="strong">SUC</emphasis> is actively utilized to upgrade the operating system and Kubernetes distribution.</para>
<para>For more information about the <emphasis role="strong">SUC</emphasis> component and how it fits in the Edge stack, see <xref linkend="components-system-upgrade-controller"/>.</para>
<para>For information on how to deploy <emphasis role="strong">SUC</emphasis>, first determine your use-case (<xref linkend="downstream-day2-fleet-determine-use-case"/>) and then refer to System Upgrade Controller installation - GitRepo (<xref linkend="components-system-upgrade-controller-fleet-gitrepo"/>), or System Upgrade Controller installation - Bundle (<xref linkend="components-system-upgrade-controller-fleet-bundle"/>).</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-determine-use-case">
<title>Determine your use-case</title>
<para>Fleet uses two types of <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link> to enable the management of Kubernetes and Helm resources.</para>
<para>Below you can find information about the purpose of these resources and the use-cases they are best suited for in the context of "Day 2" operations.</para>
<section xml:id="id-gitrepo-2">
<title>GitRepo</title>
<para>A <literal>GitRepo</literal> is a Fleet (<xref linkend="components-fleet"/>) resource that represents a Git repository from which <literal>Fleet</literal> can create <literal>Bundles</literal>. Each <literal>Bundle</literal> is created based on configuration paths defined inside of the <literal>GitRepo</literal> resource. For more information, see the <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> documentation.</para>
<para>In the context of "Day 2" operations, <literal>GitRepo</literal> resources are normally used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> in <emphasis role="strong">non air-gapped</emphasis> environments that utilize a <emphasis>Fleet GitOps</emphasis> approach.</para>
<para>Alternatively, <literal>GitRepo</literal> resources can also be used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> on <emphasis role="strong">air-gapped</emphasis> environments, <emphasis role="strong">provided you mirror your repository setup through a local git server</emphasis>.</para>
</section>
<section xml:id="id-bundle-2">
<title>Bundle</title>
<para><literal>Bundles</literal> hold <emphasis role="strong">raw</emphasis> Kubernetes resources that will be deployed on the targeted cluster. Usually they are created from a <literal>GitRepo</literal> resource, but there are use-cases where they can be deployed manually. For more information refer to the <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> documentation.</para>
<para>In the context of "Day 2" operations, <literal>Bundle</literal> resources are normally used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> in <emphasis role="strong">air-gapped</emphasis> environments that do not use some form of <emphasis>local GitOps</emphasis> procedure (e.g. a <emphasis role="strong">local git server</emphasis>).</para>
<para>Alternatively, if your use-case does not allow for a <emphasis>GitOps</emphasis> workflow (e.g. using a Git repository), <literal>Bundle</literal> resources could also be used to deploy <literal>SUC</literal> or <literal>SUC Plans</literal> in <emphasis role="strong">non air-gapped</emphasis> environments.</para>
</section>
</section>
<section xml:id="downstream-day2-upgrade-workflow">
<title>Day 2 workflow</title>
<para>The following is a "Day 2" workflow that should be followed when upgrading a downstream cluster to a specific Edge release.</para>
<orderedlist numeration="arabic">
<listitem>
<para>OS upgrade (<xref linkend="downstream-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Kubernetes version upgrade (<xref linkend="downstream-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helm chart upgrade (<xref linkend="downstream-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade">
<title>OS upgrade</title>
<para>This section describes how to perform an operating system upgrade using <xref linkend="components-fleet"/> and the <xref linkend="components-system-upgrade-controller"/>.</para>
<para>The following topics are covered as part of this section:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-components"/> - additional components used by the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-overview"/> - overview of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-requirements"/> - requirements of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment"/> - information on how to deploy <literal>SUC plans</literal>, responsible for triggering the upgrade process.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-os-upgrade-components">
<title>Components</title>
<para>This section covers the custom components that the <literal>OS upgrade</literal> process uses over the default "Day 2" components (<xref linkend="downstream-day2-fleet-components"/>).</para>
<section xml:id="downstream-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>The OS upgrade on a specific node is handled by a <link xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>.</para>
<para>A different service is created depending on what type of upgrade the OS requires from one Edge version to another:</para>
<itemizedlist>
<listitem>
<para>For Edge versions that require the same OS version (e.g. <literal>6.0</literal>), the <literal>os-pkg-update.service</literal> will be created. It uses <link xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link> to perform a <link xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">normal package upgrade</link>.</para>
</listitem>
<listitem>
<para>For Edge versions that require an OS version migration (e.g <literal>6.0</literal> → <literal>6.1</literal>), the <literal>os-migration.service</literal> will be created. It uses <link xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link> to perform:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>A <link xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">normal package upgrade</link> which ensures that all packages are at up-to-date in order to mitigate any failures in the migration related to old package versions.</para>
</listitem>
<listitem>
<para>An OS migration by utilizing the <literal>zypper migration</literal> command.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>The services mentioned above are shipped on each node through a <literal>SUC plan</literal> which must be located on the downstream cluster that is in need of an OS upgrade.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-overview">
<title>Overview</title>
<para>The upgrade of the operating system for downstream cluster nodes is done by utilizing <literal>Fleet</literal> and the <literal>System Upgrade Controller (SUC)</literal>.</para>
<para><emphasis role="strong">Fleet</emphasis> is used to deploy and manage <literal>SUC plans</literal> onto the desired cluster.</para>
<note>
<para><literal>SUC plans</literal> are <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link> that describe the steps that <literal>SUC</literal> needs to follow in order for a specific task to be executed on a set of nodes. For an example of how an <literal>SUC plan</literal> looks like, refer to the <link xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">upstream repository</link>.</para>
</note>
<para>The <literal>OS SUC plans</literal> are shipped to each cluster by deploying a <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> or <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resource to a specific Fleet <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">workspace</link>. Fleet retrieves the deployed <literal>GitRepo/Bundle</literal> and deploys its contents (the <literal>OS SUC plans</literal>) to the desired cluster(s).</para>
<note>
<para><literal>GitRepo/Bundle</literal> resources are always deployed on the <literal>management cluster</literal>. Whether to use a <literal>GitRepo</literal> or <literal>Bundle</literal> resource depends on your use-case, check <xref linkend="downstream-day2-fleet-determine-use-case"/> for more information.</para>
</note>
<para><literal>OS SUC plans</literal> describe the following workflow:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Always <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link> the nodes before OS upgrades.</para>
</listitem>
<listitem>
<para>Always upgrade <literal>control-plane</literal> nodes before <literal>worker</literal> nodes.</para>
</listitem>
<listitem>
<para>Always upgrade the cluster on a <emphasis role="strong">one</emphasis> node at a time basis.</para>
</listitem>
</orderedlist>
<para>Once the <literal>OS SUC plans</literal> are deployed, the workflow looks like this:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconciles the deployed <literal>OS SUC plans</literal> and creates a <literal>Kubernetes Job</literal> on <emphasis role="strong">each node</emphasis>.</para>
</listitem>
<listitem>
<para>The <literal>Kubernetes Job</literal> creates a systemd.service (<xref linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>) for either package upgrade, or OS migration.</para>
</listitem>
<listitem>
<para>The created <literal>systemd.service</literal> triggers the OS upgrade process on the specific node.</para>
<important>
<para>Once the OS upgrade process finishes, the corresponding node will be <literal>rebooted</literal> to apply the updates on the system.</para>
</important>
</listitem>
</orderedlist>
<para>Below you can find a diagram of the above description:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-day2-downstream-os-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2 downstream os upgrade</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-requirements">
<title>Requirements</title>
<para><emphasis>General:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC registered machine</emphasis> - All downstream cluster nodes should be registered to <literal><link xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal> which is needed so that the respective <literal>systemd.service</literal> can successfully connect to the desired RPM repository.</para>
<important>
<para>For Edge releases that require an OS version migration (e.g. <literal>6.0</literal> → <literal>6.1</literal>), make sure that your SCC key supports the migration to the new version.</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">Make sure that SUC Plan tolerations match node tolerations</emphasis> - If your Kubernetes cluster nodes have custom <emphasis role="strong">taints</emphasis>, make sure to add <link xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</link> for those taints in the <emphasis role="strong">SUC Plans</emphasis>. By default, <emphasis role="strong">SUC Plans</emphasis> have tolerations only for <emphasis role="strong">control-plane</emphasis> nodes. Default tolerations include:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Any additional tolerations must be added under the <literal>.spec.tolerations</literal> section of each Plan. <emphasis role="strong">SUC Plans</emphasis> related to the OS upgrade can be found in the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository under <literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>. <emphasis role="strong">Make sure you use the Plans from a valid repository <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</emphasis></para>
<para>An example of defining custom tolerations for the <emphasis role="strong">control-plane</emphasis> SUC Plan would look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>Air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Mirror SUSE RPM repositories</emphasis> - OS RPM repositories should be locally mirrored so that the <literal>systemd.service</literal> can have access to them. This can be achieved by using either <link xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link> or <link xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment">
<title>OS upgrade - SUC plan deployment</title>
<important>
<para>For environments previously upgraded using this procedure, users should ensure that <emphasis role="strong">one</emphasis> of the following steps is completed:</para>
<itemizedlist>
<listitem>
<para><literal>Remove any previously deployed SUC Plans related to older Edge release versions from the downstream cluster</literal> - can be done by removing the desired cluster from the existing <literal>GitRepo/Bundle</literal> <link xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">target configuration</link>, or removing the <literal>GitRepo/Bundle</literal> resource altogether.</para>
</listitem>
<listitem>
<para><literal>Reuse the existing GitRepo/Bundle resource</literal> - can be done by pointing the resource’s revision to a new tag that holds the correct fleets for the desired <literal>suse-edge/fleet-examples</literal> <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>.</para>
</listitem>
</itemizedlist>
<para>This is done in order to avoid clashes between <literal>SUC Plans</literal> for older Edge release versions.</para>
<para>If users attempt to upgrade, while there are existing <literal>SUC Plans</literal> on the downstream cluster, they will see the following fleet error:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>As mentioned in <xref linkend="downstream-day2-fleet-os-upgrade-overview"/>, OS upgrades are done by shipping <literal>SUC plans</literal> to the desired cluster through one of the following ways:</para>
<itemizedlist>
<listitem>
<para>Fleet <literal>GitRepo</literal> resource - <xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>.</para>
</listitem>
<listitem>
<para>Fleet <literal>Bundle</literal> resource - <xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle"/>.</para>
</listitem>
</itemizedlist>
<para>To determine which resource you should use, refer to <xref linkend="downstream-day2-fleet-determine-use-case"/>.</para>
<para>For use-cases where you wish to deploy the <literal>OS SUC plans</literal> from a third-party GitOps tool, refer to <xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-third-party"/></para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>SUC plan deployment - GitRepo resource</title>
<para>A <emphasis role="strong">GitRepo</emphasis> resource, that ships the needed <literal>OS SUC plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the OS upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo creation - Rancher UI</title>
<para>To create a <literal>GitRepo</literal> resource through the Rancher UI, follow their official <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentation</link>.</para>
<para>The Edge team maintains a ready to use <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">fleet</link>. Depending on your environment this fleet could be used directly or as a template.</para>
<important>
<para>Always use this fleet from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>For use-cases where no custom changes need to be included to the <literal>SUC plans</literal> that the fleet ships, users can directly refer the <literal>os-upgrade</literal> fleet from the <literal>suse-edge/fleet-examples</literal> repository.</para>
<para>In cases where custom changes are needed (e.g. to add custom tolerations), users should refer the <literal>os-upgrade</literal> fleet from a separate repository, allowing them to add the changes to the SUC plans as required.</para>
<para>An example of how a <literal>GitRepo</literal> can be configured to use the fleet from the <literal>suse-edge/fleet-examples</literal> repository, can be viewed <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">here</link>.</para>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configuration, under <literal>spec.targets</literal> specify your desired target list. By default the <literal>GitRepo</literal> resources from the <literal>suse-edge/fleet-examples</literal> are <emphasis role="strong">NOT</emphasis> mapped to any downstream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>GitRepo</literal> <emphasis role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resource your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>SUC plan deployment - Bundle resource</title>
<para>A <emphasis role="strong">Bundle</emphasis> resource, that ships the needed <literal>OS SUC Plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the OS upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<para>The Edge team maintains a ready to use <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle</link> that can be used in the below steps.</para>
<important>
<para>Always use this bundle from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>To create a bundle through Rancher’s UI:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, click <emphasis role="strong">☰ → Continuous Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<note>
<para>There might be use-cases where you would need to include custom changes to the <literal>SUC plans</literal> that the bundle ships (e.g. to add custom tolerations). Make sure to include those changes in the bundle that will be generated by the below steps.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>By manually copying the <link xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle content</link> from <literal>suse-edge/fleet-examples</literal> to the <emphasis role="strong">Create from YAML</emphasis> page.</para>
</listitem>
<listitem>
<para>By cloning the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository from the desired <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag and selecting the <emphasis role="strong">Read from File</emphasis> option in the <emphasis role="strong">Create from YAML</emphasis> page. From there, navigate to the bundle location (<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>) and select the bundle file. This will auto-populate the <emphasis role="strong">Create from YAML</emphasis> page with the bundle content.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the <literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>To match all downstream clusters change the default Bundle <literal>.spec.targets</literal> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>For a more granular downstream cluster mappings, see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> <emphasis role="strong">target</emphasis> configurations, under <literal>spec.targets</literal> provide your desired target list. By default the <literal>Bundle</literal> resources from the <literal>suse-edge/fleet-examples</literal> are <emphasis role="strong">NOT</emphasis> mapped to any downstream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>Bundle</literal> <emphasis role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resource to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-default</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>SUC Plan deployment - third-party GitOps workflow</title>
<para>There might be use-cases where users would like to incorporate the <literal>OS SUC plans</literal> to their own third-party GitOps workflow (e.g. <literal>Flux</literal>).</para>
<para>To get the OS upgrade resources that you need, first determine the Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag of the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository that you would like to use.</para>
<para>After that, resources can be found at <literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>, where:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> is a SUC plan resource for <emphasis role="strong">control-plane</emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> is a SUC plan resource for <emphasis role="strong">worker</emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> is a Secret that contains the <literal>upgrade.sh</literal> script, which is responsible for creating the systemd.service (<xref linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>).</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> is a ConfigMap that holds configurations that are consumed by the <literal>upgrade.sh</literal> script.</para>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the <literal>System Upgrade Controller</literal> and should be deployed on each downstream cluster that you wish to upgrade. For SUC deployment information, see <xref linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the <emphasis role="strong">SUC Plans</emphasis> for OS upgrade, it can be beneficial to take a look at overview (<xref linkend="downstream-day2-fleet-os-upgrade-overview"/>).</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade">
<title>Kubernetes version upgrade</title>
<important>
<para>This section covers Kubernetes upgrades for downstream clusters that have <emphasis role="strong">NOT</emphasis> been created through a Rancher (<xref linkend="components-rancher"/>) instance. For information on how to upgrade the Kubernetes version of <literal>Rancher</literal> created clusters, see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading and Rolling Back Kubernetes</link>.</para>
</important>
<para>This section describes how to perform a Kubernetes upgrade using <xref linkend="components-fleet"/> and the <xref linkend="components-system-upgrade-controller"/>.</para>
<para>The following topics are covered as part of this section:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-components"/> - additional components used by the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/> - overview of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-requirements"/> - requirements of the upgrade process.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment"/> - information on how to deploy <literal>SUC plans</literal>, responsible for triggering the upgrade process.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components">
<title>Components</title>
<para>This section covers the custom components that the <literal>K8s upgrade</literal> process uses over the default "Day 2" components (<xref linkend="downstream-day2-fleet-components"/>).</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>Container image responsible for upgrading the RKE2 version of a specific node.</para>
<para>Shipped through a Pod created by <emphasis role="strong">SUC</emphasis> based on a <emphasis role="strong">SUC Plan</emphasis>. The Plan should be located on each <emphasis role="strong">cluster</emphasis> that is in need of a RKE2 upgrade.</para>
<para>For more information regarding how the <literal>rke2-upgrade</literal> image performs the upgrade, see the <link xl:href="https://github.com/rancher/rke2-upgrade/tree/master">upstream</link> documentation.</para>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Container image responsible for upgrading the K3s version of a specific node.</para>
<para>Shipped through a Pod created by <emphasis role="strong">SUC</emphasis> based on a <emphasis role="strong">SUC Plan</emphasis>. The Plan should be located on each <emphasis role="strong">cluster</emphasis> that is in need of a K3s upgrade.</para>
<para>For more information regarding how the <literal>k3s-upgrade</literal> image performs the upgrade, see the <link xl:href="https://github.com/k3s-io/k3s-upgrade">upstream</link> documentation.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-overview">
<title>Overview</title>
<para>The Kubernetes distribution upgrade for downstream cluster nodes is done by utilizing <literal>Fleet</literal> and the <literal>System Upgrade Controller (SUC)</literal>.</para>
<para><literal>Fleet</literal> is used to deploy and manage <literal>SUC plans</literal> onto the desired cluster.</para>
<note>
<para><literal>SUC plans</literal> are <link xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources</link> that describe the steps that <emphasis role="strong">SUC</emphasis> needs to follow in order for a specific task to be executed on a set of nodes. For an example of how an <literal>SUC plan</literal> looks like, refer to the <link xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">upstream repository</link>.</para>
</note>
<para>The <literal>K8s SUC plans</literal> are shipped on each cluster by deploying a <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> or <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resource to a specific Fleet <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">workspace</link>. Fleet retrieves the deployed <literal>GitRepo/Bundle</literal> and deploys its contents (the <literal>K8s SUC plans</literal>) to the desired cluster(s).</para>
<note>
<para><literal>GitRepo/Bundle</literal> resources are always deployed on the <literal>management cluster</literal>. Whether to use a <literal>GitRepo</literal> or <literal>Bundle</literal> resource depends on your use-case, check <xref linkend="downstream-day2-fleet-determine-use-case"/> for more information.</para>
</note>
<para><literal>K8s SUC plans</literal> describe the following workflow:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Always <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link> the nodes before K8s upgrades.</para>
</listitem>
<listitem>
<para>Always upgrade <literal>control-plane</literal> nodes before <literal>worker</literal> nodes.</para>
</listitem>
<listitem>
<para>Always upgrade the <literal>control-plane</literal> nodes <emphasis role="strong">one</emphasis> node at a time and the <literal>worker</literal> nodes <emphasis role="strong">two</emphasis> nodes at a time.</para>
</listitem>
</orderedlist>
<para>Once the <literal>K8s SUC plans</literal> are deployed, the workflow looks like this:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconciles the deployed <literal>K8s SUC plans</literal> and creates a <literal>Kubernetes Job</literal> on <emphasis role="strong">each node</emphasis>.</para>
</listitem>
<listitem>
<para>Depending on the Kubernetes distribution, the Job will create a Pod that runs either the rke2-upgrade (<xref linkend="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>) or the k3s-upgrade (<xref linkend="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>) container image.</para>
</listitem>
<listitem>
<para>The created Pod will go through the following workflow:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Replace the existing <literal>rke2/k3s</literal> binary on the node with the one from the <literal>rke2-upgrade/k3s-upgrade</literal> image.</para>
</listitem>
<listitem>
<para>Kill the running <literal>rke2/k3s</literal> process.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Killing the <literal>rke2/k3s</literal> process triggers a restart, launching a new process that runs the updated binary, resulting in an upgraded Kubernetes distribution version.</para>
</listitem>
</orderedlist>
<para>Below you can find a diagram of the above description:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-day2-downstream-k8s-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2 downstream k8s upgrade</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-requirements">
<title>Requirements</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Backup your Kubernetes distribution:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>For <emphasis role="strong">RKE2 clusters</emphasis>, see the <link xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2 Backup and Restore</link> documentation.</para>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s clusters</emphasis>, see the <link xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and Restore</link> documentation.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Make sure that SUC Plan tolerations match node tolerations</emphasis> - If your Kubernetes cluster nodes have custom <emphasis role="strong">taints</emphasis>, make sure to add <link xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</link> for those taints in the <emphasis role="strong">SUC Plans</emphasis>. By default <emphasis role="strong">SUC Plans</emphasis> have tolerations only for <emphasis role="strong">control-plane</emphasis> nodes. Default tolerations include:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Any additional tolerations must be added under the <literal>.spec.tolerations</literal> section of each Plan. <emphasis role="strong">SUC Plans</emphasis> related to the Kubernetes version upgrade can be found in the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> repository under:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> - <literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis>  - <literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Make sure you use the Plans from a valid repository <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</emphasis></para>
<para>An example of defining custom tolerations for the RKE2 <emphasis role="strong">control-plane</emphasis> SUC Plan, would look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment">
<title>K8s upgrade - SUC plan deployment</title>
<important>
<para>For environments previously upgraded using this procedure, users should ensure that <emphasis role="strong">one</emphasis> of the following steps is completed:</para>
<itemizedlist>
<listitem>
<para><literal>Remove any previously deployed SUC Plans related to older Edge release versions from the downstream cluster</literal> - can be done by removing the desired cluster from the existing <literal>GitRepo/Bundle</literal> <link xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">target configuration</link>, or removing the <literal>GitRepo/Bundle</literal> resource altogether.</para>
</listitem>
<listitem>
<para><literal>Reuse the existing GitRepo/Bundle resource</literal> - can be done by pointing the resource’s revision to a new tag that holds the correct fleets for the desired <literal>suse-edge/fleet-examples</literal> <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>.</para>
</listitem>
</itemizedlist>
<para>This is done in order to avoid clashes between <literal>SUC Plans</literal> for older Edge release versions.</para>
<para>If users attempt to upgrade, while there are existing <literal>SUC Plans</literal> on the downstream cluster, they will see the following fleet error:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>As mentioned in <xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/>, Kubernetes upgrades are done by shipping <literal>SUC plans</literal> to the desired cluster through one of the following ways:</para>
<itemizedlist>
<listitem>
<para>Fleet GitRepo resource (<xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Fleet Bundle resource (<xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>To determine which resource you should use, refer to <xref linkend="downstream-day2-fleet-determine-use-case"/>.</para>
<para>For use-cases where you wish to deploy the <literal>K8s SUC plans</literal> from a third-party GitOps tool, refer to <xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party"/></para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>SUC plan deployment - GitRepo resource</title>
<para>A <emphasis role="strong">GitRepo</emphasis> resource, that ships the needed <literal>K8s SUC plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the Kubernetes upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo creation - Rancher UI</title>
<para>To create a <literal>GitRepo</literal> resource through the Rancher UI, follow their official <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentation</link>.</para>
<para>The Edge team maintains ready to use fleets for both <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link> and <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link> Kubernetes distributions. Depending on your environment, this fleet could be used directly or as a template.</para>
<important>
<para>Always use these fleets from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>For use-cases where no custom changes need to be included to the <literal>SUC plans</literal> that these fleets ship, users can directly refer the fleets from the <literal>suse-edge/fleet-examples</literal> repository.</para>
<para>In cases where custom changes are needed (e.g. to add custom tolerations), users should refer the fleets from a separate repository, allowing them to add the changes to the SUC plans as required.</para>
<para>Configuration examples for a <literal>GitRepo</literal> resource using the fleets from <literal>suse-edge/fleet-examples</literal> repository:</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configuration, under <literal>spec.targets</literal> specify your desired target list. By default the <literal>GitRepo</literal> resources from the <literal>suse-edge/fleet-examples</literal> are <emphasis role="strong">NOT</emphasis> mapped to any downstream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>GitRepo</literal> <emphasis role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resources to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>SUC plan deployment - Bundle resource</title>
<para>A <emphasis role="strong">Bundle</emphasis> resource, that ships the needed <literal>Kubernetes upgrade SUC Plans</literal>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/> (when <literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>) the resource to your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the Kubernetes upgrade process of the nodes of your targeted cluster, refer to <xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<para>The Edge team maintains ready to use bundles for both <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link> and <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link> Kubernetes distributions. Depending on your environment these bundles could be used directly or as a template.</para>
<important>
<para>Always use this bundle from a valid Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag.</para>
</important>
<para>To create a bundle through Rancher’s UI:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, click <emphasis role="strong">☰ → Continuous Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<note>
<para>There might be use-cases where you would need to include custom changes to the <literal>SUC plans</literal> that the bundle ships (e.g. to add custom tolerations). Make sure to include those changes in the bundle that will be generated by the below steps.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>By manually copying the bundle content for <link xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link> or <link xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link> from <literal>suse-edge/fleet-examples</literal> to the <emphasis role="strong">Create from YAML</emphasis> page.</para>
</listitem>
<listitem>
<para>By cloning the <link xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link> repository from the desired <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag and selecting the <emphasis role="strong">Read from File</emphasis> option in the <emphasis role="strong">Create from YAML</emphasis> page. From there, navigate to the bundle that you need (<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal> for RKE2 and <literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal> for K3s). This will auto-populate the <emphasis role="strong">Create from YAML</emphasis> page with the bundle content.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the <literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>To match all downstream clusters change the default Bundle <literal>.spec.targets</literal> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>For a more granular downstream cluster mappings, see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resources:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> <emphasis role="strong">target</emphasis> configurations, under <literal>spec.targets</literal> provide your desired target list. By default the <literal>Bundle</literal> resources from the <literal>suse-edge/fleet-examples</literal> are <emphasis role="strong">NOT</emphasis> mapped to any downstream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>Bundle</literal> <emphasis role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resources to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>SUC Plan deployment - third-party GitOps workflow</title>
<para>There might be use-cases where users would like to incorporate the <literal>Kubernetes upgrade SUC plans</literal> to their own third-party GitOps workflow (e.g. <literal>Flux</literal>).</para>
<para>To get the K8s upgrade resources that you need, first determine the Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag of the <link xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link> repository that you would like to use.</para>
<para>After that, the resources can be found at:</para>
<itemizedlist>
<listitem>
<para>For a RKE2 cluster upgrade:</para>
<itemizedlist>
<listitem>
<para>For <literal>control-plane</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>For <literal>worker</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>For a K3s cluster upgrade:</para>
<itemizedlist>
<listitem>
<para>For <literal>control-plane</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>For <literal>worker</literal> nodes - <literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the <literal>System Upgrade Controller</literal> and should be deployed on each downstream cluster that you wish to upgrade. For SUC deployment information, see <xref linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the <emphasis role="strong">SUC Plans</emphasis> for Kubernetes version upgrade, it can be beneficial to take a look at the overview (<xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/>) of the update procedure using <literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade">
<title>Helm chart upgrade</title>
<para>This section covers the following parts:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-air-gap"/> - holds information on how to ship Edge related OCI charts and images to your private registry.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure"/> - holds information on different Helm chart upgrade use-cases and their upgrade procedure.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-air-gap">
<title>Preparation for air-gapped environments</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet-2">
<title>Ensure you have access to your Helm chart Fleet</title>
<para>Depending on what your environment supports, you can take one of the following options:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Host your chart’s Fleet resources on a local Git server that is accessible by your <literal>management cluster</literal>.</para>
</listitem>
<listitem>
<para>Use Fleet’s CLI to <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">convert a Helm chart into a Bundle</link> that you can directly use and will not need to be hosted somewhere. Fleet’s CLI can be retrieved from their <link xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">release</link> page, for Mac users there is a <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link> Homebrew Formulae.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version-2">
<title>Find the required assets for your Edge release version</title>
<orderedlist numeration="arabic">
<listitem>
<para>Go to the "Day 2" <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> page and find the Edge release that you want to upgrade your chart to and click <emphasis role="strong">Assets</emphasis>.</para>
</listitem>
<listitem>
<para>From the <emphasis role="strong">"Assets"</emphasis> section, download the following files:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Release File</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Description</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Pulls the images specified in the <literal>edge-release-images.txt</literal> file and packages them inside of a '.tar.gz' archive.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Pulls the OCI chart images related to the specific Edge release and packages them inside of a '.tar.gz' archive.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Loads images from a '.tar.gz' archive, retags and pushes them to a private registry.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Takes a directory containing Edge OCI '.tgz' chart packages and loads them to a private registry.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contains a list of OCI chart images related to a specific Edge release.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contains a list of images related to a specific Edge release.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive-2">
<title>Create the Edge release images archive</title>
<para><emphasis>On a machine with internet access:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Make <literal>edge-save-images.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Generate the image archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>This will create a ready to load archive named <literal>edge-images.tar.gz</literal>.</para>
<note>
<para>If the <literal>-i|--images</literal> option is specified, the name of the archive may differ.</para>
</note>
</listitem>
<listitem>
<para>Copy this archive to your <emphasis role="strong">air-gapped</emphasis> machine:</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive-2">
<title>Create the Edge OCI chart images archive</title>
<para><emphasis>On a machine with internet access:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Make <literal>edge-save-oci-artefacts.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Generate the OCI chart image archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>This will create an archive named <literal>oci-artefacts.tar.gz</literal>.</para>
<note>
<para>If the <literal>-a|--archive</literal> option is specified, the name of the archive may differ.</para>
</note>
</listitem>
<listitem>
<para>Copy this archive to your <emphasis role="strong">air-gapped</emphasis> machine:</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine-2">
<title>Load Edge release images to your air-gapped machine</title>
<para><emphasis>On your air-gapped machine:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Log into your private registry (if required):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Make <literal>edge-load-images.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Execute the script, passing the previously <emphasis role="strong">copied</emphasis> <literal>edge-images.tar.gz</literal> archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>This will load all images from the <literal>edge-images.tar.gz</literal>, retag and push them to the registry specified under the <literal>--registry</literal> option.</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine-2">
<title>Load the Edge OCI chart images to your air-gapped machine</title>
<para><emphasis>On your air-gapped machine:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Log into your private registry (if required):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Make <literal>edge-load-oci-artefacts.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Untar the copied <literal>oci-artefacts.tar.gz</literal> archive:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>This will produce a directory with the naming template <literal>edge-release-oci-tgz-&lt;date&gt;</literal></para>
</listitem>
<listitem>
<para>Pass this directory to the <literal>edge-load-oci-artefacts.sh</literal> script to load the Edge OCI chart images to your private registry:</para>
<note>
<para>This script assumes the <literal>helm</literal> CLI has been pre-installed on your environment. For Helm installation instructions, see <link xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution-2">
<title>Configure your private registry in your Kubernetes distribution</title>
<para>For RKE2, see <link xl:href="https://docs.rke2.io/install/private_registry">Private Registry Configuration</link></para>
<para>For K3s, see <link xl:href="https://docs.k3s.io/installation/private-registry">Private Registry Configuration</link></para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure">
<title>Upgrade procedure</title>
<para>This section focuses on the following Helm upgrade procedure use-cases:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>Manually deployed Helm charts cannot be reliably upgraded. We suggest to redeploy the Helm chart using the <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/> method.</para>
</important>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>I have a new cluster and would like to deploy and manage an Edge Helm chart</title>
<para>This section covers how to:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>Prepare the fleet resources for your chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Acquire the chart’s Fleet resources from the Edge <link xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link> tag that you wish to use.</para>
</listitem>
<listitem>
<para>Navigate to the Helm chart fleet (<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">If you intend to use a GitOps workflow</emphasis>, copy the chart Fleet directory to the Git repository from where you will do GitOps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Optionally</emphasis>, if the Helm chart requires configurations to its <emphasis role="strong">values</emphasis>, edit the <literal>.helm.values</literal> configuration inside the <literal>fleet.yaml</literal> file of the copied directory.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Optionally</emphasis>, there may be use-cases where you need to add additional resources to your chart’s fleet so that it can better fit your environment. For information on how to enhance your Fleet directory, see <link xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents</link>.</para>
</listitem>
</orderedlist>
<note>
<para>In some cases, the default timeout Fleet uses for Helm operations may be insufficient, resulting in the following error:</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>In such cases, add the <link xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link> property under the <literal>helm</literal> configuration of your <literal>fleet.yaml</literal> file.</para>
</note>
<para>An <emphasis role="strong">example</emphasis> for the <literal>longhorn</literal> helm chart would look like:</para>
<itemizedlist>
<listitem>
<para>User Git repository structure:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet.yaml</literal> content populated with user <literal>Longhorn</literal> data:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>These are just example values that are used to illustrate custom configurations over the <literal>longhorn</literal> chart. They should <emphasis role="strong">NOT</emphasis> be treated as deployment guidelines for the <literal>longhorn</literal> chart.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>Deploy the fleet for your chart</title>
<para>You can deploy the fleet for your chart by either using a GitRepo (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>) or Bundle (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>).</para>
<note>
<para>While deploying your Fleet, if you get a <literal>Modified</literal> message, make sure to add a corresponding <literal>comparePatches</literal> entry to the Fleet’s <literal>diff</literal> section. For more information, see <link xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore Modified GitRepos</link>.</para>
</note>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>Fleet’s <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> resource holds information on how to access your chart’s Fleet resources and to which clusters it needs to apply those resources.</para>
<para>The <literal>GitRepo</literal> resource can be deployed through the <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher UI</link>, or manually, by <link xl:href="https://fleet.rancher.io/tut-deployment">deploying</link> the resource to the <literal>management cluster</literal>.</para>
<para>Example <emphasis role="strong">Longhorn</emphasis> <literal>GitRepo</literal> resource for <emphasis role="strong">manual</emphasis> deployment:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>Bundle</title>
<para><link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> resources hold the raw Kubernetes resources that need to be deployed by Fleet. Normally it is encouraged to use the <literal>GitRepo</literal> approach, but for use-cases where the environment is air-gapped and cannot support a local Git server, <literal>Bundles</literal> can help you in propagating your Helm chart Fleet to your target clusters.</para>
<para>A <literal>Bundle</literal> can be deployed either through the Rancher UI (<literal>Continuous Delivery → Advanced → Bundles → Create from YAML</literal>) or by manually deploying the <literal>Bundle</literal> resource in the correct Fleet namespace. For information about Fleet namespaces, see the upstream <link xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentation</link>.</para>
<para><literal>Bundles</literal> for Edge Helm charts can be created by utilizing Fleet’s <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert a Helm Chart into a Bundle</link> approach.</para>
<para>Below you can find an example on how to create a <literal>Bundle</literal> resource from the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link> and <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link> Helm chart fleet templates and manually deploy this bundle to your <literal>management cluster</literal>.</para>
<note>
<para>To illustrate the workflow, the below example uses the <link xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link> directory structure.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Navigate to the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link> Chart fleet template:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>Create a <literal>targets.yaml</literal> file that will instruct Fleet to which clusters it should deploy the Helm chart:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>For a more granular downstream cluster selection, refer to <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream Clusters</link>.</para>
</listitem>
<listitem>
<para>Convert the <literal>Longhorn</literal> Helm chart Fleet to a Bundle resource using the <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<note>
<para>Fleet’s CLI can be retrieved from their <link xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">release</link> <emphasis role="strong">Assets</emphasis> page (<literal>fleet-linux-amd64</literal>).</para>
<para>For Mac users there is a <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link> Homebrew Formulae.</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>Navigate to the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link> Chart fleet template:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>Create a <literal>targets.yaml</literal> file that will instruct Fleet to which clusters it should deploy the Helm chart:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
</listitem>
<listitem>
<para>Convert the <literal>Longhorn CRD</literal> Helm chart Fleet to a Bundle resource using the <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>Deploy the <literal>longhorn-bundle.yaml</literal> and <literal>longhorn-crd-bundle.yaml</literal> files to your <literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>Following these steps will ensure that <literal>SUSE Storage</literal> is deployed on all of the specified downstream cluster.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>Manage the deployed Helm chart</title>
<para>Once deployed with Fleet, for Helm chart upgrades, see <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>I would like to upgrade a Fleet managed Helm chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine the version to which you need to upgrade your chart so that it is compatible with the desired Edge release. Helm chart version per Edge release can be viewed from the release notes (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>In your Fleet monitored Git repository, edit the Helm chart’s <literal>fleet.yaml</literal> file with the correct chart <emphasis role="strong">version</emphasis> and <emphasis role="strong">repository</emphasis> from the release notes (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>After committing and pushing the changes to your repository, this will trigger an upgrade of the desired Helm chart</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>I would like to upgrade a Helm chart deployed via EIB</title>
<para><xref linkend="components-eib"/> deploys Helm charts by creating a <literal>HelmChart</literal> resource and utilizing the <literal>helm-controller</literal> introduced by the <link xl:href="https://docs.rke2.io/helm">RKE2</link>/<link xl:href="https://docs.k3s.io/helm">K3s</link> Helm integration feature.</para>
<para>To ensure that a Helm chart deployed via <literal>EIB</literal> is successfully upgraded, users need to do an upgrade over the respective <literal>HelmChart</literal> resources.</para>
<para>Below you can find information on:</para>
<itemizedlist>
<listitem>
<para>The general overview (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>) of the upgrade process.</para>
</listitem>
<listitem>
<para>The necessary upgrade steps (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>).</para>
</listitem>
<listitem>
<para>An example (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>) showcasing a <link xl:href="https://longhorn.io">Longhorn</link> chart upgrade using the explained method.</para>
</listitem>
<listitem>
<para>How to use the upgrade process with a different GitOps tool (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>Overview</title>
<para>Helm charts that are deployed via <literal>EIB</literal> are upgraded through a <literal>fleet</literal> called <link xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>.</para>
<para>This <literal>fleet</literal> processes <emphasis role="strong">user-provided</emphasis> data to <emphasis role="strong">update</emphasis> a specific set of HelmChart resources.</para>
<para>Updating these resources triggers the <link xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>, which <emphasis role="strong">upgrades</emphasis> the Helm charts associated with the modified <literal>HelmChart</literal> resources.</para>
<para>The user is only expected to:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Locally <link xl:href="https://helm.sh/docs/helm/helm_pull/">pull</link> the archives for each Helm chart that needs to be upgraded.</para>
</listitem>
<listitem>
<para>Pass these archives to the <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link> <literal>generate-chart-upgrade-data.sh</literal> script, which will include the data from these archives to the <literal>eib-charts-upgrader</literal> fleet.</para>
</listitem>
<listitem>
<para>Deploy the <literal>eib-charts-upgrader</literal> fleet to their <literal>management cluster</literal>. This is done through either a <literal>GitRepo</literal> or <literal>Bundle</literal> resource.</para>
</listitem>
</orderedlist>
<para>Once deployed, the <literal>eib-charts-upgrader</literal>, with the help of Fleet, will ship its resources to the desired downstream cluster.</para>
<para>These resources include:</para>
<orderedlist numeration="arabic">
<listitem>
<para>A set of <literal>Secrets</literal> holding the <emphasis role="strong">user-provided</emphasis> Helm chart data.</para>
</listitem>
<listitem>
<para>A <literal>Kubernetes Job</literal> which will deploy a <literal>Pod</literal> that will mount the previously mentioned <literal>Secrets</literal> and based on them <link xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">patch</link> the corresponding HelmChart resources.</para>
</listitem>
</orderedlist>
<para>As mentioned previously this will trigger the <literal>helm-controller</literal> which will perform the actual Helm chart upgrade.</para>
<para>Below you can find a diagram of the above description:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="fleet-day2-downstream-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2 downstream helm eib upgrade</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>Upgrade Steps</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clone the <literal>suse-edge/fleet-examples</literal> repository from the correct release <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>.</para>
</listitem>
<listitem>
<para>Create a directory in which you will store the pulled Helm chart archive(s).</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Inside of the newly created archive directory, <link xl:href="https://helm.sh/docs/helm/helm_pull/">pull</link> the archive(s) for the Helm chart(s) you wish to upgrade:</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>From <emphasis role="strong">Assets</emphasis> of the desired <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release tag</link>, download the <literal>generate-chart-upgrade-data.sh</literal> script.</para>
</listitem>
<listitem>
<para>Execute the <literal>generate-chart-upgrade-data.sh</literal> script:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>For each chart archive in the <literal>--archive-dir</literal> directory, the script generates a <literal>Kubernetes Secret YAML</literal> file containing the chart upgrade data and stores it in the <literal>base/secrets</literal> directory of the fleet specified by <literal>--fleet-path</literal>.</para>
<para>The <literal>generate-chart-upgrade-data.sh</literal> script also applies additional modifications to the fleet to ensure the generated <literal>Kubernetes Secret YAML</literal> files are correctly utilized by the workload deployed by the fleet.</para>
<important>
<para>Users should not make any changes over what the <literal>generate-chart-upgrade-data.sh</literal> script generates.</para>
</important>
</listitem>
</orderedlist>
<para>The steps below depend on the environment that you are running:</para>
<orderedlist numeration="arabic">
<listitem>
<para>For an environment that supports GitOps (e.g. is non air-gapped, or is air-gapped, but allows for local Git server support):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copy the <literal>fleets/day2/eib-charts-upgrader</literal> Fleet to the repository that you will use for GitOps.</para>
<note>
<para>Make sure that the Fleet includes the changes that have been made by the <literal>generate-chart-upgrade-data.sh</literal> script.</para>
</note>
</listitem>
<listitem>
<para>Configure a <literal>GitRepo</literal> resource that will be used to ship all the resources of the <literal>eib-charts-upgrader</literal> Fleet.</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>For <literal>GitRepo</literal> configuration and deployment through the Rancher UI, see <link xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing Fleet in the Rancher UI</link>.</para>
</listitem>
<listitem>
<para>For <literal>GitRepo</literal> manual configuration and deployment, see <link xl:href="https://fleet.rancher.io/tut-deployment">Creating a Deployment</link>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>For an environment that does not support GitOps (e.g. is air-gapped and does not allow local Git server usage):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Download the <literal>fleet-cli</literal> binary from the <literal>rancher/fleet</literal> <link xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">release</link> page (<literal>fleet-linux-amd64</literal> for Linux). For Mac users, there is a Homebrew Formulae that can be used - <link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Navigate to the <literal>eib-charts-upgrader</literal> Fleet:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Create a <literal>targets.yaml</literal> file that will instruct Fleet where to deploy your resources:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To match all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>For information on how to map target clusters, see the upstream <link xl:href="https://fleet.rancher.io/gitrepo-targets">documentation</link>.</para>
</listitem>
<listitem>
<para>Use the <literal>fleet-cli</literal> to convert the Fleet to a <literal>Bundle</literal> resource:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>This will create a Bundle (<literal>bundle.yaml</literal>) that will hold all the templated resource from the <literal>eib-charts-upgrader</literal> Fleet.</para>
<para>For more information regarding the <literal>fleet apply</literal> command, see <link xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet apply</link>.</para>
<para>For more information regarding converting Fleets to Bundles, see <link xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert a Helm Chart into a Bundle</link>.</para>
</listitem>
<listitem>
<para>Deploy the <literal>Bundle</literal>. This can be done in one of two ways:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Through Rancher’s UI - Navigate to <emphasis role="strong">Continuous Delivery → Advanced → Bundles → Create from YAML</emphasis> and either paste the <literal>bundle.yaml</literal> contents, or click the <literal>Read from File</literal> option and pass the file itself.</para>
</listitem>
<listitem>
<para>Manually - Deploy the <literal>bundle.yaml</literal> file manually inside of your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Executing these steps will result in a successfully deployed <literal>GitRepo/Bundle</literal> resource. The resource will be picked up by Fleet and its contents will be deployed onto the target clusters that the user has specified in the previous steps. For an overview of the process, refer to <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>.</para>
<para>For information on how to track the upgrade process, you can refer to <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>.</para>
<important>
<para>Once the chart upgrade has been successfully verified, remove the <literal>Bundle/GitRepo</literal> resource.</para>
<para>This will remove the no longer necessary upgrade resources from your <literal>downstream</literal> cluster, ensuring that no future version clashes might occur.</para>
</important>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>Example</title>
<note>
<para>The example below demonstrates how to upgrade a Helm chart deployed via <literal>EIB</literal> from one version to another on a <literal>downstream</literal> cluster. Note that the versions used in this example are <emphasis role="strong">not</emphasis> recommendations. For version recommendations specific to an Edge release, refer to the release notes (<xref linkend="release-notes"/>).</para>
</note>
<para><emphasis>Use-case:</emphasis></para>
<itemizedlist>
<listitem>
<para>A cluster named <literal>doc-example</literal> is running an older version of <link xl:href="https://longhorn.io">Longhorn</link>.</para>
</listitem>
<listitem>
<para>The cluster has been deployed through EIB, using the following image definition <emphasis>snippet</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal> needs to be upgraded to a version that is compatible with the Edge 3.4 release. Meaning it needs to be upgraded to <literal>107.0.0+up1.9.1</literal>.</para>
</listitem>
<listitem>
<para>It is assumed that the <literal>management cluster</literal> in charge of managing <literal>doc-example</literal> is <emphasis role="strong">air-gapped</emphasis>, without support for a local Git server and has a working Rancher setup.</para>
</listitem>
</itemizedlist>
<para>Follow the Upgrade Steps (<xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>):</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clone the <literal>suse-edge/fleet-example</literal> repository from the <literal>release-3.4.0</literal> tag.</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Create a directory where the <literal>Longhorn</literal> upgrade archive will be stored.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Pull the desired <literal>Longhorn</literal> chart archive version:</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>Outside of the <literal>archives</literal> directory, download the <literal>generate-chart-upgrade-data.sh</literal> script from the <literal>suse-edge/fleet-examples</literal> release <link xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>.</para>
</listitem>
<listitem>
<para>Directory setup should look similar to:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>Execute the <literal>generate-chart-upgrade-data.sh</literal> script:</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>The directory structure after the script execution should look similar to:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>The files changed in git should look like this:</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>Create a <literal>Bundle</literal> for the <literal>eib-charts-upgrader</literal> Fleet:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>First, navigate to the Fleet itself:</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Then create a <literal>targets.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: doc-example
EOF</screen>
</listitem>
<listitem>
<para>Then use the <literal>fleet-cli</literal> binary to convert the Fleet to a Bundle:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
<listitem>
<para>Now, transfer the <literal>bundle.yaml</literal> on your <literal>management cluster</literal> machine.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Deploy the Bundle through the Rancher UI:</para>
<figure>
<title>Deploy Bundle through Rancher UI</title>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_1.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 1</phrase></textobject>
</mediaobject>
</figure>
<para>From here, select <emphasis role="strong">Read from File</emphasis> and find the <literal>bundle.yaml</literal> file on your system.</para>
<para>This will auto-populate the <literal>Bundle</literal> inside of Rancher’s UI.</para>
<para>Select <emphasis role="strong">Create</emphasis>.</para>
</listitem>
<listitem>
<para>After a successful deployment, your Bundle would look similar to:</para>
<figure>
<title>Successfully deployed Bundle</title>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_2.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 2</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</orderedlist>
<para>After the successful deployment of the <literal>Bundle</literal>, to monitor the upgrade process:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Verify the logs of the <literal>Upgrade Pod</literal>:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_3_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 3 downstream</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Now verify the logs of the Pod created for the upgrade by the helm-controller:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>The Pod name will be with the following template - <literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>The Pod will be in the namespace where the <literal>HelmChart</literal> resource was deployed. In our case this is <literal>kube-system</literal>.</para>
<figure>
<title>Logs for successfully upgraded Longhorn chart</title>
<mediaobject>
<imageobject>
<imagedata fileref="day2_helm_chart_upgrade_example_4_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example 4 downstream</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Verify that the <literal>HelmChart</literal> version has been updated by navigating to Rancher’s <literal>HelmCharts</literal> section (<literal>More Resources → HelmCharts</literal>). Select the namespace where the chart was deployed, for this example it would be <literal>kube-system</literal>.</para>
</listitem>
<listitem>
<para>Finally check that the Longhorn Pods are running.</para>
</listitem>
</orderedlist>
<para>After making the above validations, it is safe to assume that the Longhorn Helm chart has been upgraded to the <literal>107.0.0+up1.9.1</literal> version.</para>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>Helm chart upgrade using a third-party GitOps tool</title>
<para>There might be use-cases where users would like to use this upgrade procedure with a GitOps workflow other than Fleet (e.g. <literal>Flux</literal>).</para>
<para>To produce the resources needed for the upgrade procedure, you can use the <literal>generate-chart-upgrade-data.sh</literal> script to populate the <literal>eib-charts-upgrader</literal> Fleet with the user provided data. For more information on how to do this, see <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
<para>After you have the full setup, you can use <link xl:href="https://kustomize.io">kustomize</link> to generate a full working solution that you can deploy in your cluster:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>If you want to include the solution to your GitOps workflow, you can remove the <literal>fleet.yaml</literal> file and use what is left as a valid <literal>Kustomize</literal> setup. Just do not forget to first run the <literal>generate-chart-upgrade-data.sh</literal> script, so that it can populate the <literal>Kustomize</literal> setup with the data for the Helm charts that you wish to upgrade to.</para>
<para>To understand how this workflow is intended to be used, it can be beneficial to look at <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/> and <xref linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-suse-telco-cloud-documentation">
<title>SUSE Telco Cloud Documentation</title>
<partintro>
<para>Find the SUSE Telco Cloud documentation here</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Telco Cloud</title>
<para>SUSE Telco Cloud (formerly known as SUSE Edge for Telco) is a telecom-optimized computing platform that enables telecom operators and telecom network vendors to innovate and accelerate the modernization of their networks.</para>
<para>SUSE Telco Cloud is a complete Telco-enabled cloud native stack for hosting CNFs, covering all telecom domains: Packet Core, RAN, OSS and BSS.</para>
<itemizedlist>
<listitem>
<para>Automates zero-touch rollout and lifecycle management of complex edge stack configurations at Telco scale.</para>
</listitem>
<listitem>
<para>Continuously assures quality on Telco-grade hardware, using Telco-specific configurations and workloads.</para>
</listitem>
<listitem>
<para>Consists of components that are purpose-built for the edge and hence have smaller footprint and higher performance per Watt.</para>
</listitem>
<listitem>
<para>Maintains a flexible platform strategy with vendor-neutral APIs and 100% open source.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>Concept &amp; Architecture</title>
<para>SUSE Telco Cloud is a platform designed for hosting modern, cloud native, Telco applications at scale from core to edge.</para>
<para>This page explains the architecture and components used in SUSE Telco Cloud.</para>
<section xml:id="id-suse-telco-cloud-architecture">
<title>SUSE Telco Cloud Architecture</title>
<para>The following diagram shows the high-level architecture of SUSE Telco Cloud:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="product-atip-architecture1.png" width="100%"/>
</imageobject>
<textobject><phrase>product atip architecture1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-2">
<title>Components</title>
<para>There are two different blocks, the management stack and the runtime stack:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management stack</emphasis>: This is the part of SUSE Telco Cloud that is used to manage the provision and lifecycle of the runtime stacks. It includes the following components:</para>
<itemizedlist>
<listitem>
<para>Multi-cluster management in public and private cloud environments with Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Bare-metal support with Metal3 (<xref linkend="components-metal3"/>), MetalLB (<xref linkend="components-metallb"/>) and <literal>CAPI</literal> (Cluster API) infrastructure providers</para>
</listitem>
<listitem>
<para>Comprehensive tenant isolation and <literal>IDP</literal> (Identity Provider) integrations</para>
</listitem>
<listitem>
<para>Large marketplace of third-party integrations and extensions</para>
</listitem>
<listitem>
<para>Vendor-neutral API and rich ecosystem of providers</para>
</listitem>
<listitem>
<para>Control the SUSE Linux Micro transactional updates</para>
</listitem>
<listitem>
<para>GitOps Engine for managing the lifecycle of the clusters using Git repositories with Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Runtime stack</emphasis>: This is the part of SUSE Telco Cloud that is used to run the workloads.</para>
<itemizedlist>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>) serves as the security-hardened, lightweight Kubernetes distribution, optimized for edge and compliance-focused telecom environments.</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>) to enable security features like image vulnerability scanning, deep packet inspection and automatic intra-cluster traffic control.</para>
</listitem>
<listitem>
<para>Block Storage with SUSE Storage (<xref linkend="components-suse-storage"/>) to enable a simple and easy way to use a cloud native storage solution.</para>
</listitem>
<listitem>
<para>Optimized Operating System with SUSE Linux Micro (<xref linkend="components-slmicro"/>) to enable a secure, lightweight and immutable (transactional file system) OS for running containers. SUSE Linux Micro is available on AArch64 and AMD64/Intel 64 architectures, and it also supports <literal>Real-Time Kernel</literal> for Telco and edge use cases.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>Example deployment flows</title>
<para>The following are high-level examples of workflows to understand the relationship between the management and the runtime components.</para>
<para>Directed network provisioning is the workflow that enables the deployment of a new downstream cluster with all the components preconfigured and ready to run workloads with no manual intervention.</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>Example 1: Deploying a new management cluster with all components installed</title>
<para>Using the Edge Image Builder (<xref linkend="components-eib"/>) to create a new <literal>ISO</literal> image with the management stack included. You can then use this <literal>ISO</literal> image to install a new management cluster on VMs or bare-metal.</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="product-atip-architecture2.png" width="100%"/>
</imageobject>
<textobject><phrase>product atip architecture2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>For more information about how to deploy a new management cluster, see the SUSE Telco Cloud Management Cluster guide (<xref linkend="atip-management-cluster"/>).</para>
</note>
<note>
<para>For more information about how to use the Edge Image Builder, see the Edge Image Builder guide (<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>Example 2: Deploying a single-node downstream cluster with Telco profiles to enable it to run Telco workloads</title>
<para>Once we have the management cluster up and running, we can use it to deploy a single-node downstream cluster with all Telco capabilities enabled and configured using the directed network provisioning workflow.</para>
<para>The following diagram shows the high-level workflow to deploy it:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="product-atip-architecture3.png" width="100%"/>
</imageobject>
<textobject><phrase>product atip architecture3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>For more information about how to deploy a downstream cluster, see the SUSE Telco Cloud Automated Provisioning guide. (<xref linkend="atip-automated-provisioning"/>)</para>
</note>
<note>
<para>For more information about Telco features, see the SUSE Telco Cloud Telco Features guide. (<xref linkend="atip-features"/>)</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>Example 3: Deploying a high availability downstream cluster using MetalLB as a Load Balancer</title>
<para>Once we have the management cluster up and running, we can use it to deploy a high availability downstream cluster with <literal>MetalLB</literal> as a load balancer using the directed network provisioning workflow.</para>
<para>The following diagram shows the high-level workflow to deploy it:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="product-atip-architecture4.png" width="100%"/>
</imageobject>
<textobject><phrase>product atip architecture4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>For more information about how to deploy a downstream cluster, see the SUSE Telco Cloud Automated Provisioning guide. (<xref linkend="atip-automated-provisioning"/>)</para>
</note>
<note>
<para>For more information about <literal>MetalLB</literal>, see here: (<xref linkend="components-metallb"/>)</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>Requirements &amp; Assumptions</title>
<section xml:id="id-hardware">
<title>Hardware</title>
<para>The hardware requirements for SUSE Telco Cloud are as follows:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management cluster</emphasis>: The management cluster contains components like <literal>SUSE Linux Micro</literal>, <literal>RKE2</literal>, <literal>SUSE Rancher Prime</literal>, <literal>Metal<superscript>3</superscript></literal>, and it is used to manage several downstream clusters. Depending on the number of downstream clusters to be managed, the hardware requirements for the server could vary.</para>
<itemizedlist>
<listitem>
<para>Minimum requirements for the server (<literal>VM</literal> or <literal>bare-metal</literal>) are:</para>
<itemizedlist>
<listitem>
<para>RAM: 8 GB Minimum (we recommend at least 16 GB)</para>
</listitem>
<listitem>
<para>CPU: 2 Minimum (we recommend at least 4 CPU)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Downstream clusters</emphasis>: The downstream clusters are the clusters deployed to run Telco workloads. Specific requirements are needed to enable certain Telco capabilities like <literal>SR-IOV</literal>, <literal>CPU Performance Optimization</literal>, etc.</para>
<itemizedlist>
<listitem>
<para>SR-IOV: to attach VFs (Virtual Functions) in pass-through mode to CNFs/VNFs, the NIC must support SR-IOV and VT-d/AMD-Vi be enabled in the BIOS.</para>
</listitem>
<listitem>
<para>CPU Processors: To run specific Telco workloads, the CPU Processor model should be adapted to enable most of the features available in this reference table (<xref linkend="atip-features"/>).</para>
</listitem>
<listitem>
<para>Firmware requirements for installing with virtual media:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Server Hardware</entry>
<entry align="left" valign="top">BMC Model</entry>
<entry align="left" valign="top">Management</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>Dell hardware</para></entry>
<entry align="left" valign="top"><para>15th Generation</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Supermicro hardware</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>HPE hardware</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-network">
<title>Network</title>
<para>As a reference for the network architecture, the following diagram shows a typical network architecture for a Telco environment:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="product-atip-requirements1.svg" width="100%"/>
</imageobject>
<textobject><phrase>product atip requirements1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>The network architecture is based on the following components:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management network</emphasis>: This network is used for the management of downstream cluster nodes. It is used for the out-of-band management. Usually, this network is also connected to a separate management switch, but it can be connected to the same service switch using VLANs to isolate the traffic.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Control-plane network</emphasis>: This network is used for the communication between the downstream cluster nodes and the services that are running on them. This network is also used for the communication between the nodes and the external services, like the <literal>DHCP</literal> or <literal>DNS</literal> servers. In some cases, for connected environments, the switch/router can handle traffic through the Internet.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Other networks</emphasis>: In some cases, nodes could be connected to other networks for specific purposes.</para>
</listitem>
</itemizedlist>
<note>
<para>To use the directed network provisioning workflow, the management cluster must have network connectivity to the downstream cluster server Baseboard Management Controller (BMC) so that host preparation and provisioning can be automated.</para>
</note>
</section>
<section xml:id="id-port-requirements">
<title>Port requirements</title>
<para>To operate properly, a SUSE Telco Cloud deployment requires a number of ports to be reachable on the management and the downstream Kubernetes cluster nodes.</para>
<note>
<para>The exact list depends on the deployed optional components and the selected deployment options (e.g., CNI plug-in).</para>
</note>
<section xml:id="id-management-nodes">
<title>Management Nodes</title>
<para>The following table lists the opened ports in nodes running the management cluster:</para>
<note>
<para>For CNI plug-in related ports, see CNI specific port requirements (<xref linkend="cni-specific-port-requirements"/>).</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-nodes" frame="all" rowsep="1" colsep="1">
<title>Inbound Network Rules for Management Nodes</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Source</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>Any source that requires SSH access</para></entry>
<entry align="left" valign="top"><para>SSH access to management cluster nodes</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Load balancer/proxy that does external TLS termination</para></entry>
<entry align="left" valign="top"><para>Rancher UI/API when external TLS termination is used</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Any source that requires TLS access to Rancher UI/API</para></entry>
<entry align="left" valign="top"><para>Rancher agent, Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>RKE2 (management cluster) server nodes</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> client port</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>RKE2 (management cluster) server nodes</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> peer port</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6180</para></entry>
<entry align="left" valign="top"><para>Any BMC<superscript>(1)</superscript> previously instructed by <literal>Metal<superscript>3</superscript>/ironic</literal> to pull an IPA<superscript>(2)</superscript> ramdisk image from this exposed port (non-TLS)</para></entry>
<entry align="left" valign="top"><para><literal>Ironic</literal> httpd non-TLS web server serving IPA<superscript>(2)</superscript> ISO images for virtual media based boot <?asciidoc-br?>
<?asciidoc-br?>
 In case this port is enabled, the functionally equivalent but TLS-enabled one (see below) is not opened</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6185</para></entry>
<entry align="left" valign="top"><para>Any BMC<superscript>(1)</superscript> previously instructed by <literal>Metal<superscript>3</superscript>/ironic</literal> to pull an IPA<superscript>(2)</superscript> ramdisk image from this exposed port (TLS)</para></entry>
<entry align="left" valign="top"><para><literal>Ironic</literal> httpd TLS-enabled web server serving IPA<superscript>(2)</superscript> ISO images for virtual media based boot<?asciidoc-br?>
<?asciidoc-br?>
 In case this port is enabled, the functionally equivalent but TLS-disabled one (see above) is not opened</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6385</para></entry>
<entry align="left" valign="top"><para>Any <literal>Metal<superscript>3</superscript>/ironic</literal> IPA<superscript>(1)</superscript> ramdisk image deployed &amp; running in an "enrolled" <literal>BareMetalHost</literal> instance</para></entry>
<entry align="left" valign="top"><para>Ironic API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>Any management cluster node; any external (to the management cluster) Kubernetes client</para></entry>
<entry align="left" valign="top"><para>Kubernetes API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6545</para></entry>
<entry align="left" valign="top"><para>Any management cluster node</para></entry>
<entry align="left" valign="top"><para>Pull artifacts from OCI-compliant registry (Hauler)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>RKE2 server and agent nodes (management cluster)</para></entry>
<entry align="left" valign="top"><para>RKE2 supervisor API for Node registration (opened port in all RKE2 server nodes)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>Any management cluster node</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal> metrics</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000-32767</para></entry>
<entry align="left" valign="top"><para>Any external (to the management cluster) source accessing a service exposed on the primary network through a <literal>spec.type: NodePort</literal> or <literal>spec.type: LoadBalancer</literal> <link xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service API object</link></para></entry>
<entry align="left" valign="top"><para>Available <literal>NodePort</literal> port range</para></entry>
</row>
</tbody>
</tgroup>
</table>
<para><superscript>(1)</superscript> BMC: Baseboard Management Controller<?asciidoc-br?>
<superscript>(2)</superscript> IPA: Ironic Python Agent</para>
</section>
<section xml:id="id-downstream-nodes">
<title>Downstream Nodes</title>
<para>In SUSE Telco Cloud, before any (downstream) server becomes part of a running downstream Kubernetes cluster (or runs itself a single-node downstream Kubernetes cluster), it is required to go through some of the <link xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/baremetalhost-states.md">BaremetalHost Provisioning states</link>.</para>
<itemizedlist>
<listitem>
<para>The Baseboard Management Controller (BMC) for a just declared downstream server must be accessible through the out-of-band network. BMC is instructed (from the ironic service running on the management cluster) on the initial steps to take:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pull and load the indicated IPA ramdisk image in the BMC offered <literal>virtual media</literal>.</para>
</listitem>
<listitem>
<para>Power-on the server.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>Following ports are expected to be exposed from the BMC (they could differ depending on the exact hardware):</para>
<table xml:id="table-inbound-network-rules-for-baseboard-management-controllers" frame="all" rowsep="1" colsep="1">
<title>Inbound Network Rules for Baseboard Management Controllers</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Source</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Ironic conductor (from management cluster)</para></entry>
<entry align="left" valign="top"><para>Redfish API access (HTTP)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Ironic conductor (from management cluster)</para></entry>
<entry align="left" valign="top"><para>Redfish API access (HTTPS)</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>Once the IPA ramdisk image loaded on the BMC <literal>virtual media</literal> is used to bootup the downstream server image, the hardware inspection phase begins. The following table lists the ports exposed by a running IPA ramdisk image:</para>
</listitem>
</itemizedlist>
<table xml:id="table-inbound-network-rules-for-downstream-nodes-provisioning-phase" frame="all" rowsep="1" colsep="1">
<title>Inbound Network Rules for Downstream Nodes - <literal>Metal<superscript>3</superscript>/Ironic</literal> Provisioning phase</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Source</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>Any source that requires SSH access to IPA ramdisk image</para></entry>
<entry align="left" valign="top"><para>SSH access to a being inspected downstream cluster node</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9999</para></entry>
<entry align="left" valign="top"><para>Ironic conductor (from management cluster)</para></entry>
<entry align="left" valign="top"><para>Ironic commands towards the running ramdisk image</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>Once the baremetal host is properly provisioned and has joined a downstream Kubernetes cluster, it exposes the following ports:</para>
</listitem>
</itemizedlist>
<note>
<para>For CNI plug-in related ports, see CNI specific port requirements (<xref linkend="cni-specific-port-requirements"/>).</para>
</note>
<table xml:id="table-inbound-network-rules-for-downstream-nodes" frame="all" rowsep="1" colsep="1">
<title>Inbound Network Rules for Downstream Nodes</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Source</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>Any source that requires SSH access</para></entry>
<entry align="left" valign="top"><para>SSH access to downstream cluster nodes</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Load balancer/proxy that does external TLS termination</para></entry>
<entry align="left" valign="top"><para>Rancher UI/API when external TLS termination is used</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Any source that requires TLS access to Rancher UI/API</para></entry>
<entry align="left" valign="top"><para>Rancher agent, Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>RKE2 (downstream cluster) server nodes</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> client port</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>RKE2 (downstream cluster) server nodes</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> peer port</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>Any downstream cluster node; any external (to the downstream cluster) Kubernetes client.</para></entry>
<entry align="left" valign="top"><para>Kubernetes API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>RKE2 server and agent nodes (downstream cluster)</para></entry>
<entry align="left" valign="top"><para>RKE2 supervisor API for Node registration (opened port in all RKE2 server nodes)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>Any downstream cluster node</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal> metrics</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10255</para></entry>
<entry align="left" valign="top"><para>Any downstream cluster node</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal> read-only access</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000-32767</para></entry>
<entry align="left" valign="top"><para>Any external (to the downstream cluster) source accessing a service exposed on the primary network through a <literal>spec.type: NodePort</literal> or <literal>spec.type: LoadBalancer</literal> <link xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service API object</link></para></entry>
<entry align="left" valign="top"><para>Available <literal>NodePort</literal> port range</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="cni-specific-port-requirements">
<title>CNI specific port requirements</title>
<para>Each supported CNI variant comes with its own set of port requirements. For more details, refer <link xl:href="https://docs.rke2.io/install/requirements#cni-specific-inbound-network-rules">CNI Specific Inbound Network Rules</link> in RKE2 documentation.</para>
<para>When <literal>cilium</literal> is set as default/primary CNI plug-in, following TCP port is additionally exposed when the <literal>cilium-operator</literal> workload is configured to expose metrics outside the Kubernetes cluster on which it is deployed. This ensures that an external <literal>Prometheus</literal> server instance running outside that Kubernetes cluster can still collect these metrics.</para>
<note>
<para>This is the default option when deploying <literal>cilium</literal> via the rke2-cilium Helm chart.</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-downstream-nodes-external-metrics-cilium-operator" frame="all" rowsep="1" colsep="1">
<title>Inbound Network Rules for Management/Downstream Nodes - external metrics exposure from <literal>cilium-operator</literal> enabled</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Source</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9963</para></entry>
<entry align="left" valign="top"><para>External (to the Kubernetes cluster) metrics collector</para></entry>
<entry align="left" valign="top"><para>cilium-operator metrics exposure</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>Services (DHCP, DNS, etc.)</title>
<para>Some external services like <literal>DHCP</literal>, <literal>DNS</literal>, etc. could be required depending on the kind of environment where they are deployed:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Connected environment</emphasis>: In this case, the nodes will be connected to the Internet (via routing L3 protocols) and the external services will be provided by the customer.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Disconnected / air-gap environment</emphasis>: In this case, the nodes will not have Internet IP connectivity and additional services will be required to locally mirror content required by the directed network provisioning workflow.</para>
</listitem>
<listitem>
<para><emphasis role="strong">File server</emphasis>: A file server is used to store the OS images to be provisioned on the downstream cluster nodes during the directed network provisioning workflow. The <literal>Metal<superscript>3</superscript></literal> Helm chart can deploy a media server to store the OS images — check the following section (<xref linkend="metal3-media-server"/>), but it is also possible to use an existing local webserver.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-systemd-services">
<title>Disabling systemd services</title>
<para>For Telco workloads, it is important to disable or configure properly some of the services running on the nodes to avoid any impact on the workload performance running on the nodes (latency).</para>
<itemizedlist>
<listitem>
<para><literal>rebootmgr</literal> is a service which allows to configure a strategy for reboot when the system has pending updates.
For Telco workloads, it is really important to disable or configure properly the <literal>rebootmgr</literal> service to avoid the reboot of the nodes in case of updates scheduled by the system, to avoid any impact on the services running on the nodes.</para>
</listitem>
</itemizedlist>
<note>
<para>For more information about <literal>rebootmgr</literal>, see <link xl:href="https://github.com/SUSE/rebootmgr">rebootmgr GitHub repository</link>.</para>
</note>
<para>Verify the strategy being used by running:</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>and you could disable it by running:</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>or using the <literal>rebootmgrctl</literal> command:</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para>This configuration to set the <literal>rebootmgr</literal> strategy can be automated using the directed network provisioning workflow. For more information, check the Automated Provisioning documentation (<xref linkend="atip-automated-provisioning"/>).</para>
</note>
<itemizedlist>
<listitem>
<para><literal>transactional-update</literal> is a service that allows automatic updates controlled by the system. For Telco workloads, it is important to disable the automatic updates to avoid any impact on the services running on the nodes.</para>
</listitem>
</itemizedlist>
<para>To disable the automatic updates, you can run:</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable transactional-update.timer
systemctl --now disable transactional-update-cleanup.timer</screen>
<itemizedlist>
<listitem>
<para><literal>fstrim</literal> is a service that allows to trim the filesystems automatically every week. For Telco workloads, it is important to disable the automatic trim to avoid any impact on the services running on the nodes.</para>
</listitem>
</itemizedlist>
<para>To disable the automatic trim, you can run:</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable fstrim.timer</screen>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>Setting up the management cluster</title>
<section xml:id="id-introduction-2">
<title>Introduction</title>
<para>The management cluster is the part of SUSE Telco Cloud that is used to manage the provision and lifecycle of the runtime stacks.
From a technical point of view, the management cluster contains the following components:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro</literal> as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> as the Kubernetes cluster. Depending on the use case, it can be configured to use specific CNI plugins, such as <literal>Multus</literal>, <literal>Cilium</literal>, <literal>Calico</literal>, etc.</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> as the management platform to manage the lifecycle of the clusters.</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal> as the component to manage the lifecycle of the bare-metal nodes.</para>
</listitem>
<listitem>
<para><literal>CAPI</literal> as the component to manage the lifecycle of the Kubernetes clusters (downstream clusters). The <literal>RKE2 CAPI Provider</literal> is used to manage the lifecycle of the RKE2 clusters.</para>
</listitem>
</itemizedlist>
<para>With all components mentioned above, the management cluster can manage the lifecycle of downstream clusters, using a declarative approach to manage the infrastructure and applications.</para>
<note>
<para>For more information about <literal>SUSE Linux Micro</literal>, see: SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
<para>For more information about <literal>RKE2</literal>, see: RKE2 (<xref linkend="components-rke2"/>)</para>
<para>For more information about <literal>Rancher</literal>, see: Rancher (<xref linkend="components-rancher"/>)</para>
<para>For more information about <literal>Metal<superscript>3</superscript></literal>, see: Metal3 (<xref linkend="components-metal3"/>)</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>Steps to set up the management cluster</title>
<para>The following steps are necessary to set up the management cluster (using a single node):</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="product-atip-mgmtcluster1.png" width="100%"/>
</imageobject>
<textobject><phrase>product atip mgmtcluster1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>The following are the main steps to set up the management cluster using a declarative approach:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Image preparation for connected environments (<xref linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>: The first step is to prepare the manifests and files with all the necessary configurations to be used in connected environments.</para>
<itemizedlist>
<listitem>
<para>Directory structure for connected environments (<xref linkend="mgmt-cluster-directory-structure"/>): This step creates a directory structure to be used by Edge Image Builder to store the configuration files and the image itself.</para>
</listitem>
<listitem>
<para>Management cluster definition file (<xref linkend="mgmt-cluster-image-definition-file"/>): The <literal>mgmt-cluster.yaml</literal> file is the main definition file for the management cluster. It contains the following information about the image to be created:</para>
<itemizedlist>
<listitem>
<para>Image Information: The information related to the image to be created using the base image.</para>
</listitem>
<listitem>
<para>Operating system: The operating system configurations to be used in the image.</para>
</listitem>
<listitem>
<para>Kubernetes: Helm charts and repositories, kubernetes version, network configuration, and the nodes to be used in the cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Custom folder (<xref linkend="mgmt-cluster-custom-folder"/>): The <literal>custom</literal> folder contains the configuration files and scripts to be used by Edge Image Builder to deploy a fully functional management cluster.</para>
<itemizedlist>
<listitem>
<para>Files: Contains the configuration files to be used by the management cluster.</para>
</listitem>
<listitem>
<para>Scripts: Contains the scripts to be used by the management cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Kubernetes folder (<xref linkend="mgmt-cluster-kubernetes-folder"/>): The <literal>kubernetes</literal> folder contains the configuration files to be used by the management cluster.</para>
<itemizedlist>
<listitem>
<para>Manifests: Contains the manifests to be used by the management cluster.</para>
</listitem>
<listitem>
<para>Helm: Contains the Helm values files to be used by the management cluster.</para>
</listitem>
<listitem>
<para>Config: Contains the configuration files to be used by the management cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Network folder (<xref linkend="mgmt-cluster-network-folder"/>): The <literal>network</literal> folder contains the network configuration files to be used by the management cluster nodes.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Image preparation for air-gap environments (<xref linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>: The step is to show the differences to prepare the manifests and files to be used in an air-gap scenario.</para>
<itemizedlist>
<listitem>
<para>Modifications in the definition file (<xref linkend="mgmt-cluster-image-definition-file-airgap"/>): The <literal>mgmt-cluster.yaml</literal> file must be modified to include the <literal>embeddedArtifactRegistry</literal> section with the <literal>images</literal> field set to all container images to be included into the EIB output image.</para>
</listitem>
<listitem>
<para>Modifications in the custom folder (<xref linkend="mgmt-cluster-custom-folder-airgap"/>): The <literal>custom</literal> folder must be modified to include the resources needed to run the management cluster in an air-gap environment.</para>
<itemizedlist>
<listitem>
<para>Register script: The <literal>custom/scripts/99-register.sh</literal> script must be removed when you use an air-gap environment.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Modifications in the helm values folder (<xref linkend="mgmt-cluster-helm-values-folder-airgap"/>): The <literal>helm/values</literal> folder must be modified to include the configuration needed to run the management cluster in an air-gap environment.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Image creation (<xref linkend="mgmt-cluster-image-creation"/>)</emphasis>: This step covers the creation of the image using the Edge Image Builder tool (for both, connected and air-gap scenarios). Check the prerequisites (<xref linkend="components-eib"/>) to run the Edge Image Builder tool on your system.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Management Cluster Provision (<xref linkend="mgmt-cluster-provision"/>)</emphasis>: This step covers the provisioning of the management cluster using the image created in the previous step (for both, connected and air-gap scenarios). This step can be done using a laptop, server, VM or any other AMD64/Intel 64 system with a USB port.</para>
</listitem>
</orderedlist>
<note>
<para>For more information about Edge Image Builder, see Edge Image Builder (<xref linkend="components-eib"/>) and Edge Image Builder Quick Start (<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>Image preparation for connected environments</title>
<para>Edge Image Builder is used to create the image for the management cluster, in this document we cover the minimal configuration necessary to set up the management cluster.</para>
<para>Edge Image Builder runs inside a container, so a container runtime is required such as <link xl:href="https://podman.io">Podman</link> or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link>. For this guide, we assume podman is available.</para>
<para>Also, as a prerequisite to deploy a highly available management cluster, you need to reserve three IPs in your network:</para>
<itemizedlist>
<listitem>
<para><literal>apiVIP</literal> for the API VIP Address (used to access the Kubernetes API server).</para>
</listitem>
<listitem>
<para><literal>ingressVIP</literal> for the Ingress VIP Address (consumed, for example, by the Rancher UI).</para>
</listitem>
<listitem>
<para><literal>metal3VIP</literal> for the Metal3 VIP Address.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure">
<title>Directory structure</title>
<para>When running EIB, a directory is mounted from the host, so the first thing to do is to create a directory structure to be used by EIB to store the configuration files and the image itself.
This directory has the following structure:</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── os-files
│ └── var
│   └── lib
│     └── rancher
│       └── rke2
│         └── server
│           └── manifests
│             └── rke2-ingress-config.yaml
├── kubernetes
│ ├── manifests
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── longhorn.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>The image <literal>SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso</literal> must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE Customer Center</link> or the <link xl:href="https://www.suse.com/download/sle-micro/">SUSE Download page</link>, and it must be located under the <literal>base-images</literal> folder.</para>
<para>You should check the SHA256 checksum of the image to ensure it has not been tampered with. The checksum can be found in the same location where the image was downloaded.</para>
<para>An example of the directory structure can be found in the <link xl:href="https://github.com/suse-edge/atip">SUSE Edge GitHub repository under the "telco-examples" folder</link>.</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>Management cluster definition file</title>
<para>The <literal>mgmt-cluster.yaml</literal> file is the main definition file for the management cluster. It contains the following information:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    - open-iscsi
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: $API_HOST
    apiVIP: $API_VIP
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>To explain the fields and values in the <literal>mgmt-cluster.yaml</literal> definition file, we have divided it into the following sections.</para>
<itemizedlist>
<listitem>
<para>Image section (definition file):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>where the <literal>baseImage</literal> is the original image you downloaded from the SUSE Customer Center or the SUSE Download page. <literal>outputImageName</literal> is the name of the new image that will be used to provision the management cluster.</para>
<itemizedlist>
<listitem>
<para>Operating system section (definition file):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>where the <literal>installDevice</literal> is the device to be used to install the operating system, the <literal>username</literal> and <literal>encryptedPassword</literal> are the credentials to be used to access the system, the <literal>packageList</literal> is the list of packages to be installed (<literal>jq</literal> is required internally during the installation process), and the <literal>sccRegistrationCode</literal> is the registration code used to get the packages and dependencies at build time and can be obtained from the SUSE Customer Center.
The encrypted password can be generated using the <literal>openssl</literal> command as follows:</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>This outputs something similar to:</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Kubernetes section (definition file):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>The <literal>helm</literal> section contains the list of Helm charts to be installed, the repositories to be used, and the version configuration for all of them.</para>
<para>The <literal>network</literal> section contains the configuration for the network, like the <literal>apiHost</literal> and <literal>apiVIP</literal> to be used by the <literal>RKE2</literal> component.
The <literal>apiVIP</literal> should be an IP address that is not used in the network and should not be part of the DHCP pool (in case we use DHCP). Also, when we use the <literal>apiVIP</literal> in a multi-node cluster, it is used to access the Kubernetes API server.
The <literal>apiHost</literal> is the name resolution to <literal>apiVIP</literal> to be used by the <literal>RKE2</literal> component.</para>
<para>The <literal>nodes</literal> section contains the list of nodes to be used in the cluster. In this example, a single-node cluster is being used, but it can be extended to a multi-node cluster by adding more nodes to the list (by uncommenting the lines).</para>
<note>
<itemizedlist>
<listitem>
<para>The names of the nodes must be unique in the cluster.</para>
</listitem>
<listitem>
<para>Optionally, use the <literal>initializer</literal> field to specify the bootstrap host, otherwise it will be the first node in the list.</para>
</listitem>
<listitem>
<para>The names of the nodes must be the same as the host names defined in the Network Folder (<xref linkend="mgmt-cluster-network-folder"/>) when network configuration is required.</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Custom folder</title>
<para>The <literal>custom</literal> folder contains the following subfolders:</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para>The <literal>custom/files</literal> folder contains the configuration files to be used by the management cluster.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts</literal> folder contains the scripts to be used by the management cluster.</para>
</listitem>
</itemizedlist>
<para>The <literal>custom/files</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>: contains configuration parameters for <literal>Metal<superscript>3</superscript></literal>, <literal>Rancher</literal> and <literal>MetalLB</literal>. Only modify this file if you want to change the namespaces to be used.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>: contains the configuration for the <literal>Metal<superscript>3</superscript></literal> component to be used (no modifications needed). In future versions, this script will be replaced to use instead <literal>Rancher Turtles</literal> to make it easy.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
        EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
        EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
        apiVersion: management.cattle.io/v3
        kind: Feature
        metadata:
          name: embedded-cluster-api
        spec:
          value: false
        EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>: contains the configuration for the <literal>Rancher</literal> component to be used (no modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>: contains the configuration to create the systemd service to run the scripts during the first boot (no modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single one
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>The <literal>custom/scripts</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>99-alias.sh</literal> script: contains the alias to be used by the management cluster to load the kubeconfig file at first boot (no modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para><literal>99-mgmt-setup.sh</literal> script: contains the configuration to copy the scripts during the first boot (no modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
        cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para><literal>99-register.sh</literal> script: contains the configuration to register the system using the SCC registration code. The <literal>${SCC_ACCOUNT_EMAIL}</literal> and <literal>${SCC_REGISTRATION_CODE}</literal> have to be set properly to register the system with your account.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
        zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Kubernetes folder</title>
<para>The <literal>kubernetes</literal> folder contains the following subfolders:</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para>The <literal>kubernetes/config</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>: By default, the <literal>CNI</literal> plug-in installed by default is <literal>Cilium</literal>, so you do not need to create this folder and file. Just in case you need to customize the <literal>CNI</literal> plug-in, you can use the <literal>server.yaml</literal> file under the <literal>kubernetes/config</literal> folder. It contains the following information:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
system-default-registry: registry.rancher.com</screen>
</listitem>
</itemizedlist>
<note>
<para>This is an optional file to define certain Kubernetes customization, like the CNI plug-ins to be used or many options you can check in the <link xl:href="https://docs.rke2.io/install/configuration">official documentation</link>.</para>
</note>
<para>The <literal>os-files/var/lib/rancher/rke2/server/manifests</literal> folder contains the following file:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>: contains the configuration to create the <literal>Ingress</literal> service for the management cluster (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
</itemizedlist>
<note>
<para>The <literal>HelmChartConfig</literal> must be included via <literal>os-files</literal> to the <literal>/var/lib/rancher/rke2/server/manifests</literal> directory, not via <literal>kubernetes/manifests</literal> as described in previous releases.</para>
</note>
<para>The <literal>kubernetes/manifests</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>: contains the configuration to create the <literal>NeuVector</literal> namespace (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>: contains the configuration to create the <literal>L2Advertisement</literal> for the <literal>MetalLB</literal> component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>: contains the configuration to create the <literal>IPAddressPool</literal> for the <literal>rke2-ingress-nginx</literal> component. The <literal>${INGRESS_VIP}</literal> has to be set properly to define the IP address reserved to be used by the <literal>rke2-ingress-nginx</literal> component.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para>The <literal>kubernetes/helm/values</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>: contains the configuration to create the <literal>Rancher</literal> component. The <literal>${INGRESS_VIP}</literal> must be set properly to define the IP address to be consumed by the <literal>Rancher</literal> component. The URL to access the <literal>Rancher</literal> component will be <literal>https://rancher-${INGRESS_VIP}.sslip.io</literal>.</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>: contains the configuration to create the <literal>NeuVector</literal> component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
registry: "registry.rancher.com"
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>longhorn.yaml</literal>: contains the configuration to create the <literal>Longhorn</literal> component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>: contains the configuration to create the <literal>Metal<superscript>3</superscript></literal> component. The <literal>${METAL3_VIP}</literal> must be set properly to define the IP address to be consumed by the <literal>Metal<superscript>3</superscript></literal> component.</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>The Media Server is an optional feature included in Metal<superscript>3</superscript> (by default is disabled). To use the Metal3 feature, you need to configure it on the previous manifest.
To use the Metal<superscript>3</superscript> media server, specify the following variable:</para>
<itemizedlist>
<listitem>
<para>add the <literal>enable_metal3_media_server</literal> to <literal>true</literal> to enable the media server feature in the global section.</para>
</listitem>
<listitem>
<para>include the following configuration about the media server where ${MEDIA_VOLUME_PATH} is the path to the media volume in the media (e.g <literal>/home/metal3/bmh-image-cache</literal>)</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>An external media server can be used to store the images, and in the case you want to use it with TLS, you will need to modify the following configurations:</para>
<itemizedlist>
<listitem>
<para>set to <literal>true</literal> the <literal>additionalTrustedCAs</literal> in the previous <literal>metal3.yaml</literal> file to enable the additional trusted CAs from the external media server.</para>
</listitem>
<listitem>
<para>include the following secret configuration in the folder <literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal> to store the CA certificate of the external media server.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para>The <literal>additional_ca_cert</literal> is the base64-encoded CA certificate of the external media server. You can use the following command to encode the certificate and generate the secret doing manually:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>: contains the configuration to create the <literal>Cert-Manager</literal> component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: true</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>Networking folder</title>
<para>The <literal>network</literal> folder contains as many files as nodes in the management cluster. In our case, we have only one node, so we have only one file called <literal>mgmt-cluster-node1.yaml</literal>.
The name of the file must match the host name defined in the <literal>mgmt-cluster.yaml</literal> definition file into the network/node section described above.</para>
<para>If you need to customize the networking configuration, for example, to use a specific static IP address (DHCP-less scenario), you can use the <literal>mgmt-cluster-node1.yaml</literal> file under the <literal>network</literal> folder. It contains the following information:</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: The gateway IP address.</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: The DNS server IP address.</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: The MAC address of the network interface.</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: The IP address of the management cluster.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>If you want to use DHCP to get the IP address, you can use the following configuration (the <literal>MAC</literal> address must be set properly using the <literal>${MGMT_MAC}</literal> variable):</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>Depending on the number of nodes in the management cluster, you can create more files like <literal>mgmt-cluster-node2.yaml</literal>, <literal>mgmt-cluster-node3.yaml</literal>, etc. to configure the rest of the nodes.</para>
</listitem>
<listitem>
<para>The <literal>routes</literal> section is used to define the routing table for the management cluster.</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>Image preparation for air-gap environments</title>
<para>This section describes how to prepare the image for air-gap environments showing only the differences from the previous sections. The following changes to the previous section (Image preparation for connected environments (<xref linkend="mgmt-cluster-image-preparation-connected"/>)) are required to prepare the image for air-gap environments:</para>
<itemizedlist>
<listitem>
<para>The <literal>mgmt-cluster.yaml</literal> file must be modified to include the <literal>embeddedArtifactRegistry</literal> section with the <literal>images</literal> field set to all container images to be included into the EIB output image.</para>
</listitem>
<listitem>
<para>The <literal>mgmt-cluster.yaml</literal> file must be modified to include <literal>rancher-turtles-airgap-resources</literal> helm chart.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts/99-register.sh</literal> script must be removed when use an air-gap environment.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>Modifications in the definition file</title>
<para>The <literal>mgmt-cluster.yaml</literal> file must be modified to include the <literal>embeddedArtifactRegistry</literal> section.
In this section the <literal>images</literal> field must contain the list of all container images to be included in the output image.</para>
<note>
<para>The following is an example of the <literal>mgmt-cluster.yaml</literal> file with the <literal>embeddedArtifactRegistry</literal> section included.
Make sure to the listed images contain the component versions you need.</para>
</note>
<para>The <literal>rancher-turtles-airgap-resources</literal> helm chart must also be added, this creates resources as described in the <link xl:href="https://documentation.suse.com/cloudnative/cluster-api/v0.19/en/getting-started/air-gapped-environment.html">Rancher Turtles Airgap Documentation</link>.  This also requires a turtles.yaml values file for the rancher-turtles chart to specify the necessary configuration.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/mirrored-cilium-cilium:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-cilium-operator-generic:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0
    - name: registry.rancher.com/rancher/neuvector-compliance-config:1.0.6
    - name: registry.rancher.com/rancher/neuvector-controller:5.4.5
    - name: registry.rancher.com/rancher/neuvector-enforcer:5.4.5
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.suse.com/rancher/cluster-api-addon-provider-fleet:v0.11.0
    - name: registry.rancher.com/rancher/cluster-api-operator:v0.18.1
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.7-build20250425
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher/turtles:v0.24.0
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/shell:v0.4.1
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.suse.com/rancher/cluster-api-controller:v1.10.5
    - name: registry.suse.com/rancher/cluster-api-provider-metal3:v1.10.2
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1
    - name: registry.suse.com/rancher/hardened-sriov-network-operator:v1.5.0-build20250425
    - name: registry.suse.com/rancher/ip-address-manager:v1.10.2
    - name: registry.rancher.com/rancher/kubectl:v1.32.2
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.9.5</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>Modifications in the custom folder</title>
<itemizedlist>
<listitem>
<para>The <literal>custom/scripts/99-register.sh</literal> script must be removed when using an air-gap environment. As you can see in the directory structure, the <literal>99-register.sh</literal> script is not included in the <literal>custom/scripts</literal> folder.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-helm-values-folder-airgap">
<title>Modifications in the helm values folder</title>
<itemizedlist>
<listitem>
<para>The <literal>turtles.yaml</literal>: contains the configuration required to specify airgapped operation for Rancher Turtles, note this depends on installation of the rancher-turtles-airgap-resources chart.</para>
<screen language="yaml" linenumbering="unnumbered">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"</screen>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>Image creation</title>
<para>Once the directory structure is prepared following the previous sections (for both, connected and air-gap scenarios), run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>This creates the ISO output image file that, in our case, based on the image definition described above, is <literal>eib-mgmt-cluster-image.iso</literal>.</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>Provision the management cluster</title>
<para>The previous image contains all components explained above, and it can be used to provision the management cluster using a virtual machine or a bare-metal server (using the virtual-media feature).</para>
</section>
<section xml:id="mgmt-cluster-dualstack">
<title>Dual-stack considerations and configuration</title>
<para>The examples shown in the previous sections provide guidance and examples on how to set up a single-stack IPv4 management cluster. Such a management cluster is independent of the operational status of downstream clusters, which can be individually configured to operate in either IPv4/IPv6 single-stack or dual-stack configuration, once deployed. However, they way the management cluster is configured has a direct impact on the communication protocols that can be used during the provisioning phase, where both the in-band and out-of-band communications must happen according to which protocols are supported by the management cluster and downstream host. In case some or all of the BMCs and/or downstream cluster nodes are expected to use IPv6, a dual-stack setup for the management cluster is then required.</para>
<note>
<para>Single-stack IPv6 management clusters are not yet supported.</para>
</note>
<para>In order to achieve dual-stack functionality, Kubernetes must be provided with both IPv4 and IPv6 CIDRs for PODs and Services. However, other components also require specific tuning before building the management cluster image with EIB. The Metal<superscript>3</superscript> provisioning services (Ironic) can be configured in different ways, depending on your infrastructure or requirements:</para>
<itemizedlist>
<listitem>
<para>The Ironic services can be configured to listen on all the interfaces on the system rather than a single IP address, thus, as long as the management cluster host(s) has both IPv4 and IPv6 addresses assigned to the relevant interface, any of them can potentially be used during the provisioning. Note that at this time only one of these addresses can be selected for the URL generation (for consumption by other services, e.g. the Baremetal Operator, BMCs, etc.); as a consequence, to enable IPv6 communications with the BMCs, the Baremetal Operator can be instructed to expose and pass on an IPv6 URL when dealing BMH definitions including an IPv6 address. In other words, when a BMC is identified as IPv6 capable, the provisioning will be performed via IPv6 only, and via IPv4 in all the other cases.</para>
</listitem>
<listitem>
<para>A single hostname, resolving to both IPv4 and IPv6, can be used by Metal<superscript>3</superscript> to let Ironic use those addresses for binding and URL creation. This approach allows for an easy configuration and flexible behavior (both IPv4 and IPv6 remain viable at each provisioning step), but requires an infrastructure with preexisting DNS servers, IP allocations and records already in place.</para>
</listitem>
</itemizedlist>
<para>In both cases, Kubernetes will need to know what CIDRs to use for both IPv4 and IPv6, so you can add the following lines to your <literal>kubernetes/config/server.yaml</literal> in the EIB directory, making sure to list IPv4 first:</para>
<screen language="yaml" linenumbering="unnumbered">service-cidr: 10.96.0.0/12,fd12:4567:789c::/112
cluster-cidr: 193.168.0.0/18,fd12:4567:789b::/48</screen>
<para>Some containers leverage the host networking, so modify the network configuration for the host(s), under the <literal>network</literal> directory, to enable IPv6 connectivity:</para>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    next-hop-address: ${MGMT_GATEWAY_V4}
    next-hop-interface: eth0
  - destination: ::/0
    next-hop-address: ${MGMT_GATEWAY_V6}
    next-hop-interface: eth0
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
    - 2001:4860:4860::8888
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP_V4}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    address:
    - ip: ${MGMT_CLUSTER_IP_V6}
      prefix-length: 128
    dhcp: false
    autoconf: false
    enabled: true</screen>
<para>Replace the placeholders with the gateway IP addresses, additional DNS server (if needed), the MAC address of the network interface and the the IP addressed of the management cluster. If address autoconfiguration is preferred instead, refer to the following excerpt and just set the <literal>${MGMT_MAC}</literal> variable:</para>
<screen language="yaml" linenumbering="unnumbered">interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    enabled: true
    dhcp: true
  ipv6:
    enabled: false
    dhcp: true
    autoconf: true</screen>
<para>We can now define the remaining files for a single node configuration, starting from the first option, by creating <literal>kubernetes/helm/values/metal3.yaml</literal> as:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${MGMT_CLUSTER_IP_V4}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  listenOnAll: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort
metal3-baremetal-operator:
  baremetaloperator:
    externalHttpIPv6: ${MGMT_CLUSTER_IP_V6}</screen>
<para>and <literal>kubernetes/helm/values/rancher.yaml</literal> as:</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_IP_V4}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>where <literal>${MGMT_CLUSTER_IP_V4}</literal> and <literal>${MGMT_CLUSTER_IP_V6}</literal> are the IP addresses previously assigned to the host.</para>
<para>Alternatively, to use the hostname in place of the IP addresses, modify <literal>kubernetes/helm/values/metal3.yaml</literal> to:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  provisioningHostname: `${MGMT_CLUSTER_HOSTNAME}`
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort</screen>
<para>and <literal>kubernetes/helm/values/rancher.yaml</literal> to:</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_HOSTNAME}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>where <literal>${MGMT_CLUSTER_HOSTNAME}</literal> should be a Fully Qualified Domain Name resolving to your host IP addresses.</para>
<para>For more information visit <link xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/mgmt-cluster/dual-stack">SUSE Edge GitHub repository under the "dual-stack" folder</link>, where an example directory structure can be found.</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>Telco features configuration</title>
<para>This section documents and explains the configuration of Telco-specific features on clusters deployed via SUSE Telco Cloud.</para>
<para>The directed network provisioning deployment method is used, as described in the Automated Provisioning (<xref linkend="atip-automated-provisioning"/>) section.</para>
<para>The following topics are covered in this section:</para>
<itemizedlist>
<listitem>
<para>Kernel image for real time (<xref linkend="kernel-image-for-real-time"/>): Kernel image to be used by the real-time kernel.</para>
</listitem>
<listitem>
<para>Kernel arguments for low latency and high performance (<xref linkend="kernel-args"/>): Kernel arguments to be used by the real-time kernel for maximum performance and low latency running telco workloads.</para>
</listitem>
<listitem>
<para>CPU Pinning via Tuned and kernel args (<xref linkend="cpu-tuned-configuration"/>): Isolating the CPUs via kernel arguments and Tuned profile.</para>
</listitem>
<listitem>
<para>CNI configuration (<xref linkend="cni-configuration"/>): CNI configuration to be used by the Kubernetes cluster.</para>
</listitem>
<listitem>
<para>SR-IOV configuration (<xref linkend="sriov"/>): SR-IOV configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>DPDK configuration (<xref linkend="dpdk"/>): DPDK configuration to be used by the system.</para>
</listitem>
<listitem>
<para>vRAN acceleration card (<xref linkend="acceleration"/>): Acceleration card configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Huge pages (<xref linkend="huge-pages"/>): Huge pages configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>CPU pinning on Kubernetes (<xref linkend="cpu-pinning-kubernetes"/>): Configuring Kubernetes and the applications to leverage CPU pinning.</para>
</listitem>
<listitem>
<para>NUMA-aware scheduling configuration (<xref linkend="numa-aware-scheduling"/>): NUMA-aware scheduling configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Metal LB configuration (<xref linkend="metal-lb-configuration"/>): Metal LB configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Private registry configuration (<xref linkend="private-registry"/>): Private registry configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Precision Time Protocol configuration (<xref linkend="ptp-configuration"/>): Configuration files for running PTP telco profiles.</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>Kernel image for real time</title>
<para>The real-time kernel image is not necessarily better than a standard kernel.
It is a different kernel tuned to a specific use case. The real-time kernel is tuned for lower latency at the cost of throughput. The real-time kernel is not recommended for general purpose use, but in our case, this is the recommended kernel for Telco Workloads where latency is a key factor.</para>
<para>There are four top features:</para>
<itemizedlist>
<listitem>
<para>Deterministic execution:</para>
<para>Get greater predictability — ensure critical business processes complete in time, every time and deliver high-quality service, even under heavy system loads. By shielding key system resources for high-priority processes, you can ensure greater predictability for time-sensitive applications.</para>
</listitem>
<listitem>
<para>Low jitter:</para>
<para>The low jitter built upon the highly deterministic technology helps to keep applications synchronized with the real world. This helps services that need ongoing and repeated calculation.</para>
</listitem>
<listitem>
<para>Priority inheritance:</para>
<para>Priority inheritance refers to the ability of a lower priority process to assume a higher priority when there is a higher priority process that requires the lower priority process to finish before it can accomplish its task. SUSE Linux Enterprise Real Time solves these priority inversion problems for mission-critical processes.</para>
</listitem>
<listitem>
<para>Thread interrupts:</para>
<para>Processes running in interrupt mode in a general-purpose operating system are not preemptible. With SUSE Linux Enterprise Real Time, these interrupts have been encapsulated by kernel threads, which are interruptible, and allow the hard and soft interrupts to be preempted by user-defined higher priority processes.</para>
<para>In our case, if you have installed a real-time image like <literal>SUSE Linux Micro RT</literal>, kernel real time is already installed. From the <link xl:href="https://scc.suse.com/">SUSE Customer Center</link>, you can download the real-time kernel image.</para>
<note>
<para>For more information about the real-time kernel, visit <link xl:href="https://www.suse.com/products/realtime/">SUSE Real Time</link>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="kernel-args">
<title>Kernel arguments for low latency and high performance</title>
<para>The kernel arguments are important to be configured to enable the real-time kernel to work properly giving the best performance and low latency to run telco workloads.  There are some important concepts to keep in mind when configuring the kernel arguments for this use case:</para>
<itemizedlist>
<listitem>
<para>Remove <literal>kthread_cpus</literal> when using SUSE real-time kernel. This parameter controls on which CPUs kernel threads are created. It also controls which CPUs are allowed for PID 1 and for loading kernel modules (the kmod user-space helper). This parameter is not
recognized and does not have any effect.</para>
</listitem>
<listitem>
<para>Isolate the CPU cores using <literal>isolcpus</literal>, <literal>nohz_full</literal>, <literal>rcu_nocbs</literal>, and <literal>irqaffinity</literal>. For a comprehensive list of CPU pinning techniques, refer to  CPU Pinning via Tuned and kernel args (<xref linkend="cpu-tuned-configuration"/>) chapter.</para>
</listitem>
<listitem>
<para>Add <literal>domain,nohz,managed_irq</literal> flags to <literal>isolcpus</literal> kernel argument. Without any flags, <literal>isolcpus</literal> is equivalent to specifying only the <literal>domain</literal> flag. This isolates the specified CPUs from scheduling, including kernel tasks. The <literal>nohz</literal> flag stops the scheduler tick on the specified CPUs (if only one task is runnable on a CPU), and the <literal>managed_irq</literal> flag avoids routing managed external (device) interrupts at the specified CPUs. Note that the IRQ lines of NVMe devices are fully managed by the kernel and will be routed to the non-isolated (housekeeping) cores as a consequence. For example, the command line provided at the end of this section will result in only four queues (plus an admin/control queue) allocated on the system:</para>
<screen language="shell" linenumbering="unnumbered">for I in $(grep nvme0 /proc/interrupts | cut -d ':' -f1); do cat /proc/irq/${I}/effective_affinity_list; done | column
39      0       19      20      39</screen>
<para>This behavior prevents any disruption caused by disk I/O to any time sensitive application running on the isolated cores, but might require attention and careful design for storage focused workloads.</para>
</listitem>
<listitem>
<para>Tune the ticks (kernel’s periodic timer interrupts):</para>
<itemizedlist>
<listitem>
<para><literal>skew_tick=1</literal>: ticks can sometimes happen simultaenously. Instead of all CPUs receiving their timer tick at the exact same moment, <literal>skew_tick=1</literal> makes them occur at slightly offset times. This helps reduce system jitter, resulting in more consistent and lower interrupt response times (an essential requirement for latency-sensitive applications).</para>
</listitem>
<listitem>
<para><literal>nohz=on</literal>: stops the periodic timer tick on idle CPUs.</para>
</listitem>
<listitem>
<para><literal>nohz_full=&lt;cpu-cores&gt;</literal>: Stops the periodic timer tick on specified CPUs that are dedicated for real-time applications.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Disable Machine Check Exception (MCE) handling by specifying <literal>mce=off</literal>. MCEs are hardware errors detected by the processor and disabling them can avoid noisy logs.</para>
</listitem>
<listitem>
<para>Add <literal>nowatchdog</literal> to disable the soft-lockup watchdog which is implemented as a timer running in the timer hard-interrupt context. When it expires (i.e. a soft lockup is detected), it will print a warning (in the hard interrupt context), running any latency targets. Even if it never expires, it goes onto the timer list, slightly increasing the overhead of every timer interrupt. This option also disables the NMI watchdog, so NMIs cannot interfere.</para>
</listitem>
<listitem>
<para><literal>nmi_watchdog=0</literal> disables the NMI (Non-Maskable Interrupt) watchdog. This can be omitted when <literal>nowatchdog</literal> is used.</para>
</listitem>
<listitem>
<para>RCU (Read-Copy-Update) is a kernel mechanism that enables concurrent, lock-free access for many readers to shared data. An RCU callback, a function triggered after a 'grace period', ensures all previous readers have finished so old data can be safely reclaimed. We fine-tune RCU, particularly for sensitive workloads, to offload these callbacks from dedicated (pinned) CPUs, preventing kernel operations from interfering with critical, time-sensitive tasks.</para>
<itemizedlist>
<listitem>
<para>Specify the pinned CPUs in <literal>rcu_nocbs</literal> so that RCU callbacks do not run on them. This helps reducing jitter and latency for the real-time workloads.</para>
</listitem>
<listitem>
<para><literal>rcu_nocb_poll</literal> makes the no-callback CPUs regularly 'poll' to see if callback handling is required. This can reduce the interrupt overhead.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_cpu_stall_suppress=1</literal> suppresses RCU CPU stall warnings, which can sometimes be false positives in heavily loaded real-time systems</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_expedited=1</literal> speeds up the grace period for RCU operations, making read-side critical sections more responsive</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_normal_after_boot=1</literal> When used with rcu_expedited, it allows RCU to rever to normal (non-expedited) operation after the system boot.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_task_stall_timeout=0</literal> disables the RCU task stall detector, preventing potential warnings or system halts from long-running RCU tasks.</para>
</listitem>
<listitem>
<para><literal>rcutree.kthread_prio=99</literal> sets the priority of the RCU callback kernel thread to the highest possible (99), ensuring it gets scheduled and handles RCU callbacks promptly, when needed.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Add <literal>ignition.platform.id=openstack</literal> for Metal3 and Cluster API to successfully provision/deprovision the cluster. This is used by Metal3 Python agent, which originated from Openstack Ironic.</para>
</listitem>
<listitem>
<para>Remove <literal>intel_pstate=passive</literal>. This option configures <literal>intel_pstate</literal> to work with generic cpufreq governors, but to make this work, it disables hardware-managed P-states (<literal>HWP</literal>) as a side effect. To reduce the hardware latency, this option is not recommended for real-time workloads.</para>
</listitem>
<listitem>
<para>Replace <literal>intel_idle.max_cstate=0 processor.max_cstate=1</literal> with <literal>idle=poll</literal>. To avoid C-State transitions, the <literal>idle=poll</literal> option is used to disable the C-State transitions and keep the CPU in the highest C-State. The <literal>intel_idle.max_cstate=0</literal> option disables <literal>intel_idle</literal>, so <literal>acpi_idle</literal> is used, and <literal>acpi_idle.max_cstate=1</literal> then sets max C-state for acpi_idle.
On AMD64/Intel 64 architectures, the first ACPI C-State is always <literal>POLL</literal>, but it uses a <literal>poll_idle()</literal> function, which may introduce some tiny latency by reading the clock periodically, and restarting the main loop in <literal>do_idle()</literal> after a timeout (this also involves clearing and setting the <literal>TIF_POLL</literal> task flag).
In contrast, <literal>idle=poll</literal> runs in a tight loop, busy-waiting for a task to be rescheduled. This minimizes the latency of exiting the idle state, but at the cost of keeping the CPU running at full speed in the idle thread.</para>
</listitem>
<listitem>
<para>Disable C1E in BIOS. This option is important to disable the C1E state in the BIOS to avoid the CPU from entering the C1E state when idle. The C1E state is a low-power state that can introduce latency when the CPU is idle.</para>
</listitem>
</itemizedlist>
<para>The rest of this documentation covers additional parameters, including huge pages and IOMMU.</para>
<para>This provides an example of kernel arguments for a 32-core Intel server, including the aforementioned adjustments:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
<para>Here is another configuration example for a 64-core AMD server. Among the 128 logical processors (<literal>0-127</literal>), first 8 cores (<literal>0-7</literal>) are designated for housekeeping, while the remaining 120 cores (<literal>8-127</literal>) are pinned for the applications:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=575291cf-74e8-42cf-8f2c-408a20dc00b8 skew_tick=1 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack amd_iommu=on iommu=pt irqaffinity=0-7 isolcpus=domain,nohz,managed_irq,8-127 nohz_full=8-127 rcu_nocbs=8-127 mce=off nohz=on net.ifnames=0 nowatchdog nmi_watchdog=0 nosoftlockup quiet rcu_nocb_poll rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
</section>
<section xml:id="cpu-tuned-configuration">
<title>CPU pinning via Tuned and kernel args</title>
<para><literal>tuned</literal> is a system tuning tool that monitors system conditions to optimize performance using various predefined profiles. A key feature is its ability to isolate CPU cores for specific workloads, like real-time applications. This prevents the OS from utilizing these cores and potentially increasing latency.</para>
<para>To enable and configure this feature, the first thing is to create a profile for the CPU cores we want to isolate. In this example, among 64 cores, we dedicate 60 cores (<literal>1-30,33-62</literal>) for the application and remaining 4 cores are used for housekeeping. Note that the design of isolated CPUs heavily depends on the real-time applications.</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>Then we need to modify the GRUB option to isolate CPU cores and other important parameters for CPU usage.
The following options are important to be customized with your current hardware specifications:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Isolate the cores 1-30 and 33-62. <literal>domain</literal> indicates these CPUs are part of isolation domain. <literal>nohz</literal> enables tickless operation on these isolated CPUs when they are idle, to reduce interruptions. <literal>managed_irq</literal> isolates pinned CPUs from being targeted by IRQs. This contemplates <literal>irqaffinity=0-7</literal>, which already directs mosts IRQs to the housekeeping cores.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to skew the timer interrupts across the isolated CPUs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>When enabled, kernel’s periodic timer interrupt (the 'tick') will stop on any CPU core that is idle. This primary benefits the housekeeping CPUs (<literal>0,31,32,63</literal>). This conserves power and reduces unnecessary wake-ups on those general-purpose cores.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>For the isolated cores, this stops the tick and it does so even when the CPU is running a single active task. It means it makes the CPU run in full tickless mode (or 'dyntick'). The kernel will only deliver timer interrupts when they are actually needed.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>This option offloads the RCU callback processing from specified CPU cores.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocb_poll</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>When this option is set, no-RCU-callback CPUs will regularly 'poll' to see if callback handling is required, rather than being explicitly woken up by other CPUs. This can reduce the interrupt overhead.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to run the interrupts to the housekeeping cores.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>This minimizes the latency of exiting the idle state, but at the cost of keeping the CPU running at full speed in the idle thread.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>This option disables only the NMI watchdog. This can be omitted when <literal>nowatchdog</literal> is set.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>This option disables the soft-lockup watchdog which is implemented as a timer running in the timer hard-interrupt context.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>The following commands modify the GRUB configuration and apply the changes mentioned above to be present on the next boot:</para>
<para>Edit the <literal>/etc/default/grub</literal> file with above parameters and the file will look like this:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Update the GRUB configuration:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>To validate that the parameters are applied after the reboot, the following command can be used to check the kernel command line:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<para>There is another script that can be used to tune the CPU configuration, which basically is doing the following steps:</para>
<itemizedlist>
<listitem>
<para>Set the CPU governor to <literal>performance</literal>.</para>
</listitem>
<listitem>
<para>Unset the timer migration to the isolated CPUs.</para>
</listitem>
<listitem>
<para>Migrate the kdaemon threads to the housekeeping CPUs.</para>
</listitem>
<listitem>
<para>Set the isolated CPUs latency to the lowest possible value.</para>
</listitem>
<listitem>
<para>Delay the vmstat updates to 300 seconds.</para>
</listitem>
</itemizedlist>
<para>The script is available at <link xl:href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.4/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh">SUSE Telco Cloud Examples repository</link>.</para>
</section>
<section xml:id="cni-configuration">
<title>CNI Configuration</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal> is the default CNI plug-in for SUSE Telco Cloud.
To enable Cilium on RKE2 cluster as the default plug-in, the following configuration is required in the <literal>/etc/rancher/rke2/config.yaml</literal> file:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>This can also be specified with command-line arguments, that is, <literal>--cni=cilium</literal> into the server line in <literal>/etc/systemd/system/rke2-server</literal> file.</para>
<para>To use the <literal>SR-IOV</literal> network operator described in the next section (<xref linkend="option2-sriov-helm"/>), use <literal>Multus</literal> with another CNI plug-in, like <literal>Cilium</literal> or <literal>Calico</literal>, as a secondary plug-in.</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</section>
<section xml:id="id-calico">
<title>Calico</title>
<para><literal>Calico</literal> is another CNI plug-in for SUSE Edge for Telco.
To enable Calico on RKE2 cluster as the default plug-in, the following
configuration is required in the <literal>/etc/rancher/rke2/config.yaml</literal> file:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- calico</screen>
<para>This can also be specified with command-line arguments, that is, <literal>--cni=calico</literal> into the server line in <literal>/etc/systemd/system/rke2-server</literal> file.</para>
<para>To use the <literal>SR-IOV</literal> network operator described in the next section (<xref linkend="option2-sriov-helm"/>), use <literal>Multus</literal> with another CNI plug-in, like <literal>Cilium</literal> or <literal>Calico</literal>, as a secondary plug-in.</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- calico</screen>
<note>
<para>For more information about CNI plug-ins, see <link xl:href="https://docs.rke2.io/install/network_options">Network Options</link>.</para>
</note>
</section>
<section xml:id="id-bond-cni">
<title>Bond CNI</title>
<para>In general terms, bonding provides a method for aggregating multiple network
interfaces into a single logical "bonded" interface. This is typically used to
increase service availability by introducing redundant networking paths, but can
also be used to increase bandwidth with certain bond modes.
The following CNI plug-ins are supported for the Bond CNI plugin in combination with multus:</para>
<itemizedlist>
<listitem>
<para>MACVLAN</para>
</listitem>
<listitem>
<para>Host Device</para>
</listitem>
<listitem>
<para>SR-IOV</para>
</listitem>
</itemizedlist>
<section xml:id="id-bond-cni-with-macvlan">
<title>Bond CNI with MACVLAN</title>
<para>To use the Bond CNI plugin with MACVLAN two free interfaces are needed in the
container. The following example uses 'enp8s0' and 'enp9s0'. Start by creating network
attachment definitions for them:</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp8s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp9s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para>After this, add a network attachment definition for the bond itself.</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>The IP address assignment here is static and defines the address of the bond as
'192.168.200.100' on a /24 network, with a gateway residing on the network’s first
available address. In the bond’s network attachment we also define the type of
bond we want. In this case it is active-backup.</para>
<para>To use this bond, the pod needs to know about all interfaces. An example pod
definition might look like this:</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-conf",
"interface": "net1"
},
{"name": "enp9s0-conf",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
<para>Note how the annotation refers to all networks and how it defines the mapping
between the interfaces 'enp8s0 → net1', and 'enp9s0→net2'.</para>
</section>
<section xml:id="id-bond-cni-with-host-device">
<title>Bond CNI with Host Device</title>
<para>To use the Bond CNI plugin with host device, two free interfaces are needed on
the host. These interfaces are then mapped through to the container. The
following example uses 'enp8s0' and 'enp9s0'. Start by creating network
attachment definitions for them:</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp8s0",
          "ipam": {}
        }]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp9s0",
          "ipam": {}
        }]
    }'</screen>
<para>After this, add network attachment definition for the bond itself. This is
similar to the MACVLAN use case.</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>The IP address assignment here is static and defines the address of the bond as
'192.168.200.100' on a /24 network, with a gateway residing on the network’s
first available address. In the bond’s network attachment, define the type of
bond. In this case it is active-backup.</para>
<para>To use this bond, the pod needs to know about all interfaces. An example pod
definition for bond with host devices might look like this:</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-hostdev",
"interface": "net1"
},
{"name": "enp9s0-hostdev",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
</section>
<section xml:id="id-bond-cni-with-sr-iov">
<title>Bond CNI with SR-IOV</title>
<para>Using the Bond CNI with SR-IOV is fairly straight forward. For more details on how to set up SR-IOV, see <xref linkend="sriov"/>. As described there, you have to create <literal>SriovNetworkNodePolicies</literal> that defines <literal>resourceNames</literal>, as well as number of virtual functions and such. The <literal>resourceNames</literal> are being used by the <literal>SriovNetwork</literal> which is used as interfaces in the pod definition. The bond definition is exactly the same as for the MACVLAN and host device cases.</para>
<note>
<para>Bond CNI with SR-IOV is only applicable to SRIOV Virtual Functions (VF) using the kernel driver. Userspace driver VFs - such as those used in DPDK workloads - can not be bonded with the Bond CNI.</para>
</note>
</section>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOV allows a device, such as a network adapter, to separate access to its resources among various <literal>PCIe</literal> hardware functions.
There are different ways to deploy <literal>SR-IOV</literal>, and here, we show two different options:</para>
<itemizedlist>
<listitem>
<para>Option 1: using the <literal>SR-IOV</literal> CNI device plug-ins and a config map to configure it properly.</para>
</listitem>
<listitem>
<para>Option 2 (recommended): using the <literal>SR-IOV</literal> Helm chart from Rancher Prime to make this deployment easy.</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">Option 1 - Installation of SR-IOV CNI device plug-ins and a config map to configure it properly</emphasis></para>
<itemizedlist>
<listitem>
<para>Prepare the config map for the device plug-in</para>
</listitem>
</itemizedlist>
<para>Get the information to fill the config map from the <literal>lspci</literal> command:</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>The config map consists of a <literal>JSON</literal> file that describes devices using filters to discover, and creates groups for the interfaces.
The key is understanding filters and groups. The filters are used to discover the devices and the groups are used to create the interfaces.</para>
<para>It could be possible to set filters:</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (Accelerator card)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (driver)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (physical interface name)</para>
</listitem>
</itemizedlist>
<para>It could be possible to also set filters to match more complex interface syntax, for example:</para>
<itemizedlist>
<listitem>
<para>pfNames: <literal>["eth1#1,2,3,4,5,6"]</literal> or <literal>[eth1#1-6]</literal> (physical interface name)</para>
</listitem>
</itemizedlist>
<para>Related to the groups, we could create a group for the <literal>FEC</literal> card and another group for the <literal>Intel</literal> card, even creating a prefix depending on our use case:</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>There are a lot of combinations to discover and create the resource group to allocate some <literal>VFs</literal> to the pods.</para>
<note>
<para>For more information about the filters and groups, visit <link xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sr-iov network device plug-in</link>.</para>
</note>
<para>After setting the filters and groups to match the interfaces depending on the hardware and the use case, the following config map shows an example to be used:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para>Prepare the <literal>daemonset</literal> file to deploy the device plug-in.</para>
</listitem>
</itemizedlist>
<para>The device plug-in supports several architectures (<literal>arm</literal>, <literal>amd</literal>, <literal>ppc64le</literal>), so the same file can be used for different architectures deploying several <literal>daemonset</literal> for each architecture.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>After applying the config map and the <literal>daemonset</literal>, the device plug-in will be deployed and the interfaces will be discovered and available for the pods.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Check the interfaces discovered and available in the nodes to be used by the pods:</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para>The <literal>FEC</literal> is <literal>intel.com/intel_fec_5g</literal> and the value is 1.</para>
</listitem>
<listitem>
<para>The <literal>VF</literal> is <literal>intel.com/intel_sriov_odu</literal> or <literal>intel.com/intel_sriov_oru</literal> if you deploy it with a device plug-in and the config map without Helm charts.</para>
</listitem>
</itemizedlist>
<important>
<para>If there are no interfaces here, it makes little sense to continue because the interface will not be available for pods. Review the config map and filters to solve the issue first.</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">Option 2 (recommended) - Installation using Rancher using Helm chart for SR-IOV CNI and device plug-ins</emphasis></para>
<itemizedlist>
<listitem>
<para>Get Helm if not present:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>Install SR-IOV.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">helm install sriov-crd oci://registry.suse.com/edge/charts/sriov-crd -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/charts/sriov-network-operator -n sriov-network-operator</screen>
<itemizedlist>
<listitem>
<para>Check the  deployed resources crd and pods:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>Check the label in the nodes.</para>
</listitem>
</itemizedlist>
<para>With all resources running, the label appears automatically in your node:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para>Review the <literal>daemonset</literal> to see the new <literal>sriov-network-config-daemon</literal> and <literal>sriov-rancher-nfd-worker</literal> as active and ready:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>In a few minutes (can take up to 10 min to be updated), the nodes are detected and configured with the <literal>SR-IOV</literal> capabilities:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>Check the interfaces detected.</para>
</listitem>
</itemizedlist>
<para>The interfaces discovered should be the PCI address of the network device. Check this information with the <literal>lspci</literal> command in the host.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>If your interface is not detected here, ensure that it is present in the next config map:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>If your device is not there, edit the config map, adding the right values to be discovered (should be necessary to restart the <literal>sriov-network-config-daemon</literal> daemonset).</para>
</note>
<itemizedlist>
<listitem>
<para>Create the <literal>NetworkNode Policy</literal> to configure the <literal>VFs</literal>.</para>
</listitem>
</itemizedlist>
<para>Some <literal>VFs</literal> (<literal>numVfs</literal>) from the device (<literal>rootDevices</literal>) will be created, and it will be configured with the driver <literal>deviceType</literal> and the <literal>MTU</literal>:</para>
<note>
<para>The <literal>resourceName</literal> field must not contain any special characters and must be unique across the cluster.
The example uses the <literal>deviceType: vfio-pci</literal> because <literal>dpdk</literal> will be used in combination with <literal>sr-iov</literal>. If you don’t use <literal>dpdk</literal>, the deviceType should be <literal>deviceType: netdevice</literal> (default value).</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>Validate configurations:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>Create the sr-iov network (optional, just in case a different network is needed):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>Check the network created:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal> (Data Plane Development Kit) is a set of libraries and drivers for fast packet processing. It is used to accelerate packet processing workloads running on a wide variety of CPU architectures.
The DPDK includes data plane libraries and optimized network interface controller (<literal>NIC</literal>) drivers for the following:</para>
<orderedlist numeration="arabic">
<listitem>
<para>A queue manager implements lockless queues.</para>
</listitem>
<listitem>
<para>A buffer manager pre-allocates fixed size buffers.</para>
</listitem>
<listitem>
<para>A memory manager allocates pools of objects in memory and uses a ring to store free objects; ensures that objects are spread equally on all <literal>DRAM</literal> channels.</para>
</listitem>
<listitem>
<para>Poll mode drivers (<literal>PMD</literal>) are designed to work without asynchronous notifications, reducing overhead.</para>
</listitem>
<listitem>
<para>A packet framework as a set of libraries that are helpers to develop packet processing.</para>
</listitem>
</orderedlist>
<para>The following steps will show how to enable <literal>DPDK</literal> and how to create <literal>VFs</literal> from the <literal>NICs</literal> to be used by the <literal>DPDK</literal> interfaces:</para>
<itemizedlist>
<listitem>
<para>Install the <literal>DPDK</literal> package:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Kernel parameters:</para>
</listitem>
</itemizedlist>
<para>To use DPDK, employ some drivers to enable certain parameters in the kernel:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>This option enables the use  of the <literal>vfio</literal> driver for the DPDK interfaces.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu or amd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>This option enables the use of <literal>vfio</literal> for <literal>VFs</literal>.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>To enable the parameters, add them to the <literal>/etc/default/grub</literal> file:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Update the GRUB configuration and reboot the system to apply the changes:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Load <literal>vfio-pci</literal> kernel module and enable <literal>SR-IOV</literal> on the <literal>NICs</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Create some virtual functions (<literal>VFs</literal>) from the <literal>NICs</literal>.</para>
</listitem>
</itemizedlist>
<para>To create for <literal>VFs</literal>, for example, for two different <literal>NICs</literal>, the following commands are required:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>Bind the new VFs with the <literal>vfio-pci</literal> driver:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>Review the configuration is correctly applied:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>vRAN acceleration (<literal>Intel ACC100/ACC200</literal>)</title>
<para>As communications service providers move from 4 G to 5 G networks, many are adopting virtualized radio access network (<literal>vRAN</literal>) architectures for higher channel capacity and easier deployment of edge-based services and applications. vRAN solutions are ideally located to deliver low-latency services with the flexibility to increase or decrease capacity based on the volume of real-time traffic and demand on the network.</para>
<para>One of the most compute-intensive 4 G and 5 G workloads is RAN layer 1 (<literal>L1</literal>) <literal>FEC</literal>, which resolves data transmission errors over unreliable or noisy communication channels. <literal>FEC</literal> technology detects and corrects a limited number of errors in 4 G or 5 G data, eliminating the need for retransmission. Since the <literal>FEC</literal> acceleration transaction does not contain cell state information, it can be easily virtualized, enabling pooling benefits and easy cell migration.</para>
<itemizedlist>
<listitem>
<para>Kernel parameters</para>
</listitem>
</itemizedlist>
<para>To enable the <literal>vRAN</literal> acceleration, we need to enable the following kernel parameters (if not present yet):</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>This option enables the use of vfio for the DPDK interfaces.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu or amd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>This option enables the use of vfio for VFs.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modify the GRUB file <literal>/etc/default/grub</literal> to add them to the kernel command line:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Update the GRUB configuration and reboot the system to apply the changes:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>To verify that the parameters are applied after the reboot, check the command line:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Load vfio-pci kernel modules to enable the <literal>vRAN</literal> acceleration:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Get interface information Acc100:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>Bind the physical interface (<literal>PF</literal>) with <literal>vfio-pci</literal> driver:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>Create the virtual functions (<literal>VFs</literal>) from the physical interface (<literal>PF</literal>).</para>
</listitem>
</itemizedlist>
<para>Create 2 <literal>VFs</literal> from the <literal>PF</literal> and bind with <literal>vfio-pci</literal> following the next steps:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>Configure acc100 with the proposed configuration file:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>Check the new VFs created from the FEC PF:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>Huge pages</title>
<para>When a process uses <literal>RAM</literal>, the <literal>CPU</literal> marks it as used by that process. For efficiency, the <literal>CPU</literal> allocates <literal>RAM</literal> in chunks <literal>4K</literal> bytes is the default value on many platforms. Those chunks are named pages. Pages can be swapped to disk, etc.</para>
<para>Since the process address space is virtual, the <literal>CPU</literal> and the operating system need to remember which pages belong to which process, and where each page is stored. The greater the number of pages, the longer the search for memory mapping. When a process uses <literal>1 GB</literal> of memory, that is 262144 entries to look up (<literal>1 GB</literal> / <literal>4 K</literal>). If a page table entry consumes 8 bytes, that is <literal>2 MB</literal> (262144 * 8) to look up.</para>
<para>Most current <literal>CPU</literal> architectures support larger-than-default pages, which give the <literal>CPU/OS</literal> fewer entries to look up.</para>
<itemizedlist>
<listitem>
<para>Kernel parameters</para>
</listitem>
</itemizedlist>
<para>To enable the huge pages, we should add the following kernel parameters. In this example, we configure 40 1G pages, though the huge page size and exact number should be tailored to your application’s memory requirements:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>This option allows to set the size of huge pages to 1 G</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>This is the number of huge pages defined before</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>This is the default value to get the huge pages</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modify the GRUB file <literal>/etc/default/grub</literal> to add these parameters in <literal>GRUB_CMDLINE_LINUX</literal>:</para>
<screen language="shell" linenumbering="unnumbered">default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0</screen>
<para>Update the GRUB configuration and reboot the system to apply the changes:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>To validate that the parameters are applied after the reboot, you can check the command line:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Using huge pages</para>
</listitem>
</itemizedlist>
<para>To use the huge pages, we need to mount them:</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Deploy a Kubernetes workload, creating the resources and the volumes:</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-kubernetes">
<title>CPU pinning on Kubernetes</title>
<section xml:id="id-prerequisite">
<title>Prerequisite</title>
<para>Must have the <literal>CPU</literal> tuned to the performance profile covered in this section (<xref linkend="cpu-tuned-configuration"/>).</para>
</section>
<section xml:id="id-configure-kubernetes-for-cpu-pinning">
<title>Configure Kubernetes for CPU Pinning</title>
<para>Configure kubelet arguments to implement CPU management in the <literal>RKE2</literal> cluster. Add the following configuration block such as below example to your <literal>/etc/rancher/rke2/config.yaml</literal> file. Make sure specifying the housekeeping CPU cores in <literal>kubelet-reserved</literal> and <literal>system-reserved</literal> arguments:</para>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=0,31,32,63"
- "system-reserved=cpu=0,31,32,63"</screen>
</section>
<section xml:id="id-leveraging-pinned-cpus-for-workloads">
<title>Leveraging Pinned CPUs for Workloads</title>
<para>There are three ways to use that feature using the <literal>Static Policy</literal> defined in kubelet depending on the requests and limits you define on your workload:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>BestEffort</literal> QoS Class: If you do not define any request or limit for <literal>CPU</literal>, the pod is scheduled on the first <literal>CPU</literal> available on the system.</para>
<para>An example of using the <literal>BestEffort</literal> QoS Class could be:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para><literal>Burstable</literal> QoS Class: If you define a request for CPU, which is not equal to the limits, or there is no CPU request.</para>
<para>Examples of using the <literal>Burstable</literal> QoS Class could be:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>or</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para><literal>Guaranteed</literal> QoS Class: If you define a request for CPU, which is equal to the limits.</para>
<para>An example of using the <literal>Guaranteed</literal> QoS Class could be:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="numa-aware-scheduling">
<title>NUMA-aware scheduling</title>
<para>Non-Uniform Memory Access or Non-Uniform Memory Architecture (<literal>NUMA</literal>) is a physical memory design used in <literal>SMP</literal> (multiprocessors) architecture, where the memory access time depends on the memory location relative to a processor. Under <literal>NUMA</literal>, a processor can access its own local memory faster than non-local memory, that is, memory local to another processor or memory shared between processors.</para>
<section xml:id="id-identifying-numa-nodes">
<title>Identifying NUMA nodes</title>
<para>To identify the <literal>NUMA</literal> nodes, on your system use the following command:</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>For this example, we have only one <literal>NUMA</literal> node showing 64 <literal>CPUs</literal>.</para>
<para><literal>NUMA</literal> needs to be enabled in the <literal>BIOS</literal>. If <literal>dmesg</literal> does not have records of NUMA initialization during the bootup, then <literal>NUMA</literal>-related messages in the kernel ring buffer might have been overwritten.</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>Metal LB</title>
<para><literal>MetalLB</literal> is a load-balancer implementation for bare-metal Kubernetes clusters, using standard routing protocols like <literal>L2</literal> and <literal>BGP</literal> as advertisement protocols. It is a network load balancer that can be used to expose services in a Kubernetes cluster to the outside world due to the need to use Kubernetes Services type <literal>LoadBalancer</literal> with bare-metal.</para>
<para>To enable <literal>MetalLB</literal> in the <literal>RKE2</literal> cluster, the following steps are required:</para>
<itemizedlist>
<listitem>
<para>Install <literal>MetalLB</literal> using the following command:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/metallb
  targetNamespace: metallb-system
  version: 304.0.0+up0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
  version: 304.0.1+up0.3.0
  createNamespace: true
EOF</screen>
<itemizedlist>
<listitem>
<para>Create the <literal>IpAddressPool</literal> and the <literal>L2advertisement</literal> configuration:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para>Create the endpoint service to expose the <literal>VIP</literal>:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para>Check the <literal>VIP</literal> is created and the <literal>MetalLB</literal> pods are running:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>Private registry configuration</title>
<para><literal>Containerd</literal> can be configured to connect to private registries and use them to pull private images on each node.</para>
<para>Upon startup, <literal>RKE2</literal> checks if a <literal>registries.yaml</literal> file exists at <literal>/etc/rancher/rke2/</literal> and instructs <literal>containerd</literal> to use any registries defined in the file. If you wish to use a private registry, create this file as root on each node that will use the registry.</para>
<para>To add the private registry, create the file <literal>/etc/rancher/rke2/registries.yaml</literal> with the following content:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>or without authentication:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>For the registry changes to take effect, you need to either configure this file before starting RKE2 on the node, or restart RKE2 on each configured node.</para>
<note>
<para>For more information about this, please check <link xl:href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html#_registries_configuration_file">containerd registry configuration rke2</link>.</para>
</note>
</section>
<section xml:id="ptp-configuration">
<title>Precision Time Protocol</title>
<para>Precision Time Protocol (PTP) is a network protocol developed by the Institute of Electrical and Electronics Engineers (IEEE) to enable sub-microsecond time synchronization in a computer network. Since its inception and for a couple of decades now, PTP has been in use in many industries. It has recently seen a growing adoption in the telecommunication networks as a vital element to 5G networks. While being a relatively simple protocol, its configuration can change significantly depending on the application. For this reason, multiple profiles have been defined and standardized.</para>
<para>In this section, only telco-specific profiles will be covered. Consequently time-stamping capability and a PTP hardware clock (PHC) in the NIC will be assumed. Nowadays, all telco-grade network adapters come with PTP support in hardware, but you can verify such capabilities with the following command:</para>
<screen language="console" linenumbering="unnumbered"># ethtool -T p1p1
Time stamping parameters for p1p1:
Capabilities:
        hardware-transmit
        software-transmit
        hardware-receive
        software-receive
        software-system-clock
        hardware-raw-clock
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
        off
        on
Hardware Receive Filter Modes:
        none
        all</screen>
<para>Replace <literal>p1p1</literal> with name of the interface to be used for PTP.</para>
<para>The following sections will provide guidance on how to install and configure PTP on SUSE Telco Cloud specifically, but familiarity with basic PTP concepts is expected. For a brief overview of PTP and the implementation included in SUSE Telco Cloud, refer to <link xl:href="https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html">https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html</link>.</para>
<section xml:id="id-install-ptp-software-components">
<title>Install PTP software components</title>
<para>In SUSE Telco Cloud, the PTP implementation is provided by the <literal>linuxptp</literal> package, which includes two components:</para>
<itemizedlist>
<listitem>
<para><literal>ptp4l</literal>: a daemon that controls the PHC on the NIC and runs the PTP protocol</para>
</listitem>
<listitem>
<para><literal>phc2sys</literal>: a daemon that keeps the system clock in sync with the PTP-synchronized PHC on the NIC</para>
</listitem>
</itemizedlist>
<para>Both daemons are required for the system synchronization to fully work and must be correctly configured according to your setup. This is covered in <xref linkend="ptp-telco-config"/>.</para>
<para>The easiest and best way to integrate PTP in your downstream cluster is to add the <literal>linuxptp</literal> package under <literal>packageList</literal> in the Edge Image Builder (EIB) definition file. This way the PTP control plane software will be installed automatically during the cluster provisioning. See the EIB documentation (<xref linkend="eib-configuring-rpm-packages"/>) for more information on installing packages.</para>
<para>Below find a sample EIB manifest with <literal>linuxptp</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: {micro-base-rt-image-raw}
  outputImageName: eibimage-slmicrort-telco.raw
operatingSystem:
  time:
    timezone: America/New_York
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
    enable:
      - ptp4l
      - phc2sys
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
      - open-iscsi
      - tuned
      - cpupower
      - linuxptp
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<note>
<para>The <literal>linuxptp</literal> package included in SUSE Telco Cloud does not enable <literal>ptp4l</literal> and <literal>phc2sys</literal> by default. If their system-specific configuration files are deployed at provisioning time (see <xref linkend="ptp-capi"/>), they should be enabled. Do so by adding them to the <literal>systemd</literal> section of the manifest, as in the example above.</para>
</note>
<para>Follow the usual process to build the image as described in the EIB Documentation (<xref linkend="eib-how-to-build-image"/>) and use it to deploy your cluster. If you are new to EIB, start from <xref linkend="components-eib"/> instead.</para>
</section>
<section xml:id="ptp-telco-config">
<title>Configure PTP for telco deployments</title>
<para>Many telco applications require strict phase and time synchronization with little deviance, which resulted in a definition of two telco-oriented profiles: the ITU-T G.8275.1 and ITU-T G.8275.2. They both have a high rate of sync messages and other distinctive traits, such as the use of an alternative Best Master Clock Algorithm (BMCA). Such behavior mandates specific settings in the configuration file consumed by <literal>ptp4l</literal>, provided in the following sections as a reference.</para>
<note>
<itemizedlist>
<listitem>
<para>Both sections only cover the case of an ordinary clock in Time Receiver configuration.</para>
</listitem>
<listitem>
<para>Any such profile must be used in a well-planned PTP infrastructure.</para>
</listitem>
<listitem>
<para>Your specific PTP network may require additional configuration tuning, make sure to review and adapt the provided examples if needed.</para>
</listitem>
</itemizedlist>
</note>
<section xml:id="id-ptp-profile-itu-t-g-8275-1">
<title>PTP profile ITU-T G.8275.1</title>
<para>The G.8275.1 profile has the following specifics:</para>
<itemizedlist>
<listitem>
<para>Runs directly on Ethernet and requires full network support (adjacent nodes/switches must support PTP).</para>
</listitem>
<listitem>
<para>The default domain setting is 24.</para>
</listitem>
<listitem>
<para>Dataset comparison is based on the G.8275.x algorithm and its <literal>localPriority</literal> values after <literal>priority2</literal>.</para>
</listitem>
</itemizedlist>
<para>Copy the following content to a file named <literal>/etc/ptp4l-G.8275.1.conf</literal>:</para>
<screen linenumbering="unnumbered"># Telecom G.8275.1 example configuration
[global]
domainNumber                    24
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             -3
logSyncInterval                 -4
logMinDelayReqInterval          -4
announceReceiptTimeout          3
serverOnly                      0
ptp_dst_mac                     01:80:C2:00:00:0E
network_transport               L2</screen>
<para>Once the file has been created, it must be referenced in <literal>/etc/sysconfig/ptp4l</literal> for the daemon to start correctly. This can be done by changing the <literal>OPTIONS=</literal> line to:</para>
<screen linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"</screen>
<para>More precisely:</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> requires the file name of the configuration file to use; <literal>/etc/ptp4l-G.8275.1.conf</literal> in this case</para>
</listitem>
<listitem>
<para><literal>-i</literal> requires the name of the interface to use, replace <literal>$IFNAME</literal> with a real interface name.</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> allows to better identify the ptp4l output in the system logs and is optional.</para>
</listitem>
</itemizedlist>
<para>Once the steps above are complete, the <literal>ptp4l</literal> daemon must be (re)started:</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>Check the synchronization status by observing the logs with:</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-ptp-profile-itu-t-g-8275-2">
<title>PTP profile ITU-T G.8275.2</title>
<para>The G.8275.2 profile has the following specifics:</para>
<itemizedlist>
<listitem>
<para>Runs on IP and does not require full network support (adjacent nodes/switches may not support PTP).</para>
</listitem>
<listitem>
<para>The default domain setting is 44.</para>
</listitem>
<listitem>
<para>Dataset comparison is based on the G.8275.x algorithm and its <literal>localPriority</literal> values after <literal>priority2</literal>.</para>
</listitem>
</itemizedlist>
<para>Copy the following content to a file named <literal>/etc/ptp4l-G.8275.2.conf</literal>:</para>
<screen linenumbering="unnumbered"># Telecom G.8275.2 example configuration
[global]
domainNumber                    44
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             0
serverOnly                      0
hybrid_e2e                      1
inhibit_multicast_service       1
unicast_listen                  1
unicast_req_duration            60
logSyncInterval                 -5
logMinDelayReqInterval          -4
announceReceiptTimeout          2
#
# Customize the following for slave operation:
#
[unicast_master_table]
table_id                        1
logQueryInterval                2
UDPv4                           $PEER_IP_ADDRESS
[$IFNAME]
unicast_master_table            1</screen>
<para>Make sure to replace the following placeholders:</para>
<itemizedlist>
<listitem>
<para><literal>$PEER_IP_ADDRESS</literal> - the IP address of the next PTP node to communicate with, such as the master or boundary clock that will provide synchronization.</para>
</listitem>
<listitem>
<para><literal>$IFNAME</literal> - tells <literal>ptp4l</literal> what interface to use for PTP.</para>
</listitem>
</itemizedlist>
<para>Once the file has been created, it must be referenced, along with the name of the interface to use for PTP, in <literal>/etc/sysconfig/ptp4l</literal> for the daemon to start correctly. This can be done by changing the <literal>OPTIONS=</literal> line to:</para>
<screen language="shell" linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.2.conf --message_tag ptp-8275.2"</screen>
<para>More precisely:</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> requires the file name of the configuration file to use. In this case, it is <literal>/etc/ptp4l-G.8275.2.conf</literal>.</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> allows to better identify the ptp4l output in the system logs and is optional.</para>
</listitem>
</itemizedlist>
<para>Once the steps above are complete, the <literal>ptp4l</literal> daemon must be (re)started:</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>Check the synchronization status by observing the logs with:</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-configuration-of-phc2sys">
<title>Configuration of phc2sys</title>
<para>Although not required, it is recommended that you fully complete the configuration of <literal>ptp4l</literal> before moving to <literal>phc2sys</literal>.
<literal>phc2sys</literal> does not require a configuration file and its execution parameters can be solely controlled through the <literal>OPTIONS=</literal> variable present in <literal>/etc/sysconfig/ptp4l</literal>, in a similar fashion to <literal>ptp4l</literal>:</para>
<screen linenumbering="unnumbered">OPTIONS="-s $IFNAME -w"</screen>
<para>Where <literal>$IFNAME</literal> is the name of the interface already set up in ptp4l that will be used as the source for the system clock. This is used to identify the source PHC.</para>
</section>
</section>
<section xml:id="ptp-capi">
<title>Cluster API integration</title>
<para>Whenever a cluster is deployed through a management cluster and directed network provisioning, both the configuration file and the two configuration variables in <literal>/etc/sysconfig</literal> can be deployed on the host at provisioning time. Below is an excerpt from a cluster definition, focusing on a modified <literal>RKE2ControlPlane</literal> object that deploys the same G.8275.1 configuration file on all hosts:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  serverConfig:
    cni: canal
  agentConfig:
    format: ignition
    cisProfile: cis
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /etc/ptp4l-G.8275.1.conf
              overwrite: true
              contents:
                inline: |
                  # Telecom G.8275.1 example configuration
                  [global]
                  domainNumber                    24
                  priority2                       255
                  dataset_comparison              G.8275.x
                  G.8275.portDS.localPriority     128
                  G.8275.defaultDS.localPriority  128
                  maxStepsRemoved                 255
                  logAnnounceInterval             -3
                  logSyncInterval                 -4
                  logMinDelayReqInterval          -4
                  announceReceiptTimeout          3
                  serverOnly                      0
                  ptp_dst_mac                     01:80:C2:00:00:0E
                  network_transport               L2
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/ptp4l
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): ptp4l settings
                  ## Type:           string
                  ## Default:        "-i eth0 -f /etc/ptp4l.conf"
                  ## ServiceRestart: ptp4l
                  #
                  # Arguments when starting ptp4l(8).
                  #
                  OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/phc2sys
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): phc2sys settings
                  ## Type:           string
                  ## Default:        "-s eth0 -w"
                  ## ServiceRestart: phc2sys
                  #
                  # Arguments when starting phc2sys(8).
                  #
                  OPTIONS="-s $IFNAME -w"
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Besides other variables, the above definition must be completed with the interface name and with the other Cluster API objects, as described in <xref linkend="atip-automated-provisioning"/>.</para>
<note>
<itemizedlist>
<listitem>
<para>This approach is convenient only if the hardware in the cluster is uniform and the same configuration is needed on all hosts, interface name included.</para>
</listitem>
<listitem>
<para>Alternative approaches are possible and will be covered in future releases.</para>
</listitem>
</itemizedlist>
</note>
<para>At this point, your hosts should have a working and running PTP stack and will start negotiating their PTP role.</para>
</section>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>Fully automated directed network provisioning</title>
<section xml:id="id-introduction-3">
<title>Introduction</title>
<para>Directed network provisioning is a feature that allows you to automate the provisioning of downstream clusters. This feature is useful when you have many downstream clusters to provision, and you want to automate the process.</para>
<para>A management cluster (<xref linkend="atip-management-cluster"/>) automates deployment of the following components:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro RT</literal> as the OS. Depending on the use case, configurations like networking, storage, users and kernel arguments can be customized.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> as the Kubernetes cluster. The default <literal>CNI</literal> plug-in is <literal>Cilium</literal>. Depending on the use case, certain <literal>CNI</literal> plug-ins can be used, such as <literal>Cilium+Multus</literal>.</para>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal></para>
</listitem>
<listitem>
<para><literal>SUSE Security</literal></para>
</listitem>
<listitem>
<para><literal>MetalLB</literal> can be used as the load balancer for highly available multi-node clusters.</para>
</listitem>
</itemizedlist>
<note>
<para>For more information about <literal>SUSE Linux Micro</literal>, see <xref linkend="components-slmicro"/>
For more information about <literal>RKE2</literal>, see <xref linkend="components-rke2"/>
For more information about <literal>SUSE Storage</literal>, see <xref linkend="components-suse-storage"/>
For more information about <literal>SUSE Security</literal>, see <xref linkend="components-suse-security"/></para>
</note>
<para>The following sections describe the different directed network provisioning workflows and some additional features that can be added to the provisioning process:</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
<note>
<para>The following sections show how to prepare the different scenarios for the directed network provisioning workflow using SUSE Telco Cloud.
For examples of the different configurations options for deployment (incl. air-gapped environments, DHCP and DHCP-less networks, private container registries, etc.), see the <link xl:href="https://github.com/suse-edge/atip/tree/release-3.4/telco-examples/edge-clusters">SUSE Telco Cloud repository</link>.</para>
</note>
</section>
<section xml:id="eib-edge-image-connected">
<title>Prepare downstream cluster image for connected scenarios</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) is used to prepare a modified SLEMicro base image which is provisioned on downstream cluster hosts.</para>
<para>Much of the configuration via Edge Image Builder is possible, but in this guide, we cover the minimal configurations necessary to set up the downstream cluster.</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>Prerequisites for connected scenarios</title>
<itemizedlist>
<listitem>
<para>A container runtime such as <link xl:href="https://podman.io">Podman</link> or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link> is required to run Edge Image Builder.</para>
</listitem>
<listitem>
<para>The base image will be built using the following guide <xref linkend="guides-kiwi-builder-images"/> with the profile <literal>Base-SelfInstall</literal> (or <literal>Base-RT-SelfInstall</literal> for the Real-Time kernel). The process is the same for both architectures (x86-64 and aarch64).</para>
</listitem>
</itemizedlist>
<note>
<para>It is required to use a build host with the same architecture of the images being built. In other words, to build an <literal>aarch64</literal> image, it is required to use an <literal>aarch64</literal> build host, and vice-versa for <literal>x86-64</literal> (cross-builds are not supported at this time).</para>
</note>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>Image configuration for connected scenarios</title>
<para>When running Edge Image Builder, a directory is mounted from the host, so it is necessary to create a directory structure to store the configuration files used to define the target image.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> is the image definition file, see <xref linkend="quickstart-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The base image folder will contain the output raw image generated following the guide <xref linkend="guides-kiwi-builder-images"/> with the profile <literal>Base-SelfInstall</literal> (or <literal>Base-RT-SelfInstall</literal> for the Real-Time kernel) must be copied/moved under the <literal>base-images</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>network</literal> folder is optional, see <xref linkend="add-network-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts</literal> directory contains scripts to be run on first-boot:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal> script is required to resize the OS root partition on deployment</para>
</listitem>
<listitem>
<para><literal>02-performance.sh</literal> script is optional and can be used to configure the system for performance tuning.</para>
</listitem>
<listitem>
<para><literal>03-sriov.sh</literal> script is optional and can be used to configure the system for SR-IOV.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>The <literal>custom/files</literal> directory contains the <literal>performance-settings.sh</literal> and <literal>sriov-auto-filler.sh</literal> files to be copied to the image during the image creation process.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>Downstream cluster image definition file</title>
<para>The <literal>downstream-cluster-config.yaml</literal> file is the main configuration file for the downstream cluster image. The following is a minimal example for deployment via Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Where <literal>$SCC_REGISTRATION_CODE</literal> is the registration code copied from <link xl:href="https://scc.suse.com/">SUSE Customer Center</link>, and the package list contains <literal>jq</literal> which is required.</para>
<para><literal>$ROOT_PASSWORD</literal> is the encrypted password for the root user, which can be useful for test/debugging.  It can be generated with the <literal>openssl passwd -6 PASSWORD</literal> command</para>
<para>For the production environments, it is recommended to use the SSH keys that can be added to the users block replacing the <literal>$USERKEY1</literal> with the real SSH keys.</para>
<note>
<para><literal>arch: x86_64</literal> is the architecture of the image. For arm64 architecture, use <literal>arch: aarch64</literal>.</para>
<para><literal>net.ifnames=1</literal> enables <link xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable Network Interface Naming</link></para>
<para>This matches the default configuration for the metal3 chart, but the setting must match the configured chart <literal>predictableNicNames</literal> value.</para>
<para>Also note <literal>ignition.platform.id=openstack</literal> is mandatory, without this argument SLEMicro configuration via ignition will fail in the Metal<superscript>3</superscript> automated flow.</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Growfs script</title>
<para>Currently, a custom script (<literal>custom/scripts/01-fix-growfs.sh</literal>) is required to grow the file system to match the disk size on first-boot after provisioning. The <literal>01-fix-growfs.sh</literal> script contains the following information:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="add-custom-script-performance">
<title>Performance script</title>
<para>The following optional script (<literal>custom/scripts/02-performance.sh</literal>) can be used to configure the system for performance tuning:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para>The content of <literal>custom/files/performance-settings.sh</literal> is a script that can be used to configure the system for performance tuning and can be downloaded from the following <link xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">link</link>.</para>
</section>
<section xml:id="add-custom-script-sriov">
<title>SR-IOV script</title>
<para>The following optional script (<literal>custom/scripts/03-sriov.sh</literal>) can be used to configure the system for SR-IOV:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para>The content of <literal>custom/files/sriov-auto-filler.sh</literal> is a script that can be used to configure the system for SR-IOV and can be downloaded from the following <link xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">link</link>.</para>
<note>
<para>Add your own custom scripts to be executed during the provisioning process using the same approach.
For more information, see <xref linkend="quickstart-eib"/>.</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>Additional configuration for Telco workloads</title>
<para>To enable Telco features like <literal>dpdk</literal>, <literal>sr-iov</literal> or <literal>FEC</literal>, additional packages may be required as shown in the following example.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $user1Key1
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Where <literal>$SCC_REGISTRATION_CODE</literal> is the registration code copied from <link xl:href="https://scc.suse.com/">SUSE Customer Center</link>, and the package list contains the minimum packages to be used for the Telco profiles.</para>
<note>
<para><literal>arch: x86_64</literal> is the architecture of the image. For arm64 architecture, use <literal>arch: aarch64</literal>.</para>
</note>
</section>
<section xml:id="add-network-eib">
<title>Additional script for Advanced Network Configuration</title>
<para>If you need to configure static IPs or more advanced networking scenarios as described in <xref linkend="advanced-network-configuration"/>, the following additional configuration is required.</para>
<para>In the <literal>network</literal> folder, create the following <literal>configure-network.sh</literal> file - this consumes configuration drive data on first-boot, and configures the
host networking using the <link xl:href="https://github.com/suse-edge/nm-configurator">NM Configurator tool</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>Image creation</title>
<para>Once the directory structure is prepared following the previous sections, run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>This creates the output ISO image file named <literal>eibimage-output-telco.raw</literal>, based on the definition described above.</para>
<para>The output image must then be made available via a webserver, either the media-server container enabled via the Management Cluster Documentation (<xref linkend="metal3-media-server"/>)
or some other locally accessible server.  In the examples below, we refer to this server as <literal>imagecache.local:8080</literal></para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>Prepare downstream cluster image for air-gap scenarios</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) is used to prepare a modified SLEMicro base image which is provisioned on downstream cluster hosts.</para>
<para>Much of the configuration is possible with Edge Image Builder, but in this guide, we cover the minimal configurations necessary to set up the downstream cluster for air-gap scenarios.</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>Prerequisites for air-gap scenarios</title>
<itemizedlist>
<listitem>
<para>A container runtime such as <link xl:href="https://podman.io">Podman</link> or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link> is required to run Edge Image Builder.</para>
</listitem>
<listitem>
<para>The base image will be built using the following guide <xref linkend="guides-kiwi-builder-images"/> with the profile <literal>Base-SelfInstall</literal> (or <literal>Base-RT-SelfInstall</literal> for the Real-Time kernel). The process is the same for both architectures (x86-64 and aarch64).</para>
</listitem>
<listitem>
<para>If you want to use SR-IOV or any other workload which require a container image, a local private registry must be deployed and already configured (with/without TLS and/or authentication). This registry will be used to store the images and the helm chart OCI images.</para>
</listitem>
</itemizedlist>
<note>
<para>It is required to use a build host with the same architecture of the images being built. In other words, to build an <literal>aarch64</literal> image, it is required to use an <literal>aarch64</literal> build host, and vice-versa for <literal>x86-64</literal> (cross-builds are not supported at this time).</para>
</note>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>Image configuration for air-gap scenarios</title>
<para>When running Edge Image Builder, a directory is mounted from the host, so it is necessary to create a directory structure to store the configuration files used to define the target image.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal> is the image definition file, see <xref linkend="quickstart-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The base image folder will contain the output raw image generated following the guide <xref linkend="guides-kiwi-builder-images"/> with the profile <literal>Base-SelfInstall</literal> (or <literal>Base-RT-SelfInstall</literal> for the Real-Time kernel) must be copied/moved under the <literal>base-images</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>network</literal> folder is optional, see <xref linkend="add-network-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts</literal> directory contains scripts to be run on first-boot:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal> script is required to resize the OS root partition on deployment.</para>
</listitem>
<listitem>
<para><literal>02-airgap.sh</literal> script is required to copy the images to the right place during the image creation process for air-gapped environments.</para>
</listitem>
<listitem>
<para><literal>03-performance.sh</literal> script is optional and can be used to configure the system for performance tuning.</para>
</listitem>
<listitem>
<para><literal>04-sriov.sh</literal> script is optional and can be used to configure the system for SR-IOV.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>The <literal>custom/files</literal> directory contains the <literal>rke2</literal> and the <literal>cni</literal> images to be copied to the image during the image creation process. Also, the optional <literal>performance-settings.sh</literal> and <literal>sriov-auto-filler.sh</literal> files can be included.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>Downstream cluster image definition file</title>
<para>The <literal>downstream-cluster-airgap-config.yaml</literal> file is the main configuration file for the downstream cluster image and the content has been described in the previous section (<xref linkend="add-telco-feature-eib"/>).</para>
</section>
<section xml:id="id-growfs-script">
<title>Growfs script</title>
<para>Currently, a custom script (<literal>custom/scripts/01-fix-growfs.sh</literal>) is required to grow the file system to match the disk size on first-boot after provisioning. The <literal>01-fix-growfs.sh</literal> script contains the following information:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>Air-gap script</title>
<para>The following script (<literal>custom/scripts/02-airgap.sh</literal>) is required to copy the images to the right place during the image creation process:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="add-custom-script-performance2">
<title>Performance script</title>
<para>The following optional script (<literal>custom/scripts/03-performance.sh</literal>) can be used to configure the system for performance tuning:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para>The content of <literal>custom/files/performance-settings.sh</literal> is a script that can be used to configure the system for performance tuning and can be downloaded from the following <link xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">link</link>.</para>
</section>
<section xml:id="add-custom-script-sriov2">
<title>SR-IOV script</title>
<para>The following optional script (<literal>custom/scripts/04-sriov.sh</literal>) can be used to configure the system for SR-IOV:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para>The content of <literal>custom/files/sriov-auto-filler.sh</literal> is a script that can be used to configure the system for SR-IOV and can be downloaded from the following <link xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">link</link>.</para>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>Custom files for air-gap scenarios</title>
<para>The <literal>custom/files</literal> directory contains the <literal>rke2</literal> and the <literal>cni</literal> images to be copied to the image during the image creation process.
To easily generate the images, prepare them locally using following <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">script</link> and the list of images <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-release-rke2-images.txt">here</link> to generate the artifacts required to be included in <literal>custom/files</literal>.
Also, you can download the latest <literal>rke2-install</literal> script from <link xl:href="https://get.rke2.io/">here</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>After downloading the images, the directory structure should look like this:</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>Preload your private registry with images required for air-gap scenarios and SR-IOV (optional)</title>
<para>If you want to use SR-IOV in your air-gap scenario or any other workload images, you must preload your local private registry with the images following the next steps:</para>
<itemizedlist>
<listitem>
<para>Download, extract, and push the helm-chart OCI images to the private registry</para>
</listitem>
<listitem>
<para>Download, extract, and push the rest of images required to the private registry</para>
</listitem>
</itemizedlist>
<para>The following scripts can be used to download, extract, and push the images to the private registry. We will show an example to preload the SR-IOV images, but you can also use the same approach to preload any other custom images:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Preload with helm-chart OCI images for SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>You must create a list with the helm-chart OCI images required:</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:304.0.2+up1.5.0
edge/sriov-crd-chart:304.0.2+up1.5.0
EOF</screen>
</listitem>
<listitem>
<para>Generate a local tarball file using the following <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-oci-artefacts.sh">script</link> and the list created above:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/charts/sriov-network-operator:304.0.2+up1.5.0
Pulled: registry.suse.com/edge/charts/sriov-crd:304.0.2+up1.5.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-304.0.2+up1.5.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-304.0.2+up1.5.0.tgz</screen>
</listitem>
<listitem>
<para>Upload your tarball file to your private registry (e.g. <literal>myregistry:5000</literal>) using the following <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-oci-artefacts.sh">script</link> to preload your registry with the helm chart OCI images downloaded in the previous step:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Preload with the rest of the images required for SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>In this case, we must include the `sr-iov container images for telco workloads (e.g. as a reference, you could get them from <link xl:href="https://github.com/suse-edge/charts/blob/main/charts/sriov-network-operator/1.5.0/values.yaml">helm-chart values</link>)</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</screen>
</listitem>
<listitem>
<para>Using the following <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">script</link> and the list created above, you must generate locally the tarball file with the images required:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</screen>
</listitem>
<listitem>
<para>Upload your tarball file to your private registry (e.g. <literal>myregistry:5000</literal>) using the following <link xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-images.sh">script</link> to preload your private registry with the images downloaded in the previous step:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>Image creation for air-gap scenarios</title>
<para>Once the directory structure is prepared following the previous sections, run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>This creates the output ISO image file named <literal>eibimage-output-telco.raw</literal>, based on the definition described above.</para>
<para>The output image must then be made available via a webserver, either the media-server container enabled via the Management Cluster Documentation (<xref linkend="metal3-media-server"/>)
or some other locally accessible server.  In the examples below, we refer to this server as <literal>imagecache.local:8080</literal>.</para>
</section>
</section>
<section xml:id="single-node">
<title>Downstream cluster provisioning with Directed network provisioning (single-node)</title>
<para>This section describes the workflow used to automate the provisioning of a single-node downstream cluster using directed network provisioning.
This is the simplest way to automate the provisioning of a downstream cluster.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal>, as described in the previous section (<xref linkend="eib-edge-image-connected"/>), with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on this section (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>The management server created and available to be used on the following sections. For more information, refer to the Management Cluster section <xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Workflow</emphasis></para>
<para>The following diagram shows the workflow used to automate the provisioning of a single-node downstream cluster using directed network provisioning:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="atip-automated-singlenode1.png" width="100%"/>
</imageobject>
<textobject><phrase>atip automated singlenode1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>There are two different steps to automate the provisioning of a single-node downstream cluster using directed network provisioning:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Enroll the bare-metal host to make it available for the provisioning process.</para>
</listitem>
<listitem>
<para>Provision the bare-metal host to install and configure the operating system and the Kubernetes cluster.</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">Enroll the bare-metal host</emphasis></para>
<para>The first step is to enroll the new bare-metal host in the management cluster to make it available to be provisioned.
To do that, the following file (<literal>bmh-example.yaml</literal>) has to be created in the management cluster, to specify the <literal>BMC</literal> credentials to be used and the <literal>BaremetalHost</literal> object to be enrolled:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>where:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal> — The user name for the <literal>BMC</literal> of the new bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal> — The password for the <literal>BMC</literal> of the new bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal> — The <literal>MAC</literal> address of the new bare-metal host to be used.</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal> — The <literal>URL</literal> for the bare-metal host <literal>BMC</literal> (for example, <literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>). To learn more about the different options available depending on your hardware provider, check the following <link xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">link</link>.</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>Architecture must be either <literal>x86_64</literal> or <literal>aarch64</literal>, depending on the architecture of the bare-metal host to be enrolled.</para>
</listitem>
<listitem>
<para>If no network configuration for the host has been specified, either at image build time or through the <literal>BareMetalHost</literal> definition, an autoconfiguration mechanism (DHCP, DHCPv6, SLAAC) will be used. For more details or complex configurations, check the <xref linkend="advanced-network-configuration"/>.</para>
</listitem>
</itemizedlist>
</note>
<para>Once the file is created, the following command has to be executed in the management cluster to start enrolling the new bare-metal host in the management cluster:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>The new bare-metal host object will be enrolled, changing its state from registering to inspecting and available. The changes can be checked using the following command:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para>The <literal>BaremetalHost</literal> object is in the <literal>registering</literal> state until the <literal>BMC</literal> credentials are validated. Once the credentials are validated, the <literal>BaremetalHost</literal> object changes its state to <literal>inspecting</literal>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. Check the information using the following command: <literal>kubectl get bmh -o yaml</literal>.</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">Provision step</emphasis></para>
<para>Once the bare-metal host is enrolled and available, the next step is to provision the bare-metal host to install and configure the operating system and the Kubernetes cluster.
To do that, the following file (<literal>capi-provisioning-example.yaml</literal>) has to be created in the management-cluster with the following information (the <literal>capi-provisioning-example.yaml</literal> can be generated by joining the following blocks).</para>
<note>
<para>Only values between <literal>$\{…​\}</literal> must be replaced with the real values.</para>
</note>
<para>The following block is the cluster definition, where the networking can be configured using the <literal>pods</literal> and the <literal>services</literal> blocks. Also, it contains the references to the control plane and the infrastructure (using the <literal>Metal<superscript>3</superscript></literal> provider) objects to be used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:bad:cafe::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:bad:bad:cafe::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<note>
<itemizedlist>
<listitem>
<para>Both single-stack and dual-stack deployments are possible, remove the IPv6 CIDRs from the above definition for an IPv4 only cluster.</para>
</listitem>
<listitem>
<para>Single-stack IPv6 deployments are in tech preview status and not yet officially supported.</para>
</listitem>
</itemizedlist>
</note>
<para>The <literal>Metal3Cluster</literal> object specifies the control-plane endpoint (replacing the <literal>${DOWNSTREAM_CONTROL_PLANE_IPV4}</literal>) to be configured and the <literal>noCloudProvider</literal> because a bare-metal node is used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para>The <literal>RKE2ControlPlane</literal> object specifies the control-plane configuration to be used and the <literal>Metal3MachineTemplate</literal> object specifies the control-plane image to be used.
Also, it contains the information about the number of replicas to be used (in this case, one) and the <literal>CNI</literal> plug-in to be used (in this case, <literal>Cilium</literal>).
The agentConfig block contains the <literal>Ignition</literal> format to be used and the <literal>additionalUserData</literal> to be used to configure the <literal>RKE2</literal> node with information like a systemd named <literal>rke2-preinstall.service</literal> to replace automatically the <literal>BAREMETALHOST_UUID</literal> and <literal>node-name</literal> during the provisioning process using the Ironic information.
To enable multus with cilium a file is created in the <literal>rke2</literal> server manifests directory named <literal>rke2-cilium-config.yaml</literal> with the configuration to be used.
The last block of information contains the Kubernetes version to be used. <literal>${RKE2_VERSION}</literal> is the version of <literal>RKE2</literal> to be used replacing this value (for example, <literal>v1.33.3+rke2r1</literal>).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>The <literal>Metal3MachineTemplate</literal> object specifies the following information:</para>
<itemizedlist>
<listitem>
<para>The <literal>dataTemplate</literal> to be used as a reference to the template.</para>
</listitem>
<listitem>
<para>The <literal>hostSelector</literal> to be used matching with the label created during the enrollment process.</para>
</listitem>
<listitem>
<para>The <literal>image</literal> to be used as a reference to the image generated using <literal>EIB</literal> on the previous section (<xref linkend="eib-edge-image-connected"/>), and the <literal>checksum</literal> and <literal>checksumType</literal> to be used to validate the image.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para>The <literal>Metal3DataTemplate</literal> object specifies the <literal>metaData</literal> for the downstream cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Once the file is created by joining the previous blocks, the following command must be executed in the management cluster to start provisioning the new bare-metal host:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>Downstream cluster provisioning with Directed network provisioning (multi-node)</title>
<para>This section describes the workflow used to automate the provisioning of a multi-node downstream cluster using directed network provisioning and <literal>MetalLB</literal> as a load-balancer strategy.
This is the simplest way to automate the provisioning of a downstream cluster. The following diagram shows the workflow used to automate the provisioning of a multi-node downstream cluster using directed network provisioning and <literal>MetalLB</literal>.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal>, as described in the previous section (<xref linkend="eib-edge-image-connected"/>), with the minimal configuration to set up the downstream cluster has to be located in the management cluster exactly on the path you configured on this section (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>The management server created and available to be used on the following sections. For more information, refer to the Management Cluster section: <xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Workflow</emphasis></para>
<para>The following diagram shows the workflow used to automate the provisioning of a multi-node downstream cluster using directed network provisioning:</para>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="atip-automate-multinode1.png" width="100%"/>
</imageobject>
<textobject><phrase>atip automate multinode1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>Enroll the three bare-metal hosts to make them available for the provisioning process.</para>
</listitem>
<listitem>
<para>Provision the three bare-metal hosts to install and configure the operating system and the Kubernetes cluster using <literal>MetalLB</literal>.</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">Enroll the bare-metal hosts</emphasis></para>
<para>The first step is to enroll the three bare-metal hosts in the management cluster to make them available to be provisioned.
To do that, the following files (<literal>bmh-example-node1.yaml</literal>, <literal>bmh-example-node2.yaml</literal> and <literal>bmh-example-node3.yaml</literal>) must be created in the management cluster, to specify the <literal>BMC</literal> credentials to be used and the <literal>BaremetalHost</literal> object to be enrolled in the management cluster.</para>
<note>
<itemizedlist>
<listitem>
<para>Only the values between <literal>$\{…​\}</literal> have to be replaced with the real values.</para>
</listitem>
<listitem>
<para>We will walk you through the process for only one host. The same steps apply to the other two nodes.</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>Where:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal> — The username for the BMC of the first bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal> — The password for the BMC of the first bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal> — The MAC address of the first bare-metal host to be used.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal> — The URL for the first bare-metal host BMC (for example, <literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>). The host part of the URL can be an IP address (v4 or v6) or a domain name, where the existing infrastructure allows. To learn more about the different options available depending on your hardware provider, check the following <link xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">link</link>.</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>If no network configuration for the host has been specified, either at image build time or through the <literal>BareMetalHost</literal> definition, an autoconfiguration mechanism (DHCP, DHCPv6, SLAAC) will be used. For more details or complex configurations, check the <xref linkend="advanced-network-configuration"/>.</para>
</listitem>
<listitem>
<para>Single-stack IPv6 clusters are in tech preview status and not yet officially supported.</para>
</listitem>
<listitem>
<para>Architecture must be either <literal>x86_64</literal> or <literal>aarch64</literal>, depending on the architecture of the bare-metal host to be enrolled.</para>
</listitem>
<listitem>
<para>All modern servers come with a dual-stack capable BMC, however IPv6 support (and possibly the option of using hostnames for the VirtualMedia capability) should be verified before use in production in a dual-stack environment.</para>
</listitem>
</itemizedlist>
</note>
<para>Once the file is created, the following command must be executed in the management cluster to start enrolling the bare-metal hosts in the management cluster:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>The new bare-metal host objects are enrolled, changing their state from registering to inspecting and available. The changes can be checked using the following command:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para>The <literal>BaremetalHost</literal> object is in the <literal>registering</literal> state until the <literal>BMC</literal> credentials are validated. Once the credentials are validated, the <literal>BaremetalHost</literal> object changes its state to <literal>inspecting</literal>, and this step could take some time depending on the hardware (up to 20 minutes). During the inspecting phase, the hardware information is retrieved and the Kubernetes object is updated. Check the information using the following command: <literal>kubectl get bmh -o yaml</literal>.</para>
</note>
<para><emphasis role="strong">Provision step</emphasis></para>
<para>Once the three bare-metal hosts are enrolled and available, the next step is to provision the bare-metal hosts to install and configure the operating system and the Kubernetes cluster, creating a load balancer to manage them.
To do that, the following file (<literal>capi-provisioning-example.yaml</literal>) must be created in the management cluster with the following information (the `capi-provisioning-example.yaml can be generated by joining the following blocks).</para>
<note>
<itemizedlist>
<listitem>
<para>Only values between <literal>$\{…​\}</literal> must be replaced with the real values.</para>
</listitem>
<listitem>
<para>The <literal>VIP</literal> address is a reserved IP address that is not assigned to any node and is used to configure the load balancer. In a dual-stack cluster, both an IPv4 and IPv6 can be specified, but in the following examples priority will be given to the IPv4 address.</para>
</listitem>
</itemizedlist>
</note>
<para>Below is the cluster definition, where the cluster network can be configured using the <literal>pods</literal> and the <literal>services</literal> blocks. Also, it contains the references to the control plane and the infrastructure (using the <literal>Metal<superscript>3</superscript></literal> provider) objects to be used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:1234:4321::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:5678:8765:4321::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<note>
<para>Both single-stack and dual-stack deployments are possible, remove the IPv6 CIDRs and IPv6 VIP addresses (in the subsequent sections) for an IPv4 only cluster</para>
</note>
<para>The <literal>Metal3Cluster</literal> object specifies the control-plane endpoint that uses the <literal>VIP</literal> address already reserved (replacing the <literal>${EDGE_VIP_ADDRESS_IPV4}</literal>) to be configured and the <literal>noCloudProvider</literal> because the three bare-metal nodes are used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para>The <literal>RKE2ControlPlane</literal> object specifies the control-plane configuration to be used, and the <literal>Metal3MachineTemplate</literal> object specifies the control-plane image to be used.</para>
<itemizedlist>
<listitem>
<para>The number of replicas to be used (in this case, three).</para>
</listitem>
<listitem>
<para>The advertisement mode to be used by the Load Balancer (<literal>address</literal> uses the L2 implementation), as well as the address to be used (replacing the <literal>${EDGE_VIP_ADDRESS}</literal> with the <literal>VIP</literal> address).</para>
</listitem>
<listitem>
<para>The <literal>serverConfig</literal> with the <literal>CNI</literal> plug-in to be used (in this case, <literal>Cilium</literal>), and the additional <literal>VIP</literal> address(es) and name(s) to be listed under <literal>tlsSan</literal>.</para>
</listitem>
<listitem>
<para>The agentConfig block contains the <literal>Ignition</literal> format to be used and the <literal>additionalUserData</literal> to be used to configure the <literal>RKE2</literal> node with information like:</para>
<itemizedlist>
<listitem>
<para>The systemd service named <literal>rke2-preinstall.service</literal> to replace automatically the <literal>BAREMETALHOST_UUID</literal> and <literal>node-name</literal> during the provisioning process using the Ironic information.</para>
</listitem>
<listitem>
<para>The <literal>storage</literal> block which contains the Helm charts to be used to install the <literal>MetalLB</literal> and the <literal>endpoint-copier-operator</literal>.</para>
</listitem>
<listitem>
<para>The <literal>metalLB</literal> custom resource file with the <literal>IPaddressPool</literal> and the <literal>L2Advertisement</literal> to be used (replacing <literal>${EDGE_VIP_ADDRESS_IPV4}</literal> with the <literal>VIP</literal> address).</para>
</listitem>
<listitem>
<para>The <literal>endpoint-svc.yaml</literal> file to be used to configure the <literal>kubernetes-vip</literal> service to be used by the <literal>MetalLB</literal> to manage the <literal>VIP</literal> address.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>The last block of information contains the Kubernetes version to be used. The <literal>${RKE2_VERSION}</literal> is the version of <literal>RKE2</literal> to be used replacing this value (for example, <literal>v1.33.3+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS_IPV4}
      - ${EDGE_VIP_ADDRESS_IPV6}
      - https://${EDGE_VIP_ADDRESS_IPV4}.sslip.io
      - https://${EDGE_VIP_ADDRESS_IPV6}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS_IPV4}/32
                      - ${EDGE_VIP_ADDRESS_IPV6}/128
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ipFamilyPolicy: PreferDualStack
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "Node-multinode-cluster"</screen>
<para>The <literal>Metal3MachineTemplate</literal> object specifies the following information:</para>
<itemizedlist>
<listitem>
<para>The <literal>dataTemplate</literal> to be used as a reference to the template.</para>
</listitem>
<listitem>
<para>The <literal>hostSelector</literal> to be used matching with the label created during the enrollment process.</para>
</listitem>
<listitem>
<para>The <literal>image</literal> to be used as a reference to the image generated using <literal>EIB</literal> on the previous section (<xref linkend="eib-edge-image-connected"/>), and <literal>checksum</literal> and <literal>checksumType</literal> to be used to validate the image.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para>The <literal>Metal3DataTemplate</literal> object specifies the <literal>metaData</literal> for the downstream cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>The following yaml files are an example configuration for the worker nodes.</para>
<para>A <literal>MachineDeployment</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: multinode-cluster
    nodepool: nodepool-0
  name: multinode-cluster-workers
  namespace: default
spec:
  clusterName: multinode-cluster
  replicas: 3
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: multinode-cluster
      nodepool: nodepool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: multinode-cluster
        nodepool: nodepool-0
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: RKE2ConfigTemplate
          name: multinode-cluster-workers
      clusterName: multinode-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: multinode-cluster-workers
      nodeDrainTimeout: 0s
      version: ${RKE2_VERSION}</screen>
<para>The RKE2ConfigTemplate` object specifies the configuration template to be used for multinode cluster worker nodes.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: RKE2ConfigTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "Node-multinode-cluster-worker"
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target</screen>
<para>The <literal>Metal3MachineTemplate</literal> object contain references to <literal>dataTemplate</literal>, <literal>hostSelector</literal>, and <literal>image</literal> for the worker nodes:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw</screen>
<para>The <literal>Metal3DataTemplate</literal> object specifies the <literal>metaData</literal> for the downstream cluster for the worker nodes:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-workers-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Once the file is created by joining the previous blocks, run the following command in the management cluster to start provisioning the new three bare-metal hosts:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>Advanced Network Configuration</title>
<para>The directed network provisioning workflow allows for specific network configurations in downstream clusters, such as static IPs, bonding, VLANs, IPv6, etc.</para>
<para>The following sections describe the additional steps required to enable provisioning downstream clusters using advanced network configuration.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal> has to include the network folder and the script following this section (<xref linkend="add-network-eib"/>).</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuration</emphasis></para>
<para>Before proceeding refer to one of the following sections for guidance on the steps required to enroll and provision the host(s):</para>
<itemizedlist>
<listitem>
<para>Downstream cluster provisioning with Directed network provisioning (single-node) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Downstream cluster provisioning with Directed network provisioning (multi-node) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>Any advanced network configuration must be applied at enrollment time through the <literal>BareMetalHost</literal> host definition and an associated Secret containing an <literal>nmstate</literal> formatted <literal>networkData</literal> block. The following example file defines a secret containing the required <literal>networkData</literal> that requests a static <literal>IP</literal> and <literal>VLAN</literal> for the downstream cluster host:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      identifier: mac-address
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>As you can see, the example shows the configuration to enable the interface with static IPs, as well as the configuration to enable the VLAN using the base interface, once the following variables are replaced with the actual values, according to your infrastructure:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal> — The control-plane interface to be used for the edge cluster (for example, <literal>eth0</literal>). Including <literal>identifier: mac-address</literal> the naming is inspected automatically by the MAC address so any interface name can be used.</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP}</literal> — The IP address to be used as an endpoint for the edge cluster (must match with the kubeapi-server endpoint).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX}</literal> — The CIDR to be used for the edge cluster (for example, <literal>24</literal> if you want <literal>/24</literal> or <literal>255.255.255.0</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY}</literal> — The gateway to be used for the edge cluster (for example, <literal>192.168.100.1</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_MAC}</literal> — The MAC address to be used for the control-plane interface (for example, <literal>00:0c:29:3e:3e:3e</literal>).</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal> — The DNS to be used for the edge cluster (for example, <literal>192.168.100.2</literal>).</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal> — The VLAN ID to be used for the edge cluster (for example, <literal>100</literal>).</para>
</listitem>
</itemizedlist>
<para>Any other <literal>nmstate</literal>-compliant definition can be used to configure the network for the downstream cluster to adapt to the specific requirements. For example, it is possible to specify a static dual-stack configuration:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: ${CONTROLPLANE_MAC}
      ipv4:
        enabled: true
        dhcp: false
        address:
        - ip: ${CONTROLPLANE_IP_V4}
          prefix-length: ${CONTROLPLANE_PREFIX_V4}
      ipv6:
        enabled: true
        dhcp: false
        autoconf: false
        address:
        - ip: ${CONTROLPLANE_IP_V6}
          prefix-length: ${CONTROLPLANE_PREFIX_V6}
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V4}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
      - destination: ::/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V6}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
    dns-resolver:
      config:
        server:
        - ${DNS_SERVER_V4}
        - ${DNS_SERVER_V6}</screen>
<para>As for the previous example, replace the following variables with actual values, according to your infrastructure:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_IP_V4}</literal> - the IPv4 address to assign to the host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V4}</literal> - the IPv4 prefix of the network to which the host IP belongs</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP_V6}</literal> - the IPv6 address to assign to the host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V6}</literal> - the IPv6 prefix of the network to which the host IP belongs</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V4}</literal> - the IPv4 address of the gateway for the traffic matching the default route</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V6}</literal> - the IPv6 address of the gateway for the traffic matching the default route</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal> - the name of the interface to assign the addresses to and to use for egress traffic matching the default route, for both IPv4 and IPv6</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER_V4}</literal> and/or <literal>${DNS_SERVER_V6}</literal> - the IP address(es) of the DNS server(s) to use, which can be specified as single or multiple entries. Both IPv4 and/or IPv6 addresses are supported</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>You can refer to <link xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/edge-clusters">SUSE Telco Cloud examples repo</link> for more complex examples, including IPv6 only and dual-stack configurations.</para>
</listitem>
<listitem>
<para>Single-stack IPv6 deployments are in tech preview status and not yet officially supported.</para>
</listitem>
</itemizedlist>
</note>
<para>Lastly, regardless of the network configuration details, ensure that the secret is referenced by appending <literal>preprovisioningNetworkDataName</literal> to the <literal>BaremetalHost</literal> object to successfully enroll the host in the management cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<itemizedlist>
<listitem>
<para>If you need to deploy a multi-node cluster, the same process must be done for each node.</para>
</listitem>
<listitem>
<para>The <literal>Metal3DataTemplate</literal>, <literal>networkData</literal> and <literal>Metal3 IPAM</literal> are currently not supported; only the configuration via static secrets is fully supported.</para>
</listitem>
<listitem>
<para>Architecture must be either <literal>x86_64</literal> or <literal>aarch64</literal>, depending on the architecture of the bare-metal host to be enrolled.</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="add-telco">
<title>Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)</title>
<para>The directed network provisioning workflow allows to automate the Telco features to be used in the downstream clusters to run Telco workloads on top of those servers.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal>, as described in the previous section (<xref linkend="eib-edge-image-connected"/>),  has to be located in the management cluster exactly on the path you configured on this section (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>The image generated using <literal>EIB</literal> has to include the specific Telco packages following this section (<xref linkend="add-telco-feature-eib"/>).</para>
</listitem>
<listitem>
<para>The management server created and available to be used on the following sections. For more information, refer to the Management Cluster section: <xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuration</emphasis></para>
<para>Use the following two sections as the base to enroll and provision the hosts:</para>
<itemizedlist>
<listitem>
<para>Downstream cluster provisioning with Directed network provisioning (single-node) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Downstream cluster provisioning with Directed network provisioning (multi-node) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>The Telco features covered in this section are the following:</para>
<itemizedlist>
<listitem>
<para>DPDK and VFs creation</para>
</listitem>
<listitem>
<para>SR-IOV and VFs allocation to be used by the workloads</para>
</listitem>
<listitem>
<para>CPU isolation and performance tuning</para>
</listitem>
<listitem>
<para>Huge pages configuration</para>
</listitem>
<listitem>
<para>Kernel parameters tuning</para>
</listitem>
</itemizedlist>
<note>
<para>For more information about the Telco features, see <xref linkend="atip-features"/>.</para>
</note>
<para>The changes required to enable the Telco features shown above are all inside the <literal>RKE2ControlPlane</literal> block in the provision file <literal>capi-provisioning-example.yaml</literal>. The rest of the information inside the file <literal>capi-provisioning-example.yaml</literal> is the same as the information provided in the provisioning section (<xref linkend="single-node-provision"/>).</para>
<para>To make the process clear, the changes required on that block (<literal>RKE2ControlPlane</literal>) to enable the Telco features are the following:</para>
<itemizedlist>
<listitem>
<para>The <literal>preRKE2Commands</literal> to be used to execute the commands before the <literal>RKE2</literal> installation process. In this case, use the <literal>modprobe</literal> command to enable the <literal>vfio-pci</literal> and the <literal>SR-IOV</literal> kernel modules.</para>
</listitem>
<listitem>
<para>The ignition file <literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal> to be used to define the interfaces, drivers and the number of <literal>VFs</literal> to be created and exposed to the workloads.</para>
<itemizedlist>
<listitem>
<para>The values inside the config map <literal>sriov-custom-auto-config</literal> are the only values to be replaced with real values.</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal> — The resource name to be used for the first <literal>PF</literal> interface (for example, <literal>sriov-resource-du1</literal>). It is added to the prefix <literal>rancher.io</literal> to be used as a label to be used by the workloads (for example, <literal>rancher.io/sriov-resource-du1</literal>).</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal> — The name of the first <literal>PF</literal> interface to be used (for example, <literal>eth0</literal>).</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal> — The name of the first physical function <literal>PF</literal> to be used. Generate more complex filters using this (for example, <literal>eth0#2-5</literal>).</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal> — The driver name to be used for the first <literal>VF</literal> interface (for example, <literal>vfio-pci</literal>).</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal> — The number of <literal>VFs</literal> to be created for the first <literal>PF</literal> interface (for example, <literal>8</literal>).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>The <literal>/var/sriov-auto-filler.sh</literal> to be used as a translator between the high-level config map <literal>sriov-custom-auto-config</literal> and the <literal>sriovnetworknodepolicy</literal> which contains the low-level hardware information. This script has been created to abstract the user from the complexity to know in advance the hardware information. No changes are required in this file, but it should be present if we need to enable <literal>sr-iov</literal> and create <literal>VFs</literal>.</para>
</listitem>
<listitem>
<para>The kernel arguments to be used to enable the following features:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Parameter</para></entry>
<entry align="left" valign="top"><para>Value</para></entry>
<entry align="left" valign="top"><para>Description</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Isolate the cores 1-30 and 33-62.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to skew the timer interrupts across the isolated CPUs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the timer tick on a single CPU when the system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the RCU callbacks on a single CPU when the system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the interrupts on a single CPU when the system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>Minimizes the latency of exiting the idle state.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Allows to use vfio for the dpdk interfaces.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Enables the use of vfio for VFs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Allows to set the size of huge pages to 1 G.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>Number of huge pages defined before.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Default value to enable huge pages.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Disables the watchdog.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>Disables the NMI watchdog.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>The following systemd services are used to enable the following:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> to replace automatically the <literal>BAREMETALHOST_UUID</literal> and <literal>node-name</literal> during the provisioning process using the Ironic information.</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> to enable the isolation cores of the <literal>CPU</literal> (for example, <literal>1-30,33-62</literal>).</para>
</listitem>
<listitem>
<para><literal>performance-settings.service</literal> to enable the CPU performance tuning.</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> to install the <literal>sriov</literal> Helm chart, wait until custom resources are created and run the <literal>/var/sriov-auto-filler.sh</literal> to replace the values in the config map <literal>sriov-custom-auto-config</literal> and create the <literal>sriovnetworknodepolicy</literal> to be used by the workloads.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>The <literal>${RKE2_VERSION}</literal> is the version of <literal>RKE2</literal> to be used replacing this value (for example, <literal>v1.33.3+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<para>With all these changes mentioned, the <literal>RKE2ControlPlane</literal> block in the <literal>capi-provisioning-example.yaml</literal> will look like the following:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-crd
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-network-operator
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Once the file is created by joining the previous blocks, the following command must be executed in the management cluster to start provisioning the new downstream cluster using the Telco features:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>Private registry</title>
<para>It is possible to configure a private registry as a mirror for images used by workloads.</para>
<para>To do this we create the secret containing the information about the private registry to be used by the downstream cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para>The <literal>tls.crt</literal>, <literal>tls.key</literal> and <literal>ca.crt</literal> are the certificates to be used to authenticate the private registry. The <literal>username</literal> and <literal>password</literal> are the credentials to be used to authenticate the private registry.</para>
<note>
<para>The <literal>tls.crt</literal>, <literal>tls.key</literal>, <literal>ca.crt</literal> , <literal>username</literal> and <literal>password</literal> have to be encoded in base64 format before to be used in the secret.</para>
</note>
<para>With all these changes mentioned, the <literal>RKE2ControlPlane</literal> block in the <literal>capi-provisioning-example.yaml</literal> will look like the following:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Where the <literal>registry.example.com</literal> is the example name of the private registry to be used by the downstream cluster, and it should be replaced with the real values.</para>
</section>
<section xml:id="airgap-deployment">
<title>Downstream cluster provisioning in air-gapped scenarios</title>
<para>The directed network provisioning workflow allows to automate the provisioning of downstream clusters in air-gapped scenarios.</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>Requirements for air-gapped scenarios</title>
<orderedlist numeration="arabic">
<listitem>
<para>The <literal>raw</literal> image generated using <literal>EIB</literal> must include the specific container images (helm-chart OCI and container images) required to run the downstream cluster in an air-gapped scenario. For more information, refer to this section (<xref linkend="eib-edge-image-airgap"/>).</para>
</listitem>
<listitem>
<para>In case of using SR-IOV or any other custom workload, the images required to run the workloads must be preloaded in your private registry following the preload private registry section (<xref linkend="preload-private-registry"/>).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>Enroll the bare-metal hosts in air-gap scenarios</title>
<para>The process to enroll the bare-metal hosts in the management cluster is the same as described in the previous section (<xref linkend="enroll-bare-metal-host"/>).</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>Provision the downstream cluster in air-gap scenarios</title>
<para>There are some important changes required to provision the downstream cluster in air-gapped scenarios:</para>
<orderedlist numeration="arabic">
<listitem>
<para>The <literal>RKE2ControlPlane</literal> block in the <literal>capi-provisioning-example.yaml</literal> file must include the <literal>spec.agentConfig.airGapped: true</literal> directive.</para>
</listitem>
<listitem>
<para>The private registry configuration must be included in the <literal>RKE2ControlPlane</literal> block in the <literal>capi-provisioning-airgap-example.yaml</literal> file following the private registry section (<xref linkend="atip-private-registry"/>).</para>
</listitem>
<listitem>
<para>If you are using SR-IOV or any other <literal>AdditionalUserData</literal> configuration (combustion script) which requires the helm-chart installation, you must modify the content to reference the private registry instead of using the public registry.</para>
</listitem>
</orderedlist>
<para>The following example shows the SR-IOV configuration in the <literal>AdditionalUserData</literal> block in the <literal>capi-provisioning-airgap-example.yaml</literal> file with the modifications required to reference the private registry</para>
<itemizedlist>
<listitem>
<para>Private Registry secrets references</para>
</listitem>
<listitem>
<para>Helm-Chart definition using the private registry instead of the public OCI images.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=1800
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --timeout=30m --all ; do sleep 10 ; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>Lifecycle actions</title>
<para>This section covers the lifecycle management actions for clusters deployed via SUSE Telco Cloud.</para>
<section xml:id="id-management-cluster-upgrades">
<title>Management cluster upgrades</title>
<para>The upgrade of the management cluster involves several components. For a list of the general components that require an upgrade, see the <literal>Day 2</literal> management cluster (<xref linkend="day2-mgmt-cluster"/>) documentation.</para>
<para>The upgrade procedure for components specific to this setup can be seen below.</para>
<para><emphasis role="strong">Upgrading Metal<superscript>3</superscript></emphasis></para>
<para>To upgrade <literal>Metal<superscript>3</superscript></literal>, use the following command to update the Helm repository cache and fetch the latest chart to install <literal>Metal<superscript>3</superscript></literal> from the Helm chart repository:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>After that, the easy way to upgrade is to export your current configurations to a file, and then upgrade the <literal>Metal<superscript>3</superscript></literal> version using that previous file.
If any change is required in the new version, the file can be edited before the upgrade.</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=304.0.16+up0.12.6</screen>
</section>
<section xml:id="atip-lifecycle-downstream">
<title>Downstream cluster upgrades</title>
<para>Upgrading downstream clusters involves updating several components. The following sections cover the upgrade process for each of the components.</para>
<para><emphasis role="strong">Upgrading the operating system</emphasis></para>
<para>For this process, check the following reference (<xref linkend="eib-edge-image-connected"/>) to build the new image with a new operating system version.
With this new image generated by <literal>EIB</literal>, the next provision phase uses the new operating version provided.
In the following step, the new image is used to upgrade the nodes.</para>
<para><emphasis role="strong">Upgrading the RKE2 cluster</emphasis></para>
<para>The changes required to upgrade the <literal>RKE2</literal> cluster using the automated workflow are the following:</para>
<itemizedlist>
<listitem>
<para>Change the block <literal>RKE2ControlPlane</literal> in the <literal>capi-provisioning-example.yaml</literal> shown in the following section (<xref linkend="single-node-provision"/>):</para>
<itemizedlist>
<listitem>
<para>Specify the desired <literal>rolloutStrategy</literal>.</para>
</listitem>
<listitem>
<para>Change the version of the <literal>RKE2</literal> cluster to the new version replacing <literal>${RKE2_NEW_VERSION}</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  version: ${RKE2_NEW_VERSION}
  replicas: 1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>Change the block <literal>Metal3MachineTemplate</literal> in the <literal>capi-provisioning-example.yaml</literal> shown in the following section (<xref linkend="single-node-provision"/>):</para>
<itemizedlist>
<listitem>
<para>Change the image name and checksum to the new version generated in the previous step.</para>
</listitem>
<listitem>
<para>Add the directive <literal>nodeReuse</literal> to <literal>true</literal> to avoid creating a new node.</para>
</listitem>
<listitem>
<para>Add the directive <literal>automatedCleaningMode</literal> to <literal>metadata</literal> to enable the automated cleaning for the node.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>After making these changes, the <literal>capi-provisioning-example.yaml</literal> file can be applied to the cluster using the following command:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-troubleshooting-3">
<title>Troubleshooting</title>
<partintro>
<para>This section provides guidance to diagnose and resolve common issues with SUSE Edge deployments and operations. It covers various topics, offering component-specific troubleshooting steps, key tools, and relevant log locations.</para>
</partintro>
<chapter xml:id="general-troubleshooting-principles">
<title>General Troubleshooting Principles</title>
<para>Before diving into component-specific issues, consider these general principles:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Check logs</emphasis>: Logs are the primary source of information. Most of the times the errors are self explanatory and contain hints on what failed.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Check clocks</emphasis>: Having clock differences between systems can lead to all kinds of different errors. Ensure clocks are in sync. EIB can be instructed to force clock sync at boot time, see Configuring OS Time (<xref linkend="quickstart-eib"/>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Boot Issues</emphasis>: If the system is stuck during boot, note down the last messages displayed. Access the console (physical or via BMC) to observe boot messages.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Network Issues</emphasis>: Verify network interface configuration (<literal>ip a</literal>), routing table (<literal>ip route</literal>), test connectivity from/to other nodes and external services (<literal>ping</literal>, <literal>nc</literal>). Ensure firewall rules are not blocking necessary ports.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify component status</emphasis>: Use <literal>kubectl get</literal> and <literal>kubectl describe</literal> for Kubernetes resources. Use <literal>kubectl get events --sort-by='.lastTimestamp' -n &lt;namespace&gt;</literal> to see the events on a particular Kubernetes namespace.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify services status</emphasis>: Use <literal>systemctl status &lt;service&gt;</literal> for systemd services.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Check syntax</emphasis>: Software expects certain structure and syntax on configuration files. For yaml files, for example, use <literal>yamllint</literal> or similar tools to verify the proper syntax.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Isolate the problem</emphasis>: Try to narrow down the issue to a specific component or layer (for example, network, storage, OS, Kubernetes, Metal<superscript>3</superscript>, Ironic,…​).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Documentation</emphasis>: Always refer to the official <link xl:href="https://documentation.suse.com/suse-edge/">SUSE Edge documentation</link> and also upstream documentation for detailed information.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Versions</emphasis>: SUSE Edge is an opinionated and thoroughly tested version of different SUSE components. The versions of each component per SUSE Edge release can be observed in the <link xl:href="https://documentation.suse.com/suse-edge/support-matrix/html/support-matrix/index.html">SUSE Edge support matrix</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Known issues</emphasis>: For each SUSE Edge release there is a “Known issues” section on the release notes that contains information of issues that will be fixed on future releases but can affect the current one.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="troubleshooting-kiwi">
<title>Troubleshooting Kiwi</title>
<para>Kiwi is used to generate updated SUSE Linux Micro images to be used with Edge Image Builder.</para>
<itemizedlist>
<title>Common Issues</title>
<listitem>
<para><emphasis role="strong">SL Micro Version Mismatch</emphasis>: The build host operating system version must match the operating system version being built (SL Micro 6.0 host → SL Micro 6.0 image).</para>
</listitem>
<listitem>
<para><emphasis role="strong">SELinux in Enforcing State</emphasis>: Due to certain limitations, it is currently required to disable SELinux temporarily to be able to build images with Kiwi. Check the SElinux status with <literal>getenforce</literal> and disable it before running the build process with <literal>setenforce 0</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Build host not registered</emphasis>: The build process uses the build host subscriptions to be able to pull packages from SUSE SCC. If the host is not registered it fails.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Loop Device Test Failure</emphasis>: The first time that the Kiwi build process is executed, it will fail shortly after starting with "ERROR: Early loop device test failed, please retry the container run.", this is a symptom of loop devices being created on the underlying host system that are not immediately visible inside of the container image. Re-run the Kiwi build process again and it should proceed without issue.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Missing Permissions</emphasis>: The build process expects to be run as root user (or via sudo).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Wrong Privileges</emphasis>: The build process expects the <literal>--privileged</literal> flag when running the container. Double-check that it is present.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Logs</title>
<listitem>
<para><emphasis role="strong">Build container logs</emphasis>: Check the logs of the build container. The logs are generated in the directory that was used to store the artifacts. Check docker logs or podman logs for the necessary information as well.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Temporary build directories</emphasis>: Kiwi creates temporary directories during the build process. Check these for intermediate logs or artifacts if the main output is insufficient.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting steps</title>
<listitem>
<para><emphasis role="strong">Review <literal>build-image</literal> output</emphasis>: The error message in the console output is usually very indicative.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Check build environment</emphasis>: Ensure all prerequisites for Kiwi itself (for example, docker/podman, SElinux, sufficient disk space) are met on the machine running Kiwi.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Inspect build container logs</emphasis>: Review the logs of the failed container for more detailed errors (see above).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify definition file</emphasis>: If you are using  a custom Kiwi image definition file, double-check the file for any typos or syntax.</para>
</listitem>
</orderedlist>
<note>
<para>Check the <link xl:href="https://documentation.suse.com/appliance/kiwi-9/html/kiwi/troubleshooting.html">Kiwi Troubleshooting Guide</link>.</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-image-builder">
<title>Troubleshooting Edge Image Builder (EIB)</title>
<para>EIB is used to create custom SUSE Edge images.</para>
<itemizedlist>
<title>Common Issues</title>
<listitem>
<para><emphasis role="strong">Wrong SCC code</emphasis>: Ensure the SCC code used in the EIB definition file matches the SL Micro version and architecture.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Missing dependencies</emphasis>: Ensure there are no missing packages or tools within the build environment.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Incorrect image size</emphasis>: For raw images, the <literal>diskSize</literal> parameter is required and it depends heavily on the images, RPMs, and other artifacts being included in the image.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Permissions</emphasis>: If storing a script on the custom/files directory, ensure it has executable permissions as those files are just available at combustion time but no changes are performed by EIB.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Operating system group dependencies</emphasis>: When creating an image with custom users and groups, the groups being set as “<literal>primaryGroup</literal>” should be explicitly created.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Operating system user’s sshkeys requires a home folder</emphasis>: When creating an image with users with sshkeys, the home folder needs to be created as well with <literal>createHomeDir=true</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Combustion issues</emphasis>: EIB relies on combustion for the customization of the OS and deployment of all the other SUSE Edge components. This also includes custom scripts being placed in the custom/scripts folder. Note that the combustion process is being executed at <literal>initrd</literal> time, so the system is not completely booted when the scripts are executed.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Podman machine size</emphasis>: As explained in the EIB Tips and Tricks section (<xref linkend="tips-and-tricks"/>), verify the podman machine has enough CPU/memory to run the EIB container on non-Linux operating systems.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Logs</title>
<listitem>
<para><emphasis role="strong">EIB output</emphasis>: The console output of the <literal>eib build</literal> command is crucial.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Build container logs</emphasis>: Check the logs of the build container. The logs are generated in the directory that was used to store the artifacts. Check <literal>docker logs</literal> or <literal>podman logs</literal> for  the necessary information as well.</para>
<note>
<para>For more information, see <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Debugging</link>.</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Temporary build directories</emphasis>: EIB creates temporary directories during the build process. Check these for intermediate logs or artifacts if the main output is insufficient.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Combustion logs</emphasis>: If the image being built with EIB does not boot for any reason, a root shell is available. Connect to the host console (either physically, via BMC, etc.) and check combustion logs with <literal>journalctl -u combustion</literal> and in general all the operating system logs with <literal>journalctl</literal> to find the root cause of the failure.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting steps</title>
<listitem>
<para><emphasis role="strong">Review <literal>eib-build</literal> output</emphasis>: The error message in the console output is usually very indicative.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Check build environment</emphasis>: Ensure all prerequisites for EIB itself (for example, docker/podman, sufficient disk space) are met on the machine running EIB.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Inspect build container logs</emphasis>: Review the logs of the failed container for more detailed errors (see above).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify <literal>eib</literal> configuration"</emphasis>: Double-check the <literal>eib</literal> configuration file for any typos or incorrect paths to source files or build scripts.</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Test components individually</emphasis>: If your EIB build involves custom scripts or stages, run them independently to isolate failures.</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>Check <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Edge Image Builder Debugging</link>.</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-networking">
<title>Troubleshooting Edge Networking (NMC)</title>
<para>NMC is injected on SL Micro EIB images to configure the network of the Edge hosts at boot time via combustion. It is also being executed on the Metal3 workflow as part of the inspection process. Issues can happen when the host is being booted for the first time or on the Metal3 inspection process.</para>
<itemizedlist>
<title>Common Issues</title>
<listitem>
<para><emphasis role="strong">Host not being able to boot properly the first time</emphasis>: Malformed network definition files can lead to the combustion phase to fail and then the host drops a root shell.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Files are not properly generated</emphasis>: Ensure the network files matches <link xl:href="https://nmstate.io/examples.html">NMState</link> format.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Network interfaces are not correctly configured</emphasis>: Ensure the MAC addresses match the interfaces being used on the host.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Mismatch between interface names</emphasis>: The <literal>net.ifnames=1</literal> kernel argument enables <link xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable Naming Scheme for Network Interfaces</link> so there is no <literal>eth0</literal> anymore but other naming schema such as <literal>enp2s0</literal>.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Logs</title>
<listitem>
<para><emphasis role="strong">Combustion logs</emphasis>: As nmc is being used at combustion time, check combustion logs with <literal>journalctl -u combustion</literal> on the host being provisioned.</para>
</listitem>
<listitem>
<para><emphasis role="strong">NetworkManager logs</emphasis>: On the Metal<superscript>3</superscript> deployment workflow, nmc is part of the IPA execution and it is being executed as a dependency of the NetworkManager service using systemd’s ExecStartPre functionality. Check NetworkManager logs on the IPA host as <literal>journalctl -u NetworkManager</literal> (see the Troubleshooting Directed-network provisioning (<xref linkend="troubleshooting-directed-network-provisioning"/>) section to understand how to access the host when booted with IPA).</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting steps</title>
<listitem>
<para><emphasis role="strong">Verify the yaml syntax</emphasis>: nmc configuration files are yaml files, check the proper syntax with <literal>yamllint</literal> or similar tools.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Run nmc manually</emphasis>: As nmc is part of the EIB container, to debug any issues, a local podman command can be used.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Create a temporary folder to store the nmc files.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p ${HOME}/tmp/foo</screen>
</listitem>
<listitem>
<para>Save the nmc files on that location.</para>
<screen language="shell" linenumbering="unnumbered">❯ tree --noreport ${HOME}/tmp/foo
/Users/johndoe/tmp/foo
├── host1.example.com.yaml
└── host2.example.com.yaml</screen>
</listitem>
<listitem>
<para>Run the EIB container with nmc as the entrypoint and the generate command to perform the same tasks nmc would do at combustion time:</para>
<screen language="shell" linenumbering="unnumbered">podman run -it --rm -v ${HOME}/tmp/foo:/tmp/foo:Z --entrypoint=/usr/bin/nmc registry.suse.com/edge/3.3/edge-image-builder:1.2.0 generate --config-dir /tmp/foo --output-dir /tmp/foo/

[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host2.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host1.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc] Successfully generated and stored network config</screen>
</listitem>
<listitem>
<para>Observe the logs and files being generated on the temporary folder.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-phone-home-scenarios">
<title>Troubleshooting Phone-Home scenarios</title>
<para>Phone-home scenarios involve using Elemental to connect back to the Management cluster and EIB to create an OS image including the elemental-registration bits. Issues can happen when the host is being booted for the first time, during the EIB build process or trying to register to the Management cluster.</para>
<itemizedlist>
<title>Common Issues</title>
<listitem>
<para><emphasis role="strong">System fails to register</emphasis>: Node not being registered in the UI. Ensure the host is booted properly and, is able to communicate back to Rancher, clock is in sync and the Elemental services are ok.</para>
</listitem>
<listitem>
<para><emphasis role="strong">System fails to be provisioned</emphasis>: Node is registered but it fails to be provisioned. Ensure the host is able to communicate back to Rancher, clock is in sync and the Elemental services are ok.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Logs</title>
<listitem>
<para><emphasis role="strong">System logs</emphasis>: <literal>journalctl</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Elemental-system-agent  logs</emphasis>: <literal>journalctl -u elemental-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s/RKE2 logs</emphasis>: <literal>journalctl -u k3s or journalctl -u rke2-server</literal> (or <literal>rke2-agent</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">Elemental operator pod</emphasis>: <literal>kubectl logs -n cattle-elemental-system -l app=elemental-operator</literal></para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting steps</title>
<listitem>
<para><emphasis role="strong">Review logs</emphasis>: Check Elemental operator pod logs to see if there are any issues. Check the host logs if the node is booted.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Check MachineRegistration and TPM</emphasis>: By default, TPM is used for <link xl:href="https://elemental.docs.rancher.com/authentication/">authentication</link> but there are alternatives for hosts without TPM.</para>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-directed-network-provisioning">
<title>Troubleshooting Directed-network provisioning</title>
<para>Directed-network provisioning scenarios involve using Metal<superscript>3</superscript> and CAPI elements to provision the Downstream cluster. It also includes EIB to create an OS image. Issues can happen when the host is being booted for the first time or during the inspection or provisioning processes.</para>
<itemizedlist>
<title>Common Issues</title>
<listitem>
<para><emphasis role="strong">Old firmware</emphasis>: Verify all the different firmware on the physical hosts being used are up to date. This includes the BMC firmware as some times Metal<superscript>3</superscript> <link xl:href="https://book.metal3.io/bmo/supported_hardware#redfish-and-its-variants">requires specific/updated ones</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Provisioning failed with SSL errors</emphasis>: If the webserver serving the images uses https, Metal<superscript>3</superscript> needs to be configured to inject and trust the certificate on the IPA image. See Kubernetes folder (<xref linkend="mgmt-cluster-kubernetes-folder"/>) on how to include a <literal>ca-additional.crt</literal> file to the Metal<superscript>3</superscript> chart.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Certificates issues when booting the hosts with IPA</emphasis>: Some server vendors verify the SSL connection when attaching virtual-media ISO images to the BMC, which can cause a problem because the generated certificates for the Metal3 deployment are self-signed. It can happen that the host is being booted but it drops to an UEFI shell. See Disabling TLS for virtualmedia ISO attachment (<xref linkend="disabling-tls-for-virtualmedia-iso-attachment"/>) on how to fix it.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Wrong name or label reference</emphasis>: If the cluster references a node by the wrong name or label, the cluster results as deployed but the BMH remains as “Available”. Double-check the references on the involved objects for the BMHs.</para>
</listitem>
<listitem>
<para><emphasis role="strong">BMC communication issues</emphasis>: Ensure the Metal<superscript>3</superscript> pods running on the management cluster can reach the BMC of the hosts being provisioned (usually the BMC network is very restricted).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Incorrect bare metal host state</emphasis>: The BMH object goes to different states (inspecting, preparing, provisioned, etc.) during its lifetime <link xl:href="https://book.metal3.io/bmo/state_machine">Lifetime of State machine</link>. If detected an incorrect state, check the <literal>status</literal> field of the BMH object as it contains more information as <literal>kubectl get bmh &lt;name&gt; -o jsonpath=’{.status}’| jq</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Host not being deprovisioned</emphasis>: In the event of a host being intended to be deprovisioned fails, the removal can be attempted after adding the “detached” annotation to the BMH object as:  <literal>kubectl annotate bmh/&lt;BMH&gt; baremetalhost.metal3.io/detached=””</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Image errors</emphasis>: Verify the image being built with EIB for the downstream cluster is available, has a proper checksum and it is not too large to decompress or too large for disk.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Disk size mismatch</emphasis>: By default, the disk would not expand to fill the whole disk. As explained in the Growfs script (<xref linkend="growfs-script"/>) section, a growfs script needs to be included in the image being built with EIB for the downstream cluster hosts.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Cleaning process stuck</emphasis>: The cleaning process is retried several times. If due to a problem with the host cleaning is no longer possible, disable cleaning first by setting the <literal>automatedCleanMode</literal> field to <literal>disabled</literal> on the BMH object.</para>
<warning>
<para>It is not recommended to manually remove the finalizer when the cleaning process is taking longer than desired or is failing. Doing so, removes the host record from Kubernetes but leave it in Ironic. The currently running action continues in the background, and an attempt to add the host again may fail because of the conflict.</para>
</warning>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3/Rancher Turtles/CAPI pods issues</emphasis>: The deployment flow for all the required components is:</para>
<itemizedlist>
<listitem>
<para>The Rancher Turtles controller deploys the CAPI operator controller.</para>
</listitem>
<listitem>
<para>The CAPI operator controller then deploys the provider controllers (CAPI core, CAPM3 and RKE2 controlplane/bootstrap).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Verify all the pods are running correctly and check the logs otherwise.</para>
<itemizedlist>
<title>Logs</title>
<listitem>
<para><emphasis role="strong">Metal<superscript>3</superscript> logs</emphasis>:Check logs for the different pods.</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n metal3-system -l app.kubernetes.io/component=baremetal-operator
kubectl logs -n metal3-system -l app.kubernetes.io/component=ironic</screen>
<note>
<para>The metal3-ironic pod contains at least 4 different containers (<literal>ironic-httpd</literal>,` ironic-log-watch`, <literal>ironic</literal> &amp; <literal>ironic-ipa-downloader</literal> (init)) on the same pod. Use the <literal>-c</literal>  flag when using <literal>kubectl logs</literal> to verify the logs of each of the containers.</para>
</note>
<note>
<para>The <literal>ironic-log-watch</literal> container exposes console logs from the hosts after inspection/provisioning, provided network connectivity enables sending these logs back to the management cluster. This can be useful in cases where there are provisioning errors but you do not have direct access to the BMC console logs.</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Rancher Turtles logs</emphasis>: Check logs for the different pods.</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n rancher-turtles-system -l control-plane=controller-manager
kubectl logs -n rancher-turtles-system -l app.kubernetes.io/name=cluster-api-operator
kubectl logs -n rke2-bootstrap-system -l cluster.x-k8s.io/provider=bootstrap-rke2
kubectl logs -n rke2-control-plane-system -l cluster.x-k8s.io/provider=control-plane-rke2
kubectl logs -n capi-system -l cluster.x-k8s.io/provider=cluster-api
kubectl logs -n capm3-system -l cluster.x-k8s.io/provider=infrastructure-metal3</screen>
</listitem>
<listitem>
<para><emphasis role="strong">BMC logs</emphasis>: Usually BMCs have a UI where most of the interaction can be done. There is usually a “logs” section that can be observed for potential issues (not being able to reach the image, hardware failures, etc.).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Console logs</emphasis>: Connect to the BMC console (via the BMC webui, serial, etc.) and check for errors on the logs being written.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting steps</title>
<listitem>
<para><emphasis role="strong">Check <literal>BareMetalHost</literal> status</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>kubectl get bmh -A</literal> shows the current state. Look for <literal>provisioning</literal>, <literal>ready</literal>, <literal>error</literal>, <literal>registering</literal>.</para>
</listitem>
<listitem>
<para><literal>kubectl describe bmh -n &lt;namespace&gt; &lt;bmh_name&gt;</literal> provides detailed events and conditions explaining why a BMH might be stuck.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Test RedFish connectivity</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Use <literal>curl</literal> from the Metal<superscript>3</superscript> control plane to test connectivity to the BMCs via redfish.</para>
</listitem>
<listitem>
<para>Ensure correct BMC credentials are provided in the <literal>BareMetalHost-Secret</literal> definition.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Verify turtles/CAPI/metal3 pod status</emphasis>: Ensure the containers on the management cluster are up and running: <literal>kubectl get pods -n metal3-system</literal> and <literal>kubectl get pods -n rancher-turtles-system</literal> (also see <literal>capi-system</literal>, <literal>capm3-system</literal>, <literal>rke2-bootstrap-system</literal> and <literal>rke2-control-plane-system</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify the ironic endpoint is reachable from the host being provisioned</emphasis>: The host being provisioned needs to be able to reach out the Ironic endpoint to report back to Metal<superscript>3</superscript>. Check the IP with <literal>kubectl get svc -n metal3-system metal3-metal3-ironic</literal> and try to reach it via <literal>curl/nc</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify the IPA image is reachable from the BMC</emphasis>: IPA is being served by the Ironic endpoint and it needs to be reachable from the BMC as it is being used as a virtual CD.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verify the OS image is reachable from the host being provisioned</emphasis>: The image being used to provision the host needs to be reachable from the host itself (when running IPA) as it will be downloaded temporarily and written to the disk.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Examine Metal<superscript>3</superscript> component logs</emphasis>: See above.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Retrigger BMH Insepction</emphasis>:  If an inspection failed or the hardware of an available host changed, a new inspection process can be triggered by annotating the BMH object with <literal>inspect.metal3.io: ""</literal>. See the <link xl:href="https://book.metal3.io/bmo/inspect_annotation">Metal<superscript>3</superscript> Controlling inspection</link> guide for more information.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Bare metal IPA console</emphasis>: To troubleshoot IPA issues a couple of alternatives exist:</para>
<itemizedlist>
<listitem>
<para>Enable “autologin”. This enables the root user to be logged automatically when connecting to the IPA console.</para>
<warning>
<para>This is only for debug purposes as it gives full access to the host.</para>
</warning>
<para>To enable autologin, the Metal3 helm <literal>global.ironicKernelParams</literal> value should look like: <literal>console=ttyS0 suse.autologin=ttyS0</literal> (depending on the console, <literal>ttyS0</literal> can be changed). Then a redeployment of the Metal<superscript>3</superscript> chart should be performed. (Note <literal>ttyS0</literal> is an example, this should match the actual terminal e.g may be <literal>tty1</literal> in many cases on bare metal, this can be verified by looking at the console output from the IPA ramdisk on boot where <literal>/etc/issue</literal> prints the console name).</para>
<para>Another way to do it is by changing the <literal>IRONIC_KERNEL_PARAMS</literal> parameter on the <literal>ironic-bmo</literal> configmap on the <literal>metal3-system</literal> namespace. This can be easier as it can be done via <literal>kubectl</literal> edit but it will be overwritten when updating the chart. Then the Metal<superscript>3</superscript> pod needs to be restarted with <literal>kubectl delete pod -n metal3-system -l app.kubernetes.io/component=ironic</literal>.</para>
</listitem>
<listitem>
<para>Inject an ssh key for the root user on the IPA.</para>
<warning>
<para>This is only for debug purposes as it gives full access to the host.</para>
</warning>
<para>To inject the ssh key for the root user, the Metal<superscript>3</superscript> helm <literal>debug.ironicRamdiskSshKey</literal> value should be used. Then a redeployment of the Metal<superscript>3</superscript> chart should be performed.</para>
<para>Another way to do it is by changing the <literal>IRONIC_RAMDISK_SSH_KEY</literal> parameter on the <literal>ironic-bmo configmap</literal> on the <literal>metal3-system</literal> namespace. This can be easier as it can be done via <literal>kubectl</literal> edit but it will be overwritten when updating the chart. Then the Metal<superscript>3</superscript> pod needs to be restarted with <literal>kubectl delete pod -n metal3-system -l app.kubernetes.io/component=ironic</literal></para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>Check the <link xl:href="https://cluster-api.sigs.k8s.io/user/troubleshooting">CAPI troubleshooting</link> and <link xl:href="https://book.metal3.io/troubleshooting">Metal<superscript>3</superscript> troubleshooting</link> guides.</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-other-components">
<title>Troubleshooting Other components</title>
<para>Other SUSE Edge components troubleshooting guides can be consulted on their official documentation:</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://documentation.suse.com/smart/micro-clouds/html/SLE-Micro-5.5-admin/index.html#id-1.10">SUSE Linux Micro Troubleshooting</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.rke2.io/known_issues">RKE2 Known Issues</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.k3s.io/known-issues">K3s Known Issues</link></para>
</listitem>
<listitem>
<para><link xl:href="https://ranchermanager.docs.rancher.com/troubleshooting/general-troubleshooting">Rancher General Troubleshooting</link></para>
</listitem>
<listitem>
<para><link xl:href="https://documentation.suse.com/multi-linux-manager/5.1/en/docs/administration/troubleshooting/tshoot-intro.html">SUSE Multi-Linux Manager Troubleshooting</link></para>
</listitem>
<listitem>
<para><link xl:href="https://elemental.docs.rancher.com/troubleshooting-support/">Elemental Support</link></para>
</listitem>
<listitem>
<para><link xl:href="https://turtles.docs.rancher.com/turtles/stable/en/troubleshooting/troubleshooting.html">Rancher Turtles Troubleshooting</link></para>
</listitem>
<listitem>
<para><link xl:href="https://longhorn.io/docs/1.9.1/troubleshoot/troubleshooting/">Longhorn Troubleshooting</link></para>
</listitem>
<listitem>
<para><link xl:href="https://open-docs.neuvector.com/next/troubleshooting/troubleshooting/">Neuvector Troubleshooting</link></para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/troubleshooting">Fleet Troubleshooting</link></para>
</listitem>
</itemizedlist>
<para>You can also see <link xl:href="https://www.suse.com/support/kb/">SUSE Knowledgebase</link>.</para>
</chapter>
<chapter xml:id="collecting-diagnostics-for-support">
<title>Collecting Diagnostics for Support</title>
<para>When contacting SUSE Support, providing comprehensive diagnostic information is crucial.</para>
<itemizedlist>
<title>Essential Information to Collect</title>
<listitem>
<para><emphasis role="strong">Detailed problem description</emphasis>: What happened, when did it happen, what were you doing, what is the expected behavior, and what is the actual behavior?</para>
</listitem>
<listitem>
<para><emphasis role="strong">Steps to reproduce</emphasis>: Can you reliably reproduce the issue? If so, list the exact steps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Component versions</emphasis>: SUSE Edge version, components versions (RKE2/K3, EIB, Metal<superscript>3</superscript>, Elemental,..).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Relevant logs</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>journalctl</literal> output (filtered by service if possible, or full boot logs).</para>
</listitem>
<listitem>
<para>Kubernetes pod logs (kubectl logs).</para>
</listitem>
<listitem>
<para>Metal³/Elemental component logs.</para>
</listitem>
<listitem>
<para>EIB build logs and other logs</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">System information</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>uname -a</literal></para>
</listitem>
<listitem>
<para><literal>df -h</literal></para>
</listitem>
<listitem>
<para><literal>ip a</literal></para>
</listitem>
<listitem>
<para><literal>/etc/os-release</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Configuration files</emphasis>: Relevant configuration files for Elemental, Metal<superscript>3</superscript>, EIB such as helm chart values, configmaps, etc.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Kubernetes information</emphasis>: Nodes, Services, Deployments, etc.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Kubernetes objects affected</emphasis>: BMH, MachineRegistration, etc.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>How to collect</title>
<listitem>
<para><emphasis role="strong">For logs</emphasis>: Redirect command output to files (for example, <literal>journalctl -u k3s &gt; k3s_logs.txt</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">For Kubernetes resources</emphasis>: Use <literal>kubectl get &lt;resource&gt; -o yaml &gt; &lt;resource_name&gt;.yaml</literal> to get detailed YAML definitions.</para>
</listitem>
<listitem>
<para><emphasis role="strong">For system information</emphasis>: Collect output of the commands listed above.</para>
</listitem>
<listitem>
<para><emphasis role="strong">For SL Micro</emphasis>: Check the <link xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-adm-support-slemicro.html">SUSE Linux Micro Troubleshooting Guide</link> documentation on how to gather system information for support with <literal>supportconfig</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">For RKE2/Rancher</emphasis>: Check the <link xl:href="https://www.suse.com/support/kb/doc/?id=000020191">The Rancher v2.x Linux log collector script</link> article to run The Rancher v2.x Linux log collector script.</para>
</listitem>
</itemizedlist>
<formalpara>
<title>Contact Support</title>
<para>Please check the article available at <link xl:href="https://www.suse.com/support/kb/doc/?id=000019452">How-to effectively work with SUSE Technical Support</link> and the support handbook located at <link xl:href="https://www.suse.com/support/handbook/">SUSE Technical Support Handbook</link> for more details on how to contact SUSE support.</para>
</formalpara>
</chapter>
</part>
<part xml:id="id-appendix">
<title>Appendix</title>
<chapter xml:id="id-release-notes">
<title>Release Notes</title>
<section xml:id="release-notes">
<title>Abstract</title>
<para>SUSE Edge 3.4 is a tightly integrated and comprehensively validated end-to-end solution for addressing the unique challenges of the deployment of infrastructure and cloud-native applications at the edge. Its driving focus is to provide an opinionated, yet highly flexible, highly scalable, and secure platform that spans initial deployment image building, node provisioning and onboarding, application deployment, observability, and lifecycle management.</para>
<para>The solution is designed with the notion that there is no "one-size-fits-all" edge platform due to our customers’ widely varying requirements and expectations. Edge deployments push us to solve, and continually evolve, some of the most challenging problems, including massive scalability, restricted network availability, physical space constraints, new security threats and attack vectors, variations in hardware architecture and system resources, the requirement to deploy and interface with legacy infrastructure and applications, and customer solutions that have extended lifespans.</para>
<para>SUSE Edge is built on best-of-breed open source software from the ground up, consistent with both our 30-year history in delivering secure, stable, and certified SUSE Linux platforms and our experience in providing highly scalable and feature-rich Kubernetes management with our Rancher portfolio. SUSE Edge builds on-top of these capabilities to deliver functionality that can address a wide number of market segments, including retail, medical, transportation, logistics, telecommunications, smart manufacturing, and Industrial IoT.</para>
<para>For more information on product support lifecycle updates for SUSE Edge, see <link xl:href="https://www.suse.com/lifecycle/#suse-edge-33">Product Support Lifecycle</link>.</para>
<note>
<para>SUSE Telco Cloud (formerly known as SUSE Edge for Telco) is a derivative of SUSE Edge, with additional optimizations and components that enable the platform to address the requirements found in telecommunications use-cases. Unless explicitly stated, all the release notes are applicable for both SUSE Edge 3.4, and SUSE Telco Cloud 3.4.</para>
</note>
</section>
<section xml:id="id-about">
<title>About</title>
<para>These Release Notes are, unless explicitly specified and explained, identical across all architectures, and the most recent version, along with the release notes of all other SUSE products are always available online at <link xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>.</para>
<para>Entries are only listed once, but they can be referenced in several places if they are important and belong to more than one section. Release notes usually only list changes that happened between two subsequent releases. Certain important entries from the release notes of previous product versions may be repeated. To make these entries easier to identify, they contain a note to that effect.</para>
<para>However, repeated entries are provided as a courtesy only. Therefore, if you are skipping one or more releases, check the release notes of the skipped releases also. If you are only reading the release notes of the current release, you could miss important changes that may affect system behavior. SUSE Edge versions are defined as x.y.z, where 'x' denotes the major version, 'y' denotes the minor, and 'z' denotes the patch version, also known as the "z-stream". SUSE Edge product lifecycles are defined based around a given minor release, e.g. "3.4", but ship with subsequent patch updates through its lifecycle, e.g. "3.4.1".</para>
<note>
<para>SUSE Edge z-stream releases are tightly integrated and thoroughly tested as a versioned stack. Upgrade of any individual components to a different versions to those listed above is likely to result in system downtime. While it’s possible to run Edge clusters in untested configurations, it is not recommended, and it may take longer to provide resolution through the support channels.</para>
</note>
</section>
<section xml:id="release-notes-3-4-0">
<title>Release 3.4.0</title>
<para>Availability Date: 24th September 2025</para>
<para>Full Support End Date: 20th March 2026</para>
<para>Maintenance Support End Date: 20th September 2027</para>
<para>EOL: 21st September 2027</para>
<para>Summary: SUSE Edge 3.4.0 is the first release in the SUSE Edge 3.4 release stream.</para>
<section xml:id="id-new-features">
<title>New Features</title>
<itemizedlist>
<listitem>
<para>Updated to Kubernetes 1.33 and Rancher Prime 2.12</para>
</listitem>
<listitem>
<para>Updated Rancher Turtles, Cluster API and Metal3/Ironic versions</para>
</listitem>
<listitem>
<para>Updated to SUSE Storage (Longhorn) 1.9.1 <link xl:href="https://longhorn.io/docs/1.9.1/">Release Notes</link></para>
</listitem>
<listitem>
<para>More flexible deployment of AArch64 downstream clusters is now possible via the directed network provisioning flow. See <xref linkend="atip-automated-provisioning"/> for more details.</para>
</listitem>
<listitem>
<para>Deployment of dual-stack clusters is now fully supported (single-stack ipv6 remains in <xref linkend="tech-previews"/>)</para>
</listitem>
<listitem>
<para>BGP mode for MetalLB is now available as a tech preview see <xref linkend="tech-previews"/> and <xref linkend="guides-metallb-k3s-l3"/> for more details.</para>
</listitem>
<listitem>
<para>Edge Image Builder has been updated to 1.3.0, see <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/RELEASE_NOTES.md">Upstream Release Notes</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>Bug &amp; Security Fixes</title>
<itemizedlist>
<listitem>
<para>Rancher Prime 2.12 contains several bugfixes <link xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">Upstream Rancher Release Notes</link></para>
</listitem>
<listitem>
<para>Rancher Prime 2.12 contains a fix for issues related to AppVersion when determining extension upgrade availability, which impacted operation with Edge charts <link xl:href="https://github.com/rancher/dashboard/issues/14204">Upstream Issue</link></para>
</listitem>
<listitem>
<para>SUSE Storage (Longhorn) 1.9.1 contains several bugfixes <link xl:href="https://github.com/longhorn/longhorn/releases/tag/v1.9.1">Upstream Longhorn Bug Fixes</link></para>
</listitem>
<listitem>
<para>The updated Metal<superscript>3</superscript> chart fixes an issue where the wrong MAC may be collected for bonded interfaces during inspection <link xl:href="https://bugs.launchpad.net/ironic-python-agent/+bug/2103450">Upstream IPA issue</link></para>
</listitem>
<listitem>
<para>The updated Metal<superscript>3</superscript> chart fixes an issue where the deployment may not be correctly restarted on ConfigMap updates <link xl:href="https://github.com/suse-edge/charts/issues/219">Upstream Issue</link></para>
</listitem>
<listitem>
<para>The Rancher Turtles update includes a fix which resolves an issue where MachineTemplate ownerReferences were not applied by the RKE2 CAPI provider <link xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/500">Upstream Issue</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-known-issues-8">
<title>Known Issues</title>
<warning>
<para>If deploying new clusters, please follow <xref linkend="guides-kiwi-builder-images"/> to build fresh images first as this is now the first step required to create clusters for both AMD64/Intel 64 and AArch64 architectures as well as management and downstream clusters.</para>
</warning>
<itemizedlist>
<listitem>
<para>When deploying via Edge Image Builder, <literal>HelmChartConfigs</literal> manifests may fail if they are put in the <literal>kubernetes/manifests</literal> configuration directory. Instead it is reccomended to place any <literal>HelmChartConfigs</literal> in <literal>/var/lib/rancher/{rke2/k3s}/server/manifests/</literal> using the EIB os-files interface, see <xref linkend="mgmt-cluster-directory-structure"/> for example.  Failure to do this may cause nodes to stay in <literal>NotReady</literal> state on initial startup, as discussed in <link xl:href="https://github.com/rancher/rke2/issues/8357">#8357 RKE2 issue</link></para>
</listitem>
<listitem>
<para>On RKE2/K3s 1.31, 1.32 and 1.33 versions, the directory <literal>/etc/cni</literal> being used to store CNI configurations may not trigger a notification of the files being written there to <literal>containerd</literal> due to certain conditions related to <literal>overlayfs</literal> (see the <link xl:href="https://github.com/rancher/rke2/issues/8356">#8356 RKE2 issue</link>). This in turn results in the deployment of RKE2/K3s to get stuck waiting for the CNI to start, and the RKE2/K3s nodes to stay in <literal>NotReady</literal> state. This can be seen at node level with <literal>kubectl describe node &lt;affected_node&gt;</literal>:</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Conditions:
  Type   Status  LastHeartbeatTime                LastTransitionTime               Reason           Message
  ----   ------  -----------------                ------------------               ------           -------
  Ready  False   Thu, 05 Jun 2025 17:41:28 +0000  Thu, 05 Jun 2025 14:38:16 +0000  KubeletNotReady  container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized</screen>
<para>As a workaround, a tmpfs volume can be mounted at the <literal>/etc/cni</literal> directory before RKE2 starts. It avoids the usage of overlayfs which results in containerd missing notifications and the configs should get rewritten every time the node is restarted and the pods initcontainers run again. If using EIB, this can be a <literal>04-tmpfs-cni.sh</literal> script in the <literal>custom/scripts</literal> directory (as explained here[<link xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom">https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom</link>]) that looks like:</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash
mkdir -p /etc/cni
mount -t tmpfs -o mode=0700,size=5M tmpfs /etc/cni
echo "tmpfs /etc/cni tmpfs defaults,size=5M,mode=0700 0 0" &gt;&gt; /etc/fstab</screen>
<itemizedlist>
<listitem>
<para>When onboarding remote hosts using Elemental, a race condition between <literal>dbus.service</literal> and <literal>elemental-system-agent.service</literal> might occur, resulting in <literal>rancher-system-agent.service</literal> on remote host to fail starting with errors similar to the one below. (see the <link xl:href="https://github.com/suse-edge/edge-image-builder/issues/784">#784 Edge Image Builder issue</link> for details.)</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Sep 19 19:38:07 elementalvm elemental-system-agent[3671]: time="2025-09-19T19:38:07Z" level=info msg="[6b20fe64c854da2639804884b34129bb8f718eb59578111da58d9de1509c24db_1:stderr]: Failed to restart rancher-system-agent.service: Message recipient disconnected from message bus without replying"</screen>
<para>As a workaround, a systemd override file can be created as below</para>
<screen language="bash" linenumbering="unnumbered">[Unit]
Wants=dbus.service network-online.target
After=dbus.service network-online.target time-sync.target

[Service]
ExecStartPre=/bin/bash -c 'echo "Waiting for dbus to become active..." | systemd-cat -p info -t elemental-system-agent; sleep 15; timeout 300 bash -c "while ! systemctl is-active --quiet dbus.service; do sleep 15; done"'</screen>
<para>and a custom script named <literal>30a-copy-elemental-system-agent-override.sh</literal> can be used to place the override to <literal>/etc/systemd/system/elemental-system-agent.service.d</literal> prior to EIB’s <link xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/pkg/combustion/templates/31-elemental-register.sh.tpl">31-elemental-register.sh</link> script runs during the combustion phase.</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash

/bin/mkdir -p /etc/systemd/system/elemental-system-agent.service.d
/bin/cp -f elemental-system-agent-override.conf /etc/systemd/system/elemental-system-agent.service.d/override.conf</screen>
</section>
<section xml:id="id-component-versions">
<title>Component Versions</title>
<para>The following table describes the individual components that make up the 3.4.0 release, including the version, the Helm chart version (if applicable), and from where the released artifact can be pulled in the binary format. Please follow the associated documentation for usage and deployment examples.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Name</para></entry>
<entry align="left" valign="top"><para>Version</para></entry>
<entry align="left" valign="top"><para>Helm Chart Version</para></entry>
<entry align="left" valign="top"><para>Artifact Location (URL/Image)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Linux Micro</para></entry>
<entry align="left" valign="top"><para>6.1 (latest)</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SUSE Linux Micro Download Page</link><?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso (sha256 70b9be28f2d92bc3b228412e4fc2b1d5026e691874b728e530b8063522158854)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-SelfInstall-GM.install.iso (sha256 9ce83e4545d4b36c7c6a44f7841dc3d9c6926fe32dbff694832e0fbd7c496e9d)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-GM.raw.xz (sha256 36e3efa55822113840dd76fdf6914e933a7b7e88a1dce5cb20c424ccf2fb4430)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-GM.raw.xz (sha256 2ee66735da3e1da107b4878e73ae68f5fb7309f5ec02b5dfdb94e254fda8415e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Multi-Linux Manager</para></entry>
<entry align="left" valign="top"><para>5.0.5</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE Multi-Linux Manager Download Page</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.33.3%2Bk3s1">Upstream K3s Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://github.com/rancher/rke2/releases/tag/v1.33.3%2Brke2r1">Upstream RKE2 Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.com/server-charts/prime/index.yaml">Rancher Prime Helm Repository</link><?asciidoc-br?>
<link xl:href="https://github.com/rancher/rancher/releases/download/v2.12.1/rancher-images.txt">Rancher 2.12.1 Container Images</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Storage (Longhorn)</para></entry>
<entry align="left" valign="top"><para>1.9.1</para></entry>
<entry align="left" valign="top"><para>107.0.0+up1.9.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher Charts Helm Repository</link><?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.61<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Security</para></entry>
<entry align="left" valign="top"><para>5.4.5</para></entry>
<entry align="left" valign="top"><para>107.0.0+up2.8.7</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher Charts Helm Repository</link><?asciidoc-br?>
registry.suse.com/rancher/neuvector-controller:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-enforcer:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-manager:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-compliance-config:1.0.6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-registry-adapter:0.1.8<?asciidoc-br?>
registry.suse.com/rancher/neuvector-scanner:6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-updater:0.0.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles (CAPI)</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles:304.0.6_up0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-controller:v1.10.2<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-ipam-controller:v1.10.2<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-controller:v1.10.5<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles Airgap Resources</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles-airgap-resources:304.0.6_up0.24.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>0.11.5</para></entry>
<entry align="left" valign="top"><para>304.0.16+up0.12.6</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metal3:304.0.16_up0.12.6<?asciidoc-br?>
registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic:29.0.4.3<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic-ipa-downloader:3.0.9<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metallb:304.0.0_up0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-controller:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-speaker:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.4/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.7.3</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental Dashboard Extension</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://github.com/rancher/ui-plugin-charts/tree/4.0.0/charts/elemental/3.0.1">Elemental Extension Helm Chart</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/edge-image-builder:1.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.3">NMConfigurator Upstream Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.5.2</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-operator:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-api:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-controller:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-handler:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt Dashboard Extension</para></entry>
<entry align="left" valign="top"><para>1.3.2</para></entry>
<entry align="left" valign="top"><para>304.0.3+up1.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt-dashboard-extension:304.0.3_up1.3.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.62.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/cdi:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-operator:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.3.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.3.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/endpoint-copier-operator:304.0.1_up0.3.0<?asciidoc-br?>
registry.suse.com/edge/3.4/endpoint-copier-operator:0.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (Deprecated)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/akri:304.0.0_up0.12.20<?asciidoc-br?>
registry.suse.com/edge/charts/akri-dashboard-extension:304.0.0_up1.3.1<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SR-IOV Network Operator</para></entry>
<entry align="left" valign="top"><para>1.5.0</para></entry>
<entry align="left" valign="top"><para>304.0.2+up1.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/sriov-network-operator:304.0.2_up1.5.0<?asciidoc-br?>
registry.suse.com/edge/charts/sriov-crd:304.0.2_up1.5.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.16.0</para></entry>
<entry align="left" valign="top"><para>107.0.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher Charts Helm Repository</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.16.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.1.1</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.1.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/upgrade-controller:304.0.1_up0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/upgrade-controller:0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/kubectl:1.33.4<?asciidoc-br?>
registry.suse.com/edge/3.4/release-manifest:3.4.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Kiwi Builder</para></entry>
<entry align="left" valign="top"><para>10.2.12.0</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-deprecated-features">
<title>Deprecated features</title>
<para>Unless otherwise stated, these apply to the 3.4.0 release and all subsequent z-stream versions.</para>
<itemizedlist>
<listitem>
<para>Akri was a Technology Preview offering in previous Edge releases and is now deprecated.  Removal is planned for a future release.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="tech-previews">
<title>Technology Previews</title>
<para>Unless otherwise stated, these apply to the 3.4.0 release and all subsequent z-stream versions.</para>
<itemizedlist>
<listitem>
<para>Single-stack IPv6 deployments are a Technology Preview offering and are not subject to the standard scope of support.</para>
</listitem>
<listitem>
<para>Precision Time Protocol (PTP) on downstream deployments is a Technology Preview offering and is not subject to standard scope of support.</para>
</listitem>
<listitem>
<para>BGP mode for MetalLB is a Technology Preview offering and is not subject to standard scope of support.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-component-verification">
<title>Component Verification</title>
<para>The components mentioned above may be verified using the Software Bill Of Materials (SBOM) data - for example, using <literal>cosign</literal> as outlined below:</para>
<para>Download the SUSE Edge Container public key from the <link xl:href="https://www.suse.com/support/security/keys/">SUSE Signing Keys source</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>Verify the container image hash, for example using <literal>crane</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1 --platform linux/amd64
sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374</screen>
<note>
<para>For multi-arch images it is also necessary to specify a platform when obtaining the digest, e.g <literal>--platform linux/amd64</literal> or <literal>--platform linux/arm64</literal>. Failure to do this will result in an error in the following step (<literal>Error: no matching attestations</literal>).</para>
</note>
<para>Verify with <literal>cosign</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 &gt; /dev/null
#
Verification for registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The signatures were verified against the specified public key</screen>
<para>Extract SBOM data as described at the <link xl:href="https://www.suse.com/support/security/sbom/">SUSE SBOM documentation</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>Upgrade Steps</title>
<para>Refer to the <xref linkend="day-2-operations"/> for details around how to upgrade to a new release.</para>
</section>
<section xml:id="id-product-support-lifecycle">
<title>Product Support Lifecycle</title>
<para>SUSE Edge is backed by award-winning support from SUSE, an established technology leader with a proven history of delivering enterprise-quality support services. For more information, see <link xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link> and the Support Policy page at <link xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>. If you have any questions about raising a support case, how SUSE classifies severity levels, or the scope of support, please see the Technical Support Handbook at <link xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>.</para>
<para>SUSE Edge "3.4" is supported for 24-months of production support, with an initial 6-months of "full support", followed by 18-months of "maintenance support".  After these support phases the product reaches "end of life" (EOL) and is no longer supported. More info about the lifecycle phases can be found in the table below:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Full Support (6 months)</emphasis></para></entry>
<entry align="left" valign="top"><para>Urgent and selected high-priority bug fixes will be released during the full support window, and all other patches (non-urgent, enhancements, new capabilities) will be released via the regular release schedule.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Maintenance Support (18 months)</emphasis></para></entry>
<entry align="left" valign="top"><para>During this period, only critical fixes will be released via patches. Other bug fixes may be released at SUSE’s discretion but should not be expected.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">End of Life (EOL)</emphasis></para></entry>
<entry align="left" valign="top"><para>Once a product release reaches its End of Life date, the customer may continue to use the product within the terms of product licensing agreement.
Support Plans from SUSE do not apply to product releases past their EOL date.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Unless explicitly stated, all components listed are considered Generally Available (GA), and are covered by SUSE’s standard scope of support. Some components may be listed as "Technology Preview", where SUSE is providing customers with access to early pre-GA features and functionality for evaluation, but are not subject to the standard support policies and are not recommended for production use-cases. SUSE very much welcomes feedback and suggestions on the improvements that can be made to Technology Preview components, but SUSE reserves the right to deprecate a Technology Preview feature before it becomes Generally Available if it doesn’t meet the needs of our customers or doesn’t reach a state of maturity that we require.</para>
<para>Please note that SUSE must occasionally deprecate features or change API specifications. Reasons for feature deprecation or API change could include a feature being updated or replaced by a new implementation, a new feature set, upstream technology is no longer available, or the upstream community has introduced incompatible changes. It is not intended that this will ever happen within a given minor release (x.z), and so all z-stream releases will maintain API compatibility and feature functionality. SUSE will endeavor to provide deprecation warnings with plenty of notice within the release notes, along with workarounds, suggestions, and mitigations to minimize service disruption.</para>
<para>The SUSE Edge team also welcomes community feedback, where issues can be raised within the respective code repository within <link xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>.</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>Obtaining source code</title>
<para>This SUSE product includes materials licensed to SUSE under the GNU General Public License (GPL) and various other open source licenses. The GPL requires SUSE to provide the source code that corresponds to the GPL-licensed material, and SUSE conforms to all other open-source license requirements. As such, SUSE makes all source code available, and can generally be found in the SUSE Edge GitHub repository (<link xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>), the SUSE Rancher GitHub repository (<link xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>) for dependent components, and specifically for SUSE Linux Micro, the source code is available for download at <link xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link> on "Medium 2".</para>
</section>
<section xml:id="id-legal-notices">
<title>Legal notices</title>
<para>SUSE makes no representations or warranties with regard to the contents or use of this documentation, and specifically disclaims any express or implied warranties of merchantability or fitness for any particular purpose. Further, SUSE reserves the right to revise this publication and to make changes to its content, at any time, without the obligation to notify any person or entity of such revisions or changes.</para>
<para>Further, SUSE makes no representations or warranties with regard to any software, and specifically disclaims any express or implied warranties of merchantability or fitness for any particular purpose. Further, SUSE reserves the right to make changes to any and all parts of SUSE software, at any time, without any obligation to notify any person or entity of such changes.</para>
<para>Any products or technical information provided under this Agreement may be subject to U.S. export controls and the trade laws of other countries. You agree to comply with all export control regulations and to obtain any required licenses or classifications to export, re-export, or import deliverables. You agree not to export or re-export to entities on the current U.S. export exclusion lists or to any embargoed or terrorist countries as specified in U.S. export laws. You agree to not use deliverables for prohibited nuclear, missile, or chemical/biological weaponry end uses. Refer to <link xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link> for more information on exporting SUSE software. SUSE assumes no responsibility for your failure to obtain any necessary export approvals.</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>This release notes document is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License (CC-BY-ND-4.0). You should have received a copy of the license along with this document. If not, see <link xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>.</para>
<para>SUSE has intellectual property rights relating to technology embodied in the product that is described in this document. In particular, and without limitation, these intellectual property rights may include one or more of the U.S. patents listed at <link xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link> and one or more additional patents or pending patent applications in the U.S. and other countries.</para>
<para>For SUSE trademarks, see the SUSE Trademark and Service Mark list (<link xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>). All third-party trademarks are the property of their respective owners. For SUSE brand information and usage requirements, please see the guidelines published at <link xl:href="https://brand.suse.com/">https://brand.suse.com/</link>.</para>
</section>
</chapter>
</part>
</book>
