<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book
xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink"
xmlns:its="http://www.w3.org/2005/11/its"
xmlns:xlink="http://www.w3.org/1999/xlink"
xmlns:xi="http://www.w3.org/2001/XInclude" xml:lang="pt-br">
<info>
<title>Documentação do SUSE Edge</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>26-09-2025</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="suse-edge-documentation">
<title>Documentação do SUSE Edge 3.4</title>
<para>Boas-vindas à documentação do SUSE Edge. Você vai encontrar uma visão geral
de alto nível da arquitetura, guias de inicialização rápida, designs
validados, orientação de como usar os componentes, integrações de terceiros
e melhores práticas de gerenciamento da infraestrutura e das cargas de
trabalho de computação de borda.</para>
<section xml:id="id-what-is-suse-edge">
<title>O que é o SUSE Edge?</title>
<para>O SUSE Edge é uma solução completa, amplamente validada, de estreita
integração e desenvolvida com um propósito específico para resolver os
desafios exclusivos da implantação da infraestrutura e dos aplicativos
nativos de nuvem na borda. Ele tem como objetivo principal oferecer uma
plataforma persistente, porém bastante flexível, altamente escalável e
segura que abrange a criação de imagem da implantação inicial, o
provisionamento e a integração de nós, a implantação de aplicativos, a
observabilidade e as operações do ciclo de vida completo. A plataforma foi
criada do zero, com base no melhor software de código aberto, e condiz com
os nossos mais de 30 anos de história como provedores de plataformas SUSE
Linux seguras, estáveis e certificadas e com a nossa experiência em oferecer
gerenciamento Kubernetes altamente escalável e repleto de recursos com o
nosso portfólio Rancher. O SUSE Edge é fundamentado nesses recursos para
entregar funcionalidades que atendem a inúmeros segmentos de mercado, como
varejo, medicina, transporte, logística, telecomunicações, manufatura
inteligente e IoT industrial.</para>
</section>
<section xml:id="id-design-philosophy">
<title>Filosofia do design</title>
<para>A solução foi criada com a ideia de que não existe uma plataforma de borda
do tipo "tamanho único" porque os requisitos e as expectativas dos clientes
variam de maneira significativa. As implantações de borda nos levam a
resolver (e sempre aprimorar) alguns dos problemas mais desafiadores, como
escalabilidade massiva, disponibilidade de rede restrita, limitações de
espaço físico, novas ameaças à segurança e vetores de ataque, variações na
arquitetura de hardware e nos recursos de sistema, a necessidade de
implantar e estabelecer interface com infraestruturas e aplicativos legados
e soluções de clientes com durações estendidas. Como muitos desses desafios
são diferentes das mentalidades tradicionais, por exemplo, a implantação de
infraestrutura e aplicativos em centros de dados ou na nuvem pública,
precisamos analisar o design de modo muito mais detalhado e repensar várias
hipóteses que antes eram comuns.</para>
<para>Por exemplo, consideramos importante o minimalismo, a modularidade e a
facilidade das operações. O minimalismo é essencial nos ambientes de borda,
já que as chances de um sistema falhar aumentam de acordo com a sua
complexidade. Quando há centenas, ou até centenas de milhares de locais, os
sistemas complexos vão apresentar falhas igualmente complexas. A
modularidade em nossa solução oferece mais opções aos usuários e elimina a
complexidade desnecessária da plataforma implantada. Precisamos também levar
em consideração a facilidade das operações. As pessoas tendem a cometer
erros quando repetem um processo milhares de vezes, portanto, a plataforma
deve garantir que possíveis erros sejam recuperáveis, acabando com a
necessidade de visitas técnicas, mas também buscar a consistência e a
padronização.</para>
</section>
<section xml:id="id-high-level-architecture">
<title>Arquitetura de alto nível</title>
<para>A arquitetura de alto nível do SUSE Edge está dividida em duas categorias
principais, os chamados clusters de "gerenciamento" e "downstream". O
cluster de gerenciamento é responsável pelo gerenciamento remoto de um ou
mais clusters downstream, apesar de ser reconhecido que, em determinadas
situações, os clusters downstream precisam operar sem gerenciamento remoto,
por exemplo, quando um site de borda não tem conectividade externa e precisa
operar de maneira independente. No SUSE Edge, os componentes técnicos usados
na operação de ambos os clusters de gerenciamento e downstream são muito
comuns, embora possam apresentar diferenças referentes a especificações de
sistema e aos aplicativos nos quais residem, ou seja, o cluster de
gerenciamento executa aplicativos que possibilitam as operações de ciclo de
vida e gerenciamento de sistemas, enquanto os clusters downstream atendem
aos requisitos para executar os aplicativos de usuário.</para>
<section xml:id="id-components-used-in-suse-edge">
<title>Componentes usados no SUSE Edge</title>
<para>O SUSE Edge é composto de componentes existentes tanto do SUSE quanto do
Rancher, junto com outros recursos e componentes desenvolvidos pela equipe
do Edge, para enfrentarmos as restrições e as complexidades previstas na
computação de borda. Veja abaixo uma explicação dos componentes usados nos
clusters de gerenciamento e downstream, com um diagrama simplificado de alto
nível (observe que não se trata de uma lista completa):</para>
<section xml:id="id-management-cluster">
<title>Cluster de gerenciamento</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-management-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>cluster de gerenciamento do suse edge</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Gerenciamento</emphasis>: trata-se da parte
centralizada do SUSE Edge usada para gerenciar o provisionamento e o ciclo
de vida dos clusters downstream conectados. Normalmente, o cluster de
gerenciamento inclui os seguintes componentes:</para>
<itemizedlist>
<listitem>
<para>Gerenciamento multicluster com o Rancher Prime (<xref
linkend="components-rancher"/>), que oferece um dashboard comum para
integração de clusters downstream e gerenciamento do ciclo de vida contínuo
da infraestrutura e dos aplicativos, além do isolamento total de locatários
e das integrações de <literal>IDP</literal> (provedor de identidade), um
amplo mercado de integrações e extensões de terceiros e uma API independente
de fornecedor.</para>
</listitem>
<listitem>
<para>Gerenciamento de sistemas Linux com o SUSE Multi-Linux Manager, que permite
o gerenciamento automatizado de patches e configurações do sistema
operacional Linux subjacente (*SUSE Linux Micro (<xref
linkend="components-slmicro"/>)) executado nos clusters downstream. Quando
esse componente está conteinerizado, observe que ele precisa ser executado
em um sistema separado dos outros componentes de gerenciamento, devidamente
identificado no diagrama acima como "Linux Management".</para>
</listitem>
<listitem>
<para>Um controlador dedicado de gerenciamento do ciclo de vida (<xref
linkend="components-upgrade-controller"/>) encarregado dos upgrades dos
componentes do cluster de gerenciamento para uma determinada versão do SUSE
Edge.</para>
</listitem>
<listitem>
<para>Integração remota do sistema ao Rancher Prime com Elemental (<xref
linkend="components-elemental"/>), que permite a vinculação posterior dos
nós de borda conectados aos clusters Kubernetes e a implantação de
aplicativos, por exemplo, pelo GitOps.</para>
</listitem>
<listitem>
<para>Suporte opcional completo ao ciclo de vida e gerenciamento bare metal com os
provedores de infraestrutura Metal3 (<xref linkend="components-metal3"/>),
MetalLB (<xref linkend="components-metallb"/>) e <literal>CAPI</literal>
(Cluster API), que permite o provisionamento total, de ponta a ponta, dos
sistemas bare metal com recursos de gerenciamento remoto.</para>
</listitem>
<listitem>
<para>Um mecanismo opcional do GitOps chamado Fleet (<xref
linkend="components-fleet"/>) que gerencia o provisionamento e o ciclo de
vida dos clusters downstream e dos aplicativos que residem neles.</para>
</listitem>
<listitem>
<para>O cluster de gerenciamento conta com o SUSE Linux Micro (<xref
linkend="components-slmicro"/>) como o alicerce do sistema operacional de
base e com o RKE2 (<xref linkend="components-rke2"/>) como a distribuição
Kubernetes que oferece suporte aos aplicativos do cluster de gerenciamento.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-downstream-clusters">
<title>Clusters downstream</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-downstream-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>cluster downstream do suse edge</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Downstream</emphasis>: trata-se da parte distribuída
do SUSE Edge usada para executar as cargas de trabalho dos usuários no Edge,
ou seja, o software que é executado no próprio local de borda, e que
normalmente inclui os seguintes componentes:</para>
<itemizedlist>
<listitem>
<para>Uma seleção de distribuições Kubernetes, com distribuições seguras e leves
como K3s (<xref linkend="components-k3s"/>) e RKE2 (<xref
linkend="components-rke2"/>) (<literal>RKE2</literal> é protegido,
certificado e otimizado para uso em órgãos governamentais e setores
regulamentados).</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>) para habilitar
recursos de segurança, como verificação de vulnerabilidade de imagens,
inspeção detalhada de pacotes e proteção contra ameaças e vulnerabilidades
em tempo real.</para>
</listitem>
<listitem>
<para>Armazenamento de software em blocos com o SUSE Storage (<xref
linkend="components-suse-storage"/>), que possibilita o armazenamento em
blocos leve, persistente, resiliente e escalável.</para>
</listitem>
<listitem>
<para>Um sistema operacional Linux leve, protegido e otimizado para contêiner com
o SUSE Linux Micro (<xref linkend="components-slmicro"/>), que oferece um SO
imutável e altamente resiliente para executar contêineres e máquinas
virtuais na borda. O SUSE Linux Micro está disponível para ambas as
arquiteturas AArch64 e AMD64/Intel 64, além de oferecer suporte ao
<literal>kernel Real-Time</literal> para aplicativos sensíveis à latência
(ex. casos de uso em telecomunicações).</para>
</listitem>
<listitem>
<para>Para os clusters conectados (ou seja, os que têm conectividade com o cluster
de gerenciamento), dois agentes são implantados: Agente do sistema Rancher,
para gerenciar a conectividade com o Rancher Prime, e venv-salt-minion, para
receber as instruções do SUSE Multi-Linux Manager e aplicar as atualizações
de software Linux. Eles não são obrigatórios para o gerenciamento de
clusters desconectados.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-connectivity">
<title>Conectividade</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-connected-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>arquitetura conectada do suse edge</phrase></textobject>
</mediaobject>
</informalfigure>
<para>A imagem acima apresenta uma visão geral de alto nível da arquitetura dos
clusters downstream <emphasis role="strong">conectados</emphasis> e sua
relação com o cluster de gerenciamento. É possível implantar o cluster de
gerenciamento em uma ampla variedade de plataformas de infraestrutura
subjacentes, em instalações tanto no local quanto na nuvem, dependendo da
disponibilidade de rede entre os clusters downstream e o cluster de
gerenciamento de destino. O único requisito para que isso funcione é que a
API e o URL de callback estejam acessíveis pela rede que conecta os nós do
cluster downstream à infraestrutura de gerenciamento.</para>
<para>É importante reconhecer que existem mecanismos em que a conectividade é
estabelecida e que são diferentes do mecanismo de implantação do cluster
downstream. Há uma explicação mais detalhada sobre isso na próxima seção;
mas, para um entendimento geral, há três mecanismos principais para
estabelecer os clusters downstream conectados como um cluster "gerenciado":</para>
<orderedlist numeration="arabic">
<listitem>
<para>Os clusters downstream são implantados primeiro no modo "desconectado" (por
exemplo, pelo Edge Image Builder (<xref linkend="components-eib"/>)) e
depois são importados para o cluster de gerenciamento se ou quando a
conectividade permite.</para>
</listitem>
<listitem>
<para>Os clusters downstream são configurados para usar o mecanismo de integração
incorporado (por exemplo, pelo Elemental (<xref
linkend="components-elemental"/>)) e são automaticamente registrados no
cluster de gerenciamento na primeira inicialização, o que permite a
vinculação posterior da configuração do cluster.</para>
</listitem>
<listitem>
<para>Os clusters downstream foram provisionados com os recursos de gerenciamento
bare metal (CAPI + Metal<superscript>3</superscript>) e são automaticamente
importados para o cluster de gerenciamento após a implantação e configuração
do cluster (pelo operador Rancher Turtles).</para>
</listitem>
</orderedlist>
<note>
<para>É recomendado implementar vários clusters de gerenciamento para acomodar a
escala de grandes implantações, otimizar recursos para eliminar preocupações
com largura de banda e latência em ambientes geograficamente distribuídos e
minimizar interrupções em caso de indisponibilidade ou atualização do
cluster de gerenciamento. Você encontra os limites de escalabilidade e os
requisitos de sistema atuais do cluster de gerenciamento <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements">aqui</link>.</para>
</note>
</section>
</section>
<section xml:id="id-common-edge-deployment-patterns">
<title>Padrões de implantação de borda comuns</title>
<para>Devido à variedade de ambientes operacionais e requisitos de ciclo de vida,
implementamos o suporte a alguns padrões de implantação que se adaptam de
maneira flexível aos segmentos de mercado e casos de uso em que o SUSE Edge
atua. Documentamos um guia de inicialização rápida para cada um desses
padrões de implantação para ajudar você a se familiarizar com a plataforma
SUSE Edge de acordo com as suas necessidades. Os três padrões de implantação
com suporte atualmente estão descritos a seguir, com um link para a
respectiva página do guia de início rápido.</para>
<section xml:id="id-directed-network-provisioning">
<title>Provisionamento de rede direcionado</title>
<para>O provisionamento de rede direcionado é por onde você fica sabendo dos
detalhes do hardware em que deseja implantar e tem acesso direto à interface
de gerenciamento fora da banda para orquestrar e automatizar todo o processo
de provisionamento. Nesse cenário, nossos clientes esperam uma solução capaz
de provisionar sites de borda completamente automatizados de um local
centralizado, indo muito além da criação de uma imagem de inicialização,
minimizando as operações manuais no local de borda: basta montar, ligar e
conectar as redes necessárias ao hardware físico, e o processo de automação
liga a máquina por meio do gerenciamento fora da banda (por exemplo, pela
API Redfish) e cuida do provisionamento, da integração e da implantação da
infraestrutura sem intervenção do usuário. A chave para que esse processo
funcione é que os administradores conheçam os sistemas e saibam qual
hardware está em que local, e que a implantação seja realizada centralmente.</para>
<para>Esta solução é a mais robusta, já que você interage diretamente com a
interface de gerenciamento do hardware, trabalha com um hardware conhecido e
tem menos restrições de disponibilidade de rede. Quanto às funcionalidades,
a solução usa amplamente a Cluster API e o Metal<superscript>3</superscript>
para o provisionamento automatizado, de bare metal a sistema operacional,
Kubernetes e aplicativos em camadas, e oferece a opção de se vincular aos
demais recursos de gerenciamento do ciclo de vida comuns de pós-implantação
do SUSE Edge. O início rápido da solução está disponível no <xref
linkend="quickstart-metal3"/>.</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>Provisionamento de rede "phone home"</title>
<para>Pode acontecer de você trabalhar em um ambiente onde o cluster de
gerenciamento central não pode gerenciar o hardware diretamente (por
exemplo, a rede remota é protegida por firewall ou não há uma interface de
gerenciamento fora da banda; o que é comum em hardware do tipo "PC" mais
encontrado na borda). Nesse cenário, oferecemos ferramentas para provisão
remota de clusters e suas cargas de trabalho sem a necessidade de saber para
onde o hardware é enviado quando é iniciado. É isto que a maioria das
pessoas pensa sobre computação de borda: são milhares, ou dezenas de
milhares, de sistemas um tanto desconhecidos que são iniciados nos locais de
borda e se comunicam de maneira segura com a central ("phone home"),
validando suas identidades e recebendo as instruções do que devem
fazer. Nesse caso, nossos requisitos são o provisionamento e o gerenciamento
do ciclo de vida com muito pouca intervenção do usuário, além de pré-criar a
imagem da máquina de fábrica ou simplesmente anexar uma imagem de
inicialização, por exemplo, por USB, e ligar o sistema. O principal desafio
nesse caso é abordar a escala, a consistência, a segurança e o ciclo de vida
desses dispositivos no mundo real.</para>
<para>Esta solução oferece um ótimo nível de flexibilidade e consistência no modo
como os sistemas são provisionados e integrados, seja qual for seu local,
tipo ou especificação ou quando são ligados pela primeira vez. O SUSE Edge
oferece total flexibilidade e personalização do sistema pelo Edge Image
Builder, além de aproveitar a oferta de recursos de registro Elemental do
Rancher para integração de nós e provisionamento do Kubernetes, junto com o
SUSE Multi-Linux Manager para aplicação de patches no sistema operacional. O
início rápido da solução está disponível no <xref
linkend="quickstart-elemental"/>.</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>Provisionamento com base na imagem</title>
<para>Para os clientes que precisam trabalhar em ambientes independentes,
air-gapped ou com limitações de rede, o SUSE Edge oferece uma solução que
permite gerar mídias de instalação totalmente personalizadas com todos os
artefatos de implantação necessários para habilitar clusters Kubernetes de
alta disponibilidade na borda, tanto de um quanto de vários nós, incluindo
qualquer carga de trabalho ou componentes adicionais em camadas que sejam
necessários. Tudo isso sem precisar de conectividade de rede com ambientes
externos e sem a intervenção de uma plataforma de gerenciamento
centralizada. A experiência do usuário é muito parecida com a solução "phone
home", no que se refere à mídia de instalação fornecida aos sistemas de
destino, mas a solução é "iniciada no local". Nesse cenário, é possível
conectar os clusters resultantes ao Rancher para gerenciamento contínuo (ou
seja, passar do modo de operação "desconectado" para "conectado" sem
precisar de reconfiguração ou reimplantação significativa) ou continuar a
operação de forma isolada. Observe que, em ambos os casos, é possível usar o
mesmo mecanismo consistente para automatizar as operações de ciclo de vida.</para>
<para>Além disso, é possível usar a solução para criar rapidamente clusters de
gerenciamento que podem hospedar a infraestrutura centralizada que sustenta
os modelos de provisionamento de rede tanto "direcionado" quanto "phone
home", já que pode ser a forma mais rápida e simples de provisionar todos os
tipos de infraestrutura de borda. Essa solução faz uso intensivo dos
recursos do SUSE Edge Image Builder para criar mídias de instalação
totalmente personalizadas e autônomas. O início rápido está disponível no
<xref linkend="quickstart-eib"/>.</para>
</section>
</section>
<section xml:id="id-suse-edge-stack-validation">
<title>Validação da pilha do SUSE Edge</title>
<para>Todas as versões do SUSE Edge têm componentes estreitamente integrados e
validados na íntegra que são lançados juntos. Como parte do trabalho
constante de integração e validação de pilha que testa a integração entre os
componentes e garante que o desempenho do sistema atenda às expectativas em
cenários de falha forçada, a equipe do SUSE Edge publica todas as execuções
e os resultados dos testes. Os resultados e todos os parâmetros de entrada
estão disponíveis em <link
xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link>.</para>
</section>
<section xml:id="id-full-component-list">
<title>Lista completa de componentes</title>
<para>A lista completa de componentes, com um link para a descrição de alto nível
de cada um e como são usados no SUSE Edge, está disponível abaixo:</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Extensões do Rancher Dashboard (<xref
linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>Rancher Turtles (<xref linkend="components-rancher-turtles"/>)</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux Manager</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>Akri (<xref linkend="components-akri"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>SUSE Storage (<xref linkend="components-suse-storage"/>)</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>)</para>
</listitem>
<listitem>
<para>Controller de upgrde (<xref linkend="components-upgrade-controller"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>Guias de inicialização rápida</title>
<partintro>
<para>Estes são os guias de inicialização rápida</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>Implantações automatizadas de BMC com Metal<superscript>3</superscript></title>
<para>Metal<superscript>3</superscript> é um <link
xl:href="https://metal3.io/">projeto da CNCF</link> que oferece recursos de
gerenciamento de infraestrutura bare metal para Kubernetes.</para>
<para>O Metal<superscript>3</superscript> inclui recursos nativos do Kubernetes
para gerenciar o ciclo de vida de servidores bare metal que suportam
gerenciamento com protocolos fora da banda, como o <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>Ele também conta com suporte a <link
xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link>, que
permite gerenciar os recursos da infraestrutura de vários provedores por
meio de APIs amplamente adotadas e independentes de fornecedor.</para>
<section xml:id="id-why-use-this-method">
<title>Por que usar este método</title>
<para>Este método é útil em cenários com hardware de destino que permite o
gerenciamento fora da banda e quando se deseja um fluxo de gerenciamento de
infraestrutura automatizado.</para>
<para>O cluster de gerenciamento é configurado para oferecer APIs declarativas que
permitem o gerenciamento de inventário e de estado dos servidores bare metal
do cluster downstream, incluindo inspeção automatizada, limpeza e
provisionamento/desprovisionamento.</para>
</section>
<section xml:id="id-high-level-architecture-2">
<title>Arquitetura de alto nível</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>inicialização rápida da arquitetura do metal3</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>Pré-requisitos</title>
<para>Há algumas restrições específicas relacionadas ao hardware e à rede de
servidor do cluster downstream:</para>
<itemizedlist>
<listitem>
<para>Cluster de gerenciamento</para>
<itemizedlist>
<listitem>
<para>É preciso ter conectividade de rede com a API de gerenciamento/BMC do
servidor de destino</para>
</listitem>
<listitem>
<para>É preciso ter conectividade com a rede do plano de controle do servidor de
destino</para>
</listitem>
<listitem>
<para>Para clusters de gerenciamento de vários nós, é necessário um endereço IP
reservado adicional</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Hosts para serem controlados</para>
<itemizedlist>
<listitem>
<para>É preciso ter suporte a gerenciamento fora da banda pelas interfaces do
Redfish, iDRAC ou iLO</para>
</listitem>
<listitem>
<para>É preciso ter suporte à implantação por mídia virtual (não há suporte a PXE
no momento)</para>
</listitem>
<listitem>
<para>É preciso ter conectividade de rede com o cluster de gerenciamento para
acessar as APIs de provisionamento do Metal<superscript>3</superscript></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Algumas ferramentas são necessárias e podem ser instaladas no cluster de
gerenciamento ou em um host que possa acessá-lo.</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>,
<link xl:href="https://helm.sh">Helm</link> e <link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para>Um tempo de execução do contêiner, como <link
xl:href="https://podman.io">Podman</link> ou <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link></para>
</listitem>
</itemizedlist>
<para>Faça download do arquivo de imagem de sistema operacional
<literal>SL-Micro.x86_64-6.1-Base-GM.raw</literal> pelo <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link> ou pela <link
xl:href="https://www.suse.com/download/sle-micro/">página de download da
SUSE</link>.</para>
</section>
<section xml:id="id-deployment">
<title>Implantação</title>
<section xml:id="id-setup-management-cluster">
<title>Configurar o cluster de gerenciamento</title>
<para>As etapas básicas de instalação de um cluster de gerenciamento e uso do
Metal<superscript>3</superscript> são:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Instalar um cluster de gerenciamento RKE2</para>
</listitem>
<listitem>
<para>Instalar o Rancher</para>
</listitem>
<listitem>
<para>Instalar um provedor de armazenamento (opcional)</para>
</listitem>
<listitem>
<para>Instalar as dependências do Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>Instalar as dependências da CAPI pelo Rancher Turtles</para>
</listitem>
<listitem>
<para>Criar uma imagem do sistema operacional SLEMicro para hosts do cluster
downstream</para>
</listitem>
<listitem>
<para>Registrar CRs de BareMetalHost para definir o inventário de bare metal</para>
</listitem>
<listitem>
<para>Criar um cluster downstream definindo recursos da CAPI</para>
</listitem>
</orderedlist>
<para>Este guia considera que já existe um cluster RKE2 e que o Rancher (incluindo
o cert-manager) foi instalado, por exemplo, usando o Edge Image Builder
(<xref linkend="components-eib"/>).</para>
<tip>
<para>Estas etapas também podem ser totalmente automatizadas conforme descrito na
documentação do cluster de gerenciamento (<xref
linkend="atip-management-cluster"/>).</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Instalando as dependências do Metal<superscript>3</superscript></title>
<para>O cert-manager deve ser instalado e executado caso não tenha sido junto com
a instalação do Rancher.</para>
<para>É necessário instalar um provedor de armazenamento persistente. O SUSE
Storage é recomendado, mas também é possível usar o
<literal>local-path-provisioner</literal> em ambientes de
desenvolvimento/PoC. A instrução abaixo considera que StorageClass foi <link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">marcada
como padrão</link>; do contrário, será necessária uma configuração adicional
para o gráfico do Metal<superscript>3</superscript>.</para>
<para>Um IP adicional é necessário, que pode ser gerenciado por <link
xl:href="https://metallb.universe.tf/">MetalLB</link> para fornecer um
endpoint consistente aos serviços de gerenciamento do
Metal<superscript>3</superscript>. Esse IP deve fazer parte da sub-rede do
plano de controle e ser reservado para configuração estática (e não fazer
parte de um pool DHCP).</para>
<tip>
<para>Se o cluster de gerenciamento é um nó único, é possível evitar o requisito
de IP flutuante adicional gerenciado por MetalLB. Consulte a <xref
linkend="id-single-node-configuration"/>.</para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>Vamos instalar primeiro o MetalLB:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>Na sequência, definimos um <literal>IPAddressPool</literal> e
<literal>L2Advertisement</literal> usando o IP reservado, especificado
abaixo como <literal>STATIC_IRONIC_IP</literal>:</para>
<screen language="bash" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>Agora é possível instalar o Metal<superscript>3</superscript>:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/charts/metal3 \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="$STATIC_IRONIC_IP"</screen>
</listitem>
<listitem>
<para>A execução do contêiner init pode levar cerca de dois minutos nesta
implantação, portanto, garanta que todos os pods estejam em execução antes
de continuar:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para>Não avance para as etapas seguintes até que todos os pods no namespace
<literal>metal3-system</literal> estejam em execução.</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Instalando as dependências de API do cluster</title>
<para>As dependências de API do cluster são gerenciadas pelo gráfico Helm do
Rancher Turtles:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/charts/rancher-turtles \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</screen>
<para>Após algum tempo, os pods do controlador deverão estar em execução nos
namespaces <literal>capi-system</literal>, <literal>capm3-system</literal>,
<literal>rke2-bootstrap-system</literal> e
<literal>rke2-control-plane-system</literal>.</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>Preparar a imagem do cluster downstream</title>
<para>O Kiwi (<xref linkend="guides-kiwi-builder-images"/>) e o Edge Image Builder
(<xref linkend="components-eib"/>) são usados para preparar a imagem
original do SLEMicro modificado que foi provisionado nos hosts do cluster
downstream.</para>
<para>Neste guia, abordamos a configuração mínima necessária para implantar o
cluster downstream.</para>
<section xml:id="id-image-configuration">
<title>Configuração da imagem</title>
<note>
<para>Primeiro siga o <xref linkend="guides-kiwi-builder-images"/> para gerar uma
imagem nova como a primeira etapa necessária para criar clusters.</para>
</note>
<para>Ao executar o Edge Image Builder, um diretório é montado com base no host,
portanto, é necessário criar uma estrutura de diretórios para armazenar os
arquivos de configuração usados para definir a imagem de destino.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> é o arquivo de definição
da imagem. Consulte o <xref linkend="quickstart-eib"/> para obter mais
detalhes.</para>
</listitem>
<listitem>
<para>Quando o download da imagem base é feito, ela é compactada com
<literal>xz</literal>, que deve ser descompactada com
<literal>unxz</literal> e copiada/movida para a pasta
<literal>base-images</literal>.</para>
</listitem>
<listitem>
<para>A pasta <literal>network</literal> é opcional. Consulte a <xref
linkend="metal3-add-network-eib"/> para obter mais detalhes.</para>
</listitem>
<listitem>
<para>O diretório custom/scripts contém scripts que são executados na primeira
inicialização. Atualmente, um script <literal>01-fix-growfs.sh</literal> é
necessário para redimensionar a partição raiz do sistema operacional na
implantação.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>Arquivo de definição da imagem do cluster downstream</title>
<para>O <literal>downstream-cluster-config.yaml</literal> é o arquivo de
configuração principal para a imagem do cluster downstream. Veja a seguir um
exemplo mínimo de implantação por meio do Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
  sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Em que <literal>$SCC_REGISTRATION_CODE</literal> é o código de registro
copiado do <link xl:href="https://scc.suse.com/">SUSE Customer
Center</link>, e a lista de pacotes contém o <literal>jq</literal>, que é
obrigatório.</para>
<para><literal>$ROOT_PASSWORD</literal> é a senha criptografada do usuário root,
que pode ser útil para teste/depuração. É possível gerá-la com o comando
<literal>openssl passwd -6 PASSWORD</literal>.</para>
<para>Para os ambientes de produção, a recomendação é usar as chaves SSH que podem
ser adicionadas ao bloco de usuários substituindo a
<literal>$USERKEY1</literal> pelas chaves SSH reais.</para>
<note>
<para>O <literal>net.ifnames=1</literal> habilita a <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Nomenclatura
de interface de rede previsível</link>.</para>
<para>Isso corresponde à configuração padrão do gráfico do
Metal<superscript>3</superscript>, mas a configuração deve corresponder ao
valor <literal>predictableNicNames</literal> do gráfico.</para>
<para>Veja também que o <literal>ignition.platform.id=openstack</literal> é
obrigatório. Sem esse argumento, a configuração do SUSE Linux Micro pelo
ignition vai falhar no fluxo automatizado do
Metal<superscript>3</superscript>.</para>
<para>A seção <literal>time</literal> é opcional, mas é altamente recomendado
configurá-la para evitar possíveis problemas com certificados e divergência
do relógio. Os valores inseridos neste exemplo são meramente
ilustrativos. Ajuste-os de acordo com os seus requisitos específicos.</para>
</note>
</section>
<section xml:id="growfs-script">
<title>Script growfs</title>
<para>Atualmente, é necessário um script personalizado
(<literal>custom/scripts/01-fix-growfs.sh</literal>) para expandir o sistema
de arquivos de acordo com o tamanho do disco na primeira inicialização após
o provisionamento. O script <literal>01-fix-growfs.sh</literal> contém as
seguintes informações:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>Adicione seus próprios scripts personalizados para execução durante o
processo de provisionamento usando a mesma abordagem. Para obter mais
informações, consulte o <xref linkend="quickstart-eib"/>.</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>Criação de imagem</title>
<para>Depois que a estrutura de diretórios for preparada de acordo com as seções
anteriores, execute o seguinte comando para criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>Ele cria o arquivo de imagem de saída chamado
<literal>SLE-Micro-eib-output.raw</literal>, com base na definição descrita
acima.</para>
<para>Depois disso, a imagem de saída deverá estar disponível em um servidor web,
ou o contêiner de servidor de mídia habilitado pelo gráfico do Metal3 (<xref
linkend="metal3-media-server"/>) ou um outro servidor acessível
localmente. Nos exemplos a seguir, chamamos esse servidor de
<literal>imagecache.local:8080</literal>.</para>
<note>
<para>Na implantação de imagens EIB em clusters downstream, é necessário também
incluir a soma sha256 da imagem no objeto
<literal>Metal3MachineTemplate</literal>. É possível gerá-la com este
comando:</para>
<screen language="shell" linenumbering="unnumbered">sha256sum &lt;image_file&gt; &gt; &lt;image_file&gt;.sha256
# On this example:
sha256sum SLE-Micro-eib-output.raw &gt; SLE-Micro-eib-output.raw.sha256</screen>
</note>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>Adicionando o inventário de BareMetalHost</title>
<para>Para registrar servidores bare metal para implantação automatizada, é
necessário criar dois recursos: um segredo com as credenciais de acesso ao
BMC e um recurso BareMetalHost do Metal<superscript>3</superscript> com a
definição da conexão com o BMC e outros detalhes:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>Observe o seguinte:</para>
<itemizedlist>
<listitem>
<para>O nome de usuário/senha do segredo deve ser codificado com base64. Observe
que ele não deve incluir novas linhas à direita (por exemplo, usar
<literal>echo -n</literal>, não apenas <literal>echo</literal>!)</para>
</listitem>
<listitem>
<para>É possível definir o rótulo <literal>cluster-role</literal> agora ou mais
tarde, durante a criação do cluster. No exemplo abaixo, esperamos
<literal>control-plane</literal> ou <literal>worker</literal></para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal> deve ser um MAC válido correspondente à
NIC de plano de controle do host</para>
</listitem>
<listitem>
<para>O endereço <literal>bmc</literal> é a conexão com a API de gerenciamento de
BMC. As seguintes opções são suportadas:</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;ENDEREÇO
IP&gt;/redfish/v1/Systems/&lt;ID DO SISTEMA&gt;</literal>: mídia virtual
Redfish, por exemplo, SuperMicro</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;ENDEREÇO
IP&gt;/redfish/v1/Systems/System.Embedded.1</literal>: iDRAC da Dell</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Consulte os <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">documentos
da API upstream</link> para obter mais detalhes sobre a API BareMetalHost</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>Configurando IPs estáticos</title>
<para>O exemplo do BareMetalHost acima considera que o DHCP configura a rede do
plano de controle, mas nos cenários em que a configuração manual é
necessária, como no caso dos IPs estáticos, é possível definir uma
configuração adicional, conforme descrito abaixo.</para>
<section xml:id="metal3-add-network-eib">
<title>Script adicional para configuração de rede estática</title>
<para>Ao criar a imagem base com o Edge Image Builder, na pasta
<literal>network</literal>, crie o arquivo
<literal>configure-network.sh</literal> a seguir.</para>
<para>Esse procedimento consome os dados da unidade de configuração na primeira
inicialização e configura a rede do host usando a <link
xl:href="https://github.com/suse-edge/nm-configurator">ferramenta NM
Configurator</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>Segredo adicional com a configuração de rede do host</title>
<para>É possível definir um segredo adicional com os dados no formato <link
xl:href="https://nmstate.io/">nmstate</link> suportado pela ferramenta NM
Configurator (<xref linkend="components-nmc"/>) para cada host.</para>
<para>Depois disso, o segredo será referenciado no recurso
<literal>BareMetalHost</literal> pelo campo de especificação
<literal>preprovisioningNetworkDataName</literal>.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>Em algumas situações, é possível omitir o endereço MAC. Consulte a <xref
linkend="networking-unified"/> para obter mais detalhes.</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>Preparação do BareMetalHost</title>
<para>Depois de criar o recurso BareMetalHost e os segredos associados conforme
descrito acima, será acionado um fluxo de trabalho de preparação do host:</para>
<itemizedlist>
<listitem>
<para>Uma imagem de disco RAM será iniciada depois de conectar a mídia virtual ao
BMC do host de destino</para>
</listitem>
<listitem>
<para>O disco RAM inspeciona os detalhes do hardware e prepara o host para
provisionamento (por exemplo, limpando os discos de dados anteriores)</para>
</listitem>
<listitem>
<para>Ao término desse processo, os detalhes do hardware no campo
<literal>status.hardware</literal> do BareMetalHost serão atualizados e
poderão ser verificados</para>
</listitem>
</itemizedlist>
<para>Esse processo pode levar bastante tempo, mas quando for concluído, você verá
que o estado do BareMetalHost deve mudar para <literal>available</literal>:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>Criando clusters downstream</title>
<para>Agora criamos os recursos de Cluster API que definem o cluster downstream e
os recursos de Machine que provisionam e, depois, iniciam os recursos do
BareMetalHost para formar um cluster RKE2.</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>Implantação do plano de controle</title>
<para>Para implantar o plano de controle, definimos um manifesto YAML semelhante
ao mostrado abaixo, que contém os seguintes recursos:</para>
<itemizedlist>
<listitem>
<para>O recurso Cluster define o nome do cluster, as redes e o tipo de provedor de
plano de controle/infraestrutura (neste caso, RKE2/Metal3)</para>
</listitem>
<listitem>
<para>Metal3Cluster define o endpoint do plano de controle (IP do host para nó
único, endpoint LoadBalancer para vários nós; neste exemplo, foi considerado
o nó único)</para>
</listitem>
<listitem>
<para>RKE2ControlPlane define a versão RKE2 e a configuração adicional necessária
durante a inicialização do cluster</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate define a imagem do sistema operacional que será
aplicada aos recursos do BareMetalHost, e hostSelector define quais
BareMetalHosts serão consumidos</para>
</listitem>
<listitem>
<para>Metal3DataTemplate define o metaData adicional que será passado para o
BareMetalHost (veja que o networkData não é suportado na solução Edge)</para>
</listitem>
</itemizedlist>
<note>
<para>Para simplificar, este exemplo considera um plano de controle de nó único em
que o BareMetalHost está configurado com o IP
<literal>192.168.125.200</literal>. Para ver exemplos mais avançados de
vários nós, consulte o <xref linkend="atip-automated-provisioning"/>.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Depois de adaptado ao seu ambiente, aplique o exemplo usando o
<literal>kubectl</literal> e monitore o status do cluster por meio do
<literal>clusterctl</literal>.</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>Implantação de worker/computação</title>
<para>De maneira semelhante à implantação do plano de controle, definimos um
manifesto YAML que contém os seguintes recursos:</para>
<itemizedlist>
<listitem>
<para>MachineDeployment define o número de réplicas (hosts) e o provedor de
inicialização/infraestrutura (neste caso, RKE2/Metal3)</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplate descreve a versão RKE2 e a configuração da primeira
inicialização para inicializar o host do agente</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate define a imagem do sistema operacional que será
aplicada aos recursos do BareMetalHost, e o seletor de host define quais
BareMetalHosts serão consumidos</para>
</listitem>
<listitem>
<para>Metal3DataTemplate define os metadados adicionais que serão passados para o
BareMetalHost (veja que o <literal>networkData</literal> não é suportado
atualmente)</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.33.3+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.33.3+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Quando o exemplo acima for copiado e adaptado ao seu ambiente, ele poderá
ser aplicado usando o <literal>kubectl</literal> e, depois, o status do
cluster poderá ser monitorado com o <literal>clusterctl</literal></para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait for the worker nodes to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>Desprovisionamento do cluster</title>
<para>É possível desprovisionar o cluster downstream excluindo os recursos que
foram aplicados nas etapas de criação acima:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>Esse procedimento aciona o desprovisionamento dos recursos do BareMetalHost,
o que pode levar bastante tempo. Depois disso, eles deverão voltar para o
estado disponível:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>Problemas conhecidos</title>
<itemizedlist>
<listitem>
<para>O <link
xl:href="https://github.com/metal3-io/ip-address-manager">controlador de
gerenciamento de endereços IP</link> upstream não é suportado no momento
porque ainda não é compatível com a nossa seleção de ferramentas de
configuração de rede e conjunto de ferramentas de primeira inicialização no
SLEMicro.</para>
</listitem>
<listitem>
<para>De modo semelhante, os recursos IPAM e os campos networkData de
Metal3DataTemplate não são suportados no momento.</para>
</listitem>
<listitem>
<para>Apenas há suporte para implantação por redfish-virtualmedia.</para>
</listitem>
<listitem>
<para>É possível observar um desalinhamento de nome de dispositivo de rede entre o
Ironic Python Agent (IPA) e o sistema operacional de destino (SL Micro
6.0/6.1), principalmente ao tentar configurar nomes previsíveis para os
dispositivos.</para>
</listitem>
</itemizedlist>
<para>Isso acontece porque o kernel do Ironic Python Agent (IPA) não está alinhado
ao kernel do sistema operacional de destino (SL Micro 6.0/6.1), portanto, há
um desalinhamento nos drivers de rede que permite que o IPA descubra
dispositivos de rede em um padrão de nomenclatura diferente do que o
esperado pelo SL Micro.</para>
<para>Por enquanto, há duas abordagens distintas para adotar como solução
alternativa: * Criar dois segredos diferentes com a configuração de rede: um
para aplicar ao IPA, usando os mesmos nomes de dispositivo que o IPA
descobrirá, e usá-lo como <literal>preprovisioningNetworkDataName</literal>
na definição de <literal>BareMetalHost</literal>; e o outro segredo, com os
mesmos nomes de dispositivo que o SL Micro descobrirá, para usar como
<literal>networkData.name</literal> na definição de
<literal>BareMetalHost</literal>. * Usar os UUIDs para fazer referência a
outras interfaces nos arquivos nmconnection gerenciados. Há mais detalhes na
seção <link xl:href="..tips/metal3.adoc">Dicas e truques</link>.</para>
</section>
<section xml:id="id-planned-changes">
<title>Alterações planejadas</title>
<itemizedlist>
<listitem>
<para>Habilitar suporte de recursos e configuração do IPAM pelos campos
networkData</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>Recursos adicionais</title>
<para>A documentação do SUSE Telco Cloud (<xref linkend="atip"/>) tem exemplos de
uso mais avançado do Metal<superscript>3</superscript> para casos de uso em
telecomunicações.</para>
<section xml:id="id-single-node-configuration">
<title>Configuração de nó único</title>
<para>Para ambientes de teste/PoC em que o cluster de gerenciamento é um nó único,
é possível evitar o requisito de IP flutuante adicional gerenciado por
MetalLB.</para>
<para>Nesse modo, o endpoint para as APIs do cluster de gerenciamento é o IP do
cluster de gerenciamento, portanto, ele deve ser reservado ao usar DHCP ou
configurado estaticamente para garantir que não seja alterado. Mencionado
abaixo como <literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal>.</para>
<para>Para possibilitar esse cenário, os valores do gráfico do
Metal<superscript>3</superscript> necessários são estes:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="disabling-tls-for-virtualmedia-iso-attachment">
<title>Desabilitando TLS para anexo de ISO de mídia virtual</title>
<para>Alguns fornecedores de servidor verificam a conexão SSL ao anexar imagens
ISO de mídia virtual ao BMC, o que pode causar um problema porque os
certificados gerados para a implantação do Metal<superscript>3</superscript>
são autoassinados. Para solucionar esse problema, desabilite o TLS apenas
para anexo de disco de mídia virtual com os valores do gráfico do
Metal<superscript>3</superscript> a seguir:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>Uma solução alternativa é configurar os BMCs com o certificado de CA. Nesse
caso, você pode ler os certificados do cluster usando o
<literal>kubectl</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>Depois disso, o certificado poderá ser configurado no console de BMC do
servidor, embora o processo para isso seja específico do fornecedor (e pode
não ser possível para todos os fornecedores e, consequentemente, exigir o
uso do sinalizador <literal>enable_vmedia_tls</literal>).</para>
</section>
<section xml:id="id-storage-configuration">
<title>Configuração do armazenamento</title>
<para>Para ambientes de teste/PoC em que o cluster de gerenciamento é um nó único,
não há necessidade de armazenamento persistente; mas em casos de uso de
produção, a recomendação é instalar o SUSE Storage (Longhorn) no cluster de
gerenciamento para que as imagens relacionadas ao
Metal<superscript>3</superscript> persistam durante a
reinicialização/reprogramação de um pod.</para>
<para>Para habilitar o armazenamento persistente, os valores do gráfico do
Metal<superscript>3</superscript> necessários são estes:</para>
<screen language="yaml" linenumbering="unnumbered">metal3-ironic:
  persistence:
    ironic:
      size: "5Gi"</screen>
<para>A documentação do cluster de gerenciamento do SUSE Telco Cloud (<xref
linkend="atip-management-cluster"/>) tem mais detalhes de como configurar um
cluster de gerenciamento com armazenamento persistente.</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Integração remota de host com o Elemental</title>
<para>Esta seção apresenta a solução de "provisionamento de rede 'phone home'"
como parte do SUSE Edge, em que usamos o Elemental para auxiliar na
integração de nós. O Elemental é uma pilha de software que permite o
registro remoto de hosts e o gerenciamento de sistema operacional nativo de
nuvem e totalmente centralizado com o Kubernetes. Na pilha do SUSE Edge,
usamos o recurso de registro do Elemental para fazer a integração remota de
hosts no Rancher, o que permite integrar os hosts a uma plataforma de
gerenciamento centralizada e, nela, implantar e gerenciar os clusters
Kubernetes junto com os componentes em camadas, os aplicativos e o ciclo de
vida deles, tudo de um único local.</para>
<para>Essa abordagem pode ser útil em cenários em que os dispositivos que você
deseja controlar não estão na mesma rede que o cluster de gerenciamento, ou
não têm uma integração por controlador de gerenciamento fora da banda que
permita um controle mais direto, e quando você inicializa vários sistemas
"desconhecidos" na borda e precisa integrá-los e gerenciá-los com segurança
e em escala. Esse cenário é comum nos casos de uso de varejo, IoT industrial
ou em outros ambientes de baixo controle da rede em que os dispositivos são
instalados.</para>
<section xml:id="id-high-level-architecture-3">
<title>Arquitetura de alto nível</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>inicialização rápida da arquitetura do elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>Recursos necessários</title>
<para>Veja a seguir uma descrição dos requisitos mínimos de sistema e de ambiente
para execução por meio desta inicilização rápida:</para>
<itemizedlist>
<listitem>
<para>Um host para o cluster de gerenciamento centralizado (o mesmo que hospeda o
Rancher e o Elemental):</para>
<itemizedlist>
<listitem>
<para>Mínimo de 8 GB de RAM e 20 GB de espaço em disco para desenvolvimento ou
teste (consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements#hardware-requirements">aqui</link>
para uso em produção)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>O provisionamento de um nó de destino, ou seja, o dispositivo de borda (é
possível usar uma máquina virtual para fins de demonstração ou teste)</para>
<itemizedlist>
<listitem>
<para>Mínimo de 4GB de RAM, 2 núcleos de CPU e disco de 20 GB</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Um nome de host "resolvable" para o cluster de gerenciamento ou um endereço
IP estático para usar com um serviço do tipo sslip.io</para>
</listitem>
<listitem>
<para>Um host para montar a mídia de instalação usando o Edge Image Builder</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP6, openSUSE Leap 15.6 ou outro sistema operacional compatível em
execução com suporte ao Podman</para>
</listitem>
<listitem>
<para>Com o <link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>,
o <link xl:href="https://podman.io">Podman</link> e o <link
xl:href="https://helm.sh">Helm</link> instalados</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Uma unidade flash USB de origem para inicialização (em caso de hardware
físico)</para>
</listitem>
<listitem>
<para>Uma cópia baixada da imagem ISO mais recente do SelfInstall do SUSE Linux
Micro 6.1 disponível <link
xl:href="https://www.suse.com/download/sle-micro/">aqui</link>.</para>
</listitem>
</itemizedlist>
<note>
<para>Os dados existentes nas máquinas de destino serão substituídos como parte do
processo. Faça um backup dos dados em dispositivos de armazenamento USB e
discos conectados aos nós de implantação de destino.</para>
</note>
<para>Este guia foi criado com um droplet da DigitalOcean para hospedar o cluster
upstream e com um Intel NUC como dispositivo downstream. Para montar a mídia
de instalação, o SUSE Linux Enterprise Server foi usado.</para>
</section>
<section xml:id="build-bootstrap-cluster">
<title>Criar o cluster de inicialização</title>
<para>Para começar, crie um cluster capaz de hospedar o Rancher e o
Elemental. Esse cluster precisa ser roteável na rede à qual os nós
downstream estão conectados.</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Criar o cluster Kubernetes</title>
<para>Se você usa um hiperescalador (como Azure, AWS ou Google Cloud), a maneira
mais fácil de configurar um cluster é usar as ferramentas incorporadas
dele. Para manter este guia conciso, não detalhamos o processo de cada uma
dessas opções.</para>
<para>Se você está instalando em um serviço de hospedagem bare metal, ou algum
outro, e precisa também providenciar a própria distribuição Kubernetes,
recomendamos usar o <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>.</para>
</section>
<section xml:id="id-set-up-dns">
<title>Configurar o DNS</title>
<para>Antes de continuar, você precisa configurar o acesso ao cluster. Assim como
na configuração do próprio cluster, a configuração do DNS será diferente
dependendo do local onde ele está hospedado.</para>
<tip>
<para>Se você não deseja configurar registros DNS (por exemplo, quando se trata
apenas de um servidor de teste efêmero), uma alternativa é usar um serviço
como o <link xl:href="https://sslip.io">sslip.io</link>. Com ele, você
resolve qualquer endereço IP com
<literal>&lt;endereço&gt;.sslip.io</literal>.</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Instalar o Rancher</title>
<para>Para instalar o Rancher, você precisa acessar a API Kubernetes do cluster
que acabou de criar. Esse procedimento muda dependendo da distribuição
Kubernetes que é usada.</para>
<para>Para o RKE2, o arquivo kubeconfig será gravado em
<literal>/etc/rancher/rke2/rke2.yaml</literal>. Salve-o como
<literal>~/.kube/config</literal> no sistema local. Talvez você tenha que
editar o arquivo para incluir o endereço IP ou o nome de host roteável
externamente correto.</para>
<para>Instale o Rancher facilmente com os comandos da <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Documentação
do Rancher</link>:</para>
<para>Instale o <link xl:href="https://cert-manager.io">cert-manager</link>:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true</screen>
<para>Em seguida, instale o próprio Rancher:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt; \
  --version 2.12.1</screen>
<note>
<para>Se a finalidade do sistema é de produção, use o cert-manager para configurar
um certificado real (como aquele da Let’s Encrypt).</para>
</note>
<para>Procure o nome de host que você configurou e faça login no Rancher com a
<literal>bootstrapPassword</literal> usada. Você será guiado por um curto
processo de configuração.</para>
</section>
<section xml:id="install-elemental">
<title>Instalar o Elemental</title>
<para>Com o Rancher instalado, agora você pode instalar o operador Elemental e as
CRDs necessárias. O gráfico Helm do Elemental foi publicado como um artefato
OCI, portanto, a instalação é um pouco mais simples do que a dos outros
gráficos. É possível instalá-lo do mesmo shell usado para instalar o Rancher
ou no navegador dentro do shell do Rancher.</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.7.3

helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.7.3</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>Instalar a extensão de IU do Elemental (opcional)</title>
<orderedlist numeration="arabic">
<listitem>
<para>Para usar a IU do Elemental, faça login na instância do Rancher e clique no
menu de três linhas na parte superior esquerda:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width="85%"/> </imageobject>
<textobject><phrase>Instalando a extensão 1 do Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Na guia "Available" (Disponível) nesta página, clique em "Install"
(Instalar) no cartão Elemental:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width="85%"/> </imageobject>
<textobject><phrase>Instalando a extensão 2 do Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Confirme que você deseja instalar a extensão:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width="100%"/> </imageobject>
<textobject><phrase>Instalando a extensão 3 do Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Após a instalação, será solicitado que você recarregue a página.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width="100%"/> </imageobject>
<textobject><phrase>Instalando a extensão 4 do Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Após o recarregamento, acesse a extensão do Elemental pelo app global "OS
Management".</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width="100%"/> </imageobject>
<textobject><phrase>Acessando a extensão do Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configure-elemental">
<title>Configurar o Elemental</title>
<para>Para simplificar, recomendamos definir a variável <literal>$ELEM</literal>
como o caminho completo do local desejado para o diretório de configuração:</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>Para que as máquinas sejam registradas no Elemental, precisamos criar um
objeto <literal>MachineRegistration</literal> no namespace
<literal>fleet-default</literal>.</para>
<para>Vamos criar uma versão básica desse objeto:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>O comando <literal>cat</literal> insere um caractere de escape de barra
(<literal>\</literal>) para cada <literal>$</literal> para que o Bash não os
utilize como gabarito. Remova as barras se estiver copiando manualmente.</para>
</note>
<para>Depois que o objeto é criado, encontre e observe o endpoint que foi
atribuído:</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>Você também pode fazer isso pela IU.</para>
<variablelist>
<varlistentry>
<term>Extensão de IU</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>Na extensão OS Management, clique em "Create Registration Endpoint" (Criar
endpoint de registro):</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png"
width="100%"/> </imageobject>
<textobject><phrase>Clique em Create Registration (Criar registro).</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Dê um nome para esta configuração.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png"
width="100%"/> </imageobject>
<textobject><phrase>Adicionar nome</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Você pode ignorar o campo Cloud Configuration (Configuração de nuvem) já que
os dados nele serão substituídos ao executar as etapas a seguir com o Edge
Image Builder.</para>
</note>
</listitem>
<listitem>
<para>Em seguida, role para baixo e clique em "Add Label" (Adicionar rótulo) para
cada rótulo que deseja incluir no recurso criado quando a máquina é
registrada. Isso é útil para distinguir as máquinas.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png"
width="100%"/> </imageobject>
<textobject><phrase>Adicionar rótulos</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Clique em "Create" (Criar) para salvar a configuração.</para>
</listitem>
<listitem>
<para>Após a criação do registro, você deverá ver o URL dele listado e poderá
clicar em "Copy" (Copiar) para copiar o endereço:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width="100%"/>
</imageobject>
<textobject><phrase>Copiar URL</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>Se você clicou fora dessa tela, pode clicar em "Registration Endpoints"
(Endpoints de registro) no menu esquerdo e, depois, no nome do endpoint que
acabou de criar.</para>
</tip>
<para>Esse URL será usado na próxima etapa.</para>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="build-installation-media">
<title>Criar a imagem</title>
<para>A versão atual do Elemental tem uma maneira de criar a própria mídia de
instalação. No SUSE Edge 3.4, isso é feito com o Kiwi e o Edge Image
Builder, e o sistema resultante é criado com o <link
xl:href="https://www.suse.com/products/micro/">SUSE Linux Micro</link> como
sistema operacional de base.</para>
<tip>
<para>Para saber mais detalhes sobre o Kiwi, siga o processo do construtor de
imagens do Kiwi (<xref linkend="guides-kiwi-builder-images"/>) para primeiro
criar novas imagens e, para o Edge Image Builder, consulte o Guia de
Introdução do Edge Image Builder (<xref linkend="quickstart-eib"/>) e também
a documentação do componente (<xref linkend="components-eib"/>).</para>
</tip>
<para>Em um sistema Linux com o Podman instalado, crie os diretórios e insira a
imagem base criada pelo Kiwi:</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/{micro-base-image-iso} $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.3
image:
    imageType: iso
    arch: x86_64
    baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: XXX
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para>A seção <literal>time</literal> é opcional, mas é altamente recomendado
configurá-la para evitar possíveis problemas com certificados e divergência
do relógio. Os valores inseridos neste exemplo são meramente
ilustrativos. Ajuste-os de acordo com os seus requisitos específicos.</para>
</listitem>
<listitem>
<para>A senha não codificada é <literal>eib</literal>.</para>
</listitem>
<listitem>
<para>O <literal>sccRegistrationCode</literal> é necessário para fazer download e
instalar os RPMs exigidos das fontes oficiais (se preferir, faça sideload
manual dos RPMs <literal>elemental-register</literal> e o
<literal>elemental-system-agent</literal> ).</para>
</listitem>
<listitem>
<para>O comando <literal>cat</literal> insere um caractere de escape de barra
(<literal>\</literal>) para cada <literal>$</literal> para que o Bash não os
utilize como gabarito. Remova as barras se estiver copiando manualmente.</para>
</listitem>
<listitem>
<para>O dispositivo de instalação será apagado durante a instalação.</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file eib-config.yaml</screen>
<para>Se você está inicializando um dispositivo físico, precisa gravar a imagem em
uma unidade flash USB, o que pode ser feito com o comando:</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>Inicializar os nós downstream</title>
<para>Agora que já criamos a mídia de instalação, podemos inicializar os nós
downstream com ela.</para>
<para>Para cada um dos sistemas que você quer controlar usando o Elemental,
adicione a mídia de instalação e inicialize o dispositivo. Após a
instalação, ele será reinicializado e se registrará.</para>
<para>Se você usa a extensão de IU, deve ver o nó listado em "Inventory of
Machines" (Inventário de máquinas).</para>
<note>
<para>Não remova o meio de instalação antes de ver o prompt de login. Durante a
primeira inicialização, os arquivos ainda são acessados no dispositivo USB.</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>Criar clusters downstream</title>
<para>Precisamos criar dois objetos ao provisionar um novo cluster usando o
Elemental.</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>O primeiro é o <literal>MachineInventorySelectorTemplate</literal>. Esse
objeto permite especificar um mapeamento entre os clusters e as máquinas no
inventário.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Crie um seletor que corresponda qualquer máquina no inventário com um
rótulo:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>Aplique o recurso ao cluster:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>Obtenha o nome da máquina e adicione o rótulo correspondente:</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>Crie um recurso de cluster K3s simples de nó único e aplique-o ao cluster:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.33.3+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Extensão de IU</term>
<listitem>
<para>A extensão de IU permite o uso de alguns atalhos. Observe que o
gerenciamento de vários locais pode envolver um trabalho manual excessivo.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Como já foi feito, abra o menu esquerdo de três linhas e selecione "OS
Management". Você será levado de volta à tela principal para gerenciar seus
sistemas Elemental.</para>
</listitem>
<listitem>
<para>Na barra lateral esquerda, clique em "Inventory of Machines" (Inventário de
máquinas). Essa ação abre o inventário das máquinas que foram registradas.</para>
</listitem>
<listitem>
<para>Para criar um cluster dessas máquinas, selecione os sistemas desejados,
clique na lista suspensa "Actions" (Ações) e em "Create Elemental Cluster"
(Criar cluster Elemental). Isso abre a caixa de diálogo Cluster Creation
(Criação de cluster) e cria, ao mesmo tempo, um MachineSelectorTemplate para
usar em segundo plano.</para>
</listitem>
<listitem>
<para>Nessa tela, configure o cluster que deseja criar. Nesta inicialização
rápida, o K3s v1.30.5+k3s1 foi selecionado, e as demais opções foram
mantidas como estavam.</para>
<tip>
<para>Pode ser necessário rolar para abaixo para ver mais opções.</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>Após a criação dos objetos, você deverá ver um novo cluster Kubernetes dar
arranque (partida) usando o nó com o qual você acabou de instalar.</para>
</section>
<section xml:id="id-node-reset-optional">
<title>Redefinição de nó (opcional)</title>
<para>O SUSE Rancher Elemental permite executar uma "redefinição de nó", que pode
ser acionada quando um cluster inteiro é excluído do Rancher, um único nó é
excluído do cluster ou um nó é excluído manualmente do inventário de
máquinas. Esse recurso é útil para redefinir e limpar recursos órfãos e
recuperar automaticamente o nó limpo no inventário de máquinas para que
possa ser reutilizado. Isso não está habilitado por padrão e, portanto, um
sistema que foi removido não será limpo (ou seja, os dados não serão
removidos, e os recursos do cluster Kubernetes continuarão operando nos
clusters downstream), será necessária uma intervenção manual para limpar os
dados e registrar a máquina novamente no Rancher pelo Elemental.</para>
<para>Para habilitar essa funcionalidade por padrão, você precisa garantir que o
<literal>MachineRegistration</literal> a habilite explicitamente adicionando
<literal>config.elemental.reset.enabled: true</literal>, por exemplo:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>Na sequência, todos os sistemas registrados com esse
<literal>MachineRegistration</literal> receberão automaticamente a anotação
<literal>elemental.cattle.io/resettable: 'true'</literal> na respectiva
configuração. Para fazer isso manualmente em nós individuais, por exemplo,
porque obteve um <literal>MachineInventory</literal> existente que não tem
essa anotação ou já implantou os nós, você pode modificar o
<literal>MachineInventory</literal> e adicionar a configuração
<literal>resettable</literal>, por exemplo:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>No SUSE Edge 3.1, o operador Elemental insere um marcador no sistema
operacional que aciona o processo de limpeza automaticamente; ele interrompe
todos os serviços do Kubernetes, remove todos os dados persistentes,
desinstala todos os serviços do Kubernetes, limpa os diretórios restantes do
Kubernetes/Rancher e força o novo registro no Rancher por meio da
configuração original <literal>MachineRegistration</literal> do
Elemental. Isso é feito automaticamente, não há necessidade de intervenção
manual. O script chamado está disponível em
<literal>/opt/edge/elemental_node_cleanup.sh</literal> e é acionado pelo
<literal>systemd.path</literal> depois de inserir o marcador, portanto, sua
execução é imediata.</para>
<warning>
<para>Com o uso do <literal>resettable</literal>, a funcionalidade assume que o
comportamento desejado ao remover um nó/cluster do Rancher é limpar os dados
e forçar um novo registro. A perda de dados é certa nessa situação,
portanto, use essa funcionalidade apenas se tiver certeza de que deseja
executar a redefinição automática.</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>Próximas etapas</title>
<para>Veja alguns recursos recomendados para pesquisa depois de usar este guia:</para>
<itemizedlist>
<listitem>
<para>Automação de ponta a ponta no <xref linkend="components-fleet"/></para>
</listitem>
<listitem>
<para>Mais opções de configuração de rede no <xref linkend="components-nmc"/></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Clusters independentes com o Edge Image Builder</title>
<para>O Edge Image Builder (EIB) é uma ferramenta que simplifica o processo de
geração de imagens de disco personalizadas e prontas para inicialização
(CRB) para inicializar máquinas, mesmo em cenários totalmente air-gapped. O
EIB é usado para criar imagens de implantação para uso em todas as três
áreas de implantação do SUSE Edge, já que é flexível o suficiente para
oferecer desde as menores personalizações, como adicionar um usuário ou
definir o fuso horário, até uma imagem amplamente configurada que comporta,
por exemplo, configurações de redes complexas, implantações de clusters
Kubernetes de vários nós, implantações de cargas de trabalho de clientes e
registros na plataforma de gerenciamento centralizado pelo Rancher/Elemental
e SUSE Multi-Linux Manager. O EIB é executado como uma imagem de contêiner,
o que o torna incrivelmente portátil entre as plataformas e garante que
todas as dependências necessárias sejam autossuficientes, exercendo um
impacto muito mínimo sobre os pacotes instalados do sistema usado para
operar a ferramenta.</para>
<note>
<para>Nos cenários de vários nós, o EIB implanta automaticamente o MetalLB e o
Endpoint Copier Operator para que os hosts provisionados que usam a mesma
imagem criada ingressem em um cluster Kubernetes de maneira automática.</para>
</note>
<para>Para obter mais informações, leia a introdução do Edge Image Builder (<xref
linkend="components-eib"/>).</para>
<warning>
<para>O Edge Image Builder 1.3.0 suporta a personalização de imagens do SUSE Linux
Micro 6.1. As versões mais antigas, como o SUSE Linux Enterprise Micro 5.5
ou 6.0, não são suportadas.</para>
</warning>
<section xml:id="id-prerequisites-2">
<title>Pré-requisitos</title>
<itemizedlist>
<listitem>
<para>Uma máquina host de build AMD64/Intel 64 (física ou virtual) com o SLES 15
SP6.</para>
</listitem>
<listitem>
<para>O mecanismo de contêiner Podman</para>
</listitem>
<listitem>
<para>Uma imagem ISO SelfInstall do SUSE Linux Micro 6.1 criada pelo procedimento
do construtor Kiwi (<xref linkend="guides-kiwi-builder-images"/>)</para>
</listitem>
</itemizedlist>
<note>
<para>Para fins que não sejam de produção, é possível usar o openSUSE Leap 15.6 ou
o openSUSE Tumbleweed como máquina host de build. Outros sistemas
operacionais podem funcionar, desde que um tempo de execução do contêiner
compatível esteja disponível.</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>Obtendo a imagem do EIB</title>
<para>A imagem de contêiner do EIB é publicamente disponível e pode ser baixada do
registro do SUSE Edge executando o seguinte comando no host de build da
imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>Criando o diretório de configuração de imagem</title>
<para>Como o EIB é executado dentro de um contêiner, precisamos montar um
diretório de configuração do host, o que permite especificar a configuração
desejada e, durante o processo de criação, o EIB tem o acesso aos arquivos
de entrada necessários e artefatos auxiliares. Esse diretório deve seguir
uma estrutura específica. Vamos criá-lo imaginando que ele existe em seu
diretório pessoal com o nome "eib":</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>Na etapa anterior, criamos um diretório "base-images" que hospedará a imagem
de entrada do SUSE Linux Micro 6.1. Vamos garantir que a imagem seja copiada
para o diretório de configuração:</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>Durante a execução do EIB, a imagem base original <emphasis
role="strong">não</emphasis> é modificada. Uma versão nova e personalizada é
criada com a configuração desejada na raiz do diretório de configuração do
EIB.</para>
</note>
<para>Neste momento, o diretório de configuração deve ter a seguinte aparência:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>Criando o arquivo de definição de imagem</title>
<para>O arquivo de definição descreve a maioria das opções configuráveis com
suporte no Edge Image Builder. Você encontra um exemplo completo das opções
<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/pkg/image/testdata/full-valid-example.yaml">aqui</link>,
e nós recomendamos que você consulte o <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">guia
de criação de imagens upstream</link> para ver exemplos mais abrangentes do
que este que vamos apresentar a seguir. Vamos começar com um arquivo de
definição muito básico para nossa imagem do sistema operacional:</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>Essa definição especifica que estamos gerando uma imagem de saída para um
sistema baseado em AMD64/Intel 64. A imagem que será usada como base para
modificação posterior é uma imagem <literal>iso</literal> chamada
<literal>slemicro.iso</literal>, que esperamos que esteja no local
<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>. Ela também descreve
que, depois que o EIB terminar de modificar a imagem, a imagem de saída se
chamará <literal>eib-image.iso</literal> e, por padrão, residirá em
<literal>$CONFIG_DIR</literal>.</para>
<para>Agora a estrutura do nosso diretório deve ter esta aparência:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>Nas seções a seguir, vamos analisar alguns exemplos de operações comuns:</para>
<section xml:id="id-configuring-os-users">
<title>Configurando usuários do sistema operacional</title>
<para>O EIB permite pré-configurar usuários com informações de login, como senhas
ou chaves SSH, incluindo a definição de uma senha de root fixa. Como parte
deste exemplo, vamos corrigir a senha de root, e o primeiro passo é usar o
<literal>OpenSSL</literal> para criar uma senha criptografada unidirecional:</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>A saída será semelhante a esta:</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>Em seguida, podemos adicionar uma seção chamada
<literal>operatingSystem</literal> ao arquivo de definição com uma matriz
<literal>users</literal> dentro dela. O arquivo resultante terá esta
aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>É possível também adicionar outros usuários, criar os diretórios pessoais,
definir os IDs de usuário, adicionar a autenticação de chave SSH e modificar
as informações de grupo. Consulte o <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">guia
de criação de imagens upstream</link> para ver mais exemplos.</para>
</note>
</section>
<section xml:id="configuring-os-time">
<title>Configurando o horário do sistema operacional</title>
<para>A seção <literal>time</literal> é opcional, mas sua configuração é altamente
recomendada para evitar possíveis problemas com certificados e divergência
do relógio. O EIB configura o chronyd e o <literal>/etc/localtime</literal>
dependendo desses parâmetros.</para>
<screen language="console" linenumbering="unnumbered">operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2</screen>
<itemizedlist>
<listitem>
<para>O <literal>timezone</literal> especifica o fuso horário no formato
"região/localidade" (por exemplo, "Europa/Londres"). É possível ver a lista
completa executando o comando <literal>timedatectl list-timezones</literal>
no sistema Linux.</para>
</listitem>
<listitem>
<para>ntp: define atributos relacionados à configuração do NTP (usando o chronyd).</para>
</listitem>
<listitem>
<para>forceWait: solicita que o chronyd tente sincronizar as fontes de horário
antes de iniciar outros serviços, com um tempo limite de 180 segundos.</para>
</listitem>
<listitem>
<para>pools: especifica uma lista de pools que o chronyd usará como fontes de
dados (usando o <literal>iburst</literal> para melhorar o tempo gasto na
sincronização inicial).</para>
</listitem>
<listitem>
<para>servers: especifica uma lista de servidores que o chronyd usará como fontes
de dados (usando o <literal>iburst</literal> para melhorar o tempo gasto na
sincronização inicial).</para>
</listitem>
</itemizedlist>
<note>
<para>Os valores apresentados neste exemplo são apenas para fins
ilustrativos. Ajuste-os de acordo com seus requisitos específicos.</para>
</note>
</section>
<section xml:id="adding-certificates">
<title>Adicionando certificados</title>
<para>Arquivos de certificado com a extensão ".pem" ou ".crt" armazenados no
diretório <literal>certificates</literal> serão instalados no armazenamento
de certificados global do sistema do nó:</para>
<screen language="console" linenumbering="unnumbered">.
├── definition.yaml
└── certificates
    ├── my-ca.pem
    └── my-ca.crt</screen>
<para>Consulte o <link
xl:href="https://documentation.suse.com/smart/security/html/tls-certificates/index.html#tls-adding-new-certificates">guia
"Securing Communication with TLS Certificate"</link> (Protegendo a
comunicação com o certificado TLS) para obter mais informações.</para>
</section>
<section xml:id="eib-configuring-rpm-packages">
<title>Configurando pacotes RPM</title>
<para>Um dos principais recursos do EIB é o mecanismo para adicionar outros
pacotes de software à imagem, dessa forma, o sistema pode aproveitar os
pacotes instalados assim que a instalação é concluída. O EIB permite que os
usuários especifiquem o seguinte:</para>
<itemizedlist>
<listitem>
<para>Pacotes por nome em uma lista na definição da imagem</para>
</listitem>
<listitem>
<para>Repositórios de rede nos quais pesquisar os pacotes</para>
</listitem>
<listitem>
<para>Credenciais do SUSE Customer Center (SCC) para pesquisar os pacotes listados
em repositórios oficiais da SUSE</para>
</listitem>
<listitem>
<para>Por um diretório <literal>$CONFIG_DIR/rpms</literal>, fazer sideload dos
RPMs personalizados que não existem nos repositórios de rede</para>
</listitem>
<listitem>
<para>Pelo mesmo diretório (<literal>$CONFIG_DIR/rpms/gpg-keys</literal>), chaves
GPG para habilitar a validação de pacotes de terceiros</para>
</listitem>
</itemizedlist>
<para>Em seguida, o EIB é executado por um processo de resolução de pacote no
momento da criação da imagem, usando a imagem base como entrada, e tenta
obter e instalar todos os pacotes fornecidos, especificados pela lista ou
fornecidos localmente. O EIB faz download de todos os pacotes, incluindo as
dependências, em um repositório existente na imagem de saída, e instrui o
sistema a instalá-los durante o processo da primeira inicialização. A
execução desse processo durante a criação da imagem garante que os pacotes
sejam devidamente instalados na plataforma desejada, por exemplo, o nó de
borda, durante a primeira inicialização. Isso também é vantajoso em
ambientes nos quais você deseja fazer bake dos pacotes adicionais na imagem,
em vez de extraí-los pela rede durante a operação, por exemplo, em ambientes
air-gapped ou de rede restrita.</para>
<para>Como um exemplo simples para demonstrar esse procedimento, vamos instalar o
pacote RPM <literal>nvidia-container-toolkit</literal> encontrado no
repositório NVIDIA mantido por fornecedor terceirizado:</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>O arquivo de definição resultante terá esta aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>O exemplo acima é simples, mas para uma totalidade, faça download da chave
de assinatura do pacote NVIDIA antes de gerar a imagem:</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; $CONFIG_DIR/rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>A adição de outros RPMs por esse método tem como objetivo incluir
componentes de terceiros compatíveis ou pacotes fornecidos (e mantidos) pelo
usuário. Esse mecanismo não deve ser usado para adicionar pacotes que não
costumam ser suportados no SUSE Linux Micro. Se ele for usado para adicionar
componentes de repositórios openSUSE (que não são suportados), inclusive de
versões ou pacotes de serviço mais recentes, você poderá acabar com um
configuração sem suporte, principalmente quando a resolução de dependência
resulta na substituição de partes importantes do sistema operacional, mesmo
que o sistema resultante pareça funcionar conforme o esperado. Se você tiver
qualquer dúvida, entre em contato com seu representante SUSE para obter
ajuda para determinar se a configuração desejada é suportada.</para>
</warning>
<note>
<para>Um guia mais completo com exemplos adicionais está disponível no <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">guia
de instalação de pacotes upstream</link>.</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Configurando o cluster Kubernetes e as cargas de trabalho dos usuários</title>
<para>Outro recurso do EIB é a capacidade de usá-lo para automatizar a implantação
de clusters Kubernetes altamente disponíveis, tanto de nó único quanto de
vários nós, que são "inicializados no local" (ou seja, não exigem a
coordenação de nenhuma forma de infraestrutura de gerenciamento
centralizado). O principal motivador dessa abordagem são as implantações
air-gapped, ou ambientes de rede restrita, mas ela também é útil para
inicializar rapidamente clusters independentes, mesmo com acesso à rede
total e irrestrito disponível.</para>
<para>Este método permite implantar o sistema operacional personalizado e
especificar a configuração do Kubernetes, outros componentes em camadas por
meio de gráficos Helm e cargas de trabalho de usuário por meio dos
manifestos do Kubernetes fornecidos. No entanto, o princípio do projeto por
trás desse método é que pressupomos que o usuário queira o
air-gap. Portanto, os itens especificados na definição da imagem serão
inseridos na imagem, o que inclui as cargas de trabalho fornecidas pelo
usuário. O EIB garante que as imagens descobertas exigidas pelas definições
sejam copiadas localmente e exibidas pelo registro da imagem incorporada no
sistema implantado resultante.</para>
<para>Neste exemplo, vamos usar nossa definição de imagem existente e especificar
uma configuração do Kubernetes (não há uma lista dos sistemas e suas
funções, portanto, vamos considerar um nó único), que vai instruir o EIB a
provisionar um cluster Kubernetes RKE2 de nó único. Para mostrar a automação
da implantação das cargas de trabalho fornecidas pelo usuário (por
manifesto) e dos componentes em camadas (por Helm), vamos instalar o
KubeVirt por meio do gráfico Helm do SUSE Edge e o NGINX por meio de um
manifesto do Kubernetes. A configuração adicional que precisamos para anexar
a definição da imagem existente é:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<para>O arquivo de definição completo resultante agora tem esta aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.33.3+k3s1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<note>
<para>Há mais exemplos de opções, como implantações de vários nós, rede
personalizada e opções/valores de gráfico Helm, disponíveis na <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">documentação
upstream</link>.</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>Configurando a rede</title>
<para>No último exemplo desta inicialização rápida, vamos configurar a rede que
será criada quando um sistema for provisionado com a imagem gerada pelo
EIB. É importante entender que, exceto se uma configuração de rede for
especificada, o DHCP usará o modelo padrão em todas as interfaces
descobertas no momento da inicialização. No entanto, ela nem sempre é a
configuração desejada, principalmente se o DHCP não estiver disponível e
você precisar fazer configurações estáticas, configurar estruturas de rede
mais complexas, como vínculos, LACP e VLANs, ou substituir determinados
parâmetros, por exemplo, nomes de host, servidores DNS e rotas.</para>
<para>O EIB permite fazer configurações por nó (em que o sistema em questão é
identificado exclusivamente por seu endereço MAC) ou uma substituição para
especificar uma configuração idêntica para cada máquina, o que é mais útil
quando não se sabe os endereços MAC do sistema. O EIB usa uma ferramenta
adicional chamada Network Manager Configurator, ou <literal>nmc</literal> na
forma abreviada, que foi desenvolvida pela equipe do SUSE Edge para que a
configuração de rede personalizada seja aplicada com base no esquema de rede
declarativo <link xl:href="https://nmstate.io/">nmstate.io</link> e, no
momento da inicialização, identificará o nó em que está sendo inicializado e
aplicará a configuração de rede desejada antes da ativação de quaisquer
serviços.</para>
<para>Agora vamos aplicar uma configuração de rede estática a um sistema com uma
interface única descrevendo o estado desejado da rede em um arquivo
específico do nó (com base no nome de host desejado) no diretório
<literal>network</literal> exigido:</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>O exemplo acima foi configurado para a sub-rede padrão
<literal>192.168.122.0/24</literal> considerando que o teste é executado em
uma máquina virtual. Adapte-o de acordo com o seu ambiente, lembrando do
endereço MAC. Como é possível usar a mesma imagem para provisionar vários
nós, a rede configurada pelo EIB (via <literal>nmc</literal>) depende da
capacidade de identificar exclusivamente o nó por seu endereço MAC. Como
resultado, durante a inicialização, o <literal>nmc</literal> aplicará a
configuração de rede correta a cada máquina. Isso significa que você precisa
saber os endereços MAC dos sistemas nos quais deseja instalar. Como
alternativa, o comportamento padrão é confiar no DHCP, mas você pode usar o
gancho <literal>configure-network.sh</literal> para aplicar uma configuração
comum a todos os nós. Consulte o guia de rede (<xref
linkend="components-nmc"/>) para obter mais detalhes.</para>
</warning>
<para>A estrutura do arquivo resultante deverá ter esta aparência:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>A configuração de rede que acabamos de criar será analisada, e os arquivos
de conexão necessários do NetworkManager serão automaticamente gerados e
inseridos na nova imagem de instalação que o EIB criará. Esses arquivos
serão aplicados durante o provisionamento do host, resultando na
configuração de rede completa.</para>
<note>
<para>Consulte o componente de rede de borda (<xref linkend="components-nmc"/>)
para ver uma explicação mais abrangente da configuração acima e exemplos
desse recurso.</para>
</note>
</section>
</section>
<section xml:id="eib-how-to-build-image">
<title>Criando a imagem</title>
<para>Agora que temos uma imagem base e uma definição da imagem para o EIB usar,
vamos criar a imagem. Para isso, simplesmente usaremos o
<literal>podman</literal> para chamar o contêiner do EIB com o comando
"build", especificando o arquivo de definição:</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>A saída do comando deve ter esta aparência:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Downloading file: dl-manifest-1.yaml 100% (498/498 B, 9.5 MB/s)
Pulling selected Helm charts... 100% (1/1, 43 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (3/3, 10 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (657/657 MB, 48 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (368/368 MB, 48 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (35/35 MB, 50 MB/s)
Downloading file: sha256sum-amd64.txt 100% (4.3/4.3 kB, 6.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>A imagem ISO criada é armazenada em
<literal>$CONFIG_DIR/eib-image.iso</literal>:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>Cada build cria uma pasta com marcação de data e hora em
<literal>$CONFIG_DIR/_build/</literal>, que inclui os registros do build, os
artefatos usados durante o build e os diretórios
<literal>combustion</literal> e <literal>artefacts</literal> com todos os
scripts e artefatos adicionados à imagem CRB.</para>
<para>O conteúdo do diretório deve ter esta aparência:</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:&lt;version&gt;-registry.tar.zst
│   │   │   ├── rancher_kubectl:&lt;version&gt;-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.6_virt-operator:&lt;version&gt;-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   ├── nvidia-container-toolkit-&lt;version&gt;.rpm
│   │   │       │   ├── nvidia-container-toolkit-base-&lt;version&gt;.rpm
│   │   │       │   ├── libnvidia-container1-&lt;version&gt;.rpm
│   │   │       │   └── libnvidia-container-tools-&lt;version&gt;.rpm
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt
│   │       └── kubevirt-0.4.0.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>Se houver falha no build, <literal>eib-build.log</literal> será o primeiro
registro com as informações. A partir disso, ele vai direcionar você para o
componente com falha para depuração.</para>
<para>Neste ponto, você deve ter uma imagem pronta para uso que:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Implantará o SUSE Linux Micro 6.1.</para>
</listitem>
<listitem>
<para>Configurará a senha de root.</para>
</listitem>
<listitem>
<para>Instalará o pacote <literal>nvidia-container-toolkit</literal>.</para>
</listitem>
<listitem>
<para>Configurará um registro de contêiner incorporado para enviar o conteúdo
localmente.</para>
</listitem>
<listitem>
<para>Instalará o RKE2 de nó único.</para>
</listitem>
<listitem>
<para>Configurará a rede estática.</para>
</listitem>
<listitem>
<para>Instalará o KubeVirt.</para>
</listitem>
<listitem>
<para>Implantará um manifesto fornecido pelo usuário.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>Depurando o processo de criação da imagem</title>
<para>Em caso de falha no processo de criação de imagem, consulte o <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/debugging.md">guia
de depuração upstream</link>.</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>Testando a imagem recém-criada</title>
<para>Para obter instruções de como testar a imagem do CRB recém-criada, consulte
o <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/testing-guide.md">guia
de teste de imagem upstream</link>.</para>
</section>
</chapter>
<chapter xml:id="quickstart-suma">
<title>SUSE Multi-Linux Manager</title>
<para>O SUSE Multi-Linux Manager faz parte do SUSE Edge para oferecer automação e
controle para sempre manter o SUSE Linux Micro atualizado como sistema
operacional subjacente em todos os nós da sua implantação de borda. É
possível usá-lo também para gerenciar o Kubernetes e os aplicativos
implantados no Kubernetes em seus nós de borda.</para>
<para>O objetivo desta inicialização rápida é agilizar ao máximo para você a
instalação e configuração do SUSE Multi-Linux Manager para fornecer as
atualizações de sistema operacional para seus nós de borda. O guia de
inicialização rápida não aborda tópicos como dimensionar armazenamento,
criar e gerenciar canais de software adicionais para fins de preparação ou
gerenciar usuários, grupos de sistemas e organizações para implantações de
maior porte. Para uso em produção, é altamente recomendável familiarizar-se
com a <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">documentação
completa do SUSE Multi-Linux Manager</link>.</para>
<para>As seguintes etapas são necessárias para preparar o SUSE Edge para uso
eficaz do SUSE Multi-Linux Manager:</para>
<itemizedlist>
<listitem>
<para>Implantar e configurar o SUSE Multi-Linux Manager Server.</para>
</listitem>
<listitem>
<para>Sincronizar os repositórios de pacotes do SUSE Linux Micro.</para>
</listitem>
<listitem>
<para>Criar grupos de sistemas.</para>
</listitem>
<listitem>
<para>Criar chaves de ativação.</para>
</listitem>
<listitem>
<para>Usar o Edge Image Builder para preparar a mídia de instalação para registro
do SUSE Multi-Linux Manager.</para>
</listitem>
</itemizedlist>
<section xml:id="id-deploy-suse-multi-linux-manager-server">
<title>Implantar o SUSE Multi-Linux Manager Server</title>
<para>Se você já tem uma instância do SUSE Multi-Linux Manager 5.0.5 em execução,
pode ignorar esta etapa.</para>
<para>Você pode executar o SUSE Multi-Linux Manager Server em um servidor físico
dedicado, como uma máquina virtual em seu próprio hardware, ou na nuvem. São
fornecidas imagens pré-configuradas da máquina virtual para o SUSE
Multi-Linux Server para nuvens públicas suportadas.</para>
<para>Neste início rápido, usamos a imagem "qcow2"
<literal>SUSE-Manager-Server.x86_64-5.0.4-Qcow-5.0-2025-04.qcow2</literal>
para AMD64/Intel 64, que você encontra em <link
xl:href="https://www.suse.com/download/suse-manager/">https://www.suse.com/download/suse-manager/</link>
ou no SUSE Customer Center. Essa imagem atua como uma máquina virtual em
hipervisores como o KVM. Verifique sempre se há uma versão mais recente da
imagem e use-a para novas instalações.</para>
<para>Você também pode instalar o SUSE Multi-Linux Manager Server em qualquer
outra arquitetura de hardware compatível. Nesse caso, escolha a imagem
correspondente à arquitetura.</para>
<para>Depois de fazer download da imagem, crie uma máquina virtual que atenda pelo
menos às seguintes especificações mínimas de hardware:</para>
<itemizedlist>
<listitem>
<para>16 GB de RAM</para>
</listitem>
<listitem>
<para>4 núcleos físicos ou virtuais</para>
</listitem>
<listitem>
<para>um dispositivo de blocos adicional com, no mínimo, 100 GB</para>
</listitem>
</itemizedlist>
<para>Com a imagem qcow2, não há necessidade de instalar o sistema operacional. É
possível anexar a imagem diretamente como a partição raiz.</para>
<para>Configure a rede para que os nós de borda possam acessar o SUSE Multi-Linux
Manager Server posteriormente com um nome de host que inclua o nome de
domínio completo e qualificado ("FQDN", Fully Qualified Domain Name)!</para>
<para>Quando você inicializa o SUSE Multi-Linux Manager pela primeira vez, precisa
fazer algumas configurações iniciais:</para>
<itemizedlist>
<listitem>
<para>Selecionar o layout do seu teclado</para>
</listitem>
<listitem>
<para>Aceitar o contrato de licença</para>
</listitem>
<listitem>
<para>Selecionar seu fuso horário</para>
</listitem>
<listitem>
<para>Inserir a senha de root do sistema operacional</para>
</listitem>
</itemizedlist>
<para>As etapas seguintes precisam ser realizas como usuário "root":</para>
<para>Na etapa seguinte, você precisará do código de registro para a extensão do
SUSE Multi-Linux Manager, que está disponível no SUSE Customer Center. É
possível usar o mesmo código para o registro tanto do SUSE Linux Micro
quanto do SUSE Multi-Linux Manager:</para>
<para>Registre o SUSE Linux Micro:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -r &lt;REGCODE&gt; -e &lt;your_email&gt;</screen>
<para>Registre o SUSE Multi-Linux Manager:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -p SUSE-Manager-Server/5.0/x86_64 -r &lt;REGCODE&gt;</screen>
<para>A string do produto depende da arquitetura de hardware. Por exemplo, se você
usa o SUSE Multi-Linux Manager em um sistema Arm de 64 bits, a string é
"SUSE-Manager-Server/5.0/aarch64".</para>
<para>Reiniciar</para>
<para>Atualize o sistema:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update</screen>
<para>Reinicialize para aplicar as atualizações (se houver).</para>
<para>O SUSE Multi-Linux Manager é oferecido por um contêiner gerenciado pelo
Podman. O comando <literal>mgradm</literal> cuida da instalação e da
configuração para você.</para>
<warning>
<para>É muito importante configurar o nome de host no SUSE Multi-Linux Manager
Server com um nome de domínio completo e qualificado ("FQDN", Fully
Qualified Domain Name) que os nós de borda que você deseja gerenciar possam
resolver apropriadamente na rede!</para>
</warning>
<para>Antes de instalar e configurar o contêiner do SUSE Multi-Linux Manager
Server, você precisa preparar o dispositivo de bloco extra que já
adicionou. Para isso, você deve saber o nome que a máquina virtual deu ao
dispositivo. Por exemplo, se o dispositivo de bloco é
<literal>/dev/vdb</literal>, é possível configurá-lo para uso com o SUSE
Multi-Linux Manager por meio do seguinte comando:</para>
<screen language="shell" linenumbering="unnumbered">mgr-storage-server /dev/vdb</screen>
<para>Implante o SUSE Multi-Linux Manager:</para>
<screen language="shell" linenumbering="unnumbered">mgradm install podman &lt;FQDN&gt;</screen>
<para>Insira a senha para o certificado de CA. Essa senha deve ser diferente das
suas senhas de login. Em geral, você não precisa inseri-la posteriormente,
mas é bom anotá-la.</para>
<para>Insira a senha do usuário "admin". Esse é o usuário inicial para fazer login
no SUSE Multi-Linux Manager. Você pode criar outros usuários com direitos
completos ou restritos no futuro.</para>
</section>
<section xml:id="id-configure-suse-multi-linux-manager">
<title>Configurar o SUSE Multi-Linux Manager</title>
<para>Após a conclusão da implantação, faça login na IU da web do SUSE Multi-Linux
Manager usando o nome de host que você já forneceu. O usuário inicial é
"admin". Use a senha informada na etapa anterior.</para>
<para>Para a etapa seguinte, você precisa das credenciais da sua organização, que
estão na segunda subguia da guia "Usuários" da organização no SUSE Customer
Center. Com essas credenciais, o SUSE Multi-Linux Manager pode sincronizar
todos os produtos que você assinou.</para>
<para>Selecione <literal>Admin &gt; Assistente de configuração</literal>.</para>
<para>Na guia <literal>Credenciais da Organização</literal>, crie uma nova
credencial com seu <literal>Nome de usuário</literal> e
<literal>Senha</literal> que constam no SUSE Customer Center.</para>
<para>Vá para a guia seguinte <literal>Produtos SUSE</literal>. Aguarde o término
da primeira sincronização de dados com o SUSE Customer Center.</para>
<para>Depois que a lista for preenchida, use o filtro para mostrar apenas o "Micro
6.1". Marque a caixa do SUSE Linux Micro 6.1 referente à arquitetura de
hardware na qual os nós de borda serão executados (<literal>x86_64</literal>
ou <literal>aarch64</literal>).</para>
<para>Clique em <literal>Adicionar produtos</literal> para adicionar o repositório
("canal") de pacotes principal do SUSE Linux Micro e, automaticamente, o
canal para as ferramentas do cliente SUSE Manager como um subcanal.</para>
<para>Dependendo da sua conexão com a Internet, a primeira sincronização levará
algum tempo. Você já pode iniciar as etapas seguintes:</para>
<para>Em <literal>Sistemas &gt; Grupos do Sistema</literal>, crie pelo menos um
grupo em que seus sistemas ingressarão quando forem integrados. Os grupos
são importantes para categorizar os sistemas de modo que você possa aplicar
uma configuração ou ações a todo o conjunto de sistemas de uma vez. Por
concepção, eles são similares aos rótulos no Kubernetes.</para>
<para>Clique em <literal>+ Criar Grupo</literal>.</para>
<para>Insira um nome abreviado, por exemplo, "Nós de borda", e uma descrição
longa.</para>
<para>Em <literal>Sistemas &gt; Chaves de ativação</literal>, crie pelo menos uma
chave de ativação. Pense nas chaves de ativação como perfis de configuração
que são automaticamente aplicados aos sistemas depois de integrados ao SUSE
Multi-Linux Manager. Para adicionar determinados nós de borda a grupos
diferentes ou usar outra configuração, você pode criar chaves de ativação
separadas para eles e usá-las no Edge Image Builder para criar uma mídia de
instalação personalizada.</para>
<para>Um caso de uso avançado comum das chaves de ativação é para atribuir
clusters de teste aos canais de software com as atualizações mais recentes,
e os clusters de produção aos canais de software que apenas recebem essas
atualizações mais recentes depois que você as testou no cluster de teste.</para>
<para>Clique em <literal>+ Criar chave</literal>.</para>
<para>Escolha uma descrição resumida, por exemplo, "Nós de borda". Insira um nome
exclusivo que identifique a chave, por exemplo, "edge-x86_64" para os nós de
borda com arquitetura de hardware AMD64/Intel 64. Um prefixo de número é
automaticamente adicionado à chave. Para a organização padrão, o número é
sempre "1". Se você criar mais organizações no SUSE Multi-Linux Manager e
gerar chaves para elas, esse número poderá ser diferente.</para>
<para>Se você não criou canais de software clonados, pode manter a configuração do
canal base como "padrão do SUSE Manager". Isso atribui automaticamente o
repositório de atualização correto do SUSE aos nós de borda.</para>
<para>Como "canal filho", selecione o controle deslizante para "incluir
recomendações" referente à arquitetura de hardware para a qual a chave de
ativação é usada. Dessa forma, o "SUSE-Manager-Tools-For-SL-Micro-6.1" será
adicionado ao canal.</para>
<para>Na guia "Grupos", adicione o grupo criado. Todos os nós integrados por meio
dessa chave de ativação serão automaticamente adicionados a esse grupo.</para>
</section>
<section xml:id="id-create-a-customized-installation-image-with-edge-image-builder">
<title>Criar uma imagem de instalação personalizada com o Edge Image Builder</title>
<para>Para usar o Edge Image Builder, você precisa apenas de um ambiente para
iniciar o contêiner baseado em Linux com o Podman.</para>
<para>Na verdade, para configuração mínima de laboratório, podemos usar a mesma
máquina virtual na qual o SUSE Multi-Linux Manager Server está em
execução. Verifique se você tem espaço em disco suficiente na máquina
virtual! Essa configuração não é recomendada para uso em produção. Consulte
a <xref linkend="id-prerequisites-2"/> para saber quais sistemas
operacionais host foram testados com o Edge Image Builder.</para>
<para>Faça login no host do SUSE Multi-Linux Manager Server como root.</para>
<para>Acesse o contêiner do Edge Image Builder:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
<para>Crie o diretório <literal>/opt/eib</literal> e o subdiretório
<literal>base-images</literal>:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p /opt/eib/base-images</screen>
<para>Neste início rápido, usamos a variante de "autoinstalação" da imagem do SUSE
Linux Micro. Posteriormente, essa imagem poderá ser gravada em uma unidade
removível USB física que você pode usar para instalação em servidores
físicos. Se o servidor tem a opção de anexar remotamente as ISOs de
instalação por um BMC (Baseboard Management Controller), você também pode
adotar essa abordagem. Por fim, também é possível usar essa imagem com
grande parte das ferramentas de virtualização.</para>
<para>Para pré-carregar a imagem diretamente em um nó físico ou iniciá-la de uma
VM, você também pode usar a variante de imagem "bruta".</para>
<para>Você encontra essas imagens no SUSE Customer Center ou em <link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro/</link></para>
<para>Faça download ou copie a imagem
<literal>SL-Micro.x86_64-6.1-Default-SelfInstall-GM.install.iso</literal> no
diretório <literal>base-images</literal> e dê o nome "slemicro.iso" a ela.</para>
<para>A criação de imagens AArch64 em um host de build baseado em Arm é uma prévia
de tecnologia no SUSE Edge 3.4 que deve funcionar, mas ainda não tem
suporte. Se você quer testá-la, precisa executar o Podman em uma máquina Arm
de 64 bits e substituir "x86_64" por "aarch64" em todos os exemplos e
trechos de código.</para>
<para>Em <literal>/opt/eib</literal>, crie um arquivo chamado
<literal>iso-definition.yaml</literal>. Essa é a definição do seu build para
o Edge Image Builder.</para>
<para>Este é um exemplo simples para instalar o SL Micro 6.1, definir uma senha de
root e o mapa do teclado, iniciar a IU gráfica do Cockpit e registrar o nó
no SUSE Multi-Linux Manager:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
  - username: root
    createHomeDir: true
    encryptedPassword: $6$aaBTHyqDRUMY1HAp$pmBY7.qLtoVlCGj32XR/Ogei4cngc3f4OX7fwBD/gw7HWyuNBOKYbBWnJ4pvrYwH2WUtJLKMbinVtBhMDHQIY0
  keymap: de
  systemd:
    enable:
      - cockpit.socket
  packages:
    noGPGCheck: true
  suma:
    host: ${fully qualified hostname of your SUSE Multi-Linux Manager Server}
    activationKey: 1-edge-x86_64</screen>
<para>O Edge Image Builder também configura a rede, instala automaticamente o
Kubernetes no nó e até implanta aplicativos por gráficos Helm. Consulte o
<xref linkend="quickstart-eib"/> para ver exemplos mais completos.</para>
<para>Para <literal>baseImage</literal>, especifique o nome real da ISO no
diretório <literal>base-images</literal> que deseja usar.</para>
<para>Neste exemplo, a senha de root é "root". Consulte a <xref
linkend="id-configuring-os-users"/> para criar um hash para a senha segura
que deseja usar.</para>
<para>Defina o mapa do teclado com o layout desejado para o sistema após a
instalação.</para>
<note>
<para>Usamos a opção <literal>noGPGCheck: true</literal> porque não vamos fornecer
uma chave GPG para verificar pacotes RPM. Um guia completo com uma
configuração mais segura que recomendamos para uso em produção está
disponível no <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">guia
de instalação de pacotes upstream</link>.</para>
</note>
<para>Como já foi mencionado várias vezes, o host do SUSE Multi-Linux Manager
exige um nome de host completo e qualificado que possa ser resolvido na rede
em que os nós de borda serão inicializados.</para>
<para>O valor de <literal>activationKey</literal> precisa corresponder à chave que
você criou no SUSE Multi-Linux Manager.</para>
<para>Para criar uma imagem de instalação que registre automaticamente os nós de
borda no SUSE Multi-Linux Manager após a instalação, você também precisa
preparar dois artefatos:</para>
<itemizedlist>
<listitem>
<para>pacote Salt minion para instalar o agente de gerenciamento para o SUSE
Multi-Linux Manager</para>
</listitem>
<listitem>
<para>certificado de CA do servidor SUSE Multi-Linux Manager</para>
</listitem>
</itemizedlist>
<section xml:id="id-download-the-venv-salt-minion-package">
<title>Fazer download do pacote venv-salt-minion</title>
<para>Em <literal>/opt/eib</literal>, crie o subdiretório <literal>rpms</literal>.</para>
<para>Faça download do pacote <literal>venv-salt-minion</literal> do seu servidor
SUSE Multi-Linux Manager para esse diretório. Você pode obtê-lo pela IU da
web localizando o pacote em <literal>Software &gt; Lista de canais</literal>
e fazendo download dele do canal SUSE-Manager-Tools, ou fazer download dele
do "repositório de inicialização" do SUSE Multi-Linux Manager com uma
ferramenta como curl:</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/repositories/slmicro/6/1/bootstrap/x86_64/venv-salt-minion-3006.0-8.1.x86_64.rpm</screen>
<para>O nome do pacote real pode ser diferente se uma nova versão já foi
lançada. Se houver vários pacotes para seleção, escolha sempre o mais
recente.</para>
<para>A solução alternativa para o problema documentado nas <link
xl:href="https://www.suse.com/releasenotes/x86_64/multi-linux-manager/5.1/index.html#_bootstrapping_sl_micro_6_1_clients">Notas
de lançamento</link> do SUSE Multi-Linux Manager é armazenar a versão mais
recente do pacote de chaves de build no diretório <literal>rpms</literal>
(<literal>suse-build-key-12.0-slfo.1.1_3.1.noarch.rpm</literal> quando esta
documentação foi criada). Você o encontra na seção
<literal>Software</literal> do SUSE Multi-Linux Manager, na guia
<literal>Pacotes</literal> do canal Pool do SL Micro. Há um botão
<literal>Download</literal> na visualização de <literal>Detalhes</literal>.</para>
</section>
</section>
<section xml:id="id-download-the-suse-multi-linux-manager-ca-certificate">
<title>Fazer download do certificado de CA do SUSE Multi-Linux Manager</title>
<para>Em <literal>/opt/eib</literal>, crie o subdiretório
<literal>certificates</literal>.</para>
<para>Faça download do certificado de CA do SUSE Multi-Linux Manager para esse
diretório:</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/RHN-ORG-TRUSTED-SSL-CERT</screen>
<warning>
<para>Você deve renomear o certificado para
<literal>RHN-ORG-TRUSTED-SSL-CERT.crt</literal>. Depois disso, o Edge Image
Builder garantirá que ele seja instalado e ativado no nó de borda durante a
instalação.</para>
</warning>
<para>Agora execute o Edge Image Builder:</para>
<screen language="bash" linenumbering="unnumbered">cd /opt/eib
podman run --rm -it --privileged -v /opt/eib:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>Se você usou outro nome para o arquivo de definição YAML e quer usar uma
versão diferente do Edge Image Builder, precisa adaptar o comando de acordo.</para>
<para>Após o término da criação, a ISO de instalação estará no diretório
<literal>/opt/eib</literal> como <literal>eib-image.iso</literal>.</para>
<para>Agora é possível usar essa imagem para implantar nós que tentarão fazer o
registro no SUSE Multi-Linux Manager.</para>
<para>Após a instalação completa do nó, você verá a respectiva chave listada como
<literal>pendente</literal> na seção <literal>Salt/Chaves</literal> do SUSE
Multi-Linux Manager. Depois que você aceitar a chave, o nó será integrado
automaticamente ao SUSE Multi-Liux Manager e exibido na lista
<literal>Sistemas</literal> após o término do processo. Ele receberá os
grupos do sistema que você especificou na chave de ativação.</para>
<para>Em seguida, agende uma reinicialização antes de aplicar qualquer outra
configuração.</para>
<para>É possível automatizar por completo a aceitação da chave usando as listas de
permissões conforme descrito <link
xl:href="https://docs.saltproject.io/en/latest/topics/tutorials/autoaccept_grains.html">aqui</link>.</para>
</section>
</chapter>
</part>
<part xml:id="id-components">
<title>Componentes</title>
<partintro>
<para>Lista de componentes do Edge</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para>Consulte a documentação do Rancher em <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12">https://ranchermanager.docs.rancher.com/v2.12</link>.</para>
<blockquote>
<para>O Rancher é uma plataforma de gerenciamento Kubernetes avançada e de
código-fonte aberto que simplifica a implantação, as operações e o
monitoramento de clusters Kubernetes em vários ambientes. Se você gerencia
clusters no local, na nuvem ou na borda, o Rancher oferece uma plataforma
unificada e centralizada para todas as suas necessidades do Kubernetes.</para>
</blockquote>
<section xml:id="id-key-features-of-rancher">
<title>Principais recursos do Rancher</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Gerenciamento multicluster:</emphasis> a interface
intuitiva do Rancher permite gerenciar cluster Kubernetes de qualquer lugar:
nuvens públicas, centro de dados privados e locais de borda.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Segurança e conformidade:</emphasis> o Rancher impõe
políticas de segurança, controle de acesso com base em função (RBAC,
Role-Based Access Control) e padrões de conformidade de acordo com o seu
cenário do Kubernetes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Operações de cluster simplificadas:</emphasis> o
Rancher automatiza o provisionamento de clusters, faz upgrade e soluciona
problemas, simplificando as operações do Kubernetes para equipes de todos os
tamanhos.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Catálogo de aplicativos centralizado:</emphasis> o
catálogo de aplicativos do Rancher oferece uma variedade de gráficos Helm e
operadores do Kubernetes, o que facilita a implantação e o gerenciamento de
aplicativos conteinerizados.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Entrega contínua:</emphasis> o Rancher oferece
suporte a GitOps e pipelines de CI/CD, o que automatiza e simplifica os
processos de entrega dos aplicativos.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>Uso do Rancher no SUSE Edge</title>
<para>O Rancher oferece várias funcionalidades importantes para a pilha do SUSE
Edge:</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Gerenciamento centralizado do Kubernetes</title>
<para>Nas implantações de borda típicas com vários clusters distribuídos, o
Rancher atua como um plano de controle central para gerenciar os clusters
Kubernetes. Ele oferece uma interface unificada para provisionamento,
upgrade, monitoramento e solução de problemas, simplificando as operações e
garantindo a consistência.</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>Implantação de clusters simplificada</title>
<para>O Rancher simplifica a criação de clusters Kubernetes no sistema operacional
leve SUSE Linux Micro, o que facilita a distribuição da infraestrutura de
borda com recursos avançados do Kubernetes.</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>Implantação e gerenciamento de aplicativos</title>
<para>O catálogo de aplicativos integrado do Rancher simplifica a implantação e o
gerenciamento de aplicativos conteinerizados nos clusters SUSE Edge,
facilitando a implantação de cargas de trabalho de borda.</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>Imposição de segurança e políticas</title>
<para>O Rancher oferece ferramentas de governança com base em política, controle
de acesso com base em função (RBAC, Role-Based Access Control) e integração
com provedores de autenticação externos. Isso ajuda a manter a segurança e a
conformidade das implantações do SUSE Edge, o que é crítico em ambientes
distribuídos.</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>Melhores práticas</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>O Rancher inclui o Fleet como componente incorporado que permite gerenciar
configurações de clusters e implantações de aplicativos com um código
armazenado em git.</para>
</section>
<section xml:id="id-observability">
<title>Observabilidade</title>
<para>O Rancher inclui ferramentas incorporadas de monitoramento e registro, como
Prometheus e Grafana, para insights abrangentes sobre a integridade e o
desempenho do seu cluster.</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Instalando com o Edge Image Builder</title>
<para>O SUSE Edge usa o <xref linkend="components-eib"/> para personalizar as
imagens base do sistema operacional SUSE Linux Micro. Leia a <xref
linkend="rancher-install"/> para instalação air-gapped do Rancher em
clusters Kubernetes provisionados pelo EIB.</para>
</section>
<section xml:id="id-additional-resources-2">
<title>Recursos adicionais</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Documentação do Rancher</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher Community</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Gráficos Helm</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Operadores do Kubernetes</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Extensões do Rancher Dashboard</title>
<para>As extensões permitem que usuários, desenvolvedores, parceiros e clientes
estendam e aprimorem a IU do Rancher. O SUSE Edge oferece as extensões de
dashboard KubeVirt e Akri.</para>
<para>Consulte a <literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions">Documentação
do Rancher</link></literal> para obter informações gerais sobre as extensões
do Rancher Dashboard.</para>
<section xml:id="id-installation">
<title>Instalação</title>
<para>Todos os componentes do SUSE Edge 3.4, incluindo as extensões de dashboard,
são distribuídos como artefatos OCI. Para instalar as extensões do SUSE
Edge, você pode usar a IU do Rancher Dashboard, o Helm ou o Fleet:</para>
<section xml:id="id-installing-with-rancher-dashboard-ui">
<title>Instalando a IU do Rancher Dashboard</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clique em <emphasis role="strong">Extensions</emphasis> (Extensões) na seção
<emphasis role="strong">Configuration</emphasis> (Configuração) da barra
lateral de navegação.</para>
</listitem>
<listitem>
<para>Na página de extensões, clique no menu de três pontos na parte superior
direita e selecione <emphasis role="strong">Manage Repositories</emphasis>
(Gerenciar repositórios).</para>
<para>Cada extensão é distribuída por seu próprio artefato OCI. Elas estão
disponíveis no repositório de gráficos Helm do SUSE Edge.</para>
</listitem>
<listitem>
<para>Na página <emphasis role="strong">Repositories</emphasis> (Repositórios),
clique em <literal>Create</literal> (Criar).</para>
</listitem>
<listitem>
<para>No formulário, especifique o nome e o URL do repositório e clique em
<literal>Create</literal> (Criar).</para>
<para>URL do repositório de gráficos Helm do SUSE Edge:
<literal>oci://registry.suse.com/edge/charts</literal></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-create-oci-repository.png" width="100%"/>
</imageobject>
<textobject><phrase>criar repositório oci extensões dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Veja que o repositório de extensões foi adicionado à lista com o estado
<literal>Active</literal> (Ativo).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-repositories-list.png" width="100%"/>
</imageobject>
<textobject><phrase>lista repositórios extensões dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Volte para <emphasis role="strong">Extensions</emphasis> (Extensões) na
seção <emphasis role="strong">Configuration</emphasis> (Configuração) da
barra lateral de navegação.</para>
<para>Na guia <emphasis role="strong">Available</emphasis> (Disponível), veja as
extensões disponíveis para instalação.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-available-extensions.png" width="100%"/>
</imageobject>
<textobject><phrase>extensões disponíveis extensões dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>No cartão da extensão, clique em <literal>Install</literal> (Instalar) e
confirme a instalação.</para>
<para>Após a instalação da extensão, a IU do Rancher exibe um prompt para
recarregar a página, conforme descrito na <literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions#installing-extensions">página
de instalação de extensões da documentação do Rancher</link></literal> (em
inglês).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-helm">
<title>Instalando com o Helm</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension --version 304.0.2+up1.3.2 --namespace cattle-ui-plugin-system

# Akri extension
helm install akri-dashboard-extension oci://registry.suse.com/edge/charts/akri-dashboard-extension --version 304.0.2+up1.3.1 --namespace cattle-ui-plugin-system</screen>
<note>
<para>É preciso instalar as extensões no namespace
<literal>cattle-ui-plugin-system</literal>.</para>
</note>
<note>
<para>Após a instalação de uma extensão, a IU do Rancher Dashboard deverá ser
recarregada.</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Instalando com o Fleet</title>
<para>A instalação de extensões de dashboard com o Fleet requer a definição de um
recurso <literal>gitRepo</literal> que aponte para um repositório Git com um
ou mais arquivos de configuração de bundle <literal>fleet.yaml</literal>
personalizados.</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension
  version: "304.0.2+up1.3.2"</screen>
<screen language="yaml" linenumbering="unnumbered"># Akri extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: akri-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/akri-dashboard-extension
  version: "304.0.2+up1.3.1"</screen>
<note>
<para>A propriedade <literal>releaseName</literal> é necessária e precisa
corresponder ao nome da extensão para instalá-la corretamente.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>Para obter mais informações, consulte o <xref linkend="components-fleet"/> e
o repositório <literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>.</para>
<para>Após a instalação das extensões, elas serão listadas na seção <emphasis
role="strong">Extensions</emphasis> (Extensões) nas guias <emphasis
role="strong">Installed</emphasis> (Instaladas). Como não são instaladas via
Apps/Marketplace, elas são marcadas com o rótulo
<literal>Third-Party</literal> (Terceiros).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width="100%"/> </imageobject>
<textobject><phrase>extensões dashboard instaladas</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>Extensão de dashboard KubeVirt</title>
<para>A extensão KubeVirt fornece gerenciamento de máquinas virtuais básicas para
a IU do Rancher Dashboard. Seus recursos estão descrito na <xref
linkend="kubevirt-dashboard-extension-usage"/>.</para>
</section>
<section xml:id="id-akri-dashboard-extension">
<title>Extensão de dashboard Akri</title>
<para>Akri é uma interface de recursos do Kubernetes que permite expor facilmente
dispositivos folha heterogêneos (por exemplo, câmeras IP e dispositivos USB)
como recursos em um cluster Kubernetes, além de oferecer suporte à exposição
de recursos de hardware incorporados, como GPUs e FPGAs. A Akri detecta
continuamente os nós com acesso a esses dispositivos e programa as cargas de
trabalho com base neles.</para>
<para>Com a extensão de dashboard Akri, você pode usar a interface de usuário do
Rancher Dashboard para gerenciar e monitorar dispositivos folha e executar
cargas de trabalho após a descoberta desses dispositivos.</para>
<para>Há uma descrição mais detalhada dos recursos da extensão na <xref
linkend="akri-dashboard-extension-usage"/>.</para>
</section>
</chapter>
<chapter xml:id="components-rancher-turtles">
<title>Rancher Turtles</title>
<para>Consulte a documentação do Rancher Turtles em <link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/">https://documentation.suse.com/cloudnative/cluster-api/</link></para>
<blockquote>
<para>O Rancher Turtles é um operador do Kubernetes que proporciona integração
entre o Rancher Manager e a Cluster API (CAPI), com o objetivo de levar
suporte completo para CAPI ao Rancher.</para>
</blockquote>
<section xml:id="id-key-features-of-rancher-turtles">
<title>Principais recursos do Rancher Turtles</title>
<itemizedlist>
<listitem>
<para>Importar automaticamente clusters CAPI para o Rancher instalando o Rancher
Cluster Agent nos clusters provisionados pela CAPI.</para>
</listitem>
<listitem>
<para>Instalar e configurar as dependências do controlador CAPI pelo <link
xl:href="https://cluster-api-operator.sigs.k8s.io/">CAPI Operator</link>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-rancher-turtles-use-in-suse-edge">
<title>Uso do Rancher Turtles no SUSE Edge</title>
<para>A pilha do SUSE Edge oferece um gráfico Helm agrupador que instala o Rancher
Turtles com uma configuração específica que habilita:</para>
<itemizedlist>
<listitem>
<para>Os principais componentes do controlador da CAPI</para>
</listitem>
<listitem>
<para>Os componentes do provedor de plano de controle e de inicialização do RKE2</para>
</listitem>
<listitem>
<para>Os componentes do provedor de infraestrutura do Metal3 (<xref
linkend="components-metal3"/>)</para>
</listitem>
</itemizedlist>
<para>Há suporte apenas para os provedores padrão instalados pelo gráfico
agrupador. Os provedores alternativos de plano de controle, inicialização e
infraestrutura não são suportados atualmente como parte da pilha do SUSE
Edge.</para>
</section>
<section xml:id="id-installing-rancher-turtles">
<title>Instalando o Rancher Turtles</title>
<para>É possível instalar o Rancher Turtles seguindo o Guia de Início Rápido do
Metal3 (<xref linkend="quickstart-metal3"/>) ou a documentação do cluster de
gerenciamento (<xref linkend="atip-management-cluster"/>).</para>
</section>
<section xml:id="id-additional-resources-3">
<title>Recursos adicionais</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Documentação do Rancher</link></para>
</listitem>
<listitem>
<para><link xl:href="https://cluster-api.sigs.k8s.io/">Manual da Cluster
API</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link xl:href="https://fleet.rancher.io">Fleet</link> é um mecanismo de
gerenciamento e implantação de contêineres desenvolvido para oferecer aos
usuários mais controle sobre o cluster local e monitoramento constante por
GitOps. O foco do Fleet não está apenas na capacidade de ajustar a escala,
mas também em proporcionar aos usuários um alto grau de controle e
visibilidade para monitorar exatamente o que está instalado no cluster.</para>
<para>O Fleet gerencia implantações do Git de arquivos YAML brutos no Kubernetes,
gráficos Helm, Kustomize ou qualquer combinação dos três. Seja qual for a
fonte, todos os recursos são dinamicamente convertidos em gráficos Helm, e o
Helm é usado como mecanismo para implantar todos os recursos no
cluster. Como resultado, os usuários aproveitam um alto grau de controle,
consistência e auditabilidade dos clusters.</para>
<para>Para obter informações sobre o funcionamento do Fleet, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/architecture">Arquitetura
do Fleet</link> (em inglês).</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Instalando o Fleet com o Helm</title>
<para>O Fleet está incorporado ao Rancher, mas também é possível <link
xl:href="https://fleet.rancher.io/installation">instalá-lo</link> como
aplicativo independente em qualquer cluster Kubernetes que usa o Helm.</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>Usando o Fleet com o Rancher</title>
<para>O Rancher usa o Fleet para implantar aplicativos em clusters gerenciados. A
entrega contínua com o Fleet introduz o GitOps em escala, o que foi
projetado para gerenciar aplicativos executados em uma grande quantidade de
clusters.</para>
<para>O Fleet se destaca como parte integrante do Rancher. O agente Fleet é
automaticamente implantado em clusters gerenciados com o Rancher, como parte
do processo de instalação/importação, e o cluster fica imediatamente
disponível para gerenciamento pelo Fleet.</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Acessando o Fleet na IU do Rancher</title>
<para>O Fleet vem pré-instalado no Rancher e é gerenciado pela opção <emphasis
role="strong">Continuous Delivery</emphasis> (Entrega contínua) na IU do
Rancher.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width="100%"/>
</imageobject>
<textobject><phrase>dashboard fleet</phrase></textobject>
</mediaobject>
</informalfigure>
<para>A seção Continuous Delivery (Entrega contínua) consiste nos seguintes itens:</para>
<section xml:id="id-dashboard">
<title>Dashboard</title>
<para>Uma página de visão geral de todos os repositórios do GitOps em todos os
espaços de trabalho. São mostrados apenas os espaços de trabalho com
repositórios.</para>
</section>
<section xml:id="id-git-repos">
<title>Repositórios Git</title>
<para>Uma lista de repositórios do GitOps no espaço de trabalho
selecionado. Escolha o espaço de trabalho ativo na lista suspensa na parte
superior da página.</para>
</section>
<section xml:id="id-clusters">
<title>Clusters</title>
<para>Uma lista de clusters gerenciados. Por padrão, todos os clusters gerenciados
pelo Rancher são adicionados ao espaço de trabalho
<literal>fleet-default</literal>. O espaço de trabalho
<literal>fleet-local</literal> inclui o cluster (de gerenciamento)
local. Nele, é possível <literal>Pause</literal> (Pausar) ou <literal>Force
update</literal> (Forçar a atualização) dos clusters ou mover o cluster para
outro espaço de trabalho. A edição do cluster permite atualizar rótulos e
anotações usados para agrupamento de clusters.</para>
</section>
<section xml:id="id-cluster-groups">
<title>Grupos de clusters</title>
<para>A seção "Cluster Groups" (Grupos de clusters) permite o agrupamento
personalizado de clusters no espaço de trabalho usando os seletores.</para>
</section>
<section xml:id="id-advanced">
<title>Avançado</title>
<para>A seção "Advanced" (Avançado) permite gerenciar espaços de trabalho e outros
recursos do Fleet relacionados.</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Exemplo de instalação do KubeVirt com o Rancher e o Fleet usando o Rancher
Dashboard</title>
<orderedlist numeration="arabic">
<listitem>
<para>Crie um repositório Git com o arquivo <literal>fleet.yaml</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/charts/kubevirt"
  version: "304.0.1+up0.6.0"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>No Rancher Dashboard, navegue até <emphasis role="strong">☰ &gt; Continuous
Delivery &gt; Git Repos</emphasis> (Entrega contínua > Repositórios Git) e
clique em <literal>Add Repository</literal> (Adicionar repositório).</para>
</listitem>
<listitem>
<para>O assistente de criação de repositório orienta na criação do repositório
Git. Insira o <emphasis role="strong">Name</emphasis> (Nome), <emphasis
role="strong">Repository URL</emphasis> (URL do repositório), mencionando o
repositório Git criado na etapa anterior, e selecione a ramificação ou
revisão apropriada. No caso de um repositório mais complexo, especifique os
<emphasis role="strong">Paths</emphasis> (Caminhos) para usar vários
diretórios em um único repositório.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width="100%"/>
</imageobject>
<textobject><phrase>criar repo1 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Clique em <literal>Next</literal> (Avançar).</para>
</listitem>
<listitem>
<para>Na próxima etapa, você poderá definir o local de implantação das cargas de
trabalho. A seleção de cluster inclui várias opções básicas: nenhum cluster,
todos os clusters ou escolher diretamente um cluster gerenciado ou grupo de
clusters específico (se definido). A opção "Advanced" (Avançado) permite
editar os seletores direto pelo YAML.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width="100%"/>
</imageobject>
<textobject><phrase>criar repo2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Clique em <literal>Create</literal> (Criar). O repositório é criado. A
partir de agora, as cargas de trabalho são instaladas e sincronizadas nos
clusters correspondentes à definição do repositório.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>Depurando e solucionando problemas</title>
<para>A seção de navegação "Advanced" (Avançado) apresenta visões gerais dos
recursos do Fleet de nível mais baixo. <link
xl:href="https://fleet.rancher.io/ref-bundle-stages">Bundle</link> é um
recurso interno usado para orquestração dos recursos do Git. Quando um
repositório Git é verificado, ele gera um ou mais bundles.</para>
<para>Para localizar os bundles relevantes a um repositório específico, vá até a
página de detalhes do repositório Git e clique na guia
<literal>Bundles</literal>.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width="100%"/>
</imageobject>
<textobject><phrase>bundles repositório fleet</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Para cada cluster, o bundle é aplicado a um recurso BundleDeployment que foi
criado. Para ver os detalhes do BundleDeployment, clique no botão
<literal>Graph</literal> (Gráfico) na parte superior direita da página de
detalhes do repositório Git. É carregado um gráfico de <emphasis
role="strong">Repo &gt; Bundles &gt; BundleDeployments</emphasis>
(Repositório > Bundles > BundleDeployments). Clique no BundleDeployment no
gráfico para ver seus detalhes e clique no <literal>Id</literal> para
conferir o YAML do BundleDeployment.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width="100%"/>
</imageobject>
<textobject><phrase>gráfico repositório fleet</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Para obter informações adicionais com dicas de solução de problemas no
Fleet, acesse <link
xl:href="https://fleet.rancher.io/troubleshooting">aqui</link>.</para>
</section>
<section xml:id="id-fleet-examples">
<title>Exemplos do Fleet</title>
<para>A equipe do Edge mantém um <link
xl:href="https://github.com/suse-edge/fleet-examples">repositório</link> com
exemplos de instalação de projetos do Edge com o Fleet.</para>
<para>O projeto do Fleet inclui o repositório <link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>
que abrange todos os casos de uso da <link
xl:href="https://fleet.rancher.io/gitrepo-content">estrutura de repositório
Git</link>.</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SUSE Linux Micro</title>
<para>Consulte a <link
xl:href="https://documentation.suse.com/sle-micro/6.1/">documentação oficial
do SUSE Linux Micro</link></para>
<blockquote>
<para>O SUSE Linux Micro é um sistema operacional leve e seguro para a borda. Ele
combina os componentes protegidos da empresa do SUSE Linux Enterprise com os
recursos que os desenvolvedores desejam em um sistema operacional moderno e
imutável. Como resultado, você tem uma plataforma de infraestrutura
confiável com a mais alta conformidade, porém fácil de usar.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-suse-linux-micro">
<title>Como o SUSE Edge usa o SUSE Linux Micro?</title>
<para>O SUSE Linux Micro é usado como o sistema operacional de base para a nossa
pilha de plataforma. Desse modo, contamos com uma base segura, estável e
mínima para criação.</para>
<para>O SUSE Linux Micro é exclusivo na questão do uso de instantâneos do sistema
de arquivos (Btrfs) para facilitar as reversões se houver qualquer erro no
upgrade. Esse recurso protege os upgrades remotos de toda a plataforma mesmo
sem acesso físico em caso de problemas.</para>
</section>
<section xml:id="id-best-practices-2">
<title>Melhores práticas</title>
<section xml:id="id-installation-media">
<title>Mídia de instalação</title>
<para>O SUSE Edge usa o Edge Image Builder (<xref linkend="components-eib"/>) para
pré-configurar a imagem de autoinstalação do SUSE Linux Micro.</para>
</section>
<section xml:id="id-local-administration">
<title>Administração local</title>
<para>O SUSE Linux Micro vem com o Cockpit, que permite o gerenciamento local do
host por meio de um aplicativo web.</para>
<para>Por padrão, esse serviço está desabilitado, mas é possível habilitar o
serviço <literal>cockpit.socket</literal> do systemd para iniciá-lo.</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>Problemas conhecidos</title>
<itemizedlist>
<listitem>
<para>No momento, não existe um ambiente de área de trabalho disponível no SUSE
Linux Micro, mas há uma solução conteinerizada em desenvolvimento.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>
é um projeto da CNCF que oferece recursos de gerenciamento de infraestrutura
bare metal para Kubernetes.</para>
<para>O Metal<superscript>3</superscript> inclui recursos nativos do Kubernetes
para gerenciar o ciclo de vida de servidores bare metal que suportam
gerenciamento com protocolos fora da banda, como o <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>Ele também conta com suporte a <link
xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link>, que
permite gerenciar os recursos da infraestrutura de vários provedores por
meio de APIs amplamente adotadas e independentes de fornecedor.</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>Como o SUSE Edge usa o Metal<superscript>3</superscript>?</title>
<para>Este método é útil em cenários com hardware de destino que permite o
gerenciamento fora da banda e quando se deseja um fluxo de gerenciamento de
infraestrutura automatizado.</para>
<para>Esse método fornece APIs declarativas que permitem o gerenciamento de
inventário e de estado dos servidores bare metal, incluindo inspeção,
limpeza e provisionamento/desprovisionamento automatizados.</para>
</section>
<section xml:id="id-known-issues-3">
<title>Problemas conhecidos</title>
<itemizedlist>
<listitem>
<para>No momento, não há suporte para o <link
xl:href="https://github.com/metal3-io/ip-address-manager">controlador de
gerenciamento de endereços IP</link> upstream, pois ele ainda não é
compatível com a nossa seleção de ferramentas de configuração de rede.</para>
</listitem>
<listitem>
<para>Da mesma forma, os recursos do IPAM e os campos networkData do
Metal3DataTemplate não são suportados.</para>
</listitem>
<listitem>
<para>Apenas há suporte para implantação por redfish-virtualmedia.</para>
</listitem>
<listitem>
<para>É possível observar um desalinhamento de nome de dispositivo de rede entre o
Ironic Python Agent (IPA) e o sistema operacional de destino (SL Micro
6.0/6.1), principalmente ao tentar configurar nomes previsíveis para os
dispositivos.</para>
</listitem>
</itemizedlist>
<para>Isso acontece porque o kernel do Ironic Python Agent (IPA) não está alinhado
ao kernel do sistema operacional de destino (SL Micro 6.0/6.1), portanto, há
um desalinhamento nos drivers de rede que permite que o IPA descubra
dispositivos de rede em um padrão de nomenclatura diferente do que o
esperado pelo SL Micro.</para>
<para>Por enquanto, há duas abordagens distintas para adotar como solução
alternativa: * Criar dois segredos diferentes com a configuração de rede: um
para aplicar ao IPA, usando os mesmos nomes de dispositivo que o IPA
descobrirá, e usá-lo como <literal>preprovisioningNetworkDataName</literal>
na definição de <literal>BareMetalHost</literal>; e o outro segredo, com os
mesmos nomes de dispositivo que o SL Micro descobrirá, para usar como
<literal>networkData.name</literal> na definição de
<literal>BareMetalHost</literal>. * Usar os UUIDs para fazer referência a
outras interfaces nos arquivos nmconnection gerenciados. Há mais detalhes na
seção <link xl:href="..tips/metal3.adoc">Dicas e truques</link>.</para>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para>Consulte o <link
xl:href="https://github.com/suse-edge/edge-image-builder">repositório
oficial</link>.</para>
<para>O Edge Image Builder (EIB) é uma ferramenta que simplifica a geração das
imagens de disco personalizadas e prontas para inicialização (CRB) para
máquinas de inicialização. Essas imagens permitem a implantação de ponta a
ponta da pilha de software completa do SUSE com uma única imagem.</para>
<para>O EIB cria imagens CRB para todos os cenários de provisionamento e ainda
demonstra um enorme valor para as implantações air-gapped com redes
limitadas ou totalmente isoladas.</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>Como o SUSE Edge usa o Edge Image Builder?</title>
<para>O SUSE Edge usa o EIB para configuração rápida e simplificada de imagens
personalizadas do SUSE Linux Micro para diversos cenários, incluindo
inicialização de máquinas virtuais e bare metal com:</para>
<itemizedlist>
<listitem>
<para>Implantações totalmente air-gapped do Kubernetes K3s/RKE2 (nó único e vários
nós)</para>
</listitem>
<listitem>
<para>Implantações totalmente air-gapped de gráficos Helm e manifestos do
Kubernetes</para>
</listitem>
<listitem>
<para>Registro no Rancher pela API Elemental</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>Rede personalizada (por exemplo, IP estático, nome de host, VLANs,
vinculação etc.)</para>
</listitem>
<listitem>
<para>Configurações personalizadas de sistema operacional (por exemplo, usuários,
grupos, senhas, chaves SSH, proxies, NTP, certificados SSL personalizados
etc.)</para>
</listitem>
<listitem>
<para>Instalação air-gapped de pacotes RPM no nível do host e sideloaded (com
resolução de dependências)</para>
</listitem>
<listitem>
<para>Registro do SUSE Multi-Linux Manager para gerenciamento de sistema
operacional</para>
</listitem>
<listitem>
<para>Imagens de contêiner incorporadas</para>
</listitem>
<listitem>
<para>Argumentos de linha de comando do Kernel</para>
</listitem>
<listitem>
<para>Unidades do Systemd para habilitação/desabilitação no momento da
inicialização</para>
</listitem>
<listitem>
<para>Scripts e arquivos personalizados para tarefas manuais</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>Introdução</title>
<para>A documentação completa de uso e teste do Edge Image Builder está disponível
<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.3/docs">aqui</link>.</para>
<para>Consulte também o <xref linkend="quickstart-eib"/> que apresenta um cenário
de implantação básica.</para>
<para>Depois que você se familiarizar com a ferramenta, confira outras informações
úteis na página da seção Dicas e truques do EIB (<xref
linkend="tips-and-tricks"/>).</para>
</section>
<section xml:id="id-known-issues-4">
<title>Problemas conhecidos</title>
<itemizedlist>
<listitem>
<para>O EIB cria gabaritos dos gráficos Helm para isolá-los e analisa todas as
imagens dentro dos gabaritos. Se um gráfico Helm não incluir todas as suas
imagens dentro de um gabarito e, em vez disso, fizer sideload das imagens, o
EIB não poderá isolar essas imagens automaticamente. Nesse caso, a solução é
adicionar manualmente as imagens não detectadas à seção
<literal>embeddedArtifactRegistry</literal> do arquivo de definição.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Rede de borda</title>
<para>Esta seção descreve a abordagem de configuração de rede na solução SUSE
Edge. Vamos mostrar como configurar o NetworkManager no SUSE Linux Micro de
maneira declarativa e explicar como as ferramentas relacionadas são
integradas.</para>
<section xml:id="id-overview-of-networkmanager">
<title>Visão geral do NetworkManager</title>
<para>O NetworkManager é uma ferramenta que gerencia a conexão de rede principal e
outras interfaces de conexão.</para>
<para>O NetworkManager armazena as configurações de rede como arquivos de conexão
que contêm o estado desejado. Essas conexões são armazenadas como arquivos
no diretório <literal>/etc/NetworkManager/system-connections/</literal>.</para>
<para>Os detalhes sobre o NetworkManager estão disponíveis na <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-network-configuration/index.html">documentação
do SUSE Linux Micro</link>.</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>Visão geral do nmstate</title>
<para>O nmstate é uma biblioteca muito usada (com uma ferramenta CLI auxiliar) que
oferece uma API declarativa para configurações de rede usando um esquema
predefinido.</para>
<para>Os detalhes sobre o nmstate estão disponíveis na <link
xl:href="https://nmstate.io/">documentação upstream</link>.</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>Inserir: NetworkManager Configurator (nmc)</title>
<para>As opções de personalização de rede no SUSE Edge são obtidas por uma
ferramenta CLI chamada NetworkManager Configurator, ou
<emphasis>nmc</emphasis> na forma abreviada, que aproveita a funcionalidade
da biblioteca nmstate e, desse modo, é totalmente capaz de configurar
endereços IP estáticos, servidores DNS, VLANs, vínculos, pontes etc. Essa
ferramenta permite gerar configurações de rede com base em estados desejados
predefinidos e aplicá-las a vários nós de maneira automatizada.</para>
<para>Os detalhes sobre o NetworkManager Configurator (nmc) estão disponíveis no
<link xl:href="https://github.com/suse-edge/nm-configurator">repositório
upstream</link>.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>Como o SUSE Edge usa o NetworkManager Configurator?</title>
<para>O SUSE Edge utiliza o <emphasis>nmc</emphasis> para as personalizações de
rede em diversos modelos de provisionamento:</para>
<itemizedlist>
<listitem>
<para>Configurações de rede personalizadas em cenários de provisionamento de rede
direcionado (<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>Configurações estáticas declarativas em cenários de provisionamento com base
em imagem (<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Configurando com o Edge Image Builder</title>
<para>O Edge Image Builder (EIB) é uma ferramenta que permite configurar vários
hosts com uma única imagem de sistema operacional. Nesta seção, vamos
mostrar como usar uma abordagem declarativa para descrever os estados da
rede desejados, como eles são convertidos nas respectivas conexões do
NetworkManager e, em seguida, aplicados durante o processo de
provisionamento.</para>
<section xml:id="id-prerequisites-3">
<title>Pré-requisitos</title>
<para>Se você está seguindo este guia, já deve ter os seguintes itens disponíveis:</para>
<itemizedlist>
<listitem>
<para>Host físico (ou máquina virtual) AMD64/Intel 64 com SLES 15 SP6 ou openSUSE
Leap 15.6</para>
</listitem>
<listitem>
<para>Runtime de contêiner disponível (por exemplo, Podman)</para>
</listitem>
<listitem>
<para>Cópia da imagem RAW do SUSE Linux Micro 6.1 disponível <link
xl:href="https://www.suse.com/download/sle-micro/">aqui</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Obtendo a imagem de contêiner do Edge Image Builder</title>
<para>A imagem de contêiner do EIB está disponível publicamente e pode ser baixada
do registro SUSE Edge executando o seguinte comando:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>Criando o diretório de configuração de imagem</title>
<para>Vamos começar pela criação do diretório de configuração:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>Agora vamos mover a cópia da imagem base baixada para o diretório de
configuração:</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SL-Micro.x86_64-6.1-Base-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>O EIB nunca modifica a entrada da imagem base, ele cria uma nova com as
respectivas modificações.</para>
</note>
</blockquote>
<para>Neste momento, o diretório de configuração deve ter a seguinte aparência:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>Criando o arquivo de definição de imagem</title>
<para>O arquivo de definição descreve a maioria das opções configuráveis que o
Edge Image Builder suporta.</para>
<para>Vamos começar com um arquivo de definição bem básico para nossa imagem de
sistema operacional:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.3
image:
  arch: x86_64
  imageType: raw
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para>A seção <literal>image</literal> é obrigatória e especifica a imagem de
entrada, sua arquitetura e tipo, além do nome da imagem de saída. A seção
<literal>operatingSystem</literal> é opcional e contém a configuração para
permitir o login nos sistemas provisionados com nome de usuário/senha
<literal>root/eib</literal>.</para>
<blockquote>
<note>
<para>Você pode usar sua própria senha criptografada executando <literal>openssl
passwd -6 &lt;senha&gt;</literal>.</para>
</note>
</blockquote>
<para>Neste momento, o diretório de configuração deve ter a seguinte aparência:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>Definindo as configurações de rede</title>
<para>As configurações de rede desejadas não fazem parte do arquivo de definição
da imagem que acabamos de criar. Vamos agora preenchê-las no diretório
especial <literal>network/</literal>. Para criá-lo:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>Como já foi mencionado, a ferramenta NetworkManager Configurator
(<emphasis>nmc</emphasis>) espera uma entrada no formato de esquema
predefinido. Você encontra como configurar uma ampla variedade de opções de
rede na <link xl:href="https://nmstate.io/examples.html">documentação de
exemplos upstream do NMState</link>.</para>
<para>Esse guia explica como configurar a rede em três nós diferentes:</para>
<itemizedlist>
<listitem>
<para>Um nó que usa duas interfaces Ethernet</para>
</listitem>
<listitem>
<para>Um nó que usa vínculo de rede</para>
</listitem>
<listitem>
<para>Um nó que usa ponte de rede</para>
</listitem>
</itemizedlist>
<warning>
<para>O uso de configurações de rede completamente diferentes não é recomendado em
builds de produção, especialmente na configuração de clusters Kubernetes. Em
geral, as configurações de rede devem ser homogêneas entre os nós ou, pelo
menos, entre as funções em um determinado cluster. Este guia inclui diversas
opções apenas para servir como referência de exemplo.</para>
</warning>
<blockquote>
<note>
<para>No exemplo abaixo, foi considerada a rede padrão <literal>libvirt</literal>
com um intervalo de endereços IP
<literal>192.168.122.1/24</literal>. Ajuste-a de acordo se for diferente em
seu ambiente.</para>
</note>
</blockquote>
<para>Vamos criar os estados desejados para o primeiro nó, que será chamado de
<literal>node1.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>Nesse exemplo, definimos um estado desejado de duas interfaces Ethernet
(eth0 e eth3), os endereços IP solicitados, o roteamento e a resolução DNS.</para>
<warning>
<para>Garanta que os endereços MAC de todas as interfaces Ethernet sejam
listados. Eles são usados durante o processo de provisionamento como
identificadores dos nós e servem para determinar quais configurações devem
ser aplicadas. É dessa forma que podemos configurar vários nós usando uma
única imagem ISO ou RAW.</para>
</warning>
<para>O próximo é o segundo nó que será chamado de
<literal>node2.suse.com</literal> e usará vínculo de rede:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>Nesse exemplo, definimos um estado desejado de duas interfaces Ethernet
(eth0 e eth1) que não habilitam endereçamento IP, além de um vínculo com
política round robin e o respectivo endereço que será usado para encaminhar
o tráfego de rede.</para>
<para>Por fim, vamos criar o terceiro e último arquivo de estado desejado, que
usará uma ponte de rede e será chamado de <literal>node3.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>Neste momento, o diretório de configuração deve ter a seguinte aparência:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<blockquote>
<note>
<para>Os nomes dos arquivos no diretório <literal>network/</literal> são
intencionais e correspondem aos nomes de host que serão definidos durante o
processo de provisionamento.</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>Criando a imagem de sistema operacional</title>
<para>Agora que todas as configurações necessárias foram definidas, podemos criar
a imagem executando simplesmente:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>A saída deve ter uma aparência semelhante a esta:</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>O trecho acima nos informa que o componente <literal>Network</literal> foi
configurado com sucesso, e podemos prosseguir com o provisionamento dos nós
de borda.</para>
<blockquote>
<note>
<para>É possível inspecionar um arquivo de registro
(<literal>network-config.log</literal>) e os respectivos arquivos de conexão
do NetworkManager no diretório <literal>_build</literal> resultante abaixo
do diretório com a marcação de data e hora da execução da imagem.</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>Provisionando os nós de borda</title>
<para>Vamos copiar a imagem RAW resultante:</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>Veja que copiamos a imagem criada quatro vezes, mas especificamos apenas as
configurações de rede para três nós. O motivo para isso é que também
queremos demonstrar o que acontece quando provisionamos um nó que não
corresponde a nenhuma das configurações desejadas.</para>
<blockquote>
<note>
<para>Este guia usará a virtualização nos exemplos de provisionamento de
nós. Assegure que as extensões necessárias estejam habilitadas no BIOS
(consulte <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">aqui</link>
para obter detalhes).</para>
</note>
</blockquote>
<para>Vamos usar <literal>virt-install</literal> para criar máquinas virtuais por
meio dos discos brutos copiados. Cada máquina virtual usará 10 GB de RAM e 6
vCPUs.</para>
<section xml:id="id-provisioning-the-first-node">
<title>Provisionando o primeiro nó</title>
<para>Vamos criar a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>É importante criar as interfaces de rede com os mesmos endereços MAC
daqueles no estado desejado descrito acima.</para>
</note>
</blockquote>
<para>Após o término da operação, veremos algo semelhante ao seguinte:</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Micro 6.0 (x86_64) - Kernel 6.4.0-18-default (tty1).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>Agora podemos fazer login com o par de credenciais
<literal>root:eib</literal> e também usar SSH para entrar no host, se for a
preferência no lugar de <literal>virsh console</literal> apresentado aqui.</para>
<para>Após o login, vamos confirmar se todas as configurações estão corretas.</para>
<para>Verifique se o nome de host foi devidamente definido:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>Verifique se o roteamento foi devidamente configurado:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>Verifique se a conexão com a Internet está disponível:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>Verifique se exatamente duas interfaces Ethernet foram configuradas e apenas
uma delas está ativa:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>Veja que a segunda interface é <literal>eth1</literal> em vez do
<literal>eth3</literal> predefinido em nosso estado de rede desejado. O
motivo é que o NetworkManager Configurator (<emphasis>nmc</emphasis>) é
capaz de detectar que o sistema operacional forneceu um nome diferente para
o NIC com o endereço MAC <literal>34:8a:b1:4b:16:e2</literal> e ele ajusta
suas configurações de maneira apropriada.</para>
<para>Verifique se foi isso mesmo que aconteceu analisando a fase Combustion do
provisionamento:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>Vamos agora provisionar o restante dos nós, mas vamos mostrar apenas as
diferenças na configuração final. Aplique qualquer uma ou todas as
verificações acima a todos os nós que você vai provisionar.</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>Provisionando o segundo nó</title>
<para>Vamos criar a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>Com a máquina virtual em funcionamento, podemos confirmar se esse nó usa as
interfaces vinculadas:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>Confirme se o roteamento usa o vínculo:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>Garanta que os arquivos de conexão estáticos sejam devidamente utilizados:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>Provisionando o terceiro nó</title>
<para>Vamos criar a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>Com a máquina virtual em funcionamento, podemos confirmar se esse nó usa uma
ponte de rede:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>Confirme se o roteamento usa a ponte:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>Garanta que os arquivos de conexão estáticos sejam devidamente utilizados:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>Provisionando o quarto nó</title>
<para>Por fim, vamos provisionar um nó que não corresponderá a nenhuma das
configurações predefinidas por um endereço MAC. Nesse caso, vamos usar DHCP
como padrão para configurar as interfaces de rede.</para>
<para>Vamos criar a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>Com a máquina virtual em funcionamento, podemos confirmar se esse nó usa um
endereço IP aleatório para a interface de rede:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>Verifique se o NMC não consegue aplicar configurações estáticas a esse nó:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>Verifique se a interface Ethernet foi configurada por DHCP:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="networking-unified">
<title>Configurações unificadas de nós</title>
<para>Há situações em que não é possível usar endereços MAC conhecidos. Nesses
casos, podemos recorrer à <emphasis>configuração unificada</emphasis>, que
permite especificar as configurações em um arquivo
<literal>_all.yaml</literal> que será aplicado a todos os nós provisionados.</para>
<para>Vamos criar e provisionar um nó de borda usando uma estrutura de
configuração diferente. Siga todas as etapas, desde a <xref
linkend="image-config-dir-creation"/> até a <xref
linkend="default-network-definition"/>.</para>
<para>Neste exemplo, definimos um estado desejado de duas interfaces Ethernet
(eth0 e eth1): uma usando DHCP e outra que recebeu um endereço IP estático.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>Depois que a imagem for criada com sucesso, vamos criar uma máquina virtual
com base nela:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>O processo de provisionamento pode levar alguns minutos. Depois que ele for
concluído, faça login no sistema com as credenciais fornecidas.</para>
<para>Verifique se o roteamento foi devidamente configurado:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>Verifique se a conexão com a Internet está disponível:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Verifique se as interfaces Ethernet estão configuradas e ativas:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>Configurações de rede personalizadas</title>
<para>Já abordamos a configuração de rede padrão do Edge Image Builder que usa o
NetworkManager Configurator. No entanto, há também a opção de modificá-la
por meio de um script personalizado. Essa opção é muito flexível e
independente de endereço MAC, mas tem a limitação de que seu uso não é tão
prático para inicializar vários nós com uma única imagem.</para>
<blockquote>
<note>
<para>A recomendação é usar a configuração de rede padrão por meio dos arquivos
que descrevem o estado da rede desejado no diretório
<literal>/network</literal>. Crie scripts personalizados apenas quando esse
comportamento não é cabível no seu caso de uso.</para>
</note>
</blockquote>
<para>Vamos criar e provisionar um nó de borda usando uma estrutura de
configuração diferente. Siga todas as etapas, desde a <xref
linkend="image-config-dir-creation"/> até a <xref
linkend="default-network-definition"/>.</para>
<para>Neste exemplo, vamos criar um script personalizado que aplica a configuração
estática da interface <literal>eth0</literal> a todos os nós provisionados,
além de remover e desabilitar as conexões com fio criadas automaticamente
pelo NetworkManager. Isso é vantajoso nos casos em que você quer garantir
que cada nó no cluster tenha uma configuração de rede idêntica e, sendo
assim, você não precisa se preocupar com o endereço MAC de cada nó antes da
criação da imagem.</para>
<para>Para começar, vamos armazenar o arquivo de conexão no diretório
<literal>/custom/files</literal>:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>Com a configuração estática criada, vamos também criar nosso script de rede
personalizado:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>Por padrão, o binário nmc ainda será incluído, para ser usado no script
<literal>configure-network.sh</literal> se necessário.</para>
</note>
</blockquote>
<warning>
<para>O script personalizado deve sempre ser inserido em
<literal>/network/configure-network.sh</literal> no diretório de
configuração. Se estiver presente, todos os outros arquivos serão
ignorados. NÃO é possível configurar uma rede trabalhando com as
configurações estáticas no formato YAML e um script personalizado ao mesmo
tempo.</para>
</warning>
<para>Neste momento, o diretório de configuração deve ter a seguinte aparência:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>Depois que a imagem for criada com sucesso, vamos criar uma máquina virtual
com base nela:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>O processo de provisionamento pode levar alguns minutos. Depois que ele for
concluído, faça login no sistema com as credenciais fornecidas.</para>
<para>Verifique se o roteamento foi devidamente configurado:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>Verifique se a conexão com a Internet está disponível:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>Verifique se uma interface Ethernet foi configurada estaticamente usando
nosso arquivo de conexão e se ela está ativa:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elemental é uma pilha de software que possibilita o gerenciamento de sistema
operacional nativo de nuvem e totalmente centralizado com o Kubernetes. A
pilha Elemental consiste em vários componentes que residem no próprio
Rancher ou em nós de borda. Os componentes principais são:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> : o operador principal
que reside no Rancher e processa as solicitações de registro dos clientes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis>: o cliente executado
em nós de borda que possibilita o registro por meio do
<literal>elemental-operator</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis>: um agente que
reside em nós de borda. Sua configuração é alimentada pelo
<literal>elemental-register</literal> e recebe um <literal>plan</literal>
para configurar o <literal>rancher-system-agent</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis>: após o registro
completo do nó de borda, esse componente assume o comando depois do
<literal>elemental-system-agent</literal> e aguarda outros
<literal>plans</literal> do Rancher Manager (por exemplo, instalação do
Kubernetes).</para>
</listitem>
</itemizedlist>
<para>Consulte a <link xl:href="https://elemental.docs.rancher.com/">documentação
upstream do Elemental</link> para obter todas as informações sobre o
Elemental e seu relacionamento com o Rancher.</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>Como o SUSE Edge usa o Elemental?</title>
<para>Usamos partes do Elemental para gerenciar dispositivos remotos quando o
Metal<superscript>3</superscript> não é viável (por exemplo, não existe BMC
ou o dispositivo está protegido por um gateway NAT). Essa ferramenta permite
que o operador inicialize os dispositivos em laboratório antes de saber
quando ou para onde serão enviados. Especificamente, usamos os componentes
<literal>elemental-register</literal> e
<literal>elemental-system-agent</literal> para permitir a integração de
hosts do SUSE Linux Micro no Rancher para casos de uso de provisionamento de
rede "phone home". Quando o Edge Image Builder (EIB) é usado para criar
imagens de implantação, o registro automático pelo Rancher via Elemental
pode ser feito especificando a configuração de registro no diretório de
configuração do EIB.</para>
<note>
<para>No SUSE Edge 3.4, <emphasis role="strong">não</emphasis> aproveitamos os
aspectos de gerenciamento de sistema operacional do Elemental e, portanto,
não é possível gerenciar a aplicação de patches em seu sistema operacional
pelo Rancher. Em vez de usar as ferramentas do Elemental para criar imagens
de implantação, o SUSE Edge usa a ferramenta Edge Image Builder, que consome
a configuração de registro.</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>Melhores práticas</title>
<section xml:id="id-installation-media-2">
<title>Mídia de instalação</title>
<para>A maneira recomendada do SUSE Edge para criar imagens de implantação que
possam usar o Elemental para registro no Rancher na área de implantação de
"provisionamento de rede 'phone home'" é seguir as instruções detalhadas na
inicialização rápida sobre integração de host remoto com o Elemental (<xref
linkend="quickstart-elemental"/>).</para>
</section>
<section xml:id="id-labels">
<title>Rótulos</title>
<para>O Elemental monitora o inventário com a CRD
<literal>MachineInventory</literal> e permite selecionar um inventário, por
exemplo, para selecionar máquinas nas quais implantar clusters Kubernetes
com base em rótulos. Dessa forma, os usuários podem predefinir grande parte
(se não tudo) de suas necessidades de infraestrutura antes mesmo da compra
do hardware. Além disso, como os nós podem adicionar/remover rótulos de seu
respectivo objeto de inventário (executando novamente o
<literal>elemental-register</literal> com o sinalizador adicional
<literal>--label "FOO=BAR"</literal>), podemos escrever scripts para
descobrir e permitir que o Rancher saiba onde um nó é inicializado.</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>Problemas conhecidos</title>
<itemizedlist>
<listitem>
<para>Atualmente, a IU do Elemental não sabe como criar uma mídia de instalação ou
atualizar sistemas operacionais não "Elemental Teal". Isso deve ser
resolvido em versões futuras.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-akri">
<title>Akri</title>
<para>Akri é um projeto Sandbox da CNCF que visa a descobrir dispositivos folha
para apresentá-los como recursos nativos do Kubernetes. Ele também permite
programar um pod ou job para cada dispositivo descoberto. Os dispositivos
podem ser de nó local ou de rede e usar uma ampla variedade de protocolos.</para>
<para>A documentação upstream do Akri está disponível em: <link
xl:href="https://docs.akri.sh">https://docs.akri.sh</link></para>
<section xml:id="id-how-does-suse-edge-use-akri">
<title>Como o SUSE Edge usa o Akri?</title>
<warning>
<para>Atualmente, o Akri está em prévia de tecnologia na pilha do SUSE Edge.</para>
</warning>
<para>O Akri está disponível como parte da pilha do Edge sempre que é preciso
descobrir e programar uma carga de trabalho com base nos dispositivos folha.</para>
</section>
<section xml:id="id-installing-akri">
<title>Instalando o Akri</title>
<para>O Akri está disponível como gráfico Helm no repositório Helm do Edge. A
maneira recomendada de configurar o Akri é usar o gráfico Helm especificado
para implantar os diversos componentes (agente, controlador, manipuladores
de descoberta) e usar o mecanismo de implantação de sua preferência para
implantar as CRDs de configuração do Akri.</para>
</section>
<section xml:id="id-configuring-akri">
<title>Configurando o Akri</title>
<para>O Akri é configurado usando um objeto
<literal>akri.sh/Configuration</literal>, que obtém todas as informações de
como descobrir os dispositivos e do que fazer quando um correspondente é
descoberto.</para>
<para>Veja abaixo um exemplo de configuração decomposto com a explicação de todos
os campos:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: akri.sh/v0
kind: Configuration
metadata:
  name: sample-configuration
spec:</screen>
<para>Esta parte descreve a configuração do manipulador de descoberta. Você deve
especificar o nome dele (os manipuladores disponíveis como parte do gráfico
do Akri são <literal>udev</literal>, <literal>opcua</literal> e
<literal>onvif</literal>). O <literal>discoveryDetails</literal> é
específico do manipulador. Consulte a documentação do manipulador para saber
como configurá-lo.</para>
<screen language="yaml" linenumbering="unnumbered">  discoveryHandler:
    name: debugEcho
    discoveryDetails: |+
      descriptions:
        - "foo"
        - "bar"</screen>
<para>Esta seção define a carga de trabalho que será implantada para cada
dispositivo descoberto. O exemplo mostra uma versão mínima da configuração
do <literal>Pod</literal> em <literal>brokerPodSpec</literal>, todos os
campos comuns da especificação de um pod podem ser usados aqui. Ele também
mostra a sintaxe específica do Akri para solicitar o dispositivo na seção
<literal>resources</literal>.</para>
<para>Se preferir, use um job no lugar do pod, com a chave
<literal>brokerJobSpec</literal>, e insira a parte da especificação de um
job para ele.</para>
<screen language="yaml" linenumbering="unnumbered">  brokerSpec:
    brokerPodSpec:
      containers:
      - name: broker-container
        image: rancher/hello-world
        resources:
          requests:
            "{{PLACEHOLDER}}" : "1"
          limits:
            "{{PLACEHOLDER}}" : "1"</screen>
<para>Estas duas seções mostram como configurar o Akri para implantar um serviço
por agente (<literal>instanceService</literal>) ou apontar para todos os
agentes (<literal>configurationService</literal>). Eles contêm todos os
elementos que pertencem a um serviço comum.</para>
<screen language="yaml" linenumbering="unnumbered">  instanceServiceSpec:
    type: ClusterIp
    ports:
    - name: http
      port: 80
      protocol: tcp
      targetPort: 80
  configurationServiceSpec:
    type: ClusterIp
    ports:
    - name: https
      port: 443
      protocol: tcp
      targetPort: 443</screen>
<para>O campo <literal>brokerProperties</literal> é um armazenamento de
chave/valor que será exposto como variáveis de ambiente adicionais para um
pod que solicita um dispositivo descoberto.</para>
<para>A capacidade é o número permitido de usuários simultâneos de um dispositivo
descoberto.</para>
<screen language="yaml" linenumbering="unnumbered">  brokerProperties:
    key: value
  capacity: 1</screen>
</section>
<section xml:id="id-writing-and-deploying-additional-discovery-handlers">
<title>Gravando e implantando manipuladores de descoberta adicionais</title>
<para>Se o protocolo usado por seu dispositivo não for coberto por um manipulador
de descoberta existente, você pode escrever um próprio seguindo o <link
xl:href="https://docs.akri.sh/development/handler-development">guia de
desenvolvimento de manipulador</link>.</para>
</section>
<section xml:id="akri-dashboard-extension-usage">
<title>Extensão Akri do Rancher Dashboard</title>
<para>Com a extensão de dashboard Akri, você pode usar a interface de usuário do
Rancher Dashboard para gerenciar e monitorar dispositivos folha e executar
cargas de trabalho após a descoberta desses dispositivos.</para>
<para>Consulte o <xref linkend="components-rancher-dashboard-extensions"/> para
ver a orientação de instalação.</para>
<para>Depois que a extensão for instalada, navegue até um cluster habilitado pelo
Akri usando o explorador de clusters. No grupo de navegação <emphasis
role="strong">Akri</emphasis>, você verá as seções <emphasis
role="strong">Configurations</emphasis> (Configurações) e <emphasis
role="strong">Instances</emphasis> (Instâncias).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configurations.png"
width="100%"/> </imageobject>
<textobject><phrase>configurações extensão akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>A lista de configurações apresenta informações sobre o <literal>manipulador
de descoberta de configuração</literal> e o número de instâncias. Clique no
nome para abrir a página de detalhes da configuração.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-detail.png"
width="100%"/> </imageobject>
<textobject><phrase>detalhes configuração extensão akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Você também pode editar ou criar uma nova <emphasis
role="strong">configuração</emphasis>. A extensão permite selecionar o
manipulador de descoberta, configurar o pod ou job do agente, personalizar
configurações e serviços de instâncias e definir a capacidade da
configuração.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-edit.png"
width="100%"/> </imageobject>
<textobject><phrase>edição configuração extensão akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Os dispositivos descobertos são relacionados na lista <emphasis
role="strong">Instances</emphasis> (Instâncias).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instances-list.png"
width="100%"/> </imageobject>
<textobject><phrase>lista instâncias extensão akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Clique no nome da <emphasis role="strong">instância</emphasis> para abrir
uma página de detalhes com as cargas de trabalho e o serviço da instância.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instance-detail.png"
width="100%"/> </imageobject>
<textobject><phrase>detalhes instância extensão akri</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link xl:href="https://k3s.io/">K3s</link> é uma distribuição Kubernetes
certificada e altamente disponível desenvolvida para cargas de trabalho de
produção em locais autônomos, remotos e com restrição de recursos ou dentro
de aplicações IoT.</para>
<para>Ele é empacotado como um binário único e pequeno, portanto, as instalações e
atualizações são rápidas e fáceis.</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>Como o SUSE Edge usa o K3s</title>
<para>É possível usar o K3s como distribuição Kubernetes de base para a pilha do
SUSE Edge. Ele foi projetado para ser instalado em um sistema operacional
SUSE Linux Micro.</para>
<para>O uso do K3s como distribuição Kubernetes da pilha do SUSE Edge é
recomendado apenas quando o etcd como back end não atende a suas
restrições. Se o etcd como back end for possível, será melhor usar o RKE2
(<xref linkend="components-rke2"/>).</para>
</section>
<section xml:id="id-best-practices-4">
<title>Melhores práticas</title>
<section xml:id="id-installation-2">
<title>Instalação</title>
<para>A maneira recomendada de instalar o K3s como parte da pilha do SUSE Edge é
usar o Edge Image Builder (EIB). Consulte a documentação dele (<xref
linkend="components-eib"/>) para obter mais detalhes de como configurá-lo
para implantar o K3s.</para>
<para>Ele oferece suporte automático para configuração de alta disponibilidade e
do Elemental.</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>Fleet para fluxo de trabalho do GitOps</title>
<para>A pilha do SUSE Edge usa o Fleet como ferramenta GitOps preferencial. Para
obter mais informações sobre instalação e uso, consulte a seção do Fleet
(<xref linkend="components-fleet"/>) nesta documentação.</para>
</section>
<section xml:id="id-storage-management">
<title>Gerenciamento de armazenamento</title>
<para>O K3s vem com armazenamento de caminho local pré-configurado, que é adequado
para clusters de nó único. Para clusters que abrangem vários nós,
recomendamos usar o SUSE Storage (<xref
linkend="components-suse-storage"/>).</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>Balanceamento de carga e alta disponibilidade</title>
<para>Se você instalou o K3s com o EIB, esta parte já foi coberta pela
documentação do EIB na seção de alta disponibilidade.</para>
<para>Do contrário, será necessário instalar e configurar o MetalLB de acordo com
a nossa documentação do MetalLB (<xref linkend="guides-metallb-k3s"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para>Consulte a <link xl:href="https://docs.rke2.io/">documentação oficial do
RKE2</link>.</para>
<para>O RKE2 é uma distribuição Kubernetes totalmente compatível dedicada à
segurança e à conformidade das seguintes maneiras:</para>
<itemizedlist>
<listitem>
<para>Oferecendo padrões e opções de configuração para que os clusters sejam
aprovados no CIS Kubernetes Benchmark v1.6 ou v1.23 com o mínimo de
intervenção do operador</para>
</listitem>
<listitem>
<para>Assegurando a conformidade com o FIPS 140-2</para>
</listitem>
<listitem>
<para>Verificando regularmente se há CVEs nos componentes por meio do <link
xl:href="https://trivy.dev">trivy</link> no pipeline de compilação do RKE2</para>
</listitem>
</itemizedlist>
<para>O RKE2 inicia os componentes do plano de controle como pods estáticos,
gerenciados pelo kubelet. O runtime de contêiner incorporado é containerd.</para>
<para>Nota: O RKE2 também é conhecido como RKE Government para expressar outro
caso de uso e setor a que ele atende.</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2 ou K3s</title>
<para>O K3s é uma distribuição Kubernetes leve e totalmente compatível, com foco
em borda, IoT e ARM, otimizado para facilidade de uso e ambientes com
restrição de recursos.</para>
<para>O RKE2 combina o melhor dos dois mundos: a versão 1.x do RKE (daqui em
diante chamada de RKE1) e o K3s.</para>
<para>Do K3s, ele herda a usabilidade, a facilidade de operação e o modelo de
implantação.</para>
<para>Do RKE1, ele herda o alinhamento próximo com o Kubernetes upstream. Em
alguns locais, o K3s diverge do Kubernetes upstream para otimizar as
implantações de borda, mas o RKE1 e o RKE2 mantêm um estreito alinhamento
com o upstream.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>Como o SUSE Edge usa o RKE2?</title>
<para>O RKE2 é um componente fundamental da pilha do SUSE Edge. Ele coexiste com o
SUSE Linux Micro (<xref linkend="components-slmicro"/>), oferecendo a
interface padrão do Kubernetes necessária para implantar cargas de trabalho
do Edge.</para>
</section>
<section xml:id="id-best-practices-5">
<title>Melhores práticas</title>
<section xml:id="id-installation-3">
<title>Instalação</title>
<para>A maneira recomendada de instalar o RKE2 como parte da pilha do SUSE Edge é
usar o Edge Image Builder (EIB). Consulte a documentação do EIB (<xref
linkend="components-eib"/>) para obter mais detalhes de como configurá-lo
para implantar o RKE2.</para>
<para>O EIB é flexível o suficiente para dar suporte aos parâmetros exigidos pelo
RKE2, como especificar a versão do RKE2, a configuração de <link
xl:href="https://docs.rke2.io/reference/server_config">servidores</link> ou
de <link
xl:href="https://docs.rke2.io/reference/linux_agent_config">agentes</link>,
englobando todos os casos de uso do Edge.</para>
<para>Para outros casos de uso envolvendo o Metal<superscript>3</superscript>, o
RKE2 também é usado e instalado. Nesses casos específicos, o <link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">provedor
Cluster API RKE2</link> implanta automaticamente o RKE2 nos clusters que são
provisionados com o Metal<superscript>3</superscript> usando a pilha do
Edge.</para>
<para>Nesses casos, a configuração do RKE2 deve ser aplicada às diversas CRDs
envolvidas. Um exemplo de como fornecer uma CNI diferente usando a CRD
<literal>RKE2ControlPlane</literal> tem esta aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>Para obter mais informações sobre os casos de uso do
Metal<superscript>3</superscript>, consulte o <xref
linkend="components-metal3"/>.</para>
</section>
<section xml:id="id-high-availability">
<title>Alta disponibilidade</title>
<para>Para implantações de alta disponibilidade, o EIB implanta e configura
automaticamente o MetalLB (<xref linkend="components-metallb"/>) e o
Endpoint Copier Operator (<xref linkend="components-eco"/>) para expor o
endpoint da API do RKE2 externamente.</para>
</section>
<section xml:id="id-networking">
<title>Rede</title>
<para>A pilha do SUSE Edge suporta o <link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link> e o <link
xl:href="https://docs.tigera.io/calico/latest/about/">Calico</link>, com o
Cilium como CNI padrão. O metaplug-in <link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>
também pode ser usado quando os pods exigem várias interfaces de rede. O
RKE2 independente oferece suporte a uma <link
xl:href="https://docs.rke2.io/install/network_options">ampla gama de opções
de CNI</link>.</para>
</section>
<section xml:id="id-storage">
<title>Armazenamento</title>
<para>O RKE2 não inclui nenhum tipo de classe ou operador de armazenamento
persistente. Para clusters com vários nós, é recomendado usar o SUSE Storage
(<xref linkend="components-suse-storage"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-suse-storage">
<title><link xl:href="https://www.suse.com/products/rancher/storage/">SUSE
Storage</link></title>
<para>O SUSE Storage é um sistema de armazenamento em blocos distribuído, leve,
confiável e amigável projetado para Kubernetes. Trata-se de um produto com
base no Longhorn, um projeto de código-fonte aberto inicialmente
desenvolvido pela Rancher Labs e que agora é incubado pela CNCF.</para>
<section xml:id="id-prerequisites-4">
<title>Pré-requisitos</title>
<para>Se você está seguindo este guia, já deve ter os seguintes itens disponíveis:</para>
<itemizedlist>
<listitem>
<para>No mínimo, um host com o SUSE Linux Micro 6.1 instalado, que pode ser físico
ou virtual</para>
</listitem>
<listitem>
<para>Um cluster Kubernetes instalado, K3s ou RKE2</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-suse-storage">
<title>Instalação manual do SUSE Storage</title>
<section xml:id="id-installing-open-iscsi">
<title>Instalando o Open-iSCSI</title>
<para>Um requisito essencial da implantação e do uso do SUSE Storage é a
instalação do pacote <literal>open-iscsi</literal> e a execução do daemon
<literal>iscsid</literal> em todos os nós Kubernetes. Isso é necessário
porque o Longhorn conta com o <literal>iscsiadm</literal> no host para
fornecer volumes persistentes ao Kubernetes.</para>
<para>Vamos instalá-lo:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>Como o SUSE Linux Micro é um sistema operacional imutável, é importante
notar que o pacote será instalado apenas em um novo instantâneo após a
conclusão da operação. Para carregá-lo e para que o daemon
<literal>iscsid</literal> seja executado, é necessário reinicializar no novo
instantâneo que acabamos de criar. Execute o comando de reinicialização
quando você estiver pronto:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>Para obter mais ajuda com a instalação do open-iscsi, consulte a <link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/#installing-open-iscsi">documentação
oficial do Longhorn</link> (em inglês).</para>
</tip>
</section>
<section xml:id="id-installing-suse-storage">
<title>Instalando o SUSE Storage</title>
<para>Há várias maneiras de instalar o SUSE Storage em clusters Kubernetes. Este
guia orienta na instalação pelo Helm, mas você poderá seguir a <link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/">documentação
oficial</link> se preferir outra abordagem.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Adicione o repositório de gráficos Helm do Rancher:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>Busque os gráficos mais recentes no repositório:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para>Instale o SUSE Storage no namespace <literal>longhorn-system</literal>:</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn-crd rancher-charts/longhorn-crd --namespace longhorn-system --create-namespace --version 107.0.0+up1.9.1
helm install longhorn rancher-charts/longhorn --namespace longhorn-system --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>Confirme se a implantação foi bem-sucedida:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-suse-storage-volumes">
<title>Criando volumes do SUSE Storage</title>
<para>O SUSE Storage usa os recursos do Kubernetes chamados
<literal>StorageClass</literal> para provisionar automaticamente os objetos
<literal>PersistentVolume</literal> aos pods. Pense no
<literal>StorageClass</literal> como um método para os administradores
descreverem <emphasis>classes</emphasis> ou <emphasis>perfis</emphasis> do
armazenamento que eles oferecem.</para>
<para>Vamos criar uma <literal>StorageClass</literal> com algumas opções padrão:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para>Agora que temos um <literal>StorageClass</literal>, precisamos que um
<literal>PersistentVolumeClaim</literal> faça referência a ele. Um
<literal>PersistentVolumeClaim</literal> (PVC) é uma solicitação de
armazenamento feita pelo usuário. Os PVCs consomem os recursos
<literal>PersistentVolume</literal>. As declarações podem solicitar tamanhos
e modos de acesso específicos (por exemplo, é possível montá-las como
leitura/gravação uma vez ou como somente leitura várias vezes).</para>
<para>Vamos criar um <literal>PersistentVolumeClaim</literal>:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>Pronto! Com a criação do <literal>PersistentVolumeClaim</literal>, podemos
prosseguir e anexá-lo a um <literal>Pod</literal>. Quando o
<literal>Pod</literal> é implantado, o Kubernetes cria o volume do Longhorn
e o vincula ao <literal>Pod</literal> quando há armazenamento disponível.</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>O conceito de armazenamento no Kubernetes é um tópico complexo, porém
importante. Mencionamos brevemente alguns dos recursos mais comuns do
Kubernetes, no entanto, sugerimos a familiarização com a <link
xl:href="https://longhorn.io/docs/1.9.1/terminology/">documentação de
terminologia</link> que o Longhorn oferece.</para>
</tip>
<para>Neste exemplo, o resultado deve ter esta aparência:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>Acessando a IU</title>
<para>Se você instalou o Longhorn com o kubectl ou o Helm, precisa configurar um
controle de entrada para permitir o tráfego externo no cluster. A
autenticação não está habilitada por padrão. Se o app de catálogo do Rancher
foi usado, o Rancher criou automaticamente um controlador de entrada com
controle de acesso (rancher-proxy).</para>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o endereço IP do serviço externo do Longhorn:</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para>Depois que você recuperar o endereço IP do
<literal>longhorn-frontend</literal>, poderá começar a usar a IU navegando
até ela pelo navegador.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Instalando com o Edge Image Builder</title>
<para>O SUSE Edge usa o <xref linkend="components-eib"/> para personalizar as
imagens base do sistema operacional SUSE Linux Micro. Vamos demonstrar como
fazer isso para provisionar um cluster RKE2 junto com o Longhorn.</para>
<para>Vamos criar o arquivo de definição:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>A personalização dos valores de gráficos Helm é possível por um arquivo
separado disponível em <literal>helm.charts[].valuesFile</literal>. Consulte
a <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md#kubernetes">documentação
upstream</link> para obter detalhes.</para>
</note>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>Após a criação da imagem, você poderá usá-la para instalar o sistema
operacional em um host físico ou virtual. Depois que o provisionamento é
concluído, é possível fazer login no sistema usando o par de credenciais
<literal>root:eib</literal>.</para>
<para>Verifique se o Longhorn foi implantado com sucesso:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>Essa instalação não funciona em ambientes totalmente air-gapped. Para esses
casos, consulte a <xref linkend="suse-storage-install"/>.</para>
</note>
</section>
</chapter>
<chapter xml:id="components-suse-security">
<title><link xl:href="https://www.suse.com/products/rancher/security/">SUSE
Security</link></title>
<para>O SUSE Security é uma solução de segurança para Kubernetes que proporciona
segurança de rede L7, de runtime e da cadeia de suprimento, e verificações
de conformidade, em um pacote consistente.</para>
<para>O SUSE Security é um produto implantado como uma plataforma de vários
contêineres, cada um se comunicando por diversas portas e
interfaces. Internamente, ele usa o NeuVector como componente de segurança
do contêiner subjacente. Os seguintes contêineres compõem a plataforma SUSE
Security:</para>
<itemizedlist>
<listitem>
<para>Gerenciador. Um contêiner sem estado com console baseado na
web. Normalmente, apenas um é necessário e pode ser executado em qualquer
local. Uma falha no gerenciador não afeta as operações do controlador ou do
executor. No entanto, determinadas notificações (eventos) e dados de conexão
recentes são armazenados em cache na memória pelo gerenciador, portanto, a
visualização deles pode ser afetada.</para>
</listitem>
<listitem>
<para>Controlador. O "plano de controle" do SUSE Security deve ser implantado em
uma configuração de alta disponibilidade, assim a configuração não se perde
em caso de falha no nó. Ele pode ser executado em qualquer local, embora os
clientes geralmente prefiram colocá-los em nós de "gerenciamento", mestre ou
de infraestrutura devido à sua criticidade.</para>
</listitem>
<listitem>
<para>Executor. Esse contêiner é implantado como um DaemonSet para que haja um
executor em cada nó que será protegido. Em geral, a implantação é feita em
cada nó do worker, mas é possível permitir a programação para que os nós
mestre e de infraestrutura também sejam implantados lá. Nota: Se o executor
não estiver em um nó do cluster e as conexões vierem de um pod nesse nó, o
SUSE Security os identificará como cargas de trabalho "não gerenciadas".</para>
</listitem>
<listitem>
<para>Verificador. Faz a verificação de vulnerabilidades usando o banco de dados
CVE incorporado, conforme direcionado pelo controlador. É possível implantar
vários verificadores para aumentar a capacidade de verificação. Os
verificadores podem ser executados em qualquer local, mas costumam executar
em nós onde os controladores são executados. Veja a seguir as considerações
de dimensionamento dos nós do verificador. Também é possível invocar um
verificador de forma independente, quando usado para verificação de fase de
compilação, por exemplo, em um pipeline que aciona a verificação, recupera
os resultados e interrompe o verificador. O verificador inclui o banco de
dados CVE mais recente, portanto, ele deve ser atualizado diariamente.</para>
</listitem>
<listitem>
<para>Atualizador. Aciona uma atualização do verificador pelo cron job do
Kubernetes quando uma atualização do banco de dados CVE é desejada. Faça
essa configuração em seu ambiente.</para>
</listitem>
</itemizedlist>
<para>Você encontra a documentação mais detalhada sobre a integração e as melhores
práticas do SUSE Security <link
xl:href="https://open-docs.neuvector.com/">aqui</link>.</para>
<section xml:id="id-how-does-suse-edge-use-suse-security">
<title>Como o SUSE Edge usa o SUSE Security?</title>
<para>O SUSE Edge oferece uma configuração mais simples do SUSE Security como
ponto de partida para implantações de borda.</para>
</section>
<section xml:id="id-important-notes">
<title>Observações importantes</title>
<itemizedlist>
<listitem>
<para>O contêiner <literal>Scanner</literal> deve ter memória suficiente para
extrair a imagem que será verificada na memória e expandi-la. Para verificar
imagens com mais de 1 GB, aumente a memória do verificador um pouco acima do
maior tamanho esperado da imagem.</para>
</listitem>
<listitem>
<para>O modo de proteção espera grandes volumes de conexões de rede. O
<literal>Enforcer</literal> requer CPU e memória no modo de proteção
(bloqueio de firewall integrado) para armazenar e inspecionar as conexões e
a possível carga (DLP). Aumentar a memória e dedicar um núcleo de CPU ao
<literal>Enforcer</literal> garantem a capacidade de filtragem de pacotes
adequada.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Instalando com o Edge Image Builder</title>
<para>O SUSE Edge usa o <xref linkend="components-eib"/> para personalizar as
imagens base do sistema operacional SUSE Linux Micro. Siga a <xref
linkend="suse-security-install"/> para uma instalação air-gapped do SUSE
Security em clusters Kubernetes provisionados pelo EIB.</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para>Consulte a <link xl:href="https://metallb.universe.tf/">documentação oficial
do MetalLB</link>.</para>
<blockquote>
<para>O MetalLB é uma implementação de balanceador de carga para clusters
Kubernetes bare metal, usando protocolos de roteamento padrão.</para>
<para>Em ambientes bare metal, a configuração de balanceadores de carga de rede é
visivelmente mais complexa do que em ambientes de nuvem. Ao contrário das
chamadas de API simples nas configurações de nuvem, o bare metal requer
aplicações de rede dedicadas ou uma combinação de balanceadores de carga e
configurações de IP virtual (VIP, Virtual IP) para gerenciar a alta
disponibilidade (HA, High Availability) ou resolver o ponto único de falha
(SPOF, Single Point of Failure) inerentes em um balanceador de carga de nó
único. Essas configurações não são automatizadas com facilidade, criando
desafios para as implantações do Kubernetes em que a escala dos componentes
aumenta e reduz de forma dinâmica.</para>
<para>O MetalLB resolve esses desafios usando o modelo do Kubernetes para criar
serviços do tipo LoadBalancer como se operassem em um ambiente de nuvem,
mesmo nas configurações bare metal.</para>
<para>Há duas abordagens diferentes, pelo <link
xl:href="https://metallb.universe.tf/concepts/layer2/">modo L2</link>
(usando <emphasis>truques</emphasis> de ARP) ou pelo <link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>. Basicamente,
o L2 não precisa de equipamento de rede especial, mas o BGP costuma ser
melhor. Isso depende do caso de uso.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>Como o SUSE Edge usa o MetalLB?</title>
<para>O SUSE Edge usa o MetalLB de três maneiras principais:</para>
<itemizedlist>
<listitem>
<para>Como solução de balanceador de carga: o MetalLB atua como solução de
balanceador de carga para máquinas bare metal.</para>
</listitem>
<listitem>
<para>Em uma configuração do K3s/RKE2 de alta disponibilidade: o MetalLB permite o
balanceamento de carga da API do Kubernetes usando um endereço IP virtual.</para>
</listitem>
<listitem>
<para>Como uma solução de L3 do BGP, em que o MetalLB anuncia as rotas para os IPs
de serviço aos roteadores vizinhos.</para>
</listitem>
</itemizedlist>
<note>
<para>Para expor a API, o Endpoint Copier Operator (<xref
linkend="components-eco"/>) é usado para sincronização dos endpoints da API
K8s do serviço <literal>kubernetes</literal> com um serviço LoadBalancer
<literal>kubernetes-vip</literal>.</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>Melhores práticas</title>
<para>A instalação do MetalLB no modo de L2 está descrita no <xref
linkend="guides-metallb-k3s"/>, e no modo de L3 no <xref
linkend="guides-metallb-k3s-l3"/>.</para>
<para>Um guia para instalação do MetalLB na frente do
<literal>kube-api-server</literal> para obter uma topologia de alta
disponibilidade está disponível no <xref
linkend="guides-metallb-kubernetes"/>.</para>
</section>
<section xml:id="id-known-issues-6">
<title>Problemas conhecidos</title>
<itemizedlist>
<listitem>
<para>O K3s vem com a própria solução de balanceador de carga chamada
<literal>Klipper</literal>. Para usar o MetalLB, é necessário desabilitar o
<literal>Klipper</literal>. Para fazer isso, inicie o servidor K3s com a
opção <literal>--disable servicelb</literal>, conforme descrito na <link
xl:href="https://docs.k3s.io/networking">documentação do K3s</link>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eco">
<title>Endpoint Copier Operator</title>
<para><link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link> é um operador do Kubernetes que tem como finalidade
criar uma cópia de um serviço e endpoint do Kubernetes e mantê-los
sincronizados.</para>
<section xml:id="id-how-does-suse-edge-use-endpoint-copier-operator">
<title>Como o SUSE Edge usa o Endpoint Copier Operator?</title>
<para>No SUSE Edge, o Endpoint Copier Operator desempenha um papel crucial na
configuração de alta disponibilidade (HA, High Availability) para clusters
K3s/RKE2. Para que isso seja possível, crie um serviço
<literal>kubernetes-vip</literal> do tipo <literal>LoadBalancer</literal>,
garantindo que seu endpoint esteja sempre sincronizado com o endpoint do
kubernetes. O MetalLB (<xref linkend="components-metallb"/>) é usado para
gerenciar o serviço <literal>kubernetes-vip</literal>, já que o endereço IP
exposto é usado em outros nós para ingressar no cluster.</para>
</section>
<section xml:id="id-best-practices-7">
<title>Melhores práticas</title>
<para>Você encontra a documentação completa sobre como usar o Endpoint Copier
Operator <link
xl:href="https://github.com/suse-edge/endpoint-copier-operator/blob/main/README.md">aqui</link>.</para>
<para>Consulte também nosso guia (<xref linkend="guides-metallb-k3s"/>) para obter
uma configuração de alta disponibilidade do K3s/RKE2 por meio do Endpoint
Copier Operator e do MetalLB.</para>
</section>
<section xml:id="id-known-issues-7">
<title>Problemas conhecidos</title>
<para>No momento, o funcionamento do Endpoint Copier Operator está limitado apenas
a um serviço/endpoint. Há planos para estender o suporte a vários
serviços/endpoints no futuro.</para>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>Esta seção descreve como usar o Edge Virtualization para executar máquinas
virtuais em nós de borda. O Edge Virtualization foi projetado para casos de
uso de virtualização leve, em que se espera o uso de um fluxo de trabalho
comum para implantação e gerenciamento de aplicativos tanto virtualizados
quanto conteinerizados.</para>
<para>O SUSE Edge Virtualization oferece suporte a dois métodos de execução de
máquinas virtuais:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Implantação manual de máquinas virtuais pelo libvirt+qemu-kvm no nível do
host (não há envolvimento do Kubernetes)</para>
</listitem>
<listitem>
<para>Implantação do operador KubeVirt para gerenciamento de máquinas virtuais com
base no Kubernetes</para>
</listitem>
</orderedlist>
<para>As duas opções são válidas, mas apenas a segunda está descrita abaixo. Para
usar os mecanismos de virtualização padrão e prontos para uso incluídos no
SUSE Linux Micro, há um guia completo <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/chap-virtualization-introduction.html">aqui</link>
e, apesar de ter sido escrito inicialmente para o SUSE Linux Enterprise
Server, os conceitos são quase idênticos.</para>
<para>A princípio, este guia explica como implantar os componentes de
virtualização adicionais em um sistema que já foi pré-implantado, mas
acompanha uma seção que descreve como incorporar essa configuração à
implantação inicial usando o Edge Image Builder. Se você não quer ler os
conceitos básicos e definir as configurações manualmente, pule direto para
essa seção.</para>
<section xml:id="id-kubevirt-overview">
<title>Visão geral do KubeVirt</title>
<para>O KubeVirt permite o gerenciamento de máquinas virtuais com o Kubernetes
junto ao restante de suas cargas de trabalho conteinerizadas. Para fazer
isso, ele executa a parte referente ao espaço do usuário da pilha de
virtualização do Linux em um contêiner. Isso minimiza os requisitos no
sistema host, o que facilita a configuração e o gerenciamento.</para>
<informalexample>
<para>Os detalhes sobre a arquitetura do KubeVirt estão disponíveis na <link
xl:href="https://kubevirt.io/user-guide/architecture/">documentação
upstream</link>.</para>
</informalexample>
</section>
<section xml:id="id-prerequisites-5">
<title>Pré-requisitos</title>
<para>Se você está seguindo este guia, já deve ter os seguintes itens disponíveis:</para>
<itemizedlist>
<listitem>
<para>No mínimo, um host físico com o SUSE Linux Micro 6.1 instalado e com as
extensões de virtualização habilitadas no BIOS (consulte <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">aqui</link>
para obter detalhes).</para>
</listitem>
<listitem>
<para>Em todos os nós, um cluster Kubernetes do K3s/RKE2 já implantado com um
<literal>kubeconfig</literal> apropriado que permite o acesso de
superusuário ao cluster.</para>
</listitem>
<listitem>
<para>Acesso ao usuário root. Nestas instruções, consideramos você como usuário
root, <emphasis>sem</emphasis> escalar seus privilégios por meio do
<literal>sudo</literal>.</para>
</listitem>
<listitem>
<para>Você tem o <link xl:href="https://helm.sh/docs/intro/install/">Helm</link>
disponível localmente com uma conexão de rede adequada para poder enviar as
configurações ao cluster Kubernetes e fazer download das imagens
necessárias.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Instalação manual do Edge Virtualization</title>
<para>Este guia não orientará na implantação do Kubernetes, mas ele considera que
você já instalou a versão apropriada ao SUSE Edge do <link
xl:href="https://k3s.io/">K3s</link> ou do <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> e configurou o
kubeconfig de acordo para que os comandos <literal>kubectl</literal> padrão
sejam executados como superusuário. Pressupomos que seu nó constitua um
cluster de nó único, apesar de não haver diferenças significativas esperadas
nas implantações de vários nós.</para>
<para>O SUSE Edge Virtualization é implantado por meio de três gráficos Helm
separados, especificamente:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>: os componentes de virtualização
principais, ou seja, as CRDs do Kubernetes, os operadores e outros
componentes necessários para que o Kubernetes possa implantar e gerenciar
máquinas virtuais.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Extensão de dashboard KubeVirt</emphasis>: uma
extensão de IU opcional do Rancher que permite o gerenciamento básico de
máquinas virtuais, por exemplo, iniciar/parar máquinas virtuais e acessar o
console.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>: um
componente adicional que permite a integração de armazenamento persistente
para KubeVirt, oferecendo recursos para as máquinas virtuais usarem os back
ends de armazenamento do Kubernetes existentes para dados, mas também
permitindo que os usuários importem ou clonem volumes de dados em máquinas
virtuais.</para>
</listitem>
</itemizedlist>
<para>Cada um desses gráficos Helm é versionado de acordo com a versão do SUSE
Edge que você usa. Para uso em produção/com suporte, aplique os artefatos
disponíveis no SUSE Registry.</para>
<para>Primeiro, garanta que o acesso ao <literal>kubectl</literal> esteja
funcionando:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>Esse comando deve retornar algo semelhante ao seguinte:</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.30.5+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1</screen>
<para>Agora você pode instalar os gráficos Helm <emphasis
role="strong">KubeVirt</emphasis> e <emphasis role="strong">Containerized
Data Importer (CDI)</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/charts/kubevirt --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/charts/cdi --namespace cdi-system --create-namespace</screen>
<para>Em alguns minutos, os componentes KubeVirt e CDI estarão implantados. Para
validar isso, verifique todos os recursos implantados no namespace
<literal>kubevirt-system</literal> e <literal>cdi-system</literal>.</para>
<para>Verifique os recursos do KubeVirt:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>Esse comando deve retornar algo semelhante ao seguinte:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>Verifique os recursos do CDI:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>Esse comando deve retornar algo semelhante ao seguinte:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para>Para verificar se as definições de recursos personalizados (CRDs, Custom
Resource Definitions) <literal>VirtualMachine</literal> foram implantadas,
faça a validação com:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para>Esse comando deve retornar a definição do objeto
<literal>VirtualMachine</literal>, com o seguinte conteúdo:</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>Implantando máquinas virtuais</title>
<para>Agora que KubeVirt e CDI estão implantados, vamos definir uma máquina
virtual simples com base no <link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE
Tumbleweed</link>. Essa máquina virtual tem as configurações mais simples,
usando a configuração de "rede de pod" padrão idêntica a qualquer outro
pod. Ela também utiliza um armazenamento não persistente, o que garante que
seja efêmero como em qualquer contêiner que não tenha um <link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>A saída deve mostrar que uma <literal>VirtualMachine</literal> foi criada:</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>Essa definição da <literal>VirtualMachine</literal> é mínima e especifica
pouco da configuração. Ela simplesmente descreve que se trata de uma máquina
do tipo "<link xl:href="https://wiki.qemu.org/Features/Q35">q35</link>" com
2 GB de memória, que usa uma imagem de disco baseada em um <literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>
efêmero (ou seja, uma imagem de disco armazenada na imagem de contêiner de
um repositório de imagens remoto), e especifica um disco cloudInit
codificado com base64, que usamos apenas para criação de usuários e
imposição de senha no momento da inicialização (usamos <literal>base64
-d</literal> para decodificá-lo).</para>
<blockquote>
<note>
<para>Essa imagem da máquina virtual é somente para teste. Ela não tem suporte
oficial e serve apenas como exemplo para documentação.</para>
</note>
</blockquote>
<para>A inicialização da máquina leva alguns minutos porque ela precisa fazer
download da imagem de disco do openSUSE Tumbleweed. Depois disso, você
poderá consultar as informações para ver mais detalhes sobre a máquina
virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>Esse comando deve mostrar o nó em que a máquina virtual foi iniciada e o
endereço IP dela. Como ela usa a rede de pod, lembre-se de que o endereço IP
relatado será exatamente igual ao de qualquer outro pod e, dessa forma,
roteável:</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>Ao executar esses comandos nos próprios nós do cluster Kubernetes, com uma
CNI para rotear o tráfego diretamente aos pods (por exemplo, Cilium), você
pode executar <literal>ssh</literal> diretamente na máquina. Substitua o
seguinte endereço IP pelo que foi atribuído à sua máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>Depois que você estiver nessa máquina virtual, poderá explorá-la, mas
lembre-se de que ela tem recursos limitados e apenas 1 GB de espaço em
disco. Quando concluir, use <literal>Ctrl-D</literal> ou
<literal>exit</literal> para se desconectar da sessão SSH.</para>
<para>O processo da máquina virtual ainda é agrupado em um pod padrão do
Kubernetes. A CRD <literal>VirtualMachine</literal> é uma representação da
máquina virtual desejada, mas o processo em que a máquina virtual foi
realmente iniciada é realizado pelo pod <literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>,
um pod padrão do Kubernetes, assim como com qualquer outro aplicativo. Para
cada máquina virtual iniciada, você vê que existe um pod
<literal>virt-launcher</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>Em seguida, o pod <literal>virt-launcher</literal> deve aparecer para a
máquina Tumbleweed que definimos:</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>Se você observar o pod <literal>virt-launcher</literal>, verá que ele está
executando os processos <literal>libvirt</literal> e
<literal>qemu-kvm</literal>. É possível entrar e analisar o conteúdo do pod,
levando em conta que você precisa adaptar o seguinte comando com o nome do
seu pod:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Depois que você estiver no pod, execute os comandos <literal>virsh</literal>
enquanto observa os processos. Você verá o binário
<literal>qemu-system-x86_64</literal> em execução, junto com alguns
processos para monitorar a máquina virtual, e também o local da imagem de
disco e como a rede está conectada (como um dispositivo TAP):</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>Por fim, vamos excluir a máquina virtual para limpeza:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>Usando o virtctl</title>
<para>Junto com a ferramenta CLI padrão do Kubernetes, ou seja,
<literal>kubectl</literal>, o KubeVirt integra um utilitário CLI
complementar que permite estabelecer interface com seu cluster para suprir
algumas lacunas entre o cenário da virtualização e o cenário para o qual o
Kubernetes foi projetado. Por exemplo, a ferramenta
<literal>virtctl</literal> permite gerenciar o ciclo de vida das máquinas
virtuais (início, parada, reinicialização etc.), concedendo acesso aos
consoles virtuais, fazendo upload das imagens de máquina virtual e
estabelecendo interface com as estruturas do Kubernetes, como serviços, sem
usar diretamente a API ou as CRDs.</para>
<para>Vamos fazer download da versão estável mais recente da ferramenta
<literal>virtctl</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.5.2
$ wget https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64</screen>
<para>Se você usa uma arquitetura diferente ou uma máquina não Linux, pode
encontrar outras versões <link
xl:href="https://github.com/kubevirt/kubevirt/releases">aqui</link>. Antes
de continuar, você precisa torná-la executável, o que pode ser útil para
movê-la até um local em seu <literal>$PATH</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-$VERSION-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>Na sequência, use a ferramenta de linha de comando
<literal>virtctl</literal> para criar máquinas virtuais. Vamos replicar
nossa máquina virtual anterior, observando que estamos fazendo pipe da saída
diretamente em <literal>kubectl apply</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>Isso deve mostrar a máquina virtual em execução (desta vez, ela deve ser
iniciada de forma bem mais rápida, já que a imagem de contêiner estará
armazenada em cache):</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>Vamos usar <literal>virtctl</literal> para conexão direta com a máquina
virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para>Há vários outros comandos que o <literal>virtctl</literal> pode usar. Por
exemplo, o <literal>virtctl console</literal> concede a você acesso ao
console serial quando a rede não está funcionando, e você pode usar o
<literal>virtctl guestosinfo</literal> para obter as informações completas
do sistema operacional, desde que o convidado tenha o
<literal>qemu-guest-agent</literal> instalado e em execução.</para>
<para>Por fim, vamos pausar e retomar a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para>Observe que o objeto <literal>VirtualMachine</literal> aparece como
<emphasis role="strong">Paused</emphasis> (Pausado), e o objeto
<literal>VirtualMachineInstance</literal> como <emphasis
role="strong">Running</emphasis> (Em execução), mas <emphasis
role="strong">READY=False</emphasis> (PRONTO=Falso):</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>Veja também que não é mais possível se conectar à máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>Vamos retomar a máquina virtual e tentar novamente:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>Agora devemos conseguir reestabelecer a conexão:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>Por fim, vamos remover a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>Rede de entrada simples</title>
<para>Nesta seção, mostramos como é possível expor máquinas virtuais como serviços
padrão do Kubernetes e disponibilizá-los pelo serviço de entrada do
Kubernetes, por exemplo, <link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINX
com RKE2</link> ou <link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">Traefik
com K3s</link>. Este documento pressupõe que esses componentes já foram
devidamente configurados e que você tem o ponteiro de DNS, por exemplo,
usando um curinga, para apontar para os nós do servidor Kubernetes ou IP
virtual de entrada para uma resolução de entrada correta.</para>
<blockquote>
<note>
<para>A partir do SUSE Edge 3.1, se você usa o K3s em uma configuração de nó de
vários servidores, talvez tenha que configurar um VIP com base no MetalLB
para entrada. Isso não é necessário no RKE2.</para>
</note>
</blockquote>
<para>No ambiente de exemplo, outra máquina virtual openSUSE Tumbleweed foi
implantada, o cloud-init foi usado para instalar o NGINX como servidor web
simples no momento da inicialização e uma mensagem simples foi configurada
para ser exibida para verificar se ela funciona conforme o esperado quando
uma chamada é feita. Para ver como isso é feito, basta executar
<literal>base64 -d</literal> na seção cloud-init na saída a seguir.</para>
<para>Agora vamos criar a máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>Quando a máquina virtual é iniciada com sucesso, podemos usar o comando
<literal>virtctl</literal> para expor
<literal>VirtualMachineInstance</literal> com a porta externa
<literal>8080</literal> e a porta de destino <literal>80</literal> (na qual
o NGINX escuta por padrão). Usamos o comando <literal>virtctl</literal> aqui
porque ele reconhece o mapeamento entre o objeto e o pod da máquina
virtual. Isso cria um novo serviço:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>Desse modo, o serviço apropriado será criado automaticamente:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>Se depois você usar <literal>kubectl create ingress</literal>, poderemos
criar a seguir um objeto de entrada que aponte para esse serviço. Adapte o
URL aqui (conhecido como "host" no objeto <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link>)
de acordo com sua configuração do DNS e garanta que ele aponte para a porta
<literal>8080</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>Com a configuração correta do DNS, você pode executar curl no URL
imediatamente:</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>Vamos fazer a limpeza removendo a máquina virtual e os respectivos recursos
de entrada e serviço:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Usando a extensão de IU do Rancher</title>
<para>O SUSE Edge Virtualization oferece uma extensão de IU para o Rancher
Manager, que permite o gerenciamento básico de máquinas virtuais usando a
interface de usuário do Rancher Dashboard.</para>
<section xml:id="id-installation-4">
<title>Instalação</title>
<para>Consulte as extensões do Rancher Dashboard (<xref
linkend="components-rancher-dashboard-extensions"/>) para obter orientação
de instalação.</para>
</section>
<section xml:id="kubevirt-dashboard-extension-usage">
<title>Usando a extensão KubeVirt do Rancher Dashboard</title>
<para>A extensão introduz uma nova seção <emphasis
role="strong">KubeVirt</emphasis> no explorador de clusters, que é
adicionada a qualquer cluster gerenciado que tenha o KubeVirt instalado.</para>
<para>A extensão permite interagir diretamente com os recursos de máquina virtual
do KubeVirt para gerenciar o ciclo de vida das máquinas virtuais.</para>
<section xml:id="id-creating-a-virtual-machine">
<title>Criando uma máquina virtual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Navegue até o <emphasis role="strong">explorador de clustersr</emphasis>
clicando no cluster gerenciado habilitado para KubeVirt na navegação
esquerda.</para>
</listitem>
<listitem>
<para>Navegue até a página <emphasis role="strong">KubeVirt &gt; Virtual
Machines</emphasis> (Máquinas virtuais) e clique em <literal>Create from
YAML</literal> (Criar do YAML) na parte superior direita da tela.</para>
</listitem>
<listitem>
<para>Preencha ou cole uma definição da máquina virtual e clique em
<literal>Create</literal> (Criar). Use a definição da máquina virtual da
seção Implantando máquinas virtuais como modelo.</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width="100%"/>
</imageobject>
<textobject><phrase>página máquinas virtuais</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-virtual-machine-actions">
<title>Ações da máquina virtual</title>
<para>É possível usar o menu de ações acessado pela lista suspensa <emphasis
role="strong">⋮</emphasis> à direita de cada máquina virtual para executar
as ações iniciar, parar, pausar ou fazer reboot suave. Se preferir, use as
ações em grupo na parte superior da lista selecionando as máquinas virtuais
nas quais executar a ação.</para>
<para>A execução das ações pode afetar a estratégia de execução da máquina
virtual. <link
xl:href="https://kubevirt.io/user-guide/compute/run_strategies/#virtctl">Consulte
a tabela na documentação do KubeVirt</link> para obter mais detalhes.</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>Acessando o console da máquina virtual</title>
<para>A lista de "máquinas virtuais" oferece a lista suspensa
<literal>Console</literal>, que permite conectar-se à máquina usando
<emphasis role="strong">VNC ou console serial</emphasis>. Essa ação está
disponível apenas em máquinas que estão em execução.</para>
<para>Em alguns casos, leva um curto tempo para que o console fique disponível em
uma máquina virtual que acabou de ser iniciada.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width="100%"/>
</imageobject>
<textobject><phrase>iu console vnc</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Instalando com o Edge Image Builder</title>
<para>O SUSE Edge usa o <xref linkend="components-eib"/> para personalizar as
imagens base do sistema operacional SUSE Linux Micro. Siga a <xref
linkend="kubevirt-install"/> para saber sobre a instalação air-gapped tanto
do KubeVirt quanto da CDI em clusters Kubernetes provisionados pelo EIB.</para>
</section>
</chapter>
<chapter xml:id="components-system-upgrade-controller">
<title>System Upgrade Controller</title>
<para>Consulte a <link
xl:href="https://github.com/rancher/system-upgrade-controller">documentação
do System Upgrade Controller</link>.</para>
<blockquote>
<para>O System Upgrade Controller (SUC) tem como objetivo oferecer um controlador
de upgrade nativo do Kubernetes genérico (para nós). Ele introduz uma nova
CRD, o plano, para definir todas as suas políticas/requisitos de upgrade. O
plano é uma intenção evidente de mudar nós no cluster.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-system-upgrade-controller">
<title>Como o SUSE Edge usa o System Upgrade Controller?</title>
<para>O SUSE Edge usa o <literal>SUC</literal> para facilitar diversas operações
de "dia 2" relacionadas a upgrades de versão de sistema operacional e
Kubernetes em clusters de gerenciamento e downstream.</para>
<para>As operações de "dia 2" são definidas em <literal>planos do
SUC</literal>. Com base nesses planos, o <literal>SUC</literal> implanta as
cargas de trabalho em cada nó para executar a respectiva operação de "dia
2".</para>
<para>O <literal>SUC</literal> também é usado no <xref
linkend="components-upgrade-controller"/>. Para saber mais sobre as
principais diferenças entre o SUC e o Controller de upgrade, consulte a
<xref linkend="components-upgrade-controller-uc-vs-suc"/>.</para>
</section>
<section xml:id="components-system-upgrade-controller-install">
<title>Instalando o System Upgrade Controller</title>
<important>
<para>A partir do Rancher <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.10.0">v2.10.0</link>,
o <literal>System Upgrade Controller</literal> é instalado automaticamente.</para>
<para>Siga as etapas abaixo <emphasis role="strong">apenas</emphasis> se o seu
ambiente <emphasis role="strong">não</emphasis> é gerenciado pelo Rancher,
ou se a versão do Rancher é inferior a <literal>v2.10.0</literal>.</para>
</important>
<para>Recomendamos a instalação do SUC pelo Fleet (<xref
linkend="components-fleet"/>), localizado no repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>.</para>
<note>
<para>Os recursos disponíveis no repositório
<literal>suse-edge/fleet-examples</literal> sempre <emphasis
role="strong">devem</emphasis> ser usados de uma versão válida de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">fleet-examples</link>.
Para determinar a versão que você precisa usar, consulte as Notas de
lançamento (<xref linkend="release-notes"/>).</para>
</note>
<para>Se não for possível usar o Fleet para instalação do SUC, instale-o por meio
do repositório de gráficos Helm do Rancher ou incorpore o gráfico Helm do
Rancher ao seu próprio fluxo de trabalho do GitOps de terceiros.</para>
<para>Esta seção aborda o seguinte:</para>
<itemizedlist>
<listitem>
<para>Instalação do Fleet (<xref
linkend="components-system-upgrade-controller-fleet"/>)</para>
</listitem>
<listitem>
<para>Instalação do Helm (<xref
linkend="components-system-upgrade-controller-helm"/>)</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet">
<title>Instalação do System Upgrade Controller pelo Fleet</title>
<para>Com o Fleet, há dois recursos possíveis para serem usados na implantação do
SUC:</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>: para
casos de uso em que um servidor Git externo/local está disponível. Para
obter instruções de instalação, consulte Instalação do System Upgrade
Controller – GitRepo (<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>).</para>
</listitem>
<listitem>
<para>Recurso <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>:
para casos de uso air-gapped que não oferecem suporte à opção de servidor
Git local. Para obter instruções de instalação, consulte Instalação do
System Upgrade Controller – Bundle (<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet-gitrepo">
<title>Instalação do System Upgrade Controller – GitRepo</title>
<note>
<para>É possível também realizar esse processo pela IU do Rancher, se
disponível. Para obter mais informações, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Acessando
o Fleet na IU do Rancher</link> (em inglês).</para>
</note>
<para>Em seu cluster de <emphasis role="strong">gerenciamento</emphasis>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Determine os clusters em que deseja implantar o SUC. Para fazer isso,
implante um recurso <literal>GitRepo</literal> do SUC no espaço de trabalho
correto do Fleet em seu cluster de <emphasis
role="strong">gerenciamento</emphasis>. Por padrão, o Fleet tem dois espaços
de trabalho:</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal>: para recursos que precisam ser implantados
no cluster de <emphasis role="strong">gerenciamento</emphasis>.</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal>: para recursos que precisam ser implantados
em clusters <emphasis role="strong">downstream</emphasis>.</para>
<para>Para obter mais informações sobre espaços de trabalho do Fleet, consulte a
documentação <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">upstream</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Implante o recurso <literal>GitRepo</literal> :</para>
<itemizedlist>
<listitem>
<para>Para implantar o SUC no cluster de gerenciamento:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-local -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF</screen>
</listitem>
<listitem>
<para>Para implantar o SUC em clusters downstream:</para>
<note>
<para>Antes de implantar o recurso a seguir, você <emphasis
role="strong">deve</emphasis> especificar uma configuração válida de
<literal>targets</literal>, para que o Fleet saiba em quais clusters
downstream implantar seu recurso. Para obter informações de como mapear para
clusters downstream, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Valide que o recurso <literal>GitRepo</literal> foi implantado:</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get gitrepo system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.4.0   1/1</screen>
</listitem>
<listitem>
<para>Valide a implantação do System Upgrade Controller:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-fleet-bundle">
<title>Instalação do System Upgrade Controller – Bundle</title>
<para>Esta seção ilustra como criar e implantar um recurso
<literal>Bundle</literal> de uma configuração padrão do Fleet usando <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Em uma máquina com acesso à rede, faça download de
<literal>fleet-cli</literal>:</para>
<note>
<para>Garanta que a versão do fleet-cli do download seja a mesma do Fleet que foi
implantado em seu cluster.</para>
</note>
<itemizedlist>
<listitem>
<para>Para usuários do Mac, existe um Homebrew Formulae <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Para usuários do Linux e Windows, os binários estão presentes como <emphasis
role="strong">ativos</emphasis> para cada <link
xl:href="https://github.com/rancher/fleet/releases">versão</link> do Fleet.</para>
<itemizedlist>
<listitem>
<para>Linux AMD:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Torne o <literal>fleet-cli</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x fleet-cli</screen>
</listitem>
<listitem>
<para>Clone a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do <literal>suse-edge/fleet-examples</literal> que deseja usar:</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Navegue até a instância do Fleet do SUC, localizada no repositório
<literal>fleet-examples</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller</screen>
</listitem>
<listitem>
<para>Determine os clusters em que deseja implantar o SUC. Para fazer isso,
implante o bundle do SUC no espaço de trabalho correto do Fleet em seu
cluster de gerenciamento. Por padrão, o Fleet tem dois espaços de trabalho:</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal>: para recursos que precisam ser implantados
no cluster de <emphasis role="strong">gerenciamento</emphasis>.</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal>: para recursos que precisam ser implantados
em clusters <emphasis role="strong">downstream</emphasis>.</para>
<para>Para obter mais informações sobre espaços de trabalho do Fleet, consulte a
documentação <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">upstream</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Se você pretende implantar o SUC apenas em clusters downstream, crie um
arquivo <literal>targets.yaml</literal> correspondente aos clusters
específicos:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>Para obter informações sobre como mapear para clusters downstream, consulte
<link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
<listitem>
<para>Avance para a criação do bundle :</para>
<note>
<para>Certifique-se de que você <emphasis role="strong">não</emphasis> fez
download de fleet-cli no diretório
<literal>fleet-examples/fleets/day2/system-upgrade-controller</literal>; do
contrário, ele será empacotado com o bundle, o que não é aconselhável.</para>
</note>
<itemizedlist>
<listitem>
<para>Para implantar o SUC no cluster de gerenciamento, execute:</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>Para implantar o SUC em clusters downstream, execute:</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
<para>Para obter mais informações sobre esse processo, consulte <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Converter um gráfico Helm em bundle).</para>
<para>Para obter mais informações sobre o comando <literal>fleet-cli
apply</literal>, consulte <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Transfira o bundle <literal>system-upgrade-controller-bundle.yaml</literal>
para sua máquina do cluster de gerenciamento:</para>
<screen language="bash" linenumbering="unnumbered">scp system-upgrade-controller-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para>No cluster de gerenciamento, implante o bundle
<literal>system-upgrade-controller-bundle.yaml</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>No cluster de gerenciamento, valide se o bundle foi implantado:</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get bundle system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1</screen>
</listitem>
<listitem>
<para>De acordo com o espaço de trabalho do Fleet em que você implantou o bundle,
navegue até o cluster e valide a implantação do SUC:</para>
<note>
<para>O SUC é sempre implantado no namespace <emphasis
role="strong">cattle-system</emphasis>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-helm">
<title>Instalação do System Upgrade Controller com o Helm</title>
<orderedlist numeration="arabic">
<listitem>
<para>Adicione o repositório de gráficos do Rancher:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>Implante o gráfico do SUC:</para>
<screen language="bash" linenumbering="unnumbered">helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 107.0.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace</screen>
<para>Esse procedimento instala o SUC versão 0.16.0, que é exigido pela plataforma
Edge 3.4.</para>
</listitem>
<listitem>
<para>Valide a implantação do SUC:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans">
<title>Monitorando os planos do System Upgrade Controller</title>
<para>É possível ver os planos do SUC destas maneiras:</para>
<itemizedlist>
<listitem>
<para>Pela IU do Rancher (<xref
linkend="components-system-upgrade-controller-monitor-plans-rancher"/>).</para>
</listitem>
<listitem>
<para>Por monitoramento manual (<xref
linkend="components-system-upgrade-controller-monitor-plans-manual"/>)
dentro do cluster.</para>
</listitem>
</itemizedlist>
<important>
<para>Os pods implantados para planos do SUC são mantidos ativos por <emphasis
role="strong">15</emphasis> minutos após a execução bem-sucedida. Depois
disso, eles serão removidos pelo job correspondente que os criou. Para ter
acesso aos registros do pod após esse período de tempo, você deve habilitar
o registro no cluster. Para obter informações de como fazer isso no Rancher,
consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/logging">Integração
do Rancher com serviços de registro</link> (em inglês).</para>
</important>
<section xml:id="components-system-upgrade-controller-monitor-plans-rancher">
<title>Monitorando os planos do System Upgrade Controller – IU do Rancher</title>
<para>Para consultar os registros do pod referentes ao plano do SUC específico:</para>
<orderedlist numeration="arabic">
<listitem>
<para>No canto superior esquerdo, <emphasis role="strong">☰ →
&lt;nome-do-seu-cluster&gt;</emphasis></para>
</listitem>
<listitem>
<para>Selecione Workloads → Pods (Cargas de trabalho → Pods).</para>
</listitem>
<listitem>
<para>Selecione o menu suspenso <literal>Only User Namespaces</literal> (Somente
namespaces de usuário) e adicione o namespace
<literal>cattle-system</literal>.</para>
</listitem>
<listitem>
<para>Na barra de filtro Pod, escreva o nome do pod do plano do SUC, que terá este
formato de gabarito:
<literal>apply-&lt;nome_do_plano&gt;-on-&lt;nome_do_nó&gt;</literal></para>
<note>
<para>Pode haver pods tanto <literal>Completed</literal> (Concluídos) quanto
<literal>Unknown</literal> (Desconhecidos) para um plano do SUC
específico. Isso é esperado e acontece por causa da natureza de alguns
upgrades.</para>
</note>
</listitem>
<listitem>
<para>Selecione o pod dos quais deseja revisar os registros e navegue até
<emphasis role="strong">⋮ → View Logs</emphasis> (Ver registros).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans-manual">
<title>Monitorando os planos do System Upgrade Controller – Manual</title>
<note>
<para>As etapas abaixo pressupõem que o <literal>kubectl</literal> foi configurado
para conectar-se ao cluster em que os <emphasis role="strong">planos do
SUC</emphasis> foram implantados.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Liste os planos do <emphasis role="strong">SUC</emphasis> implantados:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get plans -n cattle-system</screen>
</listitem>
<listitem>
<para>Obtenha o pod para o plano do <emphasis role="strong">SUC</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l upgrade.cattle.io/plan=&lt;plan_name&gt; -n cattle-system</screen>
<note>
<para>Pode haver pods tanto <literal>Completed</literal> (Concluídos) quanto
<literal>Unknown</literal> (Desconhecidos) para um plano do SUC
específico. Isso é esperado e acontece por causa da natureza de alguns
upgrades.</para>
</note>
</listitem>
<listitem>
<para>Obtenha os registros do pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n cattle-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="components-upgrade-controller">
<title>Controller de upgrade</title>
<para>Um controlador do Kubernetes que faz upgrades dos seguintes componentes da
plataforma SUSE Edge:</para>
<itemizedlist>
<listitem>
<para>Sistema operacional (SUSE Linux Micro)</para>
</listitem>
<listitem>
<para>Kubernetes (K3s e RKE2)</para>
</listitem>
<listitem>
<para>Componentes adicionais (Rancher, Elemental, SUSE Security etc.)</para>
</listitem>
</itemizedlist>
<para>O <link xl:href="https://github.com/suse-edge/upgrade-controller">Controller
de upgrade</link> otimiza o processo de upgrade dos componentes mencionados
acima, pois encapsula suas complexidades em um único recurso
<literal>voltado para o usuário</literal> que funciona como um <emphasis
role="strong">acionador</emphasis> de upgrade. Os usuários precisam apenas
configurar esse recurso, e o <literal>Controller de upgrade</literal> cuida
do restante.</para>
<note>
<para>Atualmente, o <literal>Controller de upgrade</literal> oferece suporte a
upgrades da plataforma SUSE Edge apenas para clusters de <emphasis
role="strong">gerenciamento não air-gapped</emphasis>. Consulte a <xref
linkend="components-upgrade-controller-known-issues"/> para obter mais
informações.</para>
</note>
<section xml:id="id-how-does-suse-edge-use-upgrade-controller">
<title>Como o SUSE Edge usa o Controller de upgrade?</title>
<para>O <emphasis role="strong">Controller de upgrade</emphasis> é essencial para
automação das operações de "dia 2" (que antes eram manuais) necessárias para
fazer upgrade dos clusters de gerenciamento de uma versão de lançamento do
SUSE Edge para a seguinte.</para>
<para>Para atingir essa automação, o Controller de upgrade usa ferramentas como o
System Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>) e o <link
xl:href="https://github.com/k3s-io/helm-controller/">Helm Controller</link>.</para>
<para>Para obter mais detalhes de como o Controller de upgrade funciona, consulte
a <xref linkend="components-upgrade-controller-how"/>.</para>
<para>Para saber as limitações conhecidas do Controller de upgrade, consulte a
<xref linkend="components-upgrade-controller-known-issues"/>.</para>
<para>Para obter informações sobre a diferença entre o Controller de upgrade e o
System Upgrade Controller, consulte a <xref
linkend="components-upgrade-controller-uc-vs-suc"/>.</para>
</section>
<section xml:id="components-upgrade-controller-uc-vs-suc">
<title>Comparação entre Controller de upgrade e System Upgrade Controller</title>
<para>O System Upgrade Controller (SUC) (<xref
linkend="components-system-upgrade-controller"/>) é uma ferramenta de uso
geral que propaga as instruções de upgrade para os nós específicos do
Kubernetes.</para>
<para>Apesar de oferecer suporte a algumas operações de "dia 2" para a plataforma
SUSE Edge, ele <emphasis role="strong">não</emphasis> abrange todas
elas. Mesmo para as operações com suporte, os usuários ainda precisam
configurar, manter e implantar manualmente vários <literal>planos do
SUC</literal>, um processo propenso a erros que pode provocar problemas
inesperados.</para>
<para>Isso levou à necessidade de uma ferramenta para <emphasis
role="strong">automatizar</emphasis> e <emphasis
role="strong">abstrair</emphasis> a complexidade de gerenciar várias
operações de "dia 2" na plataforma SUSE Edge. E, assim, o
<literal>Controller de upgrade</literal> foi desenvolvido. Ele simplifica o
processo de upgrade com a introdução de um único <literal>recurso voltado
para o usuário</literal> que conduz o upgrade. Os usuários precisam
gerenciar apenas esse recurso, e o <literal>Controller de upgrade</literal>
cuida do restante.</para>
</section>
<section xml:id="components-upgrade-controller-installation">
<title>Instalando o Controller de upgrade</title>
<section xml:id="id-prerequisites-6">
<title>Pré-requisitos</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://helm.sh/docs/intro/install/">Helm</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://cert-manager.io/v1.15-docs/installation/helm/#installing-with-helm">cert-manager</link></para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller-install"/>)</para>
</listitem>
<listitem>
<para>Um cluster Kubernetes; seja K3s ou RKE2</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-steps">
<title>Etapas</title>
<orderedlist numeration="arabic">
<listitem>
<para>Instale o gráfico Helm do Controller de upgrade no cluster de gerenciamento:</para>
<screen language="bash" linenumbering="unnumbered">helm install upgrade-controller oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1 --create-namespace --namespace upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Valide a implantação do Controller de upgrade:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Valide o pod do Controller de upgrade:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Valide os registros do pod do Controller de upgrade:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n upgrade-controller-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-how">
<title>Como funciona o Controller de upgrade?</title>
<para>Para fazer upgrade de uma versão do Edge, o Controller de upgrade lança dois
novos <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> do Kubernetes:</para>
<itemizedlist>
<listitem>
<para>UpgradePlan (<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>): criado
pelo usuário; armazena as configurações referentes ao upgrade de uma versão
do Edge.</para>
</listitem>
<listitem>
<para>ReleaseManifest (<xref
linkend="components-upgrade-controller-extensions-release-manifest"/>):
criado pelo Controller de upgrade; armazena as versões de componente
específicas a uma determinada versão de lançamento do Edge. <emphasis
role="strong">Os usuários não devem editar esse arquivo.</emphasis></para>
</listitem>
</itemizedlist>
<para>O Controller de upgrade cria um recurso <literal>ReleaseManifest</literal>
que armazena os dados do componente referentes à versão de lançamento do
Edge especificada pelo usuário na propriedade
<literal>releaseVersion</literal> do recurso <literal>UpgradePlan</literal>.</para>
<para>Usando os dados do componente do <literal>ReleaseManifest</literal>, o
Controller de upgrade faz upgrade do componente da versão do Edge na
seguinte ordem:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Sistema operacional (SO) (<xref
linkend="components-upgrade-controller-how-os"/>).</para>
</listitem>
<listitem>
<para>Kubernetes (<xref linkend="components-upgrade-controller-how-k8s"/>).</para>
</listitem>
<listitem>
<para>Componentes adicionais (<xref
linkend="components-upgrade-controller-how-additional"/>).</para>
</listitem>
</orderedlist>
<note>
<para>Durante o processo de upgrade, o Controller de upgrade envia continuamente
as informações de upgrade para o <literal>UpgradePlan</literal> criado. Para
obter mais informações sobre como acompanhar o processo de upgrade, consulte
Acompanhando o processo de upgrade (<xref
linkend="components-upgrade-controller-how-track"/>).</para>
</note>
<section xml:id="components-upgrade-controller-how-os">
<title>Upgrade do sistema operacional</title>
<para>Para fazer upgrade do sistema operacional, o Controller de upgrade cria
planos do SUC (<xref linkend="components-system-upgrade-controller"/>) com o
seguinte gabarito de nomenclatura:</para>
<itemizedlist>
<listitem>
<para>Para planos do SUC relacionados a upgrades de sistema operacional de nó do
plano de controle:
<literal>control-plane-&lt;nome-do-so&gt;-&lt;versão-do-so&gt;-&lt;sufixo&gt;</literal>.</para>
</listitem>
<listitem>
<para>Para planos do SUC relacionados a upgrades de sistema operacional de nó do
worker:
<literal>workers-&lt;nome-do-so&gt;-&lt;versão-do-so&gt;-&lt;sufixo&gt;</literal>.</para>
</listitem>
</itemizedlist>
<para>Com base nesses planos, o SUC cria as cargas de trabalho em cada nó do
cluster que faz o upgrade real do sistema operacional.</para>
<para>Dependendo do <literal>ReleaseManifest</literal>, o upgrade de sistema
operacional pode incluir:</para>
<itemizedlist>
<listitem>
<para>Atualizações somente de pacotes: para casos de uso em que a versão do
sistema operacional não é alterada entre os lançamentos do Edge.</para>
</listitem>
<listitem>
<para>Migração completa do sistema operacional: para casos de uso em que a versão
do sistema operacional é alterada entre os lançamentos do Edge.</para>
</listitem>
</itemizedlist>
<para>O upgrade é feito <emphasis role="strong">um</emphasis> nó de cada vez,
começando primeiro pelos nós do plano de controle. Apenas quando o upgrade
do nó do plano de controle é concluído que o upgrade dos nós do worker é
iniciado.</para>
<note>
<para>O Controller de upgrade configura os planos do SUC de sistema operacional
para fazer uma <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drenagem</link>
dos nós do cluster, se o cluster tiver mais de um <emphasis
role="strong">um</emphasis> nó do tipo especificado.</para>
<para>Para os clusters em que os nós do plano de controle são <emphasis
role="strong">mais de</emphasis> um, e existe <emphasis role="strong">apenas
um</emphasis> nó do worker, a drenagem será realizada somente para os nós do
plano de controle e vice-versa.</para>
<para>Para obter informações sobre como desabilitar completamente as drenagens de
nós, consulte a seção UpgradePlan (<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>).</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-k8s">
<title>Upgrade do Kubernetes</title>
<para>Para fazer upgrade da distribuição Kubernetes de um cluster, o Controller de
upgrade cria planos do SUC (<xref
linkend="components-system-upgrade-controller"/>) com o seguinte gabarito de
nomenclatura:</para>
<itemizedlist>
<listitem>
<para>Para os planos do SUC relacionados a ugrades do Kubernetes em nós do plano
de controle:
<literal>control-plane-&lt;versão-do-k8s&gt;-&lt;sufixo&gt;</literal>.</para>
</listitem>
<listitem>
<para>Para os planos do SUC relacionados a upgrades do Kubernetes em nós do
worker: <literal>workers-&lt;versão-do-k8s&gt;-&lt;sufixo&gt;</literal>.</para>
</listitem>
</itemizedlist>
<para>Com base nesses planos, o SUC cria as cargas de trabalho em cada nó do
cluster que faz o upgrade real do Kubernetes.</para>
<para>O upgrade do Kubernetes é feito <emphasis role="strong">um</emphasis> nó de
cada vez, começando primeiro pelos nós do plano de controle. Apenas quando o
upgrade do nó do plano de controle é concluído que o upgrade dos nós do
worker é iniciado.</para>
<note>
<para>O Controller de upgrade configura os planos do SUC do Kubernetes para fazer
uma <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drenagem</link>
dos nós do cluster, se o cluster tiver mais de um <emphasis
role="strong">um</emphasis> nó do tipo especificado.</para>
<para>Para os clusters em que os nós do plano de controle são <emphasis
role="strong">mais de</emphasis> um, e existe <emphasis role="strong">apenas
um</emphasis> nó do worker, a drenagem será realizada somente para os nós do
plano de controle e vice-versa.</para>
<para>Para obter informações sobre como desabilitar completamente as drenagens de
nós, consulte a <xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-additional">
<title>Upgrades de componentes adicionais</title>
<para>Atualmente, todos os componentes adicionais são instalados por gráficos
Helm. Para ver a lista completa dos componentes de uma versão específica,
consulte as Notas de lançamento (<xref linkend="release-notes"/>).</para>
<para>Para gráficos Helm implantados pelo EIB (<xref linkend="components-eib"/>),
o Controller de upgrade atualiza o <link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart CR</link>
existente de cada componente.</para>
<para>Para gráficos Helm implantados fora do EIB, o Controller de upgrade cria um
recurso <literal>HelmChart</literal> para cada componente.</para>
<para>Após a criação/atualização do recurso <literal>HelmChart</literal>, o
Controller de upgrade se baseia no <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>
para detectar essa alteração e prosseguir com o upgrade real do componente.</para>
<para>O upgrade dos gráficos será feito em sequência de acordo com a ordem
apresentada no <literal>ReleaseManifest</literal>. É possível também
especificar mais valores no <literal>UpgradePlan</literal>. Se uma versão de
gráfico permanecer inalterada no novo lançamento do SUSE Edge, o upgrade
dele não será feito. Para obter mais informações sobre isso, consulte a
<xref linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</section>
</section>
<section xml:id="components-upgrade-controller-extensions">
<title>Extensões de API Kubernetes</title>
<para>Extensões para a API Kubernetes lançadas pelo Controller de upgrade.</para>
<section xml:id="components-upgrade-controller-extensions-upgrade-plan">
<title>UpgradePlan</title>
<para>O Controller de upgrade lança um novo <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recurso
personalizado</link> do Kubernetes chamado <literal>UpgradePlan</literal>.</para>
<para>O <literal>UpgradePlan</literal> atua como um mecanismo de instrução para o
Controller de upgrade e oferece suporte às seguintes configurações:</para>
<itemizedlist>
<listitem>
<para><literal>releaseVersion</literal>: a versão de lançamento do Edge para a
qual o upgrade do cluster deve ser feito. Ela deve seguir o controle de
versão <link xl:href="https://semver.org">semântico</link> e ser recuperada
das Notas de lançamento (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para><literal>disableDrain</literal> (<emphasis
role="strong">opcional</emphasis>): indica se o Controller de upgrade deve
ou não desabilitar as <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drenagens</link>
de nós. Útil quando você tem cargas de trabalho com <link
xl:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">orçamentos
de interrupção</link>.</para>
<itemizedlist>
<listitem>
<para>Exemplo de desabilitação de drenagem de nó do plano de controle:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true</screen>
</listitem>
<listitem>
<para>Exemplo de desabilitação de drenagem de nó do plano de controle e do worker:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true
    worker: true</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>helm</literal> (<emphasis role="strong">opcional</emphasis>):
especifica valores adicionais para componentes instalados pelo Helm.</para>
<warning>
<para>Somente é aconselhável usar esse campo para valores que sejam críticos aos
upgrades. As atualizações nos valores de gráficos padrão deverão ser feitas
depois que o upgrade dos respectivos gráficos for feito para a versão
seguinte.</para>
</warning>
<itemizedlist>
<listitem>
<para>Exemplo:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  helm:
  - chart: foo
    values:
      bar: baz</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-upgrade-controller-extensions-release-manifest">
<title>ReleaseManifest</title>
<para>O Controller de upgrade lança um novo <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recurso
personalizado</link> do Kubernetes chamado
<literal>ReleaseManifest</literal>.</para>
<para>O recurso <literal>ReleaseManifest</literal> é criado pelo Controller de
upgrade e armazena os dados de componente referentes a <emphasis
role="strong">uma</emphasis> versão de lançamento específica do Edge. Isso
significa que o upgrade de cada versão de lançamento do Edge será
representado por um recurso <literal>ReleaseManifest</literal> diferente.</para>
<warning>
<para>O Controller de upgrade sempre deve criar o ReleaseManifest.</para>
<para>Não é aconselhável criar manualmente ou editar os recursos
<literal>ReleaseManifest</literal> . O usuários que decidirem por esse
método devem fazê-lo <emphasis role="strong">por sua conta e
risco</emphasis>.</para>
</warning>
<para>O ReleaseManifest inclui os seguintes dados de componente, mas sem
limitação:</para>
<itemizedlist>
<listitem>
<para>Dados de sistema operacional: versão, arquiteturas suportadas, dados
adicionais de upgrade etc.</para>
</listitem>
<listitem>
<para>Dados de distribuição Kubernetes: versões suportadas do <link
xl:href="https://docs.rke2.io">RKE2</link>/<link
xl:href="https://k3s.io">K3s</link></para>
</listitem>
<listitem>
<para>Dados de componentes adicionais: dados de gráficos Helm do SUSE (local,
versão, nome etc.)</para>
</listitem>
</itemizedlist>
<para>Para ver um exemplo da aparência de um ReleaseManifest, consulte a
documentação <link
xl:href="https://github.com/suse-edge/upgrade-controller/blob/main/config/samples/lifecycle_v1alpha1_releasemanifest.yaml">upstream</link>.
<emphasis>Tenha em mente que se trata apenas de um exemplo, sem a intenção
de criar um recurso <literal>ReleaseManifest</literal> válido.</emphasis></para>
</section>
</section>
<section xml:id="components-upgrade-controller-how-track">
<title>Acompanhando o processo de upgrade</title>
<para>Esta seção apresenta como acompanhar e depurar o processo de upgrade que o
Controller de upgrade inicia após a criação do recurso
<literal>UpgradePlan</literal>.</para>
<section xml:id="components-upgrade-controller-how-track-general">
<title>Geral</title>
<para>Acesse as informações gerais sobre o estado do processo de upgrade em
Condições de status do plano de upgrade.</para>
<para>Veja o status do recurso UpgradePlan da seguinte maneira:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get upgradeplan &lt;upgradeplan_name&gt; -n upgrade-controller-system -o yaml</screen>
<formalpara>
<title>Exemplo de execução do UpgradePlan:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Control plane nodes are being upgraded
    reason: InProgress
    status: "False"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Kubernetes upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Rancher upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Longhorn upgrade is not yet started
    reason: Pending
    status: Unknown
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: MetalLB upgrade is not yet started
    reason: Pending
    status: Unknown
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: CDI upgrade is not yet started
    reason: Pending
    status: Unknown
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: KubeVirt upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: NeuVector upgrade is not yet started
    reason: Pending
    status: Unknown
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: EndpointCopierOperator upgrade is not yet started
    reason: Pending
    status: Unknown
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Elemental upgrade is not yet started
    reason: Pending
    status: Unknown
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: SRIOV upgrade is not yet started
    reason: Pending
    status: Unknown
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Akri upgrade is not yet started
    reason: Pending
    status: Unknown
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Metal3 upgrade is not yet started
    reason: Pending
    status: Unknown
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: RancherTurtles upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherTurtlesUpgraded
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
<para>Aqui você vê todos os componentes para os quais o Controller de upgrade
tentará programar um upgrade. Cada condição segue o gabarito abaixo:</para>
<itemizedlist>
<listitem>
<para><literal>lastTransitionTime</literal>: a hora em que a condição do
componente mudou de um status para outro pela última vez.</para>
</listitem>
<listitem>
<para><literal>message</literal>: mensagem que indica o estado atual do upgrade da
condição do componente específico.</para>
</listitem>
<listitem>
<para><literal>reason</literal>: o estado atual do upgrade da condição do
componente específico. Os possíveis <literal>motivos</literal> incluem:</para>
<itemizedlist>
<listitem>
<para><literal>Succeeded</literal>: o upgrade do componente específico foi
bem-sucedido.</para>
</listitem>
<listitem>
<para><literal>Failed</literal>: falha no upgrade do componente específico.</para>
</listitem>
<listitem>
<para><literal>InProgress</literal>: o upgrade do componente específico está em
andamento.</para>
</listitem>
<listitem>
<para><literal>Pending</literal>: o upgrade do componente específico ainda não foi
programado.</para>
</listitem>
<listitem>
<para><literal>Skipped</literal>: o componente específico não foi encontrado no
cluster, portanto, o upgrade dele será ignorado.</para>
</listitem>
<listitem>
<para><literal>Error</literal>: o componente específico encontrou um erro
temporário.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>status</literal>: o status do <literal>type</literal> (tipo) da
condição atual, que pode ser <literal>True</literal> (Verdadeiro),
<literal>False</literal> (Falso) ou <literal>Unknown</literal>
(Desconhecido).</para>
</listitem>
<listitem>
<para><literal>type</literal>: indicador do componente que está passando por
upgrade.</para>
</listitem>
</itemizedlist>
<para>O Controller de upgrade cria planos do SUC para as condições de componente
do tipo <literal>OSUpgraded</literal> e
<literal>KubernetesUpgraded</literal>. Para acompanhar em mais detalhes os
planos do SUC criados para esses componentes, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<para>É possível acompanhar em mais detalhes todos os outros tipos de condição de
componente visualizando os recursos criados para eles pelo <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>.
Para obter mais informações, consulte a <xref
linkend="components-upgrade-controller-how-track-helm"/>.</para>
<para>É possível marcar um UpgradePlan programado pelo Controller de upgrade como
<literal>successful</literal> (bem-sucedido) se:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Não há condições de componente <literal>Pending</literal> ou
<literal>InProgress</literal>.</para>
</listitem>
<listitem>
<para>A propriedade <literal>lastSuccessfulReleaseVersion</literal> aponta para a
<literal>releaseVersion</literal>, que é especificada na configuração do
UpgradePlan. <emphasis>O Controller de upgrade adiciona essa propriedade ao
status do UpgradePlan após o processo de upgrade bem-sucedido.</emphasis></para>
</listitem>
</orderedlist>
<formalpara>
<title>Exemplo de <literal>UpgradePlan</literal> bem-sucedido:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:48Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:59Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart rancher upgrade succeeded
    reason: Succeeded
    status: "True"
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart longhorn is not installed
    reason: Skipped
    status: "False"
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Specified version of chart metallb is already installed
    reason: Skipped
    status: "False"
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart cdi is not installed
    reason: Skipped
    status: "False"
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart kubevirt is not installed
    reason: Skipped
    status: "False"
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart neuvector-crd is not installed
    reason: Skipped
    status: "False"
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Specified version of chart endpoint-copier-operator is already installed
    reason: Skipped
    status: "False"
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Chart elemental-operator upgrade succeeded
    reason: Succeeded
    status: "True"
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:27:15Z"
    message: Chart sriov-crd is not installed
    reason: Skipped
    status: "False"
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:27:16Z"
    message: Chart akri is not installed
    reason: Skipped
    status: "False"
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:27:19Z"
    message: Chart metal3 is not installed
    reason: Skipped
    status: "False"
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:27:27Z"
    message: Chart rancher-turtles is not installed
    reason: Skipped
    status: "False"
    type: RancherTurtlesUpgraded
  lastSuccessfulReleaseVersion: 3.4
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
</section>
<section xml:id="components-upgrade-controller-how-track-helm">
<title>Helm Controller</title>
<para>Esta seção explica como acompanhar os recursos criados pelo <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>.</para>
<note>
<para>As etapas a seguir consideram que o <literal>kubectl</literal> foi
configurado para conectar-se ao cluster no qual o Controller de upgrade foi
implantado.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Localize o recurso <literal>HelmChart</literal> para o componente
específico:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmcharts -n kube-system</screen>
</listitem>
<listitem>
<para>Usando o nome do recurso <literal>HelmChart</literal>, localize o pod de
upgrade que foi criado pelo <literal>helm-controller</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n kube-system

# Example for Rancher
kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n kube-system
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-tv9wn   0/1     Completed   0          16m</screen>
</listitem>
<listitem>
<para>Exiba os registros do pod específico do componente:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n kube-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-known-issues">
<title>Limitações conhecidas</title>
<itemizedlist>
<listitem>
<para>Os upgrades de clusters downstream ainda não são gerenciados pelo Controller
de upgrade. Para obter mais informações sobre como fazer upgrade de clusters
downstream, consulte o <xref linkend="day2-downstream-clusters"/>.</para>
</listitem>
<listitem>
<para>O Controller de upgrade espera que os gráficos Helm adicionais do SUSE Edge
implantados pelo EIB (<xref linkend="components-eib"/>) tenham o <link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart CR</link>
implantado no namespace <literal>kube-system</literal>. Para isso, configure
a propriedade <literal>installationNamespace</literal> no arquivo de
definição do EIB. Para obter mais informações, consulte a documentação <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">upstream</link>.</para>
</listitem>
<listitem>
<para>Atualmente, o Controller de upgrade não tem como determinar a versão de
lançamento do Edge que está em execução no cluster de gerenciamento. Insira
uma versão de lançamento do Edge superior àquela que está em execução no
cluster.</para>
</listitem>
<listitem>
<para>Atualmente, o Controller de upgrade oferece suporte apenas a upgrades de
ambientes <emphasis role="strong">não air-gapped</emphasis>. Os upgrades
<emphasis role="strong">air-gapped</emphasis> ainda não são possíveis.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-suma">
<title>SUSE Multi-Linux Manager</title>
<para>O SUSE Multi-Linux Manager está incluído no SUSE Edge para oferecer a
automação e o controle que mantêm o sistema operacional SUSE Linux Micro
subjacente sempre atualizado em todos os nós da implantação de borda.</para>
<para>Para obter mais informações, consulte o <xref linkend="quickstart-suma"/> e
a <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">documentação
do SUSE Multi-Linux Manager</link>.</para>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>Guias de Procedimentos</title>
<partintro>
<para>Guias de procedimentos e melhores práticas</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>MetalLB no K3s (usando o modo de camada 2)</title>
<para>O MetalLB é uma implementação de balanceador de carga para clusters
Kubernetes bare metal, usando protocolos de roteamento padrão.</para>
<para>Neste guia, demonstramos como implantar o MetalLB no modo de camada 2 (L2).</para>
<section xml:id="id-why-use-metallb">
<title>Por que usar o MetalLB?</title>
<para>O MetalLB é uma opção atrativa para balanceamento de carga em clusters
Kubernetes bare metal por vários motivos:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Integração nativa com o Kubernetes: o MetalLB se integra perfeitamente ao
Kubernetes, o que facilita a implantação e o gerenciamento usando as
ferramentas e práticas conhecidas do Kubernetes.</para>
</listitem>
<listitem>
<para>Compatibilidade com bare metal: ao contrário dos balanceadores de carga com
base na nuvem, o MetalLB foi especificamente projetado para implantações no
local, em que os balanceadores de carga tradicionais podem não estar
disponíveis ou ser viáveis.</para>
</listitem>
<listitem>
<para>Compatível com vários protocolos: o MetalLB é compatível com os modos de
camada 2 e BGP (Border Gateway Protocol), proporcionando flexibilidade de
acordo com as diferentes arquiteturas e requisitos de rede.</para>
</listitem>
<listitem>
<para>Alta disponibilidade: ao distribuir as responsabilidades de balanceamento de
carga por vários nós, o MetalLB garante alta disponibilidade e
confiabilidade aos seus serviços.</para>
</listitem>
<listitem>
<para>Escalabilidade: o MetalLB processa implantações de grande escala, ajustando
a escala no cluster Kubernetes para atender à crescente demanda.</para>
</listitem>
</orderedlist>
<para>No modo de camada 2, um nó assume a responsabilidade de anunciar um serviço
para a rede local. Da perspectiva da rede, simplesmente parece que a máquina
tem vários endereços IP atribuídos à interface de rede.</para>
<para>A maior vantagem do modo de camada 2 é a sua universalidade: ele funciona em
qualquer rede Ethernet, sem a necessidade de hardware especial nem de
roteadores sofisticados.</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>MetalLB no K3s (usando L2)</title>
<para>Neste início rápido, o modo L2 será usado. Isso significa que não precisamos
de nenhum equipamento de rede especial, mas de três IPs livres no intervalo
de rede.</para>
</section>
<section xml:id="id-prerequisites-7">
<title>Pré-requisitos</title>
<itemizedlist>
<listitem>
<para>Um cluster K3s no qual o MetalLB será implantado.</para>
</listitem>
</itemizedlist>
<warning>
<para>O K3S vem com o próprio balanceador de carga de serviço chamado
Klipper. Você <link
xl:href="https://metallb.universe.tf/configuration/k3s/">precisa
desabilitá-lo para executar o MetalLB</link>. Para desabilitar o Klipper, o
K3s precisa ser instalado com o sinalizador
<literal>--disable=servicelb</literal>.</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>Três endereços IP livres dentro do intervalo de rede. Neste exemplo,
<literal>192.168.122.10-192.168.122.12</literal></para>
</listitem>
</itemizedlist>
<important>
<para>Garanta que esses endereços IP não estejam atribuídos. Em um ambiente DHCP,
esses endereços não devem fazer parte do pool DHCP para evitar atribuições
duplas.</para>
</important>
</section>
<section xml:id="id-deployment-2">
<title>Implantação</title>
<para>Vamos usar o gráfico Helm do MetalLB publicado como parte da solução SUSE
Edge:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>Configuração</title>
<para>Neste ponto, a instalação foi concluída. Agora é hora de <link
xl:href="https://metallb.universe.tf/configuration/">configurar</link>
usando nossos valores de exemplo:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>Agora ele está pronto para uso. É possível personalizar vários componentes
no modo L2, por exemplo:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">Serviços
IPv6 e de pilha dupla</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">Controlar
a alocação de endereços automática</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">Reduzir
o escopo da alocação de endereços para namespaces e serviços
específicos</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">Limitando
o conjunto de nós dos quais o serviço pode ser anunciado</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">Especificar
as interfaces de rede das quais o IP do LB pode ser anunciado</link></para>
</listitem>
</itemizedlist>
<para>E muito mais para <link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>.</para>
<section xml:id="traefik-and-metallb">
<title>Traefik e MetalLB</title>
<para>O Traefik é implantado por padrão com o K3s (<link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">ele pode
ser desabilitado</link> com <literal>--disable=traefik</literal>) e exposto
como <literal>LoadBalancer</literal> (para uso com o Klipper). No entanto,
como o Klipper precisa ser desabilitado, o serviço Traefik para entrada
ainda é um tipo de <literal>LoadBalancer</literal>. Sendo assim, no momento
da implantação do MetalLB, o primeiro IP será automaticamente atribuído à
entrada do Traefik.</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>Isso será aplicado mais adiante (<xref linkend="ingress-with-metallb"/>) no
processo.</para>
</section>
</section>
<section xml:id="id-usage">
<title>Utilização</title>
<para>Vamos criar uma implantação de exemplo:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>E, por fim, o serviço:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>Vamos vê-lo em ação:</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<section xml:id="ingress-with-metallb">
<title>Ingress com MetalLB</title>
<para>Como o Traefik já atua como controlador de entrada, podemos expor o tráfego
HTTP/HTTPS por um objeto <literal>Ingress</literal>, por exemplo:</para>
<screen language="bash" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>E depois:</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>Verifique se o MetalLB funciona corretamente:</para>
<screen language="bash" linenumbering="unnumbered">% arping hellok3s.${IP}.sslip.io

ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>No exemplo acima, o tráfego flui da seguinte maneira:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal> é resolvido para o IP atual.</para>
</listitem>
<listitem>
<para>Em seguida, o tráfego é processado pelo pod
<literal>metallb-speaker</literal>.</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal> redireciona o tráfego para o controlador
<literal>traefik</literal>.</para>
</listitem>
<listitem>
<para>Por fim, o Traefik encaminha a solicitação para o serviço
<literal>hello-kubernetes</literal>.</para>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="guides-metallb-k3s-l3">
<title>MetalLB em K3s (usando o modo de camada 3)</title>
<para>O MetalLB é uma implementação de balanceador de carga para clusters
Kubernetes bare metal, usando protocolos de roteamento padrão.</para>
<para>Neste guia, vamos demonstrar como implantar o MetalLB no modo de camada 3
(L3) do BGP.</para>
<section xml:id="id-why-use-metallb-2">
<title>Por que usar o MetalLB?</title>
<para>O MetalLB é uma opção atrativa para balanceamento de carga em clusters
Kubernetes bare metal por vários motivos:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Integração nativa com o Kubernetes: o MetalLB se integra perfeitamente ao
Kubernetes, o que facilita a implantação e o gerenciamento usando as
ferramentas e práticas conhecidas do Kubernetes.</para>
</listitem>
<listitem>
<para>Compatibilidade com bare metal: ao contrário dos balanceadores de carga com
base na nuvem, o MetalLB foi especificamente projetado para implantações no
local, em que os balanceadores de carga tradicionais podem não estar
disponíveis ou ser viáveis.</para>
</listitem>
<listitem>
<para>Suporte a vários protocolos: o MetalLB oferece suporte aos modos de camada 2
e 3 do BGP (Border Gateway Protocol), proporcionando flexibilidade para
diferentes arquiteturas e requisitos de rede.</para>
</listitem>
<listitem>
<para>Alta disponibilidade: ao distribuir as responsabilidades de balanceamento de
carga por vários nós, o MetalLB garante alta disponibilidade e
confiabilidade aos seus serviços.</para>
</listitem>
<listitem>
<para>Escalabilidade: o MetalLB processa implantações de grande escala, ajustando
a escala no cluster Kubernetes para atender à crescente demanda.</para>
</listitem>
</orderedlist>
<para>No modo de camada 2, um nó assume a responsabilidade de anunciar um serviço
para a rede local. Da perspectiva da rede, simplesmente parece que a máquina
tem vários endereços IP atribuídos à interface de rede.</para>
<para>A maior vantagem do modo de camada 2 é a sua universalidade: ele funciona em
qualquer rede Ethernet, sem a necessidade de hardware especial nem de
roteadores sofisticados.</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l3">
<title>MetalLB em K3s (usando L3)</title>
<para>Neste início rápido, o modo de L3 é usado. Por essa razão, precisamos ter os
roteadores vizinhos com recursos de BGP dentro do intervalo de rede.</para>
</section>
<section xml:id="id-prerequisites-8">
<title>Pré-requisitos</title>
<itemizedlist>
<listitem>
<para>Um cluster K3s no qual o MetalLB será implantado.</para>
</listitem>
<listitem>
<para>Roteadores na rede com suporte ao protocolo BGP.</para>
</listitem>
<listitem>
<para>Um endereço IP livre no intervalo de rede para o serviço. Neste exemplo,
<literal>192.168.10.100</literal></para>
</listitem>
</itemizedlist>
<important>
<para>Você deve garantir que esse endereço IP não esteja atribuído. Em um ambiente
DHCP, esse endereço não deve fazer parte do pool DHCP para evitar
atribuições duplas.</para>
</important>
</section>
<section xml:id="id-configuration-to-advertise-service-ip-addresses">
<title>Configuração para anunciar endereços IP de serviços</title>
<para>O BGP anuncia imediatamente o endereço IP de um serviço a todos os pares
configurados. Esses pares, que normalmente são roteadores, receberão a rota
para o endereço IP de cada serviço com uma máscara de rede de 32 bits. Neste
exemplo, usaremos um roteador com base em FRR que está na mesma rede que o
nosso cluster. Em seguida, usaremos o recurso BGP do MetalLB para anunciar
um serviço a esse roteador com base em FRR.</para>
</section>
<section xml:id="id-deployment-3">
<title>Implantação</title>
<para>Vamos usar o gráfico Helm do MetalLB publicado como parte da solução SUSE
Edge:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration-2">
<title>Configuração</title>
<orderedlist numeration="arabic">
<listitem>
<para>Neste ponto, a instalação é concluída. Crie um
<literal>IPAddressPool</literal>:</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: bgp-pool
  namespace: metallb-system
  labels:
    app: httpd
spec:
  addresses:
  - 192.168.10.100/32
  autoAssign: true
  avoidBuggyIPs: false
  serviceAllocation:
    namespaces:
    - metallb-system
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - key: serviceType
        operator: In
        values:
        - httpd
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>Configure um <literal>BGPPeer</literal>.</para>
</listitem>
</orderedlist>
<note>
<para>O roteador FRR tem ASN 1000, enquanto o nosso <literal>BGPPeer</literal>
terá 1001. Podemos ver também que o roteador FRR tem o endereço IP
192.168.3.140.</para>
</note>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: mypeertest
spec:
  peerAddress: 192.168.3.140
  peerASN: 1000
  myASN: 1001
  routerID: 4.4.4.4
EOF</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>Crie o BGPAdvertisement (L3):</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-test
  namespace: metallb-system
spec:
  ipAddressPools:
  - bgp-pool
EOF</screen>
</section>
<section xml:id="id-usage-2">
<title>Utilização</title>
<orderedlist numeration="arabic">
<listitem>
<para>Crie um aplicativo de exemplo com um serviço. Neste caso, o endereço IP do
<literal>IPAddressPool</literal> é <literal>192.168.10.100</literal> para o
serviço.</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment
  namespace: metallb-system
  labels:
    app: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      pod-label: httpd
  template:
    metadata:
      labels:
        pod-label: httpd
    spec:
      containers:
      - name: httpdcontainer
        image: image: docker.io/library/httpd:2.4
        ports:
          - containerPort: 80
            protocol: TCP
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: http-service
  namespace: metallb-system
  labels:
    serviceType: httpd
spec:
  selector:
    pod-label: httpd
  type: LoadBalancer
  ports:
  - protocol: TCP
    port: 8080
    name: 8080-tcp
    targetPort: 80
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>Para verificar, faça login no roteador FRR para ver as rotas criadas com
base no anúncio feito por BGP.</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered">42178089cba5# show ip bgp all

For address family: IPv4 Unicast
BGP table version is 3, local router ID is 2.2.2.2, vrf id 0
Default local pref 100, local AS 1000
Status codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
* i172.16.0.0/24    1.1.1.1                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
* i172.17.0.0/24    3.3.3.3                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
*= 192.168.10.100/32
                    192.168.3.162                          0 1001 i
*=                  192.168.3.163                          0 1001 i
*&gt;                  192.168.3.161                          0 1001 i

Displayed  3 routes and 7 total paths
kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>Se esse roteador é o gateway padrão da sua rede, você pode executar o
comando <literal>curl</literal> de uma caixa nessa rede para verificar se
ele pode acessar o aplicativo de amostra httpd.</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered"># curl http://192.168.10.100:8080
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
#</screen>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>MetalLB na frente do servidor da API Kubernetes</title>
<para>Este guia demonstra o uso do serviço MetalLB para expor a API RKE2/K3s
externamente em um cluster HA com três nós de plano de controle. Para isso,
um serviço Kubernetes do tipo <literal>LoadBalancer</literal> será criado
manualmente. Em seguida, o objeto <literal>EndpointSlices</literal> será
automaticamente criado para manter os IPs de todos os nós de plano de
controle disponíveis no cluster. Para sempre manter o EndpointSlices
sincronizado com os eventos que ocorrem no cluster (adição/remoção de nó ou
um nó que fica offline), o Endpoint Copier Operator (<xref
linkend="components-eco"/>) será implantado. O operador monitora os eventos
ocorridos no EndpointSlices padrão do <literal>kubernetes</literal> e
atualiza o que é gerenciado automaticamente para mantê-los em
sincronia. Como o serviço gerenciado é do tipo
<literal>LoadBalancer</literal>, o MetalLB atribui a ele um
<literal>ExternalIP</literal> estático. Esse <literal>ExternalIP</literal>
será usado para comunicação com o API Server.</para>
<section xml:id="id-prerequisites-9">
<title>Pré-requisitos</title>
<itemizedlist>
<listitem>
<para>Três hosts para implantar o RKE2/K3s em cima.</para>
<itemizedlist>
<listitem>
<para>Certifique-se de que os hosts tenham nomes diferentes.</para>
</listitem>
<listitem>
<para>Para teste, eles podem ser máquinas virtuais.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Pelo menos 2 IPs disponíveis na rede (um para o Traefik/Nginx e outro para o
serviço gerenciado).</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-rke2k3s">
<title>Instalando o RKE2/K3s</title>
<note>
<para>Se você não deseja usar um cluster novo, mas um existente, ignore esta etapa
e avance para a próxima.</para>
</note>
<para>Primeiramente, é necessário reservar um IP livre para usar mais adiante como
<literal>ExternalIP</literal> do serviço gerenciado.</para>
<para>Acesse o primeiro host por SSH e instale a distribuição desejada no modo de
cluster.</para>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC="server \
 --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" sh -

systemctl enable rke2-server.service
systemctl start rke2-server.service

# Fetch the cluster token:
RKE2_TOKEN=$(tr -d '\n' &lt; /var/lib/rancher/rke2/server/node-token)</screen>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>Verifique se o sinalizador <literal>--disable=servicelb</literal> foi
inserido no comando <literal>k3s server</literal>.</para>
</note>
<important>
<para>A partir de agora, os comandos devem ser executados na máquina local.</para>
</important>
<para>Para acessar o servidor da API de um ambiente externo, o IP da VM do
RKE2/K3s será usado.</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
export KUBE_DISTRIBUTION=&lt;k3s/rke2&gt;

scp ${NODE_IP}:/etc/rancher/${KUBE_DISTRIBUTION}/${KUBE_DISTRIBUTION}.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-cluster">
<title>Configurando um cluster existente</title>
<note>
<para>Esta etapa é válida apenas se você pretende usar um cluster RKE2/K3s
existente.</para>
</note>
<para>Modifique os sinalizadores <literal>tls-san</literal> para usar um cluster
existente. Além disso, o LB <literal>servicelb</literal> deve ser
desabilitado no K3s.</para>
<para>Para alterar os sinalizadores dos servidores RKE2 ou K3s, você precisa
modificar o arquivo <literal>/etc/systemd/system/rke2.service</literal> ou
<literal>/etc/systemd/system/k3s.service</literal> em todas as VMs no
cluster, dependendo da distribuição.</para>
<para>Insira os sinalizadores em <literal>ExecStart</literal>. Por exemplo:</para>
<para>Para RKE2:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/rke2 \
    server \
        '--write-kubeconfig-mode=644' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>Para K3s:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>Na sequência, os seguintes comandos devem ser executados para carregar as
novas configurações:</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart ${KUBE_DISTRIBUTION}</screen>
</section>
<section xml:id="id-installing-metallb">
<title>Instalando o MetalLB</title>
<para>Para implantar o <literal>MetalLB</literal>, use o guia do MetalLB no K3s
(<xref linkend="guides-metallb-k3s"/>).</para>
<para><emphasis role="strong">NOTA:</emphasis> Garanta que o endereço IP
<literal>VIP_SERVICE_IP</literal> não se sobreponha ao
<literal>IPAddressPools</literal> existente no cluster.</para>
<para>Crie um <literal>IpAddressPool</literal> separado e um
<literal>L2Advertisement</literal> para uso apenas com o serviço gerenciado.</para>
<para><emphasis role="strong">NOTA:</emphasis> O IPAddressPool a seguir será
atribuído a um serviço do tipo <literal>LoadBalancer</literal> no namespace
<literal>padrão</literal>. Se houver vários serviços
<literal>LoadBalancer</literal> nele, outros <link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">ServiceSelectors</link>
poderão ser configurados para corresponder a esse serviço VIP de maneira
explícita.</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: kubernetes-vip-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Instalando o Endpoint Copier Operator</title>
<screen language="bash" linenumbering="unnumbered">helm install \
endpoint-copier-operator oci://registry.suse.com/edge/charts/endpoint-copier-operator \
--namespace endpoint-copier-operator \
--create-namespace</screen>
<para>O comando acima implanta o operador
<literal>endpoint-copier-operator</literal> com duas réplicas: uma será a
líder e a outra assumirá a função de líder se necessário.</para>
<para>Agora o serviço <literal>kubernetes-vip</literal> deve estar implantado, que
será reconciliado pelo operador, e será criado um EndpointSlices com as
portas e o IP configurados.</para>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  type: LoadBalancer
EOF</screen>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer
EOF</screen>
<para>Verifique se o serviço <literal>kubernetes-vip</literal> tem o endereço IP
correto:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para>Verifique se os recursos <literal>kubernetes-vip-*</literal> e
<literal>kubernetes</literal> do EndpointSlices no namespace
<literal>padrão</literal> apontam para os mesmos IPs.</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpointslices | grep kubernetes</screen>
<para>Se tudo estiver correto, a última coisa a se fazer é usar o
<literal>VIP_SERVICE_IP</literal> no <literal>Kubeconfig</literal>.</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>A partir de agora, o <literal>kubectl</literal> sempre vai passar pelo
serviço <literal>kubernetes-vip</literal>.</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>Adicionando nós do plano de controle</title>
<para>Para monitorar o processo inteiro, é possível abrir mais duas guias do
terminal.</para>
<para>Primeiro terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>Segundo terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpointslices</screen>
<para>Agora execute os comandos a seguir no segundo e no terceiro nó.</para>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
systemctl enable rke2-server.service


mkdir -p /etc/rancher/rke2/
cat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml
server: https://${VIP_SERVICE_IP}:9345
token: ${RKE2_TOKEN}
EOF

systemctl start rke2-server.service</screen>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Implantações air-gapped com o Edge Image Builder</title>
<section xml:id="id-intro">
<title>Introdução</title>
<para>Este guia mostra como implantar vários componentes do SUSE Edge totalmente
air-gapped no SUSE Linux Micro 6.1 usando o Edge Image Builder (EIB) (<xref
linkend="components-eib"/>). Dessa forma, você pode inicializar em uma
imagem personalizada e pronta para uso (CRB) criada pelo EIB e ter os
componentes especificados implantados em um cluster RKE2 ou K3s, sem conexão
com a Internet nem qualquer etapa manual. Essa configuração é extremamente
prática para clientes que desejam fazer bake prévio de todos os artefatos
necessários para implantação na imagem do sistema operacional, assim eles
ficam disponíveis logo após a inicialização.</para>
<para>Vamos abordar a instalação air-gapped do:</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-security"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-storage"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>O EIB analisa e faz pré-download de todas as imagens referenciadas nos
gráficos Helm e nos manifestos do Kubernetes fornecidos. No entanto, alguns
deles podem tentar obter as imagens do contêiner e criar recursos do
Kubernetes com base nas imagens em runtime. Nesses casos, precisamos
especificar manualmente as imagens necessárias no arquivo de definição para
configurar um ambiente completamente air-gapped.</para>
</warning>
</section>
<section xml:id="id-prerequisites-10">
<title>Pré-requisitos</title>
<para>Se você está seguindo este guia, consideramos que já esteja familiarizado
com o EIB (<xref linkend="components-eib"/>). Do contrário, consulte o Guia
de Início Rápido (<xref linkend="quickstart-eib"/>) para entender melhor os
conceitos apresentados na prática a seguir.</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Configuração da rede libvirt</title>
<note>
<para>Para demonstrar a implantação air-gapped, este guia usa a rede air-gapped
simulada <literal>libvirt</literal> e a configuração a seguir é adaptada a
ela. Em suas próprias implantações, você pode precisar modificar a
configuração <literal>host1.local.yaml</literal> que será introduzida na
etapa seguinte.</para>
</note>
<para>Se você prefere usar a mesma configuração de rede
<literal>libvirt</literal>, siga adiante. Do contrário, pule para a <xref
linkend="config-dir-creation"/>.</para>
<para>Vamos criar uma configuração de rede isolada com o intervalo de endereços IP
<literal>192.168.100.2/24</literal> para DHCP:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>Agora resta apenas criar e iniciar a rede:</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>Configuração do diretório base</title>
<para>A configuração do diretório base é igual em todos os diversos componentes,
portanto, vamos defini-la aqui.</para>
<para>Vamos primeiro criar os subdiretórios necessários:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>Adicione qualquer imagem base que você queira usar ao diretório
<literal>base-images</literal>. O foco deste guia é a autoinstalação da ISO
disponível <link
xl:href="https://www.suse.com/download/sle-micro/">aqui</link>.</para>
<para>Vamos copiar a imagem baixada:</para>
<screen language="shell" linenumbering="unnumbered">cp SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>O EIB nunca modifica a entrada da imagem base.</para>
</note>
<para>Vamos criar um arquivo com a configuração de rede desejada:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>Esta configuração garante que os seguintes elementos estejam presentes nos
sistemas provisionados (usando o endereço MAC especificado):</para>
<itemizedlist>
<listitem>
<para>uma interface Ethernet com endereço IP estático</para>
</listitem>
<listitem>
<para>roteamento</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>nome de host (<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>A estrutura do arquivo resultante deve ter a seguinte aparência:</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>Arquivo de definição de base</title>
<para>O Edge Image Builder usa <emphasis>arquivos de definição</emphasis> para
modificar as imagens do SUSE Linux Micro. Esses arquivos contêm a maioria
das opções configuráveis. Muitas dessas opções se repetem nas diferentes
seções de componentes, portanto, vamos listá-las e explicá-las aqui.</para>
<tip>
<para>A lista completa das opções de personalização no arquivo de definição está
disponível na <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#image-definition-file">documentação
upstream</link></para>
</tip>
<para>Vamos analisar os seguintes campos que estarão presentes em todos os
arquivos de definição:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para>A seção <literal>image</literal> é obrigatória e especifica a imagem de
entrada, sua arquitetura e tipo, além do nome da imagem de saída.</para>
<para>A seção <literal>operatingSystem</literal> é opcional e contém uma
configuração para permitir o login nos sistemas provisionados com o nome de
usuário/senha <literal>root/eib</literal>.</para>
<para>A seção <literal>kubernetes</literal> é opcional e define o tipo e a versão
do Kubernetes. Vamos usar a distribuição RKE2. Em vez disso, use
<literal>kubernetes.version: v1.33.3+k3s1</literal> se quiser o K3s. A menos
que seja claramente configurado no campo
<literal>kubernetes.nodes</literal>, todos os clusters que inicializamos
neste guia são de nó único.</para>
<para>A seção <literal>embeddedArtifactRegistry</literal> incluirá todas as
imagens que apenas são referenciadas e extraídas em runtime para o
componente específico.</para>
</section>
<section xml:id="rancher-install">
<title>Instalação do Rancher</title>
<note>
<para>A exibição da implantação do Rancher (<xref linkend="components-rancher"/>)
será muito reduzida para fins de demonstração. Em sua implantação real,
podem ser necessários artefatos adicionais, dependendo da sua configuração.</para>
</note>
<para>Os ativos da versão do <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">Rancher
2.12.1</link> contêm um arquivo <literal>rancher-images.txt</literal> que
lista todas as imagens necessárias para uma instalação air-gapped.</para>
<para>Há mais de 600 imagens do contêiner no total, o que significa que a imagem
CRB resultante teria cerca de 30 GB. No caso da nossa instalação do Rancher,
vamos reduzir a lista para a menor configuração de trabalho. A partir disso,
você pode readicionar qualquer imagem que possa precisar em sua implantação.</para>
<para>Vamos criar o arquivo de definição e incluir a lista de imagens reduzida:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.18.2
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v8.0.0
    - name: registry.rancher.com/rancher/compliance-operator:v1.1.0
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.23-build20250612
    - name: registry.rancher.com/rancher/hardened-calico:v3.30.2-build20250711
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.26.0-build20250611
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.21-k3s1-build20250612
    - name: registry.rancher.com/rancher/hardened-flannel:v0.27.1-build20250710
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.33.3-rke2r1-build20250716
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-multus-dynamic-networks-controller:v0.3.7-build20250711
    - name: registry.rancher.com/rancher/hardened-multus-thick:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.9.1-build20250704
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.13
    - name: registry.rancher.com/rancher/kubectl:v1.33.1
    - name: registry.rancher.com/rancher/kuberlr-kubectl:v5.0.0
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.31
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher131
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.10.2
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.rancher.com/rancher/prom-prometheus:v3.2.1
    - name: registry.rancher.com/rancher/prometheus-federator:v4.1.0
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.5-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.5-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.12.1
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v7.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/remotedialer-proxy:v0.5.0
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.33.1-0.20250516163953-99d91538b132-build20250612
    - name: registry.rancher.com/rancher/rke2-runtime:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/scc-operator:v0.1.1
    - name: registry.rancher.com/rancher/security-scan:v0.7.1
    - name: registry.rancher.com/rancher/shell:v0.5.0
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.13-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.rancher.com/rancher/ui-plugin-catalog:4.0.3
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.1
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.2
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.3
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.6.0</screen>
<para>Em comparação com a lista completa de mais de 600 imagens do contêiner, essa
versão reduzida contém apenas cerca de 60, o que faz com que a nova imagem
CRB tenha somente 7 GB.</para>
<para>Vamos criar também um arquivo de valores do Helm para o Rancher:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para>A definição de <literal>systemDefaultRegistry</literal> como
<literal>registry.rancher.com</literal> permite que o Rancher procure
automaticamente as imagens no registro de artefatos incorporado que foi
iniciado na imagem CRB no momento da inicialização. A omissão desse campo
pode impedir que as imagens do contêiner sejam encontradas no nó.</para>
</warning>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>A saída deve ter uma aparência semelhante a esta:</para>
<screen language="console" linenumbering="unnumbered">Downloading file: dl-manifest-1.yaml 100% |██████████████████████████████████████████████████████████████████████████████| (583/583 kB, 12 MB/s)
Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████| (56/56, 8 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% |███████████████████████████████████████████████████████████| (644/644 MB, 29 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% |█████████████████████████████████████████████████████████| (400/400 MB, 29 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% |███████████████████████████████████████████████████████████████████████████| (36/36 MB, 30 MB/s)
Downloading file: sha256sum-amd64.txt 100% |█████████████████████████████████████████████████████████████████████████████| (4.3/4.3 kB, 29 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Após o provisionamento do nó usando a imagem criada, poderemos verificar a
instalação do Rancher:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cattle-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>A saída deve mostrar que tudo foi implantado com sucesso, com uma aparência
similar a esta:</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/helm-operation-6l6ld                        0/2     Completed   0          107s
pod/helm-operation-8tk2v                        0/2     Completed   0          2m2s
pod/helm-operation-blnrr                        0/2     Completed   0          2m49s
pod/helm-operation-hdcmt                        0/2     Completed   0          3m19s
pod/helm-operation-m74c7                        0/2     Completed   0          97s
pod/helm-operation-qzzr4                        0/2     Completed   0          2m30s
pod/helm-operation-s9jh5                        0/2     Completed   0          3m
pod/helm-operation-tq7ts                        0/2     Completed   0          2m41s
pod/rancher-99d599967-ftjkk                     1/1     Running     0          4m15s
pod/rancher-webhook-79798674c5-6w28t            1/1     Running     0          2m27s
pod/system-upgrade-controller-56696956b-trq5c   1/1     Running     0          104s

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/rancher           ClusterIP   10.43.255.80   &lt;none&gt;        80/TCP,443/TCP   4m15s
service/rancher-webhook   ClusterIP   10.43.7.238    &lt;none&gt;        443/TCP          2m27s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rancher                     1/1     1            1           4m15s
deployment.apps/rancher-webhook             1/1     1            1           2m27s
deployment.apps/system-upgrade-controller   1/1     1            1           104s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/rancher-99d599967                     1         1         1       4m15s
replicaset.apps/rancher-webhook-79798674c5            1         1         1       2m27s
replicaset.apps/system-upgrade-controller-56696956b   1         1         1       104s</screen>
<para>Vamos acessar <literal>https://192.168.100.50.sslip.io</literal> e fazer
login com a senha <literal>adminadminadmin</literal> que já definimos, o que
nos leva ao dashboard do Rancher:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width="100%"/>
</imageobject>
<textobject><phrase>rancher air-gapped</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="suse-security-install">
<title>Instalação do SUSE Security</title>
<para>Ao contrário da instalação do Rancher, a do SUSE Security não requer nenhum
processamento adicional no EIB. O EIB isola automaticamente as imagens
necessárias ao seu componente subjacente NeuVector.</para>
<para>Vamos criar o arquivo de definição:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>Vamos criar também um arquivo de valores do Helm para o NeuVector:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>A saída deve ter uma aparência semelhante a esta:</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 4 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████| (5/5, 13 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Após o provisionamento do nó usando a imagem criada, poderemos verificar a
instalação do SUSE Security:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>A saída deve mostrar que tudo foi implantado com sucesso, com uma aparência
similar a esta:</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/neuvector-cert-upgrader-job-bxbnz           0/1     Completed   0          3m39s
pod/neuvector-controller-pod-7d854bfdc7-nhxjf   1/1     Running     0          3m44s
pod/neuvector-enforcer-pod-ct8jm                1/1     Running     0          3m44s

NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.234.241   &lt;none&gt;        443/TCP                         3m44s
service/neuvector-svc-controller          ClusterIP   None            &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   3m44s
service/neuvector-svc-crd-webhook         ClusterIP   10.43.50.190    &lt;none&gt;        443/TCP                         3m44s

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          3m44s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           3m44s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-7d854bfdc7   1         1         1       3m44s

NAME                                        SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-cert-upgrader-pod   0 0 1 1 *   &lt;none&gt;     True      0        &lt;none&gt;          3m44s
cronjob.batch/neuvector-updater-pod         0 0 * * *   &lt;none&gt;     False     0        &lt;none&gt;          3m44s

NAME                                    STATUS     COMPLETIONS   DURATION   AGE
job.batch/neuvector-cert-upgrader-job   Complete   1/1           7s         3m39s</screen>
</section>
<section xml:id="suse-storage-install">
<title>Instalação do SUSE Storage</title>
<para>A <link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/airgap/">documentação
oficial</link> do Longhorn contém um arquivo
<literal>longhorn-images.txt</literal> que lista todas as imagens
necessárias para uma instalação air-gapped. Vamos incluir as respectivas
contrapartes espelhadas do registro de contêiner do Rancher em nosso arquivo
de definição. Vamos criá-lo:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: [reg-code]
    packageList:
      - open-iscsi
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 107.0.0+up1.9.1
      - name: longhorn-crd
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        version: 107.0.0+up1.9.1
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.52
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1</screen>
<note>
<para>Veja que o arquivo de definição lista o pacote
<literal>open-iscsi</literal>. Ele é necessário porque o Longhorn precisa
que o daemon <literal>iscsiadm</literal> seja executado em nós diferentes
para fornecer volumes persistentes ao Kubernetes.</para>
</note>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>A saída deve ter uma aparência semelhante a esta:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 20956 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Após o provisionamento do nó usando a imagem criada, poderemos verificar a
instalação do Longhorn:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>A saída deve mostrar que tudo foi implantado com sucesso, com uma aparência
similar a esta:</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/csi-attacher-787fd9c6c8-sf42d                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-tb82p                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-zhc6s                       1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-b2v9s                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-hwllt                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-mlrpk                    1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-t54zk                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-vdt5d                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-x9kh4                        1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-r62gr                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-vrwjn                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-z65nb                    1/1     Running   0          2m28s
pod/engine-image-ei-4623b511-9vhkb                      1/1     Running   0          3m13s
pod/instance-manager-6f95fd57d4a4cd0459e469d75a300552   1/1     Running   0          2m43s
pod/longhorn-csi-plugin-gx98x                           3/3     Running   0          2m28s
pod/longhorn-driver-deployer-55f9c88499-fbm6q           1/1     Running   0          3m28s
pod/longhorn-manager-dpdp7                              2/2     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-gg5hq                        1/1     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-s49jc                        1/1     Running   0          3m28s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.77.89    &lt;none&gt;        9502/TCP   3m28s
service/longhorn-backend              ClusterIP   10.43.56.17    &lt;none&gt;        9500/TCP   3m28s
service/longhorn-conversion-webhook   ClusterIP   10.43.54.73    &lt;none&gt;        9501/TCP   3m28s
service/longhorn-frontend             ClusterIP   10.43.22.82    &lt;none&gt;        80/TCP     3m28s
service/longhorn-recovery-backend     ClusterIP   10.43.45.143   &lt;none&gt;        9503/TCP   3m28s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-4623b511   1         1         1       1            1           &lt;none&gt;          3m13s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          2m28s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          3m28s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           2m28s
deployment.apps/csi-provisioner            3/3     3            3           2m28s
deployment.apps/csi-resizer                3/3     3            3           2m28s
deployment.apps/csi-snapshotter            3/3     3            3           2m28s
deployment.apps/longhorn-driver-deployer   1/1     1            1           3m28s
deployment.apps/longhorn-ui                2/2     2            2           3m28s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-787fd9c6c8               3         3         3       2m28s
replicaset.apps/csi-provisioner-74486b95c6            3         3         3       2m28s
replicaset.apps/csi-resizer-859d4557fd                3         3         3       2m28s
replicaset.apps/csi-snapshotter-6f69c6c8cc            3         3         3       2m28s
replicaset.apps/longhorn-driver-deployer-55f9c88499   1         1         1       3m28s
replicaset.apps/longhorn-ui-59c85fcf94                2         2         2       3m28s</screen>
</section>
<section xml:id="kubevirt-install">
<title>Instalação do KubeVirt e CDI</title>
<para>Os gráficos Helm para KubeVirt e CDI instalam apenas os respectivos
operadores. Cabe aos operadores implantar o restante dos sistemas, ou seja,
precisamos incluir todas as imagens do contêiner necessárias em nosso
arquivo de definição. Vamos criá-lo:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: kubevirt
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/virt-api:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-controller:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-handler:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2-150700.3.5.2</screen>
<para>Vamos criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>A saída deve ter uma aparência semelhante a esta:</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 48 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 4 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Após o provisionamento do nó usando a imagem criada, poderemos verificar a
instalação do KubeVirt e do CDI.</para>
<para>Verifique o KubeVirt:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>A saída deve mostrar que tudo foi implantado com sucesso, com uma aparência
similar a esta:</para>
<screen language="console" linenumbering="unnumbered">NAME                                  READY   STATUS    RESTARTS   AGE
pod/virt-api-59cb997648-mmt67         1/1     Running   0          2m34s
pod/virt-controller-69786b785-7cc96   1/1     Running   0          2m8s
pod/virt-controller-69786b785-wq2dz   1/1     Running   0          2m8s
pod/virt-handler-2l4dm                1/1     Running   0          2m8s
pod/virt-operator-7c444cff46-nps4l    1/1     Running   0          3m1s
pod/virt-operator-7c444cff46-r25xq    1/1     Running   0          3m1s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.167.109   &lt;none&gt;        443/TCP   2m36s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m36s
service/virt-api                      ClusterIP   10.43.18.202    &lt;none&gt;        443/TCP   2m36s
service/virt-exportproxy              ClusterIP   10.43.142.188   &lt;none&gt;        443/TCP   2m36s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   2m8s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m34s
deployment.apps/virt-controller   2/2     2            2           2m8s
deployment.apps/virt-operator     2/2     2            2           3m1s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-59cb997648         1         1         1       2m34s
replicaset.apps/virt-controller-69786b785   2         2         2       2m8s
replicaset.apps/virt-operator-7c444cff46    2         2         2       3m1s

NAME                            AGE    PHASE
kubevirt.kubevirt.io/kubevirt   3m1s   Deployed</screen>
<para>Verifique o CDI:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>A saída deve mostrar que tudo foi implantado com sucesso, com uma aparência
similar a esta:</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-5598c9bf47-pqfxw     1/1     Running   0          3m44s
pod/cdi-deployment-7cbc5db7f8-g46z7    1/1     Running   0          3m44s
pod/cdi-operator-777c865745-2qcnj      1/1     Running   0          3m48s
pod/cdi-uploadproxy-646f4cd7f7-fzkv7   1/1     Running   0          3m44s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.2.224    &lt;none&gt;        443/TCP    3m44s
service/cdi-prometheus-metrics   ClusterIP   10.43.237.13   &lt;none&gt;        8080/TCP   3m44s
service/cdi-uploadproxy          ClusterIP   10.43.114.91   &lt;none&gt;        443/TCP    3m44s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m44s
deployment.apps/cdi-deployment    1/1     1            1           3m44s
deployment.apps/cdi-operator      1/1     1            1           3m48s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m44s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-5598c9bf47     1         1         1       3m44s
replicaset.apps/cdi-deployment-7cbc5db7f8    1         1         1       3m44s
replicaset.apps/cdi-operator-777c865745      1         1         1       3m48s
replicaset.apps/cdi-uploadproxy-646f4cd7f7   1         1         1       3m44s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>Resolução de Problemas</title>
<para>Se você tiver problemas ao criar as imagens ou se quiser testar e depurar o
processo em mais detalhes, consulte a <link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">documentação
upstream</link>.</para>
</section>
</chapter>
<chapter xml:id="guides-kiwi-builder-images">
<title>Criando imagens atualizadas do SUSE Linux Micro com o Kiwi</title>
<para>Esta seção explica como gerar imagens atualizadas do SUSE Linux Micro para
usar com o Edge Image Builder, usando Cluster API (CAPI) +
Metal<superscript>3</superscript>, ou como gravar a imagem do disco
diretamente em um dispositivo em blocos. Esse processo é útil quando há
necessidade de incluir os patches mais recentes nas imagens de inicialização
do sistema inicial (para minimizar a transferência de patches após a
instalação) ou quando a CAPI é usada, em que é preferível reinstalar o
sistema operacional com uma nova imagem em vez de fazer upgrade dos hosts no
local.</para>
<para>Esse processo usa o <link
xl:href="https://osinside.github.io/kiwi/">Kiwi</link> para a criação da
imagem. O SUSE Edge vem com uma versão conteinerizada que simplifica o
processo geral com um utilitário auxiliar integrado, o que permite
especificar o <emphasis role="strong">perfil</emphasis> de destino
obrigatório. O perfil define o tipo de imagem de saída necessário, sendo os
mais comuns relacionados a seguir:</para>
<itemizedlist>
<listitem>
<para>"<emphasis role="strong">Base</emphasis>": uma imagem de disco do SUSE Linux
Micro com um conjunto de pacotes reduzido (inclui o podman).</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-SelfInstall</emphasis>": imagem SelfInstall
baseada na imagem "Base" acima.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-RT</emphasis>": igual à imagem "Base" acima,
mas usa o kernel em tempo real (rt) no lugar.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-RT-SelfInstall</emphasis>": uma imagem
SelfInstall conforme a "Base-RT" acima.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Default</emphasis>": uma imagem de disco do SUSE
Linux Micro conforme a "Base" acima, mas com algumas outras ferramentas,
como pilha de virtualização, Cockpit e salt-minion.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Default-SelfInstall</emphasis>": imagem SelfInstall
baseada na imagem "Default" acima.</para>
</listitem>
</itemizedlist>
<para>Consulte a documentação do <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-deployment-images/index.html#alp-images-installer-type">SUSE
Linux Micro 6.1</link> para obter mais detalhes.</para>
<para>Esse processo funciona em ambas as arquiteturas AMD64/Intel 64 e AArch64,
embora nem todos os perfis de imagem estejam disponíveis nas duas
arquiteturas. Por exemplo, no SUSE Edge 3.4, em que o SUSE Linux Micro 6.1 é
usado, um perfil com o kernel Real-Time (ou seja, "Base-RT" ou
"Base-RT-SelfInstall") não está disponível para AArch64 no momento.</para>
<note>
<para>É necessário usar um host de build com a mesma arquitetura das imagens que
estão sendo criadas. Em outras palavras, para criar uma imagem AArch64, é
necessário usar um host de build AArch64, e vice-versa para
AMD64/Intel 64. Não há suporte para builds cruzados no momento.</para>
</note>
<section xml:id="id-prerequisites-11">
<title>Pré-requisitos</title>
<para>O construtor de imagens Kiwi requer o seguinte:</para>
<itemizedlist>
<listitem>
<para>Um host SUSE Linux Micro 6.1 ("sistema de build") com a mesma arquitetura da
imagem que está sendo criada.</para>
</listitem>
<listitem>
<para>O sistema de build já deve ter sido registrado pelo
<literal>SUSEConnect</literal> (o registro é usado para obter os pacotes
mais recentes dos repositórios SUSE).</para>
</listitem>
<listitem>
<para>Uma conexão de Internet para obter os pacotes necessários. Se conectado por
proxy, o host de build precisa ser pré-configurado.</para>
</listitem>
<listitem>
<para>É necessário desabilitar o SELinux no host de build (já que ocorre a
rotulagem do SELinux no contêiner e isso pode entrar em conflito com a
política do host).</para>
</listitem>
<listitem>
<para>Pelo menos 10 GB de espaço livre no disco para acomodar a imagem do
contêiner, a raiz do build e uma ou mais imagens de saída resultantes.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started-2">
<title>Introdução</title>
<para>Devido a algumas limitações, é necessário desabilitar o SELinux. Conecte-se
ao host de build da imagem do SUSE Linux Micro 6.1 e confirme se o SELinux
está desabilitado:</para>
<screen language="console" linenumbering="unnumbered"># setenforce 0</screen>
<para>Crie um diretório de saída para compartilhar com o contêiner de build do
Kiwi no qual salvar as imagens resultantes:</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/output</screen>
<para>Extraia a imagem mais recente do construtor Kiwi do SUSE Registry:</para>
<screen language="console" linenumbering="unnumbered"># podman pull registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
(...)</screen>
</section>
<section xml:id="id-building-the-default-image">
<title>Criando a imagem padrão</title>
<para>Este é o comportamento padrão do contêiner de imagens do Kiwi quando nenhum
argumento é inserido durante a execução da imagem do contêiner. O seguinte
comando executa o <literal>podman</literal> com dois diretórios mapeados
para o contêiner:</para>
<itemizedlist>
<listitem>
<para>O diretório do repositório de pacotes <literal>/etc/zypp/repos.d</literal>
do SUSE Linux Micro do host subjacente.</para>
</listitem>
<listitem>
<para>O diretório de saída <literal>~/output</literal> criado acima.</para>
</listitem>
</itemizedlist>
<para>O contêiner de imagens do Kiwi requer a execução do script auxiliar
<literal>build-image</literal> como:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<note>
<para>Se você está executando o script pela primeira vez, é esperado que ocorra
uma <emphasis role="strong">falha</emphasis> nele logo depois de ser
iniciado com o erro: "<emphasis role="strong">ERROR: Early loop device test
failed, please retry the container run.</emphasis>" (ERRO: Falha no teste do
dispositivo no ciclo inicial. Tente executar o contêiner
novamente.). Trata-se de um sintoma em que os dispositivos que são criados
em loop no sistema host subjacente não ficam imediatamente visíveis dentro
da imagem do contêiner. Você apenas precisa executar o comando novamente, e
ele deverá prosseguir sem problemas.</para>
</note>
<para>Após alguns minutos, as imagens estarão disponíveis no diretório de saída
local:</para>
<screen language="console" linenumbering="unnumbered">(...)
INFO: Image build successful, generated images are available in the 'output' directory.

# ls -1 output/
SLE-Micro.x86_64-6.1.changes
SLE-Micro.x86_64-6.1.packages
SLE-Micro.x86_64-6.1.raw
SLE-Micro.x86_64-6.1.verified
build
kiwi.result
kiwi.result.json</screen>
</section>
<section xml:id="id-building-images-with-other-profiles">
<title>Criando imagens com outros perfis</title>
<para>Para criar perfis de imagens diferentes, é usada a opção de comando
"<emphasis role="strong">-p</emphasis>" no script auxiliar da imagem do
contêiner do Kiwi. Por exemplo, para criar a imagem ISO "<emphasis
role="strong">Default-SelfInstall</emphasis>":</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall
(...)</screen>
<note>
<para>Para evitar perda de dados, o Kiwi recusará a execução se houver imagens no
diretório <literal>output</literal>. É necessário remover o conteúdo do
diretório de saída antes de prosseguir com o <literal>rm -f
output/*</literal>.</para>
</note>
<para>Uma alternativa é criar a imagem ISO SelfInstall com o kernel RealTime
("<emphasis role="strong">kernel-rt</emphasis>"):</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Base-RT-SelfInstall
(...)</screen>
</section>
<section xml:id="id-building-images-with-large-sector-sizes">
<title>Criando imagens com tamanho de setor grande</title>
<para>Alguns modelos de hardware exigem imagens com tamanho de setor grande, ou
seja, <emphasis role="strong">4096 bytes</emphasis> em vez do padrão de 512
bytes. O construtor Kiwi conteinerizado permite gerar imagens com tamanho de
bloco grande especificando o parâmetro "<emphasis
role="strong">-b</emphasis>". Por exemplo, para criar a imagem "<emphasis
role="strong">Default-SelfInstall</emphasis>" com um tamanho de setor
grande:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall -b
(...)</screen>
</section>
<section xml:id="id-using-a-custom-kiwi-image-definition-file">
<title>Usando um arquivo de definição de imagem personalizado do Kiwi</title>
<para>Para casos de uso avançados, é possível usar um arquivo de definição de
imagem personalizado do Kiwi (<literal>SL-Micro.kiwi</literal>) junto com os
scripts necessários após a criação. Para isso, substitua as definições
padrão predefinidas pela equipe do SUSE Edge.</para>
<para>Crie e mapeie um novo diretório para a imagem do contêiner em que o script
auxiliar faz a busca (<literal>/micro-sdk/defs</literal>):</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/mydefs/
# cp /path/to/SL-Micro.kiwi ~/mydefs/
# cp /path/to/config.sh ~/mydefs/
# podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output -v ~/mydefs/:/micro-sdk/defs/ \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<warning>
<para>Isso é necessário apenas em casos de uso avançados e pode causar problemas
de suporte. Contate seu representante SUSE para receber mais conselhos e
orientações.</para>
</warning>
<para>Para acessar os arquivos de definição de imagem padrão do Kiwi incluídos no
contêiner, use os seguintes comandos:</para>
<screen language="console" linenumbering="unnumbered">$ podman create --name kiwi-builder registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi .
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi.4096 .
$ podman rm kiwi-builder
$ ls ./SL-Micro.*
(...)</screen>
</section>
</chapter>
<chapter xml:id="guides-clusterclass-example">
<title>Usando clusterclass para implantar clusters downstream</title>
<section xml:id="id-introduction">
<title>Introdução</title>
<para>O provisionamento de clusters Kubernetes é uma tarefa complexa que demanda
profunda experiência na configuração de componentes de cluster. À medida que
as configurações se tornam cada vez mais complexas, ou as demandas de
diversos provedores geram inúmeras definições de recursos específicas do
provedor, a criação de clusters pode ser um processo assustador. Felizmente,
a Kubernetes Cluster API (CAPI) oferece uma abordagem declarativa e mais
refinada que é aprimorada pelo ClusterClass. Esse recurso apresenta um
modelo orientado por gabarito que permite definir uma classe de cluster
reutilizável que encapsula a complexidade e promove a consistência.</para>
</section>
<section xml:id="id-what-is-clusterclass">
<title>O que é ClusterClass?</title>
<para>O projeto CAPI lançou o recurso ClusterClass como uma mudança de paradigma
no gerenciamento do ciclo de vida de clusters Kubernetes por meio da adoção
de uma metodologia baseada em gabarito para criação de instâncias de
cluster. Em vez de definir recursos separadamente para cada cluster, os
usuários definem um ClusterClass, que serve como um diagrama de referência
abrangente e reutilizável. Essa representação abstrata encapsula o estado e
a configuração desejados de um cluster Kubernetes, o que permite a criação
rápida e consistente de vários clusters que seguem as especificações
definidas. Essa abstração reduz a sobrecarga de configuração, resultando em
manifestos de implantação mais gerenciáveis. Isso significa que os
componentes principais de um cluster de carga de trabalho são definidos no
nível da classe, permitindo que os usuários recorram a esses gabaritos como
variantes de cluster Kubernetes que podem ser reutilizadas uma ou várias
vezes para provisionamento de clusters. A implementação do ClusterClass
oferece diversas vantagens importantes que abordam os desafios inerentes ao
gerenciamento tradicional da CAPI em escala:</para>
<itemizedlist>
<listitem>
<para>Redução significativa na complexidade e detalhamento do YAML</para>
</listitem>
<listitem>
<para>Processos otimizados de manutenção e atualização</para>
</listitem>
<listitem>
<para>Consistência e padronização aprimoradas nas implantações</para>
</listitem>
<listitem>
<para>Escalabilidade e recursos de automação aprimorados</para>
</listitem>
<listitem>
<para>Gerenciamento declarativo e controle de versão robusto</para>
</listitem>
</itemizedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="clusterclass.png" width="100%"/>
</imageobject>
<textobject><phrase>clusterclass</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-example-of-current-capi-provisioning-file">
<title>Exemplo de arquivo de provisionamento da CAPI atual</title>
<para>A implantação de um cluster Kubernetes por meio do provedor Cluster API
(CAPI) e RKE2 requer a definição de vários recursos personalizados. Esses
recursos definem o estado desejado do cluster e sua infraestrutura
subjacente, o que permite à CAPI orquestrar o provisionamento e o ciclo de
vida de gerenciamento. O trecho do código a seguir ilustra os tipos de
recursos que devem ser configurados:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Cluster</emphasis>: esse recurso encapsula as
configurações de alto nível, incluindo a topologia de rede que controla a
comunicação entre os nós e a descoberta de serviços. Ele também estabelece
vínculos essenciais com a especificação do plano de controle e o recurso do
provedor de infraestrutura designado e, desse modo, informa a CAPI sobre a
arquitetura desejada do cluster e a infraestrutura subjacente na qual ele
deverá ser provisionado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3Cluster</emphasis>: esse recurso define os
atributos no nível da infraestrutura exclusivos do Metal3, por exemplo, o
endpoint externo pelo qual o servidor da API Kubernetes estará acessível.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlane</emphasis>: esse recurso define as
características e o comportamento dos nós do plano de controle do
cluster. Nessa especificação, os parâmetros, como o número desejado de
réplicas do plano de controle (essencial para garantir alta disponibilidade
e tolerância a falhas), a versão da distribuição Kubernetes específica
(alinhada à versão do RKE2 selecionada) e a estratégia de distribuição das
atualizações para os componentes do plano de controle são configurados. Além
disso, esse recurso determina a interface de rede de contêiner (CNI,
Container Network Interface) que será usada dentro do cluster e facilita a
injeção das configurações específicas do agente, em geral aproveitando o
Ignition para provisionamento contínuo e automatizado dos agentes do RKE2
nos nós do plano de controle.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3MachineTemplate</emphasis>: esse recurso
funciona como um diagrama de referência para criação das instâncias de
computação individuais que formam os nós do worker do cluster Kubernetes
definindo a imagem que será usada.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3DataTemplate</emphasis>: como complemento ao
Metal3MachineTemplate, o recurso Metal3DataTemplate permite especificar
metadados adicionais para instâncias de máquina recém-provisionadas.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: emea-spa-cluster-3
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: emea-spa-cluster-3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  controlPlaneEndpoint:
    host: 192.168.122.203
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: emea-spa-cluster-3
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  registrationMethod: "control-plane-endpoint"
  registrationAddress: 192.168.122.203
  serverConfig:
    cni: cilium
    cniMultusEnable: true
    tlsSan:
      - 192.168.122.203
      - https://192.168.122.203.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - 192.168.122.203/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          node: group-3
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
</section>
<section xml:id="id-transforming-the-capi-provisioning-file-to-clusterclass">
<title>Transformando o arquivo de provisionamento da CAPI em ClusterClass</title>
<section xml:id="id-clusterclass-definition">
<title>Definição do ClusterClass</title>
<para>O código a seguir define o recurso ClusterClass, um gabarito declarativo
para implantação consistente de um tipo específico de cluster
Kubernetes. Essa especificação inclui as configurações comuns de
infraestrutura e de plano de controle, o que permite o provisionamento
eficiente e o gerenciamento do ciclo de vida uniforme de toda a frota de
clusters. Há algumas variáveis no exemplo abaixo do clusterclass que serão
substituídas pelos valores reais durante o processo de criação de instância
do cluster. As seguintes variáveis são usadas no exemplo:</para>
<itemizedlist>
<listitem>
<para><literal>controlPlaneMachineTemplate</literal>: esse é o nome para definir a
referência de gabarito da máquina do ControlPlane que será usada</para>
</listitem>
<listitem>
<para><literal>controlPlaneEndpointHost</literal>: esse é o nome de host ou
endereço IP do endpoint do plano de controle</para>
</listitem>
<listitem>
<para><literal>tlsSan</literal>: esse é o nome alternativo da entidade TLS para o
endpoint do plano de controle</para>
</listitem>
</itemizedlist>
<para>O arquivo de definição de clusterclass é definido com base nos três recursos
a seguir:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ClusterClass</emphasis>: esse recurso encapsula a
definição da classe do cluster inteira, incluindo os gabaritos de plano de
controle e de infraestrutura. Além disso, ele inclui a lista das variáveis
que serão substituídas durante o processo de criação de instâncias.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlaneTemplate</emphasis>: esse recurso
define o gabarito do plano de controle, especificando a configuração
desejada para os nós do plano de controle. Ele inclui os parâmetros, como
número de réplicas, versão do Kubernetes e CNI, que serão usados. Alguns
parâmetros também serão substituídos pelos valores corretos durante o
processo de criação de instâncias.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3ClusterTemplate</emphasis>: esse recurso
define o gabarito de infraestrutura, especificando a configuração desejada
da infraestrutura subjacente. Ele inclui parâmetros, como o endpoint do
plano de controle e o sinalizador noCloudProvider. Alguns parâmetros também
serão substituídos pelos valores corretos durante o processo de criação de
instâncias.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlaneTemplate
metadata:
  name: example-controlplane-type2
  namespace: emea-spa
spec:
  template:
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: example-controlplane    # This will be replaced by the patch applied in each cluster instances
        namespace: emea-spa
      replicas: 1
      version: v1.33.3+rke2r1
      rolloutStrategy:
        type: "RollingUpdate"
        rollingUpdate:
          maxSurge: 1
      registrationMethod: "control-plane-endpoint"
      registrationAddress: "default"  # This will be replaced by the patch applied in each cluster instances
      serverConfig:
        cni: cilium
        cniMultusEnable: true
        tlsSan:
          - "default"  # This will be replaced by the patch applied in each cluster instances
      agentConfig:
        format: ignition
        additionalUserData:
          config: |
            default
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3ClusterTemplate
metadata:
  name: example-cluster-template-type2
  namespace: emea-spa
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: "default"  # This will be replaced by the patch applied in each cluster instances
        port: 6443
      noCloudProvider: true
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: example-clusterclass-type2
  namespace: emea-spa
spec:
  variables:
    - name: controlPlaneMachineTemplate
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: controlPlaneEndpointHost
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: tlsSan
      required: true
      schema:
        openAPIV3Schema:
          type: array
          items:
            type: string
  infrastructure:
    ref:
      kind: Metal3ClusterTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: example-cluster-template-type2
  controlPlane:
    ref:
      kind: RKE2ControlPlaneTemplate
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      name: example-controlplane-type2
  patches:
    - name: setControlPlaneMachineTemplate
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/infrastructureRef/name"
              valueFrom:
                variable: controlPlaneMachineTemplate
    - name: setControlPlaneEndpoint
      definitions:
        - selector:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: Metal3ClusterTemplate
            matchResources:
              infrastructureCluster: true  # Added to select InfraCluster
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/controlPlaneEndpoint/host"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setRegistrationAddress
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/registrationAddress"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setTlsSan
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/serverConfig/tlsSan"
              valueFrom:
                variable: tlsSan
    - name: updateAdditionalUserData
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/agentConfig/additionalUserData"
              valueFrom:
                template: |
                  config: |
                    variant: fcos
                    version: 1.4.0
                    storage:
                      files:
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: endpoint-copier-operator
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                                targetNamespace: endpoint-copier-operator
                                version: 304.0.1+up0.3.0
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: metallb
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/metallb
                                targetNamespace: metallb-system
                                version: 304.0.0+up0.14.9
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: metallb.io/v1beta1
                              kind: IPAddressPool
                              metadata:
                                name: kubernetes-vip-ip-pool
                                namespace: metallb-system
                              spec:
                                addresses:
                                  - {{ .controlPlaneEndpointHost }}/32
                                serviceAllocation:
                                  priority: 100
                                  namespaces:
                                    - default
                                  serviceSelectors:
                                    - matchExpressions:
                                      - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                              ---
                              apiVersion: metallb.io/v1beta1
                              kind: L2Advertisement
                              metadata:
                                name: ip-pool-l2-adv
                                namespace: metallb-system
                              spec:
                                ipAddressPools:
                                  - kubernetes-vip-ip-pool
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: v1
                              kind: Service
                              metadata:
                                name: kubernetes-vip
                                namespace: default
                                labels:
                                  serviceType: kubernetes-vip
                              spec:
                                ports:
                                - name: rke2-api
                                  port: 9345
                                  protocol: TCP
                                  targetPort: 9345
                                - name: k8s-api
                                  port: 6443
                                  protocol: TCP
                                  targetPort: 6443
                                type: LoadBalancer
                    systemd:
                      units:
                        - name: rke2-preinstall.service
                          enabled: true
                          contents: |
                            [Unit]
                            Description=rke2-preinstall
                            Wants=network-online.target
                            Before=rke2-install.service
                            ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                            [Service]
                            Type=oneshot
                            User=root
                            ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                            ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                            ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                            ExecStartPost=/bin/sh -c "umount /mnt"
                            [Install]
                            WantedBy=multi-user.target</screen>
</section>
<section xml:id="id-cluster-instance-definition">
<title>Definição da instância do cluster</title>
<para>No contexto do ClusterClass, uma instância de cluster se refere à criação de
instância em execução específica de um cluster que foi criado com base no
ClusterClass definido. Isso representa a implantação concreta com suas
configurações, recursos e estado operacional exclusivos, derivados
diretamente do diagrama de referência especificado no ClusterClass. Isso
inclui o conjunto específico de máquinas, as configurações de rede e os
componentes associados do Kubernetes que estão ativamente em execução. Saber
o que é uma instância de cluster é essencial para gerenciar o ciclo de vida,
fazer upgrades, executar operações de ajuste de escala e monitorar um
determinado cluster implantado que foi provisionado usando a estrutura do
ClusterClass.</para>
<para>Para definir uma instância de cluster, precisamos definir os seguintes
recursos:</para>
<itemizedlist>
<listitem>
<para>Cluster</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate</para>
</listitem>
<listitem>
<para>Metal3DataTemplate</para>
</listitem>
</itemizedlist>
<para>As variáveis definidas no gabarito (arquivo de definição de clusterclass)
serão substituídas pelos valores finais para esta criação de instância do
cluster:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  topology:
    class: example-clusterclass-type2  # Correct way to reference ClusterClass
    version: v1.33.3+rke2r1
    controlPlane:
      replicas: 1
    variables:                         # Variables to be replaced for this cluster instance
      - name: controlPlaneMachineTemplate
        value: emea-spa-cluster-3-machinetemplate
      - name: controlPlaneEndpointHost
        value: 192.168.122.203
      - name: tlsSan
        value:
          - 192.168.122.203
          - https://192.168.122.203.sslip.io
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3-machinetemplate
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          cluster-type: type2
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine</screen>
<para>Essa abordagem permite um processo mais ágil, implantando um cluster com
apenas três recursos assim que você define o clusterclass.</para>
</section>
</section>
</chapter>
</part>
<part xml:id="tips-and-tricks">
<title>Dicas e truques</title>
<partintro>
<para>Dicas e truques para componentes do Edge</para>
</partintro>
<chapter xml:id="tips-edge-image-builder">
<title>Edge Image Builder</title>
<section xml:id="id-common">
<title>Comuns</title>
<itemizedlist>
<listitem>
<para>Se você está em um ambiente não Linux e seguindo estas instruções para criar
uma imagem, provavelmente está executando o <literal>Podman</literal> em uma
máquina virtual. Por padrão, essa máquina virtual está configurada para ter
uma pequena quantidade de recursos do sistema alocada para ela e pode
provocar instabilidade no <literal>Edge Image Builder</literal> durante
operações de uso intensivo de recursos, como o processo de resolução de
RPM. Será necessário ajustar os recursos da máquina do podman usando o
Podman Desktop (ícone de engrenagem de configurações → ícone de edição da
máquina do podman) ou diretamente pelo <link
xl:href="https://docs.podman.io/en/stable/markdown/podman-machine-set.1.html">comando</link>
<literal>podman-machine-set</literal>.</para>
</listitem>
<listitem>
<para>Neste momento, o <literal>Edge Image Builder</literal> não pode criar
imagens em uma configuração de arquitetura cruzada, ou seja, você deve
executá-lo em:</para>
<itemizedlist>
<listitem>
<para>Sistemas AArch64 (como Apple Silicon) para criar imagens
<literal>aarch64</literal> do SL Micro</para>
</listitem>
<listitem>
<para>Sistemas AMD64/Intel 64 para criar imagens <literal>x86_64</literal> do SL
Micro</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-kubernetes">
<title>Kubernetes</title>
<itemizedlist>
<listitem>
<para>A criação de clusters Kubernetes de vários nós requer o ajuste da seção
<literal>kubernetes</literal> no arquivo de definição para:</para>
<itemizedlist>
<listitem>
<para>listar todos os nós de servidor e de agente em
<literal>kubernetes.nodes</literal></para>
</listitem>
<listitem>
<para>definir um endereço IP virtual que será usado para que todos os nós não
inicializadores ingressem no cluster em
<literal>kubernetes.network.apiVIP</literal></para>
</listitem>
<listitem>
<para>(opcional) definir um host de API para especificar o endereço do domínio
para acessar o cluster em
<literal>kubernetes.network.apiHost</literal>. Para saber mais sobre essa
configuração, consulte os <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">documentos
da seção do Kubernetes</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>O <literal>Edge Image Builder</literal> se baseia nos nomes de host dos
diferentes nós para determinar o tipo de Kubernetes deles
(<literal>server</literal> ou <literal>agent</literal>). Essa configuração é
gerenciada no arquivo de definição, mas para a configuração geral de rede
das máquinas, podemos usar a configuração de DHCP, conforme descrito no
<xref linkend="components-nmc"/>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="tips-elemental">
<title>Elemental</title>
<section xml:id="id-common-2">
<title>Comuns</title>
<section xml:id="id-expose-rancher-service">
<title>Expor o serviço do Rancher</title>
<para>Ao usar o RKE2 ou K3s, precisamos expor os serviços (neste contexto, do
Rancher) do cluster de gerenciamento já que eles não são expostos por
padrão. No RKE2, há um controlador de entrada NGINX, enquanto o k3s usa o
Traefik. O fluxo de trabalho atual sugere o uso do MetalLB para anunciar um
serviço (anúncio por L2 ou BGP) e do respectivo controlador de entrada para
criar uma entrada usando <literal>HelmChartConfig</literal>, já que a
criação de um novo objeto de entrada substitui a configuração existente.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Instalar o Rancher Prime (pelo Helm) e configurar os valores necessários</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-192.168.64.101.sslip.io
replicas: 1
bootstrapPassword: Admin
global.cattle.psp.enabled: "false"</screen>
<tip>
<para>Consulte a documentação <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Instalação
do Rancher</link> (em inglês) para obter mais detalhes.</para>
</tip>
</listitem>
<listitem>
<para>Criar um serviço LoadBalancer para expor o Rancher</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local
EOF</screen>
</listitem>
<listitem>
<para>Criar um pool de endereços IP para o serviço usando o endereço IP que já
configuramos nos valores do Helm</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.64.101/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}
EOF</screen>
</listitem>
<listitem>
<para>Criar um anúncio por L2 para o pool de endereços IP</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ingress-ippool
EOF</screen>
</listitem>
<listitem>
<para>Garantir que o Elemental esteja devidamente instalado</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Instale o operador Elemental e a respectiva IU nos nós de gerenciamento.</para>
</listitem>
<listitem>
<para>Adicione a configuração do Elemental ao nó downstream junto com um código de
registro, pois isso solicitará que o Edge Image Builder inclua a opção de
registro remoto na máquina.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<tip>
<para>Consulte a <xref linkend="install-elemental"/> e a <xref
linkend="configure-elemental"/> para obter informações e exemplos
adicionais.</para>
</tip>
</section>
</section>
<section xml:id="id-hardware-specific">
<title>Específico do hardware</title>
<section xml:id="id-trusted-platform-module">
<title>Trusted Platform Module</title>
<para>É necessário gerenciar de maneira apropriada a configuração do <link
xl:href="https://elemental.docs.rancher.com/tpm/">Trusted Platform
Module</link> (TPM). Se isso não for feito, haverá erros semelhantes a
estes:</para>
<screen language="console" linenumbering="unnumbered">Nov 25 18:17:06 eled elemental-register[4038]: Error: registering machine: cannot generate authentication token: opening tpm for getting attestation data: TPM device not available</screen>
<para>Para mitigá-los, siga uma das abordagens abaixo:</para>
<itemizedlist>
<listitem>
<para>Habilitar o TPM nas configurações da máquina virtual</para>
</listitem>
</itemizedlist>
<para><emphasis>Exemplo com UTM no MacOS</emphasis></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="tpm.png" width="100%"/> </imageobject>
<textobject><phrase>TPM</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para>Emular o TPM usando um valor negativo para a semente do TPM no recurso
<literal>MachineRegistration</literal></para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: true
        emulated-tpm-seed: -1</screen>
<itemizedlist>
<listitem>
<para>Desabilitar o TPM no recurso <literal>MachineRegistration</literal></para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: false</screen>
</section>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>Integração de terceiros</title>
<partintro>
<para>Como integrar ferramentas de terceiros</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link xl:href="https://nats.io/">NATS</link> é uma tecnologia conectiva
projetada para o mundo cada vez mais hiperconectado. Trata-se de uma
tecnologia única que permite que os aplicativos se comuniquem de forma
segura com qualquer combinação de fornecedores de nuvem, ambientes locais e
dispositivos de borda, web e móveis. O NATS consiste em uma família de
produtos de código-fonte aberto estreitamente integrados, mas que podem ser
implantados com facilidade e de modo independente. O NATS é usado
mundialmente por milhares de empresas, atendendo a casos de uso como
microsserviços, computação de borda, dispositivos móveis e IoT, e pode ser
usado para expandir ou substituir o serviço de mensagens tradicional.</para>
<section xml:id="id-architecture">
<title>Arquitetura</title>
<para>O NATS é uma infraestrutura que permite a troca de dados entre aplicativos
na forma de mensagens.</para>
<section xml:id="id-nats-client-applications">
<title>Aplicativos clientes do NATS</title>
<para>É possível usar as bibliotecas de clientes do NATS para permitir que os
aplicativos publiquem, assinem, solicitem e respondam entre instâncias
diferentes. Esses aplicativos costumam ser chamados de <literal>aplicativos
clientes</literal>.</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>Infraestrutura de serviço do NATS</title>
<para>Os serviços do NATS são fornecidos por um ou mais processos do servidor
NATS, que são configurados para interconexão uns com os outros e oferecem a
infraestrutura de serviço do NATS, que pode ajustar a escala de um único
processo do servidor NATS executado em um dispositivo de endpoint para um
supercluster público global com muitos clusters, envolvendo todos os
principais provedores de nuvem de todas as regiões do mundo.</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>Design de mensagens simples</title>
<para>O NATS facilita a comunicação dos aplicativos ao enviar e receber
mensagens. Essas mensagens são endereçadas e identificadas por strings de
assunto e não dependem do local da rede. Os dados são codificados e
estruturados como uma mensagem e enviados por um publicador. A mensagem é
recebida, decodificada e processada por um ou mais subscritores.</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>O NATS tem um sistema de persistência distribuída incorporado chamado
JetStream. O JetStream foi criado para solucionar os problemas de
transmissão identificados nas tecnologias atuais: complexidade, fragilidade
e falta de escalabilidade. O JetStream também soluciona o problema de
acoplamento entre publicador e subscritor (os subscritores precisam estar em
funcionamento para receber a mensagem quando ela for publicada). Há mais
informações sobre o NATS JetStream <link
xl:href="https://docs.nats.io/nats-concepts/jetstream">aqui</link>.</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>Instalação</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>Instalando o NATS no K3s</title>
<para>O NATS foi projetado para várias arquiteturas para ser facilmente instalado
no K3s (<xref linkend="components-k3s"/>).</para>
<para>Vamos criar um arquivo de valores para substituir os valores padrão do NATS.</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>Agora vamos instalar o NATS pelo Helm:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>Com o arquivo <literal>values.yaml</literal> acima, os seguintes componentes
estarão no namespace <literal>nats</literal>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Versão de alta disponibilidade do NATS Statefulset com três contêineres:
servidor NATS, recarregador de configurações e sidecars de métricas.</para>
</listitem>
<listitem>
<para>Contêiner NATS box, que vem com um conjunto de utilitários do
<literal>NATS</literal> que são usados para verificar a configuração.</para>
</listitem>
<listitem>
<para>O JetStream também aproveita o back end de chave-valor incluído nos
<literal>PVCs</literal> vinculados aos pods.</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>Testando a configuração</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>Crie uma assinatura para a entidade de teste:</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>Envie uma mensagem para a entidade de teste:</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>Limpando</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>NATS como back end para K3s</title>
<para>Um componente que o K3s usa é o <link
xl:href="https://github.com/k3s-io/kine">KINE</link>, um shim que permite a
substituição do etcd por back ends de armazenamento alternativos
originalmente destinados a bancos de dados relacionais. Como o JetStream
fornece uma API de chave-valor, é possível usar o NATS como back end para o
cluster K3s.</para>
<para>Já existe uma PR mesclada que simplifica o NATS incorporado no K3s, mas a
alteração ainda <link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">não
está incluída</link> nas versões do K3s.</para>
<para>Por esse motivo, o binário do K3s deve ser criado manualmente.</para>
<section xml:id="id-building-k3s">
<title>Criando o K3s</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>O seguinte comando adiciona o <literal>nats</literal> às tags de build para
habilitar o recurso NATS incorporado no K3s:</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>Substitua &lt;node-ip&gt; pelo IP real do nó onde o K3s será iniciado:</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>A criação local do K3s requer o plug-in buildx Docker CLI. Ele pode ser
<link xl:href="https://github.com/docker/buildx#manual-download">manualmente
instalado</link> em caso de falha no <literal>$ make local</literal>.</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>Instalando a CLI do NATS</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>Executando o NATS como back end do K3s</title>
<para>Vamos acessar o nó por <literal>ssh</literal> e executar o K3s com o
sinalizador <literal>--datastore-endpoint</literal> apontando para
<literal>nats</literal>.</para>
<note>
<para>O comando a seguir inicia o K3s como um processo em primeiro plano, assim é
possível acompanhar os registros facilmente para ver se há problemas. Para
não bloquear o terminal atual, adicione o sinalizador
<literal>&amp;</literal> antes do comando para iniciá-lo como um processo em
segundo plano.</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>Para tornar o servidor K3s com o NATS como back end permanente em sua VM
<literal>slemicro</literal>, execute o script abaixo para criar um serviço
<literal>systemd</literal> com as configurações necessárias.</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>Resolução de Problemas</title>
<para>É possível executar os seguintes comandos no nó para verificar se tudo
funciona corretamente com o fluxo:</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-suse-linux-micro">
<title>GPUs NVIDIA no SUSE Linux Micro</title>
<section xml:id="id-intro-2">
<title>Introdução</title>
<para>Este guia demonstra como implementar o suporte à GPU NVIDIA no nível do host
usando os <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">drivers de
código-fonte aberto</link> predefinidos no SUSE Linux Micro 6.1. Esses
drivers são integrados ao sistema operacional, e não dinamicamente
carregados pelo NVIDIA <link
xl:href="https://github.com/NVIDIA/gpu-operator">GPU Operator</link>. Essa
configuração é altamente aconselhável para clientes que desejam fazer bake
prévio de todos os artefatos necessários para implantação na imagem, e
quando não é um requisito a seleção dinâmica da versão do driver, ou seja, o
usuário ter que selecionar a versão do driver pelo Kubernetes. Inicialmente,
este guia explica como implantar componentes adicionais em um sistema que já
tenha sido pré-implantado, mas prossegue com uma seção que descreve como
incorporar essa configuração à implantação inicial por meio do Edge Image
Builder. Se você não quer passar pelas etapas básicas e configurar tudo
manualmente, avance direto para essa seção.</para>
<para>É importante destacar que o suporte a esses drivers é oferecido pela SUSE e
NVIDIA em estreita colaboração, sendo que os drivers são criados e
distribuídos pela SUSE como parte dos repositórios de pacotes. No entanto,
se você tiver qualquer problema ou dúvida sobre a combinação dos drivers
usada, contate os gerentes de conta da SUSE ou NVIDIA para receber mais
ajuda. Se você pretende usar o <link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE), certifique-se de que tenha uma <link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">GPU
certificada para NVAIE</link>, o que <emphasis>pode</emphasis> exigir o uso
de drivers proprietários da NVIDIA. Se estiver em dúvida, fale com seu
representante NVIDIA.</para>
<para>Este guia <emphasis>não</emphasis> aborda mais informações sobre a
integração do NVIDIA GPU Operator. A integração do NVIDIA GPU Operator para
Kubernetes não é explicada aqui, mas você ainda pode seguir a maioria das
etapas neste guia para configurar o sistema operacional subjacente e
simplesmente permitir que o GPU Operator use os drivers
<emphasis>pré-instalados</emphasis> por meio do sinalizador
<literal>driver.enabled=false</literal> no gráfico Helm do NVIDIA GPU
Operator, que apenas vai selecionar os drivers instalados no host. A NVIDIA
apresenta instruções mais detalhadas <link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">aqui</link>.</para>
</section>
<section xml:id="id-prerequisites-12">
<title>Pré-requisitos</title>
<para>Se você está seguindo este guia, já deve ter os seguintes itens disponíveis:</para>
<itemizedlist>
<listitem>
<para>No mínimo, um host com o SUSE Linux Micro 6.1 instalado, que pode ser físico
ou virtual.</para>
</listitem>
<listitem>
<para>Seus hosts são anexados a uma assinatura porque isso é exigido para acesso
ao pacote. Há uma avaliação disponível <link
xl:href="https://www.suse.com/download/sle-micro/">aqui</link>.</para>
</listitem>
<listitem>
<para>Uma <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">GPU
NVIDIA compatível</link> instalada (ou <emphasis>completamente</emphasis>
transferida para a máquina virtual na qual o SUSE Linux Micro está em
execução).</para>
</listitem>
<listitem>
<para>Acesso ao usuário root. Nestas instruções, consideramos você como usuário
root, <emphasis>sem</emphasis> escalar seus privilégios por meio do
<literal>sudo</literal>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>Instalação manual</title>
<para>Nesta seção, você vai instalar os drivers da NVIDIA diretamente no sistema
operacional SUSE Linux Micro, já que o driver de código aberto da NVIDIA
agora faz parte dos repositórios de pacotes principais do SUSE Linux Micro,
o que o torna tão fácil de instalar quanto os pacotes RPM obrigatórios. Não
há necessidade de compilação nem download de pacotes executáveis. Veja a
seguir como implantar a geração "G06" do driver, que oferece suporte às GPUs
mais recentes (consulte <link
xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">aqui</link>
para obter mais informações). Sendo assim, selecione a geração do driver
apropriada à GPU NVIDIA do seu sistema. Para GPUs modernas, o driver "G06" é
a opção mais comum.</para>
<para>Antes de começar, é importante reconhecer que, além do driver de código
aberto da NVIDIA que a SUSE distribui como parte do SUSE Linux Micro, você
precisa de componentes NVIDIA adicionais para sua configuração. Eles podem
ser bibliotecas OpenGL, kits de ferramentas CUDA, utilitários de linha de
comando, como <literal>nvidia-smi</literal>, e componentes de integração de
contêiner, como <literal>nvidia-container-toolkit</literal>. Muitos desses
componentes não são distribuídos pela SUSE porque são softwares
proprietários da NVIDIA, ou porque não faz sentido distribuí-los no lugar da
NVIDIA. Portanto, como parte das instruções, vamos configurar repositórios
adicionais para termos acesso a esses componentes e percorrer alguns
exemplos de como usar essas ferramentas para obter um sistema totalmente
funcional. É importante diferenciar entre os repositórios SUSE e NVIDIA, já
que pode haver incompatibilidade entre as versões dos pacotes que a NVIDIA
disponibiliza e as que a SUSE compilou. Geralmente, isso acontece quando a
SUSE cria uma nova versão do driver de código aberto disponível e leva
alguns dias até que os pacotes equivalentes sejam disponibilizados nos
repositórios NVIDIA para compatibilidade.</para>
<para>A recomendação é garantir que você selecione uma versão do driver compatível
com a GPU e que atenda aos possíveis requisitos de CUDA verificando o
seguinte:</para>
<itemizedlist>
<listitem>
<para>As <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">Notas de
lançamento de CUDA</link></para>
</listitem>
<listitem>
<para>A versão do driver que você pretende implantar tem uma versão correspondente
no <link
xl:href="https://download.nvidia.com/suse/sle15sp6/x86_64/">repositório
NVIDIA</link> e é certeza de que há versões de pacote equivalentes para os
componentes de suporte disponíveis</para>
</listitem>
</itemizedlist>
<tip>
<para>Para encontrar as versões de driver de código aberto da NVIDIA, execute
<literal>zypper se -s nvidia-open-driver</literal> na máquina de destino
<emphasis>ou</emphasis> pesquise no SUSE Customer Center por
"nvidia-open-driver" no <link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.1&amp;arch=x86_64">SUSE
Linux Micro 6.1 para AMD64/Intel 64</link>.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width="100%"/>
</imageobject>
<textobject><phrase>SUSE Customer Center</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>Depois que você confirmar a disponibilidade de uma versão equivalente nos
repositórios NVIDIA, estará pronto para instalar os pacotes no sistema
operacional host. Para isso, precisamos abrir uma sessão
<literal>transactional-update</literal>, que cria um novo instantâneo de
leitura/gravação do sistema operacional subjacente para que possamos fazer
alterações na plataforma imutável (para obter mais instruções sobre
<literal>transactional-update</literal>, consulte <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-transactional-updates/transactional-updates.html">aqui</link>):</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>Quando estiver no shell <literal>transactional-update</literal>, adicione
outro repositório de pacotes da NVIDIA para que possamos obter os
utilitários adicionais, por exemplo, <literal>nvidia-smi</literal>:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-suse-main
zypper --gpg-auto-import-keys refresh</screen>
<para>Depois disso, você poderá instalar o driver e
<literal>nvidia-compute-utils</literal> para os utilitários adicionais. Se
os utilitários não forem necessários, você poderá omiti-los. No entanto, é
importante instalá-los neste estágio para fins de teste:</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>Se houver falha na instalação, poderá ser indicativo de uma
incompatibilidade de dependência entre a versão do driver selecionada e a
que a NVIDIA distribui em seus repositórios. Consulte a seção anterior para
verificar se as suas versões são compatíveis. Tente instalar outra versão do
driver. Por exemplo, se os repositórios NVIDIA tiverem uma versão mais
antiga, tente especificar
<literal>nvidia-open-driver-G06-signed-kmp=550.54.14</literal> em seu
comando de instalação para especificar uma versão equivalente.</para>
</note>
<para>Em seguida, se você <emphasis>não</emphasis> usa uma GPU compatível
(lembrando que a lista está disponível <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">aqui</link>),
pode conferir se o driver funciona habilitando o suporte no nível do módulo,
mas a sua milhagem poderá variar. Pule esta etapa se você usa uma GPU
<emphasis>compatível</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>Agora que você instalou os pacotes, saia da sessão
<literal>transactional-update</literal>:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>Antes de prosseguir, garanta que já tenha saído da sessão
<literal>transactional-update</literal>.</para>
</note>
<para>Agora que você instalou os drivers, faça a reinicialização. Como o SUSE
Linux Micro é um sistema operacional imutável, ele precisa ser
reinicializado no novo instantâneo criado na etapa anterior. Os drivers são
instalados apenas nesse novo instantâneo, já que não é possível carregá-los
sem a reinicialização nele, o que é feito automaticamente. Execute o comando
de reinicialização quando estiver pronto:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>Após a reinicialização bem-sucedida do sistema, faça login novamente e use a
ferramenta <literal>nvidia-smi</literal> para verificar se o driver foi
carregado com sucesso e pode acessar e enumerar as GPUs:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>A saída desse comando deve ser similar à mostrada abaixo, levando em
consideração que temos duas GPUs no exemplo a seguir:</para>
<screen language="shell" linenumbering="unnumbered">+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>E assim concluímos o processo de instalação e verificação dos drivers da
NVIDIA no sistema SUSE Linux Micro.</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>Validação adicional da instalação manual</title>
<para>Neste estágio, a única coisa que podemos verificar é que, no nível do host,
é possível acessar o dispositivo NVIDIA e os drivers são carregados com
sucesso. No entanto, para garantir que tudo esteja funcionando, há um teste
simples para comprovar que a GPU pode receber as instruções de um aplicativo
no espaço do usuário, de preferência por meio de um contêiner, e pela
biblioteca CUDA, que é o mais comum para uma carga de trabalho real. Para
isso, é possível fazer outra modificação no sistema operacional host
instalando o <literal>nvidia-container-toolkit</literal> (<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container Toolkit</link>). Em primeiro lugar, abra outro shell
<literal>transactional-update</literal>, lembrando que talvez isso já tenha
sido feito em uma transação única na etapa anterior, e veja como fazer isso
de maneira totalmente automatizada em uma seção posterior:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>Em seguida, instale o pacote <literal>nvidia-container-toolkit</literal> do
repositório NVIDIA Container Toolkit:</para>
<itemizedlist>
<listitem>
<para>O <literal>nvidia-container-toolkit.repo</literal> a seguir contém um
repositório estável (<literal>nvidia-container-toolkit</literal>) e outro
experimental (<literal>nvidia-container-toolkit-experimental</literal>). O
repositório estável é recomendado para uso em produção. O repositório
experimental está desabilitado por padrão.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>Quando estiver pronto, saia do shell
<literal>transactional-update</literal>:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>…​e reinicie a máquina em um novo instantâneo:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>Como antes, você precisa garantir que saiu do
<literal>transactional-shell</literal> e reinicializou a máquina para que as
alterações tenham efeito.</para>
</note>
<para>Depois que a máquina for reinicializada, verifique se o sistema pode
enumerar corretamente os dispositivos usando o NVIDIA Container Toolkit. A
saída deve ser detalhada, com mensagens INFO e WARN, mas sem mensagens
ERROR:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>Esse procedimento garante que os contêineres iniciados na máquina possam
usar os dispositivos de GPU NVIDIA que foram descobertos. Quando estiver
pronto, você poderá executar um contêiner baseado em podman. Usar o
<literal>podman</literal> é uma boa forma de validar o acesso ao dispositivo
NVIDIA de dentro de um contêiner, o que traz segurança para fazer o mesmo
com o Kubernetes no futuro. Conceda ao <literal>podman</literal> acesso para
os dispositivos NVIDIA identificados que foram processados pelo comando
anterior, com base na <link
xl:href="https://registry.suse.com/repositories/bci-bci-base-15sp6">SLE
BCI</link>, e simplesmente execute o comando Bash:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>Agora você executará os comandos de um contêiner podman temporário. Ele não
tem acesso ao seu sistema subjacente e é efêmero, portanto, o que quer que
se faça nele não vai persistir, e você não poderá danificar nada no host
subjacente. Como estamos em um contêiner, podemos instalar as bibliotecas
CUDA necessárias; mais uma vez, verificando a versão correta do CUDA para
seu driver <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">aqui</link>,
embora a saída anterior de <literal>nvidia-smi</literal> deva mostrar a
versão necessária do CUDA. No exemplo a seguir, vamos instalar o
<emphasis>CUDA 12.3</emphasis> e extrair muitos exemplos, demonstrações e
kits de desenvolvimento para você validar o GPU na íntegra:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-suse
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>Após a devida instalação, não saia do contêiner. Executaremos o exemplo
<literal>deviceQuery</literal> de CUDA, que valida de maneira completa o
acesso da GPU por CUDA, e de dentro do próprio contêiner:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>Se tudo correr bem, você verá uma saída como esta mostrada a seguir, com
destaque para a mensagem <literal>Result = PASS</literal> no fim do comando
e para a identificação correta das duas GPUs feita pelo sistema,
considerando que seu ambiente pode ter apenas uma:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>A partir deste ponto, você pode continuar executando qualquer outra carga de
trabalho CUDA. Use os compiladores e qualquer outro elemento do ecossistema
CUDA para realizar mais testes. Ao concluir, saia do contêiner, lembrando
que tudo o que você instalou nele é efêmero (será perdido!) e não afeta o
sistema operacional subjacente:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Implementação com o Kubernetes</title>
<para>Agora que comprovamos a instalação e o uso do driver de código aberto da
NVIDIA no SUSE Linux Micro, vamos explorar a configuração do Kubernetes na
mesma máquina. Este guia não orienta na implantação do Kubernetes, mas ele
considera que você já tenha o <link xl:href="https://k3s.io/">K3s</link> ou
o <link xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>
instalado e que o kubeconfig esteja devidamente configurado, de modo que os
comandos <literal>kubectl</literal> padrão possam ser executados como
superusuário. Supomos que o seu nó forme um cluster de nó único, embora as
etapas principais sejam similares nos clusters de vários nós. Primeiramente,
garanta que o acesso ao <literal>kubectl</literal> esteja funcionando:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>Esse comando deve retornar algo semelhante ao seguinte:</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.33.3+rke2r1</screen>
<para>Você deve descobrir que a instalação do k3s/rke2 detectou o NVIDIA Container
Toolkit no host e configurou automaticamente a integração do runtime NVIDIA
ao <literal>containerd</literal> (a interface de tempo de execução do
contêiner que o k3s/rke2 usa). Para confirmar isso, verifique o arquivo
<literal>config.toml</literal> de containerd:</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>Isso deve retornar algo semelhante ao que está mostrado a seguir. O local
equivalente do K3s é
<literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>:</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>Se estas entradas não estiverem presentes, talvez a detecção tenha
falhado. O motivo pode ser a máquina ou os serviços do Kubernetes que não
foram reiniciados. Se necessário, adicione-os manualmente conforme explicado
acima.</para>
</note>
<para>Depois disso, precisamos configurar o <literal>RuntimeClass</literal> NVIDIA
como runtime do Kubernetes adicional ao padrão, garantindo que as
solicitações dos usuários para pods que precisam de acesso à GPU possam usar
o NVIDIA Container Toolkit para fazer isso, por meio do comando
<literal>nvidia-container-runtime</literal>, conforme definido na
configuração de <literal>containerd</literal>:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>A próxima etapa é configurar o <link
xl:href="https://github.com/NVIDIA/k8s-device-plugin">plug-in de dispositivo
NVIDIA</link>, que configura o Kubernetes para aproveitar as GPUs NVIDIA
como recursos no cluster que pode ser usado, trabalhando em conjunto com o
NVIDIA Container Toolkit. Inicialmente, essa ferramenta detecta todos os
recursos no host subjacente, incluindo GPUs, drivers e outros (como GL), e
permite solicitar recursos para a GPU e consumi-los como parte dos seus
aplicativos.</para>
<para>Primeiro, você precisa adicionar e atualizar o repositório Helm para o
plug-in de dispositivo NVIDIA:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>Agora você pode instalar o plug-in de dispositivo NVIDIA:</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>Após alguns minutos, você verá um novo pod em execução que concluirá a
detecção em seus nós disponíveis e os marcará com o número de GPUs que foram
detectadas:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>Agora você está pronto para criar um pod NVIDIA que tenta usar essa
GPU. Vamos tentar com o contêiner de benchmark CUDA:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>Se tudo correu bem, você pode consultar os registros para ver as informações
de benchmark:</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
        -fullscreen       (run n-body simulation in fullscreen mode)
        -fp64             (use double precision floating point values for simulation)
        -hostmem          (stores simulation data in host memory)
        -benchmark        (run benchmark to measure performance)
        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
        -compare          (compares simulation results running once on the default GPU and once on the CPU)
        -cpu              (run n-body simulation on the CPU)
        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>Por fim, se os seus aplicativos exigem OpenGL, instale as bibliotecas OpenGL
da NVIDIA necessárias no nível do host; e o plug-in de dispositivo NVIDIA e
o NVIDIA Container Toolkit podem torná-las disponíveis aos contêineres. Para
isso, instale o pacote da seguinte maneira:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>Você precisa reinicializar para disponibilizar esse pacote aos seus
aplicativos. O plug-in de dispositivo NVIDIA deve fazer essa nova detecção
automaticamente por meio do NVIDIA Container Toolkit.</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Reunindo tudo com o Edge Image Builder</title>
<para>Você demonstrou a total funcionalidade dos aplicativos e das GPUs no SUSE
Linux Micro e agora quer usar o <xref linkend="components-eib"/> para
disponibilizar tudo em uma imagem de disco ISO ou RAW
implantável/consumível. Este guia não explica como usar o Edge Image
Builder, mas aborda as configurações necessárias para criar essa
imagem. Veja a seguir um exemplo de definição de imagem, junto com os
arquivos de configuração necessários do Kubernetes, para garantir que todos
os componentes obrigatórios sejam implantados imediatamente. Esta é a
estrutura de diretórios do Edge Image Builder para o exemplo apresentado
abaixo:</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>Vamos explorar esses arquivos. Primeiro, esta é uma definição da imagem de
amostra para um cluster de nó único com o K3s e que também implanta os
utilitários e os pacotes OpenGL (<literal>eib-config-iso.yaml</literal>):</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: [snip]
kubernetes:
  version: v1.33.3+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>Este é apenas um exemplo. Você pode precisar personalizá-lo de acordo com os
seus requisitos e expectativas. Além disso, se você usa o SUSE Linux Micro,
precisa inserir seu próprio <literal>sccRegistrationCode</literal> para
resolver as dependências de pacotes e obter os drivers da NVIDIA.</para>
</note>
<para>Além disso, precisamos adicionar outros componentes para que sejam
carregados pelo Kubernetes no momento da inicialização. O diretório do EIB
precisa primeiro de um diretório <literal>kubernetes</literal>, com os
subdiretórios para configuração, os valores dos gráficos Helm e todos os
manifestos adicionais necessários:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>Agora vamos definir a configuração (opcional) do Kubernetes escolhendo uma
CNI (que assume o Cilium como padrão se nada for selecionado) e habilitando
o SELinux:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>Agora garanta que o RuntimeClass NVIDIA tenha sido criado no cluster
Kubernetes:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>Usamos o Helm Controller incorporado para implantar o plug-in de dispositivo
NVIDIA pelo próprio Kubernetes. Vamos inserir uma classe de runtime no
arquivo de valores do gráfico:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>Precisamos obter a chave pública do RPM do NVIDIA Container Toolkit antes de
continuar:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>Todos os artefatos necessários, incluindo o binário do Kubernetes, as
imagens do contêiner, os gráficos Helm (e todas as imagens referenciadas),
serão automaticamente isolados. Isso significa que, por padrão, o sistema
não deve exigir conectividade de Internet no momento da implantação. Agora
você precisa apenas obter a ISO do SUSE Linux Micro da <link
xl:href="https://www.suse.com/download/sle-micro/">página de downloads da
SUSE</link> (e salvá-la no diretório <literal>base-images</literal>) e
chamar a ferramenta Edge Image Builder para gerar a ISO para você. Para
concluir o exemplo, este é o comando que foi usado para criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-config-iso.yaml</screen>
<para>Para obter mais instruções, consulte a <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">documentação</link>
(em inglês) do Edge Image Builder.</para>
</section>
<section xml:id="id-resolving-issues">
<title>Resolvendo problemas</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smi não encontra a GPU</title>
<para>Verifique as mensagens do kernel usando o <literal>dmesg</literal>. Se isso
indicar que não é possível alocar o <literal>NvKMSKapDevice</literal>, adote
a solução alternativa para a GPU sem suporte:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>NOTA</emphasis>: Será necessário recarregar, ou reinicializar, o
módulo do kernel se você alterar a configuração dele na etapa anterior para
que as alterações entrem em vigor.</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="day-2-operations">
<title>Operações de dia 2</title>
<partintro>
<para>Esta seção explica como os administradores podem realizar as tarefas de
operações de "dia 2" nos clusters de gerenciamento e downstream.</para>
</partintro>
<chapter xml:id="day2-migration">
<title>Migração para o Edge 3.4</title>
<para>Esta seção explica como migrar os clusters de
<literal>gerenciamento</literal> e <literal>downstream</literal> do
<literal>Edge 3.3</literal> para o <literal>Edge 3.4.0</literal>.</para>
<important>
<para>Sempre faça as migrações de cluster da versão do <literal>Z-stream mais
recente</literal> do <literal>Edge 3.3</literal>.</para>
<para>Migre sempre para a versão <literal>Edge 3.4.0</literal>. Para as próximas
atualizações após a migração, consulte as seções de cluster de gerenciamento
(<xref linkend="day2-mgmt-cluster"/>) e downstream (<xref
linkend="day2-downstream-clusters"/>).</para>
</important>
<section xml:id="day2-migration-mgmt">
<title>Cluster de gerenciamento</title>
<para>Esta seção aborda os seguintes tópicos:</para>
<para><xref linkend="day2-migration-mgmt-prereq"/>: etapas de pré-requisito para
concluir antes de iniciar a migração.</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller"/>: como migrar um
cluster de <literal>gerenciamento</literal> usando o <xref
linkend="components-upgrade-controller"/>.</para>
<para><xref linkend="day2-migration-mgmt-fleet"/>: como migrar um cluster de
<literal>gerenciamento</literal> usando o <xref
linkend="components-fleet"/>.</para>
<section xml:id="day2-migration-mgmt-prereq">
<title>Pré-requisitos</title>
<section xml:id="id-upgrade-the-bare-metal-operator-crds">
<title>Fazer upgrade das CRDs do Bare Metal Operator</title>
<note>
<para>Aplica-se apenas a clusters que exigem upgrade do gráfico do <xref
linkend="components-metal3"/>.</para>
</note>
<para>O gráfico Helm do <literal>Metal<superscript>3</superscript></literal>
inclui as CRDs do <link
xl:href="https://book.metal3.io/bmo/introduction.html">Bare Metal Operator
(BMO)</link> aproveitando o diretório <link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you">CRD</link>
do Helm.</para>
<para>Entretanto, essa abordagem tem algumas limitações, especificamente a
incapacidade de fazer upgrade das CRDs nesse diretório usando o Helm. Para
obter mais informações, consulte a <link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations">documentação
do Helm</link>.</para>
<para>Como resultado, antes de fazer upgrade do Metal<superscript>3</superscript>
para uma versão compatível com o <literal>Edge 3.4.0</literal>, os usuários
devem fazer o upgrade manual das CRDs do BMO subjacentes.</para>
<para>Em uma máquina com o <literal>Helm</literal> instalado e o
<literal>kubectl</literal> configurado para apontar para o cluster de
<literal>gerenciamento</literal>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Aplique manualmente as CRDs do BMO:</para>
<screen language="bash" linenumbering="unnumbered">helm show crds oci://registry.suse.com/edge/charts/metal3 --version 304.0.16+up0.12.6 | kubectl apply -f -</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller">
<title>Controller de upgrade</title>
<important>
<para>O <literal>Controller de upgrade</literal> oferece suporte a migrações de
versão do Edge somente para clusters de <emphasis
role="strong">gerenciamento não air-gapped</emphasis>.</para>
</important>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-prereq"/>:
pré-requisitos específicos para o <literal>Controller de upgrade</literal>.</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-migration"/>: etapas
para migrar o cluster de <literal>gerenciamento</literal> para uma nova
versão do Edge usando o <literal>Controller de upgrade</literal>.</para>
<section xml:id="day2-migration-mgmt-upgrade-controller-prereq">
<title>Pré-requisitos</title>
<section xml:id="id-edge-3-4-upgrade-controller">
<title>Controller de upgrade do Edge 3.4</title>
<para>Antes de usar o <literal>Controller de upgrade</literal>, verifique se ele
executa uma versão com capacidade de migração para o lançamento desejado do
Edge.</para>
<para>Para fazer isso:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Se você já tem o <literal>Controller de upgrade</literal> implantado de uma
versão anterior do Edge, faça upgrade do respectivo gráfico:</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade upgrade-controller -n upgrade-controller-system oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1</screen>
</listitem>
<listitem>
<para>Se você <emphasis role="strong">não</emphasis> tem o <literal>Controller de
upgrade</literal> implantado, siga a <xref
linkend="components-upgrade-controller-installation"/>.</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller-migration">
<title>Etapas de migração</title>
<para>A execução da migração de um cluster de <literal>gerenciamento</literal> com
o <literal>Controller de upgrade</literal> é basicamente similar à execução
de um upgrade.</para>
<para>A única diferença é que o seu <literal>UpgradePlan</literal> <emphasis
role="strong">deve</emphasis> especificar a versão de lançamento
<literal>3.4.0</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  # Change to the namespace of your Upgrade Controller
  namespace: CHANGE_ME
spec:
  releaseVersion: 3.4.0</screen>
<para>Para obter informações sobre como usar o <literal>UpgradePlan</literal>
acima para fazer uma migração, consulte Processo de upgrade do Controller de
upgrade (<xref linkend="management-day2-upgrade-controller"/>).</para>
</section>
</section>
<section xml:id="day2-migration-mgmt-fleet">
<title>Fleet</title>
<note>
<para>Sempre que possível, siga as instruções da <xref
linkend="day2-migration-mgmt-upgrade-controller"/> para a migração.</para>
<para>Consulte esta seção apenas para casos de uso não cobertos pelo
<literal>Controller de upgrade</literal>.</para>
</note>
<para>A execução da migração de um cluster de <literal>gerenciamento</literal> com
o <literal>Fleet</literal> é basicamente similar a de um upgrade.</para>
<para>As <emphasis role="strong">principais</emphasis> diferenças são:</para>
<orderedlist numeration="arabic">
<listitem>
<para>As instâncias do Fleet <emphasis role="strong">devem ser usadas</emphasis> a
partir da versão <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link>
do repositório <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>O upgrade dos gráficos agendados <emphasis role="strong">deve</emphasis> ser
feito para versões compatíveis com o <literal>Edge 3.4.0</literal>. Para ver
a lista de componentes do <literal>Edge 3.4.0</literal>, consulte a <xref
linkend="release-notes-3-4-0"/>.</para>
</listitem>
</orderedlist>
<important>
<para>Para garantir a migração bem-sucedida do <literal>Edge 3.4.0</literal>, é
importante que os usuários sigam os pontos descritos anteriormente.</para>
</important>
<para>Considerando os pontos acima, os usuários podem seguir a documentação do
Fleet sobre cluster de <literal>gerenciamento</literal> (<xref
linkend="management-day2-fleet"/>) para obter um guia completo das etapas
necessárias à migração.</para>
</section>
</section>
<section xml:id="day2-migration-downstream">
<title>Clusters downstream</title>
<para><xref linkend="day2-migration-downstream-fleet"/>: como migrar um cluster
<literal>downstream</literal> usando o <xref linkend="components-fleet"/>.</para>
<section xml:id="day2-migration-downstream-fleet">
<title>Fleet</title>
<para>A execução da migração de um cluster <literal>downstream</literal> com o
<literal>Fleet</literal> é basicamente similar a de um upgrade.</para>
<para>As <emphasis role="strong">principais</emphasis> diferenças são:</para>
<orderedlist numeration="arabic">
<listitem>
<para>As instâncias do Fleet <emphasis role="strong">devem ser usadas</emphasis> a
partir da versão <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link>
do repositório <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>O upgrade dos gráficos agendados <emphasis role="strong">deve</emphasis> ser
feito para versões compatíveis com o <literal>Edge 3.4.0</literal>. Para ver
a lista de componentes do <literal>Edge 3.4.0</literal>, consulte a <xref
linkend="release-notes-3-4-0"/>.</para>
</listitem>
</orderedlist>
<important>
<para>Para garantir a migração bem-sucedida do <literal>Edge 3.4.0</literal>, é
importante que os usuários sigam os pontos descritos anteriormente.</para>
</important>
<para>Considerando os pontos acima, os usuários podem seguir a documentação do
Fleet sobre cluster <literal>downstream</literal> (<xref
linkend="downstream-day2-fleet"/>) para obter um guia completo das etapas
necessárias à migração.</para>
</section>
</section>
</chapter>
<chapter xml:id="day2-mgmt-cluster">
<title>Cluster de gerenciamento</title>
<para>Atualmente, há duas maneiras de realizar as operações de "dia 2" no cluster
de <literal>gerenciamento</literal>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Usando o <xref linkend="components-upgrade-controller"/>: <xref
linkend="management-day2-upgrade-controller"/></para>
</listitem>
<listitem>
<para>Usando o <xref linkend="components-fleet"/>: <xref
linkend="management-day2-fleet"/></para>
</listitem>
</orderedlist>
<section xml:id="management-day2-upgrade-controller">
<title>Controller de upgrade</title>
<important>
<para>O <literal>Controller de upgrade</literal> oferece suporte apenas a
operações de <literal>dia 2</literal> em clusters de <emphasis
role="strong">gerenciamento não air-gapped</emphasis>.</para>
</important>
<para>Esta seção explica como realizar as diversas operações de <literal>dia
2</literal> relacionadas ao upgrade do cluster de
<literal>gerenciamento</literal> de uma versão da plataforma Edge para
outra.</para>
<para>O Controller de upgrade (<xref linkend="components-upgrade-controller"/>)
automatiza as operações de <literal>dia 2</literal>, o que inclui:</para>
<itemizedlist>
<listitem>
<para>Upgrade de sistema operacional SUSE Linux Micro (<xref
linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Upgrade de Kubernetes com <xref linkend="components-rke2"/> ou <xref
linkend="components-k3s"/></para>
</listitem>
<listitem>
<para>Upgrade de componentes adicionais do SUSE (SUSE Rancher Prime, SUSE Security
etc.)</para>
</listitem>
</itemizedlist>
<section xml:id="id-prerequisites-13">
<title>Pré-requisitos</title>
<para>Antes de fazer upgrade do cluster de <literal>gerenciamento</literal>, é
necessário cumprir os seguintes pré-requisitos:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Nós registrados do SCC</literal>: assegure que os nós do cluster do
seu sistema operacional sejam registrados com uma chave de assinatura
compatível com a versão de sistema operacional especificada no lançamento do
Edge (<xref linkend="release-notes"/>) para o qual você pretende fazer
upgrade.</para>
</listitem>
<listitem>
<para><literal>Controller de upgrade</literal>: garanta que o <literal>Controller
de upgrade</literal> tenha sido implantado no cluster de
<literal>gerenciamento</literal>. Para ver as etapas de instalação, consulte
a <xref linkend="components-upgrade-controller-installation"/>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade">
<title>Atualizar</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine a versão de lançamento do Edge (<xref linkend="release-notes"/>)
para a qual você quer fazer upgrade do seu cluster de
<literal>gerenciamento</literal>.</para>
</listitem>
<listitem>
<para>No cluster de <literal>gerenciamento</literal>, implante um
<literal>UpgradePlan</literal> que especifique a <literal>versão de
lançamento</literal> desejada. É necessário implantar o
<literal>UpgradePlan</literal> no namespace do <literal>Controller de
upgrade</literal>.</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n &lt;upgrade_controller_namespace&gt; -f - &lt;&lt;EOF
apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
spec:
  # Version retrieved from release notes
  releaseVersion: 3.X.Y
EOF</screen>
<note>
<para>Em alguns casos de uso, talvez você queira fazer configurações adicionais no
<literal>UpgradePlan</literal>. Para saber todas as configurações possíveis,
consulte a <xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</note>
</listitem>
<listitem>
<para>A implantação do <literal>UpgradePlan</literal> no namespace do
<literal>Controller de upgrade</literal> inicia o <literal>processo de
upgrade</literal>.</para>
<note>
<para>Para obter mais informações sobre o <literal>processo de upgrade</literal>
real, consulte a <xref linkend="components-upgrade-controller-how"/>.</para>
<para>Para obter informações sobre como acompanhar o <literal>processo de
upgrade</literal>, consulte a <xref
linkend="components-upgrade-controller-how-track"/>.</para>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet">
<title>Fleet</title>
<para>Esta seção apresenta informações sobre como executar operações de "dia 2"
usando o componente Fleet (<xref linkend="components-fleet"/>).</para>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-components"/>: componentes padrão
usados para todas as operações de "dia 2".</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-determine-use-case"/>: apresenta uma
visão geral dos recursos personalizados do Fleet que serão usados e como se
adaptam aos diferentes casos de uso das operações de "dia 2".</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-upgrade-workflow"/>: fornece um guia de fluxo
de trabalho para execução de operações de "dia 2" com o Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade"/>: descreve como fazer
upgrades de sistema operacional com o Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade"/>: descreve como fazer
upgrades de versão do Kubernetes com o Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade"/>: descreve como fazer
upgrades de gráficos Helm com o Fleet.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-components">
<title>Componentes</title>
<para>Veja a seguir uma descrição dos componentes padrão que você deve configurar
no cluster de <literal>gerenciamento</literal> para realizar operações de
"dia 2" com sucesso pelo Fleet.</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<para>(<emphasis role="strong">Opcional</emphasis>) Responsável por gerenciar os
<literal>clusters downstream</literal> e implantar o <literal>System Upgrade
Controller</literal> no <literal>cluster de gerenciamento</literal>.</para>
<para>Para obter mais informações, consulte o <xref
linkend="components-rancher"/>.</para>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System Upgrade Controller (SUC)</title>
<para><emphasis role="strong">System Upgrade Controller</emphasis> é responsável
por executar tarefas em nós especificados com base nos dados de configuração
fornecidos por um recurso personalizado chamado <literal>plano</literal>.</para>
<para>O <emphasis role="strong">SUC</emphasis> é ativamente usado para fazer
upgrade do sistema operacional e da distribuição Kubernetes.</para>
<para>Para obter mais informações sobre o componente <emphasis
role="strong">SUC</emphasis> e como ele se adapta à pilha do Edge, consulte
o <xref linkend="components-system-upgrade-controller"/>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-determine-use-case">
<title>Determinar seu caso de uso</title>
<para>O Fleet usa dois tipos de <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> para permitir o gerenciamento de recursos do
Kubernetes e do Helm.</para>
<para>Veja a seguir as informações sobre a finalidade desses recursos e para quais
casos de uso eles são mais adequados no contexto das operações de "dia 2".</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para>O <literal>GitRepo</literal> é um recurso do Fleet (<xref
linkend="components-fleet"/>) que representa um repositório Git do qual o
<literal>Fleet</literal> pode criar <literal>Bundles</literal>. Cada
<literal>Bundle</literal> é criado com base nos caminhos de configuração
definidos no recurso <literal>GitRepo</literal>. Para obter mais
informações, consulte a documentação do <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>.</para>
<para>No contexto das operações de "dia 2", os recursos <literal>GitRepo</literal>
costumam ser usados para implantar o <literal>SUC</literal> ou os
<literal>planos do SUC</literal> em ambientes <emphasis role="strong">não
air-gapped</emphasis> que usam a abordagem <emphasis>GitOps do
Fleet</emphasis>.</para>
<para>Se preferir, use os recursos <literal>GitRepo</literal> para implantar o
<literal>SUC</literal> ou os <literal>planos do SUC</literal> em ambientes
<emphasis role="strong">air-gapped</emphasis>, <emphasis role="strong">desde
que você espelhe a configuração do seu repositório por um servidor Git
local</emphasis>.</para>
</section>
<section xml:id="id-bundle">
<title>Bundle</title>
<para>Os <literal>Bundles</literal> armazenam recursos <emphasis
role="strong">brutos</emphasis> do Kubernetes que serão implantados no
cluster de destino. Normalmente, eles são criados de um recurso
<literal>GitRepo</literal>, mas há casos de uso em que podem ser implantados
manualmente. Para obter mais informações, consulte a documentação do <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>.</para>
<para>No contexto das operações de "dia 2", os recursos <literal>Bundle</literal>
costumam ser usados para implantar o <literal>SUC</literal> ou
<literal>planos do SUC</literal> em ambientes <emphasis
role="strong">air-gapped</emphasis> que não usam nenhuma forma de
procedimento do <emphasis>GitOps local</emphasis> (por exemplo, um <emphasis
role="strong">servidor git local</emphasis>).</para>
<para>Se o seu caso de uso não possibilita um fluxo de trabalho
<emphasis>GitOps</emphasis> (por exemplo, usando um repositório Git), a
alternativa é usar os recursos <literal>Bundle</literal> para implantar o
<literal>SUC</literal> ou os <literal>planos do SUC</literal> em ambientes
<emphasis role="strong">não air-gapped</emphasis>.</para>
</section>
</section>
<section xml:id="management-day2-upgrade-workflow">
<title>Fluxo de trabalho de dia 2</title>
<para>Veja a seguir um fluxo de trabalho de "dia 2" que deve ser seguido para
fazer upgrade de um cluster de gerenciamento para uma versão específica do
Edge.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Upgrade de sistema operacional (<xref
linkend="management-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Upgrade de versão do Kubernetes (<xref
linkend="management-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Upgrade de gráfico Helm (<xref
linkend="management-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade">
<title>Upgrade de sistema operacional</title>
<para>Esta seção descreve como fazer upgrade de um sistema operacional usando o
<xref linkend="components-fleet"/> e o <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-components"/>: componentes
adicionais usados pelo processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-overview"/>: visão geral do
processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-requirements"/>: requisitos
do processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-plan-deployment"/>:
informações de como implantar os <literal>planos do SUC</literal>,
responsáveis por acionar o processo de upgrade.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-os-upgrade-components">
<title>Componentes</title>
<para>Esta seção aborda os componentes personalizados que o processo de
<literal>upgrade de sistema operacional</literal> usa com os componentes de
"dia 2" padrão (<xref linkend="management-day2-fleet-components"/>).</para>
<section xml:id="management-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>O upgrade de sistema operacional em um nó específico é executado pelo <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>.</para>
<para>É criado um serviço diferente dependendo do tipo de upgrade que o sistema
operacional requer de uma versão do Edge para outra:</para>
<itemizedlist>
<listitem>
<para>Para versões do Edge que requerem a mesma versão de sistema operacional (por
exemplo, <literal>6.0</literal>), o <literal>os-pkg-update.service</literal>
é criado. Ele usa o <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para fazer <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">upgrade
de um pacote normal</link>.</para>
</listitem>
<listitem>
<para>Para versões do Edge que requerem a migração da versão de sistema
operacional (por exemplo, <literal>6.0</literal> → <literal>6.1</literal>),
o <literal>os-migration.service</literal> é criado. Ele usa o <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para fazer o seguinte:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>O <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">upgrade
de pacote normal</link>, que garante que todos os pacotes sejam atualizados
para mitigar falhas na migração relacionadas a versões antigas dos pacotes.</para>
</listitem>
<listitem>
<para>A migração de sistema operacional usando o comando <literal>zypper
migration</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>Os serviços mencionados acima são distribuídos para cada nó por meio de um
<literal>plano do SUC</literal>, que deve estar localizado no cluster de
gerenciamento que precisa do upgrade de sistema operacional.</para>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-overview">
<title>Visão geral</title>
<para>O upgrade do sistema operacional para nós de cluster de gerenciamento é
feito pelo <literal>Fleet</literal> e pelo <literal>System Upgrade
Controller (SUC)</literal>.</para>
<para>O <emphasis role="strong">Fleet</emphasis> é usado para implantar e
gerenciar os <literal>planos do SUC</literal> no cluster desejado.</para>
<note>
<para>Os <literal>planos do SUC</literal> são <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que descrevem as etapas que o <literal>SUC</literal>
precisa seguir para execução de uma tarefa específica em um conjunto de
nós. Para ver um exemplo de como é um <literal>plano do SUC</literal>,
consulte o <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositório
upstream</link>.</para>
</note>
<para>Os <literal>planos do SUC de sistema operacional</literal> são enviados a
cada cluster por meio da implantação de um recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> ou <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> em um <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espaço
de trabalho</link> do Fleet específico. O Fleet recupera o
<literal>GitRepo/Bundle</literal> implantado e implanta seu conteúdo (os
<literal>planos do SUC de sistema operacional</literal>) no(s) cluster(s)
desejado(s).</para>
<note>
<para>Os recursos <literal>GitRepo/Bundle</literal> sempre são implantados no
<literal>cluster de gerenciamento</literal>. O uso do recurso
<literal>GitRepo</literal> ou <literal>Bundle</literal> depende do seu caso
de uso. Consulte a <xref
linkend="management-day2-fleet-determine-use-case"/> para obter mais
informações.</para>
</note>
<para>Os <literal>planos do SUC de sistema operacional</literal> descrevem o
seguinte fluxo de trabalho:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Sempre use o comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
nos nós antes dos upgrades de sistema operacional.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade dos nós do <literal>control-plane</literal> antes dos
nós do <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade do cluster <emphasis role="strong">um</emphasis> nó de
cada vez.</para>
</listitem>
</orderedlist>
<para>Depois que os <literal>planos do SUC de sistema operacional</literal> forem
implantados, o fluxo de trabalho terá esta aparência:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O SUC reconcilia os <literal>planos do SUC de sistema operacional</literal>
implantados e cria um <literal>Kubernetes Job</literal> em <emphasis
role="strong">cada nó</emphasis>.</para>
</listitem>
<listitem>
<para>O <literal>Kubernetes Job</literal> cria um systemd.service (<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>)
para upgrade de pacote ou migração de sistema operacional.</para>
</listitem>
<listitem>
<para>O <literal>systemd.service</literal> criado aciona o processo de upgrade de
sistema operacional no nó específico.</para>
<important>
<para>Após o término do processo de upgrade do sistema operacional, o nó
correspondente será <literal>reinicializado</literal> para aplicar as
atualizações ao sistema.</para>
</important>
</listitem>
</orderedlist>
<para>Veja a seguir um diagrama da descrição acima:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>upgrade so gerenciamento dia2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-os-upgrade-requirements">
<title>Requisitos</title>
<para><emphasis>Geral:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Máquina registrada no SCC</emphasis>: todos os nós
do cluster de gerenciamento devem ser registrados no <literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal> para
que o respectivo <literal>systemd.service</literal> possa se conectar com
sucesso ao repositório RPM desejado.</para>
<important>
<para>Para lançamentos do Edge que exigem a migração da versão do sistema
operacional (por exemplo, <literal>6.0</literal> → <literal>6.1</literal>),
verifique se a chave do SCC oferece suporte à migração para a nova versão.</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">Garantir que as tolerâncias do plano do SUC
correspondam às do nó</emphasis>: se os nós do cluster Kubernetes têm
<emphasis role="strong">taints</emphasis> personalizados, adicione <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerâncias</link>
a esses taints nos <emphasis role="strong">planos do SUC</emphasis>. Por
padrão, os <emphasis role="strong">planos do SUC</emphasis> têm tolerâncias
apenas para nós do <emphasis role="strong">plano de controle</emphasis>. As
tolerâncias padrão incluem:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>As tolerâncias adicionais devem ser incluídas na seção
<literal>.spec.tolerations</literal> de cada plano. Os <emphasis
role="strong">planos do SUC</emphasis> relacionados a upgrade de sistema
operacional estão disponíveis no repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
em
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>.
<emphasis role="strong">Use os planos de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do repositório.</emphasis></para>
<para>Um exemplo de definição de tolerâncias personalizadas para o plano do SUC de
<emphasis role="strong">plano de controle</emphasis> tem esta aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>Air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Espelhar repositórios RPM do SUSE</emphasis>: os
repositórios RPM de sistema operacional devem ser espelhados localmente para
que o <literal>systemd.service</literal> tenha acesso a eles. Para isso, use
<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>
ou <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment">
<title>Upgrade de sistema operacional: implantação do plano do SUC</title>
<important>
<para>Em ambientes que já passaram por upgrade usando esse procedimento, os
usuários devem garantir que <emphasis role="strong">uma</emphasis> das
seguintes etapas seja concluída:</para>
<itemizedlist>
<listitem>
<para><literal>Remover os planos do SUC que já foram implantados relacionados a
versões de lançamento mais antigas do Edge do cluster de
gerenciamento</literal>: para fazer isso, remova o cluster desejado da <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuração
de destino</link> do <literal>GitRepo/Bundle</literal>, ou remova o recurso
<literal>GitRepo/Bundle</literal> completamente.</para>
</listitem>
<listitem>
<para><literal>Reutilizar o recurso GitRepo/Bundle existente</literal>: para fazer
isso, aponte a revisão do recurso para uma nova tag que inclua as instâncias
do Fleet corretas para a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Isso é feito para evitar conflitos entre os <literal>planos do SUC</literal>
de versões de lançamento mais antigas do Edge.</para>
<para>Se os usuários tentarem fazer upgrade e já houver <literal>planos do
SUC</literal> no cluster de gerenciamento, eles verão o seguinte erro no
Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Conforme mencionado na <xref
linkend="management-day2-fleet-os-upgrade-overview"/>, os upgrades de
sistema operacional são feitos enviando os <literal>planos do SUC</literal>
ao cluster desejado de uma destas maneiras:</para>
<itemizedlist>
<listitem>
<para>Recurso <literal>GitRepo</literal> do Fleet: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>.</para>
</listitem>
<listitem>
<para>Recurso <literal>Bundle</literal> do Fleet: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle"/>.</para>
</listitem>
</itemizedlist>
<para>Para determinar o recurso que você deve usar, consulte a <xref
linkend="management-day2-fleet-determine-use-case"/>.</para>
<para>Para casos de uso de implantação dos <literal>planos do SUC de sistema
operacional</literal> de uma ferramenta GitOps de terceiros, consulte a
<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>Implantação do plano do SUC: recurso GitRepo</title>
<para>Um recurso <emphasis role="strong">GitRepo</emphasis>, que distribui os
<literal>planos do SUC de sistema operacional</literal> necessários, pode
ser implantado de uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>)
em seu <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do sistema
operacional dos nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>Criação do GitRepo: IU do Rancher</title>
<para>Para criar um recurso <literal>GitRepo</literal> usando a IU do Rancher,
consulte a <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentação</link>
oficial do produto.</para>
<para>A equipe do Edge mantém uma <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">instância
do Fleet</link> pronta para uso. Dependendo do seu ambiente, ela pode ser
usada diretamente ou como gabarito.</para>
<important>
<para>Use sempre essa instância do Fleet de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para casos de uso em que não há necessidade de incluir alterações
personalizadas nos <literal>planos do SUC</literal> que a instância do Fleet
envia, os usuários podem fazer referência à instância do Fleet de
<literal>os-upgrade</literal> do repositório
<literal>suse-edge/fleet-examples</literal>.</para>
<para>Nos casos em que as alterações personalizadas são necessárias (por exemplo,
para adicionar tolerâncias personalizadas), os usuários devem fazer
referência à instância do Fleet de <literal>os-upgrade</literal> de um
repositório separado, para que possam adicioná-las aos planos do SUC
conforme necessário.</para>
<para>Há um exemplo de como configurar um <literal>GitRepo</literal> para usar a
instância do Fleet do repositório
<literal>suse-edge/fleet-examples</literal> disponível <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">aqui</link>.</para>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>Criação do GitRepo: manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edite a configuração do <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Remova a seção <literal>spec.targets</literal> (necessário apenas para
clusters downstream).</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Aponte o namespace do <literal>GitRepo</literal> para o namespace
<literal>fleet-local</literal>, feito para implantar o recurso no cluster de
gerenciamento.</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique o recurso <emphasis role="strong">GitRepo</emphasis> ao
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">GitRepo</emphasis> criado no
namespace <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-local

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>Implantação do plano do SUC – recurso Bundle</title>
<para>O recurso <emphasis role="strong">Bundle</emphasis>, que envia os
<literal>planos do SUC de sistema operacional</literal> necessários, pode
ser implantado de uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal> : <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Por meio da implantação manual do recurso (<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do sistema
operacional dos nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>Criação do bundle – IU do Rancher</title>
<para>A equipe do Edge mantém um <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle</link>
pronto para uso que pode ser usado nas etapas a seguir.</para>
<important>
<para>Sempre use esse bundle de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para criar um bundle pela IU do Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>No canto superior esquerdo, clique em <emphasis role="strong">☰ → Continuous
Delivery</emphasis> (Entrega contínua)</para>
</listitem>
<listitem>
<para>Vá para <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avançado > Bundles).</para>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create from YAML</emphasis> (Criar do
YAML).</para>
</listitem>
<listitem>
<para>Nesse local, você pode criar o bundle de uma das seguintes maneiras:</para>
<note>
<para>Há casos de uso em que você precisa incluir alterações personalizadas nos
<literal>planos do SUC</literal> que o bundle envia (por exemplo, para
adicionar tolerâncias personalizadas). Inclua essas alterações no bundle que
será gerado pelas etapas a seguir.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie manualmente o <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">conteúdo
do bundle</link> de <literal>suse-edge/fleet-examples</literal> para a
página <emphasis role="strong">Create from YAML</emphasis> (Criar do YAML).</para>
</listitem>
<listitem>
<para>Clone o repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
da tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada e selecione a opção <emphasis role="strong">Read from
File</emphasis> (Ler arquivo) na página <emphasis role="strong">Create from
YAML</emphasis> (Criar do YAML). Dessa página, navegue até o local do bundle
(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)
e selecione o arquivo do bundle. Esse procedimento preencherá
automaticamente a página <emphasis role="strong">Create from YAML</emphasis>
(Criar do YAML) com o conteúdo do bundle.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Edite o bundle na IU do Rancher:</para>
<itemizedlist>
<listitem>
<para>Altere o <emphasis role="strong">namespace</emphasis> do
<literal>Bundle</literal> para apontar para o namespace
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>Altere os clusters de <emphasis role="strong">destino</emphasis> para que o
<literal>Bundle</literal> aponte para o seu cluster <literal>local</literal>
(gerenciamento):</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Em alguns casos de uso, o cluster <literal>local</literal> pode ter um nome
diferente.</para>
<para>Para recuperar o nome do cluster <literal>local</literal>, execute o comando
abaixo:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create</emphasis> (Criar).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>Criação do bundle – manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o recurso <emphasis role="strong">Bundle</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edite a configuração do <literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Altere os clusters de <emphasis role="strong">destino</emphasis> para que o
<literal>Bundle</literal> aponte para o seu cluster <literal>local</literal>
(gerenciamento):</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Em alguns casos de uso, o cluster <literal>local</literal> pode ter um nome
diferente.</para>
<para>Para recuperar o nome do cluster <literal>local</literal>, execute o comando
abaixo:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Altere o <emphasis role="strong">namespace</emphasis> do
<literal>Bundle</literal> para apontar para o namespace
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique o recurso <emphasis role="strong">Bundle</emphasis> ao seu
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">Bundle</emphasis> criado no
namespace <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-local</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>Implantação do plano do SUC: fluxo de trabalho do GitOps de terceiros</title>
<para>Em alguns casos de uso, talvez você queira incorporar os <literal>planos do
SUC de sistema operacional</literal> ao fluxo de trabalho do GitOps de
terceiros (por exemplo, o <literal>Flux</literal>).</para>
<para>Para obter os recursos de upgrade de sistema operacional que você precisa,
primeiro determine a tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do Edge do repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
que deseja usar.</para>
<para>Depois disso, os recursos estarão disponíveis em
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>,
em que:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> é um recurso do plano do SUC para
nós do <emphasis role="strong">plano de controle</emphasis>.</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> é um recurso do plano do SUC para nós do
<emphasis role="strong">worker</emphasis>.</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> é um segredo que contém o script
<literal>upgrade.sh</literal>, responsável por criar o systemd.service
(<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>).</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> é um ConfigMap que armazena as
configurações consumidas pelo script <literal>upgrade.sh</literal>.</para>
</listitem>
</itemizedlist>
<important>
<para>Esses recursos do <literal>plano</literal> são interpretados pelo
<literal>System Upgrade Controller</literal> e devem ser implantados em cada
cluster downstream do qual você deseja fazer upgrade. Para obter informações
sobre a implantação do SUC, consulte a <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para entender melhor como usar o fluxo de trabalho do GitOps para implantar
os <emphasis role="strong">planos do SUC</emphasis> para upgrade de sistema
operacional, consulte a visão geral (<xref
linkend="management-day2-fleet-os-upgrade-overview"/>).</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade">
<title>Upgrade da versão do Kubernetes</title>
<para>Esta seção descreve como fazer um upgrade do Kubernetes usando o <xref
linkend="components-fleet"/> e o <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-components"/>: componentes
adicionais usados pelo processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-overview"/>: visão geral do
processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-requirements"/>: requisitos
do processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment"/>:
informações de como implantar os <literal>planos do SUC</literal>,
responsáveis por acionar o processo de upgrade.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-k8s-upgrade-components">
<title>Componentes</title>
<para>Esta seção aborda os componentes personalizados que o processo de
<literal>upgrade do K8s</literal> usa com os componentes de "dia 2" padrão
(<xref linkend="management-day2-fleet-components"/>).</para>
<section xml:id="management-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>Imagem do contêiner responsável por fazer upgrade da versão do RKE2 de um nó
específico.</para>
<para>Incluída em um pod criado pelo <emphasis role="strong">SUC</emphasis> com
base no <emphasis role="strong">plano do SUC</emphasis>. O plano deve estar
localizado em cada <emphasis role="strong">cluster</emphasis> que precisa de
upgrade do RKE2.</para>
<para>Para obter mais informações sobre como a imagem
<literal>rke2-upgrade</literal> faz o upgrade, consulte a documentação <link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">upstream</link>.</para>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Imagem do contêiner responsável por fazer upgrade da versão do K3s de um nó
específico.</para>
<para>Incluída em um pod criado pelo <emphasis role="strong">SUC</emphasis> com
base no <emphasis role="strong">plano do SUC</emphasis>. O plano deve estar
localizado em cada <emphasis role="strong">cluster</emphasis> que precisa de
upgrade do K3s.</para>
<para>Para obter mais informações sobre como a imagem
<literal>k3s-upgrade</literal> faz o upgrade, consulte a documentação <link
xl:href="https://github.com/k3s-io/k3s-upgrade">upstream</link>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-overview">
<title>Visão geral</title>
<para>O upgrade de distribuições Kubernetes para nós de cluster de gerenciamento é
feito pelo <literal>Fleet</literal> e pelo <literal>System Upgrade
Controller (SUC)</literal>.</para>
<para>O <literal>Fleet</literal> é usado para implantar e gerenciar
<literal>planos do SUC</literal> no cluster desejado.</para>
<note>
<para>Os <literal>planos do SUC</literal> são <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que descrevem as etapas que o <emphasis
role="strong">SUC</emphasis> precisa seguir para execução de uma tarefa
específica em um conjunto de nós. Para ver um exemplo de como é um
<literal>plano do SUC</literal>, consulte o <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositório
upstream</link>.</para>
</note>
<para>Os <literal>planos do SUC do K8s</literal> são enviados a cada cluster por
meio da implantação de um recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> ou <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> em um <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espaço
de trabalho</link> do Fleet específico. O Fleet recupera o
<literal>GitRepo/Bundle</literal> implantado e implanta seu conteúdo (os
<literal>planos do SUC do K8s</literal>) no(s) cluster(s) desejado(s).</para>
<note>
<para>Os recursos <literal>GitRepo/Bundle</literal> sempre são implantados no
<literal>cluster de gerenciamento</literal>. O uso do recurso
<literal>GitRepo</literal> ou <literal>Bundle</literal> depende do seu caso
de uso. Consulte a <xref
linkend="management-day2-fleet-determine-use-case"/> para obter mais
informações.</para>
</note>
<para>Os <literal>planos do SUC do K8s</literal> descrevem o seguinte fluxo de
trabalho:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Sempre use o comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
nos nós antes dos upgrades do K8s.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade dos nós do <literal>control-plane</literal> antes dos
nós do <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade dos nós do <literal>control-plane</literal> <emphasis
role="strong">um</emphasis> de cada vez, e dos nós do
<literal>worker</literal> <emphasis role="strong">dois</emphasis> nós de
cada vez.</para>
</listitem>
</orderedlist>
<para>Depois que os <literal>planos do SUC do K8s</literal> forem implantados, o
fluxo de trabalho terá esta aparência:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O SUC reconcilia os <literal>planos do SUC do K8s</literal> implantados e
cria um <literal>Kubernetes Job</literal> em <emphasis role="strong">cada
nó</emphasis>.</para>
</listitem>
<listitem>
<para>Dependendo da distribuição Kubernetes, o job cria um pod que executa a
imagem de contêiner do rke2-upgrade (<xref
linkend="management-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>) ou do
k3s-upgrade (<xref
linkend="management-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>).</para>
</listitem>
<listitem>
<para>O pod criado vai percorrer o seguinte fluxo de trabalho:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Substitua o binário <literal>rke2/k3s</literal> existente no nó pelo da
imagem <literal>rke2-upgrade/k3s-upgrade</literal>.</para>
</listitem>
<listitem>
<para>Cancele o processo do <literal>rke2/k3s</literal> em execução.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>O cancelamento do processo do <literal>rke2/k3s</literal> aciona a
reinicialização, o que inicia um novo processo que executa o binário
atualizado, resultando em uma versão atualizada da distribuição Kubernetes.</para>
</listitem>
</orderedlist>
<para>Veja a seguir um diagrama da descrição acima:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>upgrade k8s gerenciamento dia2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-requirements">
<title>Requisitos</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Faça backup da distribuição Kubernetes:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Para <emphasis role="strong">clusters RKE2</emphasis>, consulte a
documentação sobre <link
xl:href="https://docs.rke2.io/datastore/backup_restore">backup e restauração
do RKE2</link>.</para>
</listitem>
<listitem>
<para>Para <emphasis role="strong">clusters K3s</emphasis>, consulte a
documentação sobre <link
xl:href="https://docs.k3s.io/datastore/backup-restore">backup e restauração
do K3s</link>.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Garantir que as tolerâncias do plano do SUC
correspondam às do nó</emphasis>: se os nós do cluster Kubernetes têm
<emphasis role="strong">taints</emphasis> personalizados, adicione <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerâncias</link>
a esses taints nos <emphasis role="strong">planos do SUC</emphasis>. Por
padrão, os <emphasis role="strong">planos do SUC</emphasis> têm tolerâncias
apenas para nós do <emphasis role="strong">plano de controle</emphasis>. As
tolerâncias padrão incluem:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>As tolerâncias adicionais devem ser incluídas na seção
<literal>.spec.tolerations</literal> de cada plano. Os <emphasis
role="strong">planos do SUC</emphasis> relacionados ao upgrade de versões do
Kubernetes estão disponíveis no repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
em:</para>
<itemizedlist>
<listitem>
<para>Para o <emphasis role="strong">RKE2</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>Para o <emphasis role="strong">K3s</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Use os planos de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do repositório.</emphasis></para>
<para>Um exemplo de definição de tolerâncias personalizadas para o plano do SUC de
<emphasis role="strong">plano de controle</emphasis> do RKE2 tem esta
aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment">
<title>Upgrade do K8s: implantação do plano do SUC</title>
<important>
<para>Em ambientes que já passaram por upgrade usando esse procedimento, os
usuários devem garantir que <emphasis role="strong">uma</emphasis> das
seguintes etapas seja concluída:</para>
<itemizedlist>
<listitem>
<para><literal>Remover os planos do SUC que já foram implantados relacionados a
versões de lançamento mais antigas do Edge do cluster de
gerenciamento</literal>: para fazer isso, remova o cluster desejado da <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuração
de destino</link> do <literal>GitRepo/Bundle</literal>, ou remova o recurso
<literal>GitRepo/Bundle</literal> completamente.</para>
</listitem>
<listitem>
<para><literal>Reutilizar o recurso GitRepo/Bundle existente</literal>: para fazer
isso, aponte a revisão do recurso para uma nova tag que inclua as instâncias
do Fleet corretas para a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Isso é feito para evitar conflitos entre os <literal>planos do SUC</literal>
de versões de lançamento mais antigas do Edge.</para>
<para>Se os usuários tentarem fazer upgrade e já houver <literal>planos do
SUC</literal> no cluster de gerenciamento, eles verão o seguinte erro no
Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Conforme mencionado na <xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>, os upgrades do
Kubernetes são feitos enviando os <literal>planos do SUC</literal> para o
cluster desejado de uma destas maneiras:</para>
<itemizedlist>
<listitem>
<para>Recurso GitRepo do Fleet (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Recurso Bundle do Fleet (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>Para determinar o recurso que você deve usar, consulte a <xref
linkend="management-day2-fleet-determine-use-case"/>.</para>
<para>Para casos de uso de implantação dos <literal>planos do SUC do K8s</literal>
de uma ferramenta GitOps de terceiros, consulte a <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>Implantação do plano do SUC: recurso GitRepo</title>
<para>Um recurso <emphasis role="strong">GitRepo</emphasis>, que distribui os
<literal>planos do SUC do K8s</literal> necessários, pode ser implantado de
uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do Kubernetes dos
nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>Criação do GitRepo: IU do Rancher</title>
<para>Para criar um recurso <literal>GitRepo</literal> usando a IU do Rancher,
consulte a <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentação</link>
oficial do produto.</para>
<para>A equipe do Edge mantém instâncias do Fleet prontas para uso para ambas as
distribuições Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>
e <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>.
Dependendo do seu ambiente, é possível usar a instância do Fleet diretamente
ou como gabarito.</para>
<important>
<para>Sempre use as instâncias do Fleet de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para casos de uso em que não há necessidade de incluir alterações
personalizadas nos <literal>planos do SUC</literal> que as instâncias do
Fleet envia, os usuários podem fazer referência a essas instâncias do Fleet
diretamente do repositório <literal>suse-edge/fleet-examples</literal>.</para>
<para>Nos casos em que as alterações personalizadas são necessárias (por exemplo,
para adicionar tolerâncias personalizadas), os usuários devem fazer
referência às instâncias do Fleet de um repositório separado, para que
possam adicionas as alterações aos planos do SUC conforme necessário.</para>
<para>Exemplos de configuração do recurso <literal>GitRepo</literal> usando as
instâncias do Fleet do repositório
<literal>suse-edge/fleet-examples</literal>:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>Criação do GitRepo: manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clusters <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para clusters <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite a configuração do <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Remova a seção <literal>spec.targets</literal> (necessário apenas para
clusters downstream).</para>
<itemizedlist>
<listitem>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aponte o namespace do <literal>GitRepo</literal> para o namespace
<literal>fleet-local</literal>, feito para implantar o recurso no cluster de
gerenciamento.</para>
<itemizedlist>
<listitem>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique os recursos <emphasis role="strong">GitRepo</emphasis> ao
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">GitRepo</emphasis> criado no
namespace <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-local

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-local

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>Implantação do plano do SUC – recurso Bundle</title>
<para>O recurso <emphasis role="strong">Bundle</emphasis>, que envia os
<literal>planos do SUC de upgrade do Kubernetes</literal> necessários, pode
ser implantado de uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do Kubernetes dos
nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>Criação do bundle – IU do Rancher</title>
<para>A equipe do Edge mantém bundles prontos para uso para ambas as distribuições
Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>
e <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>.
Dependendo do seu ambiente, é possível usar os bundles diretamente ou como
gabaritos.</para>
<important>
<para>Sempre use esse bundle de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para criar um bundle pela IU do Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>No canto superior esquerdo, clique em <emphasis role="strong">☰ → Continuous
Delivery</emphasis> (Entrega contínua)</para>
</listitem>
<listitem>
<para>Vá para <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avançado > Bundles).</para>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create from YAML</emphasis> (Criar do
YAML).</para>
</listitem>
<listitem>
<para>Nesse local, você pode criar o bundle de uma das seguintes maneiras:</para>
<note>
<para>Há casos de uso em que você precisa incluir alterações personalizadas nos
<literal>planos do SUC</literal> que o bundle envia (por exemplo, para
adicionar tolerâncias personalizadas). Inclua essas alterações no bundle que
será gerado pelas etapas a seguir.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie manualmente o conteúdo do bundle para <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>
ou <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>
de <literal>suse-edge/fleet-examples</literal> para a página <emphasis
role="strong">Create from YAML</emphasis> (Criar do YAML).</para>
</listitem>
<listitem>
<para>Clone o repositório <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
da tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada e selecione a opção <emphasis role="strong">Read from
File</emphasis> (Ler arquivo) na página <emphasis role="strong">Create from
YAML</emphasis> (Criar do YAML). Dessa página, navegue até o bundle
necessário
(<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>
para RKE2 e
<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>
para K3s). Esse procedimento preencherá automaticamente a página <emphasis
role="strong">Create from YAML</emphasis> (Criar do YAML) com o conteúdo do
bundle.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Edite o bundle na IU do Rancher:</para>
<itemizedlist>
<listitem>
<para>Altere o <emphasis role="strong">namespace</emphasis> do
<literal>Bundle</literal> para apontar para o namespace
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>Altere os clusters de <emphasis role="strong">destino</emphasis> para que o
<literal>Bundle</literal> aponte para o seu cluster <literal>local</literal>
(gerenciamento):</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Em alguns casos de uso, o cluster <literal>local</literal> pode ter um nome
diferente.</para>
<para>Para recuperar o nome do cluster <literal>local</literal>, execute o comando
abaixo:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create</emphasis> (Criar).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>Criação do bundle – manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha os recursos <emphasis role="strong">Bundle</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clusters <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Para clusters <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite a configuração do <literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Altere os clusters de <emphasis role="strong">destino</emphasis> para que o
<literal>Bundle</literal> aponte para o seu cluster <literal>local</literal>
(gerenciamento):</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Em alguns casos de uso, o cluster <literal>local</literal> pode ter um nome
diferente.</para>
<para>Para recuperar o nome do cluster <literal>local</literal>, execute o comando
abaixo:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Altere o <emphasis role="strong">namespace</emphasis> do
<literal>Bundle</literal> para apontar para o namespace
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique os recursos <emphasis role="strong">Bundle</emphasis> ao
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">Bundle</emphasis> criado no
namespace <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-local

# For K3s
kubectl get bundles k3s-upgrade -n fleet-local

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>Implantação do plano do SUC: fluxo de trabalho do GitOps de terceiros</title>
<para>Em alguns casos de uso, talvez você queira incorporar os <literal>planos do
SUC de upgrade do Kubernetes</literal> ao fluxo de trabalho do GitOps de
terceiros (por exemplo, o <literal>Flux</literal>).</para>
<para>Para obter os recursos de upgrade do K8s upgrade que você precisa, primeiro
determine a tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do Edge do repositório <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
que deseja usar.</para>
<para>Depois disso, os recursos estarão em:</para>
<itemizedlist>
<listitem>
<para>Para ugrade do cluster RKE2:</para>
<itemizedlist>
<listitem>
<para>Para os nós do <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para os nós de <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Para upgrade do cluster K3s:</para>
<itemizedlist>
<listitem>
<para>Para os nós de <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para os nós de <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>Esses recursos do <literal>plano</literal> são interpretados pelo
<literal>System Upgrade Controller</literal> e devem ser implantados em cada
cluster downstream do qual você deseja fazer upgrade. Para obter informações
sobre a implantação do SUC, consulte a <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para entender melhor como usar o fluxo de trabalho do GitOps para implantar
os <emphasis role="strong">planos do SUC</emphasis> para upgrade de versão
do Kubernetes, consulte a visão geral (<xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>) do procedimento de
atualização com o <literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade">
<title>Upgrade do gráfico Helm</title>
<para>Esta seção aborda as seguintes partes:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-air-gap"/>: armazena
informações sobre como enviar gráficos e imagens OCI relacionados ao Edge
para seu registro particular.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure"/>: armazena
informações sobre diversos casos de uso de upgrade de gráficos Helm e o
respectivo procedimento de upgrade.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-air-gap">
<title>Preparação para ambientes air-gapped</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet">
<title>Garantir que você tenha acesso ao Fleet por gráfico Helm</title>
<para>Dependendo da compatibilidade do seu ambiente, você poderá seguir uma destas
opções:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Hospede os recursos do Fleet do seu gráfico em um servidor Git local que
possa ser acessado pelo <literal>cluster de gerenciamento</literal>.</para>
</listitem>
<listitem>
<para>Use a CLI do Fleet para <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">converter
um gráfico Helm em bundle</link> que você possa usar diretamente, sem
precisar hospedá-lo. É possível recuperar a CLI do Fleet da página da <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">versão</link>.
Para usuários do Mac, há um Homebrew Formulae <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Encontrar os ativos necessários à sua versão de lançamento do Edge</title>
<orderedlist numeration="arabic">
<listitem>
<para>Vá para a página de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versões</link>
do "dia 2" e localize a versão do Edge para a qual você quer fazer upgrade
do gráfico e clique em <emphasis role="strong">Assets</emphasis> (Ativos).</para>
</listitem>
<listitem>
<para>Na seção <emphasis role="strong">"Assets"</emphasis> (Ativos), faça download
dos seguintes arquivos:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Arquivo de versão</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Descrição</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Obtém as imagens especificadas no arquivo
<literal>edge-release-images.txt</literal> e as compacta em um arquivo
".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Obtém as imagens de gráfico OCI relacionadas à versão especifica do Edge e
as compacta em um arquivo ".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Carrega as imagens de um arquivo ".tar.gz", faz a remarcação e as envia a um
registro particular.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Detecta o diretório que contém os pacotes ".tgz" de gráficos OCI do Edge e
os carrega em um registro particular.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contém uma lista de imagens de gráfico OCI relacionadas a uma versão
específica do Edge.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contém uma lista de imagens relacionadas a uma versão específica do Edge.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive">
<title>Criar o arquivo de imagens da versão do Edge</title>
<para><emphasis>Em uma máquina com acesso à Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Torne o <literal>edge-save-images.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Gere o arquivo de imagens:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esse procedimento cria um arquivo pronto para carregamento chamado
<literal>edge-images.tar.gz</literal>.</para>
<note>
<para>Se a opção <literal>-i|--images</literal> foi especificada, o nome do
arquivo pode ser diferente.</para>
</note>
</listitem>
<listitem>
<para>Copie esse arquivo para sua máquina <emphasis
role="strong">air-gapped</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive">
<title>Criar o arquivo de imagens de gráfico OCI do Edge</title>
<para><emphasis>Em uma máquina com acesso à Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Torne o <literal>edge-save-oci-artefacts.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Gere o arquivo de imagens de gráfico OCI:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esse procedimento cria um arquivo chamado
<literal>oci-artefacts.tar.gz</literal>.</para>
<note>
<para>Se a opção <literal>-a|--archive</literal> foi especificada, o nome do
arquivo pode ser diferente.</para>
</note>
</listitem>
<listitem>
<para>Copie esse arquivo para sua máquina <emphasis
role="strong">air-gapped</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine">
<title>Carregar as imagens da versão do Edge para sua máquina air-gapped</title>
<para><emphasis>Na máquina air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Faça login no seu registro particular (se necessário):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Torne o <literal>edge-load-images.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Execute o script, especificando o arquivo já <emphasis
role="strong">copiado</emphasis> <literal>edge-images.tar.gz</literal>:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>Desse modo, todas as imagens do <literal>edge-images.tar.gz</literal> serão
carregadas, remarcadas e enviadas ao registro especificado na opção
<literal>--registry</literal>.</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine">
<title>Carregar as imagens de gráfico OCI do Edge em sua máquina air-gapped</title>
<para><emphasis>Na máquina air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Faça login no seu registro particular (se necessário):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Torne o <literal>edge-load-oci-artefacts.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Descompacte o arquivo <literal>oci-artefacts.tar.gz</literal> copiado:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>Isso cria um diretório com o gabarito de nomenclatura
<literal>edge-release-oci-tgz-&lt;data&gt;</literal>.</para>
</listitem>
<listitem>
<para>Especifique esse diretório no script
<literal>edge-load-oci-artefacts.sh</literal> para carregar as imagens de
gráfico OCI do Edge em seu registro particular:</para>
<note>
<para>Esse script assume que a CLI <literal>helm</literal> foi pré-instalada em
seu ambiente. Para obter instruções de instalação do Helm, consulte <link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>
(Instalando o Helm).</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution">
<title>Configurar o registro particular na distribuição Kubernetes</title>
<para>Para RKE2, consulte <link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration</link> (Configuração de registro particular)</para>
<para>Para K3s, consulte <link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration</link> (Configuração de registro particular)</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure">
<title>Procedimento de upgrade</title>
<para>Esta seção tem como foco os seguintes casos de uso do procedimento de
upgrade do Helm:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>Não é possível fazer upgrade confiável de gráficos Helm implantados
manualmente. Sugerimos reimplantá-los usando o método da <xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/>.</para>
</important>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>Eu tenho um novo cluster e desejo implantar e gerenciar um gráfico Helm do
Edge</title>
<para>Esta seção explica como:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>Preparar os recursos do Fleet para seu gráfico</title>
<orderedlist numeration="arabic">
<listitem>
<para>Adquira os recursos do Fleet do gráfico com base na tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do Edge que deseja usar.</para>
</listitem>
<listitem>
<para>Navegue até a instância do Fleet do gráfico Helm
(<literal>fleets/day2/chart-templates/&lt;gráfico&gt;</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Se você pretende usar um fluxo de trabalho do
GitOps</emphasis>, copie o diretório do Fleet do gráfico para o repositório
Git do qual você vai executar o GitOps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, se o gráfico Helm exigir
configurações em seus <emphasis role="strong">valores</emphasis>, edite a
configuração <literal>.helm.values</literal> no arquivo
<literal>fleet.yaml</literal> do diretório copiado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, pode haver casos de uso em
que você tenha que adicionar outros recursos à instância do Fleet do seu
gráfico para que se adapte melhor ao seu ambiente. Para obter informações de
como aprimorar o diretório do Fleet, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository
Contents</link> (Conteúdo do repositório Git).</para>
</listitem>
</orderedlist>
<note>
<para>Em alguns casos, o tempo limite padrão que o Fleet usa nas operações do Helm
pode ser insuficiente e resultar no seguinte erro:</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>Nesses casos, adicione a propriedade <link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>
à configuração <literal>helm</literal> do arquivo
<literal>fleet.yaml</literal>.</para>
</note>
<para>Esta é a aparência de um <emphasis role="strong">exemplo</emphasis> de
gráfico Helm do <literal>longhorn</literal>:</para>
<itemizedlist>
<listitem>
<para>Estrutura de repositórios Git do usuário:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>Conteúdo do <literal>fleet.yaml</literal> preenchido com os dados do
<literal>Longhorn</literal> do usuário:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>Estes são apenas valores de exemplo usados para ilustrar as configurações
comuns no gráfico do <literal>longhorn</literal>. Eles <emphasis
role="strong">NÃO</emphasis> devem ser considerados diretrizes de
implantação para o gráfico do <literal>longhorn</literal>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>Implantar o Fleet para seu gráfico</title>
<para>É possível implantar o Fleet para seu gráfico usando o GitRepo (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>)
ou o bundle (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>).</para>
<note>
<para>Durante a implantação do Fleet, se você receber a mensagem
<literal>Modified</literal> (Modificado), adicione uma entrada
<literal>comparePatches</literal> correspondente à seção
<literal>diff</literal> do Fleet. Para obter mais informações, consulte
<link xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to
Ignore Modified GitRepos</link> (Gerando diffs para ignorar GitRepos
modificados).</para>
</note>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>O recurso <link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> do Fleet
armazena as informações sobre como acessar os recursos do Fleet do seu
gráfico e a quais clusters ele precisa aplicar esses recursos.</para>
<para>É possível implantar o recurso <literal>GitRepo</literal> usando a <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">IU
do Rancher</link> ou, manualmente, <link
xl:href="https://fleet.rancher.io/tut-deployment">implantando</link> o
recurso no <literal>cluster de gerenciamento</literal>.</para>
<para>Exemplo de recurso <literal>GitRepo</literal> do <emphasis
role="strong">Longhorn</emphasis> para implantação <emphasis
role="strong">manual</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-local
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url</screen>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>Bundle</title>
<para>Os recursos <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> armazenam os
recursos brutos do Kubernetes que o Fleet precisa implantar. Normalmente, a
recomendação é aplicar a abordagem do <literal>GitRepo</literal>, mas para
casos de uso em que o ambiente é air-gapped e não oferece suporte a um
servidor Git local, os <literal>Bundles</literal> podem ajudar na propagação
do Fleet do gráfico Helm aos clusters de destino.</para>
<para>É possível implantar um <literal>Bundle</literal> pela IU do Rancher
(<literal>Continuous Delivery → Advanced → Bundles → Create from
YAML</literal> (Entrega contínua → Avançado → Bundles → Criar do YAML)) ou
pela implantação manual do recurso <literal>Bundle</literal> no namespace
correto do Fleet. Para obter informações sobre os namespaces do Fleet,
consulte a <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentação</link>
upstream.</para>
<para>É possível criar <literal>Bundles</literal> para gráficos Helm do Edge
usando a abordagem <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Converter
um gráfico Helm em bundle</link> do Fleet.</para>
<para>Veja a seguir um exemplo de como criar um recurso <literal>Bundle</literal>
com base nos gabaritos de instância Fleet <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
e <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
do gráfico Helm e implantar manualmente esse bundle no <literal>cluster de
gerenciamento</literal>.</para>
<note>
<para>Para ilustrar o fluxo de trabalho, o exemplo a seguir usa a estrutura de
diretório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Navegue até o gabarito de instância Fleet do gráfico <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>Crie um arquivo <literal>targets.yaml</literal> para instruir o Fleet sobre
os clusters em que ele deve implantar o gráfico Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
<note>
<para>Em alguns casos de uso, o nome do seu cluster local pode ser diferente.</para>
<para>Para recuperar o nome do seu cluster local, execute este comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Converta o gráfico Helm <literal>Longhorn</literal> do Fleet em um recurso
bundle usando <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<note>
<para>É possível recuperar a CLI do Fleet da página <emphasis
role="strong">Assets</emphasis> (Ativos) da <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">versão</link>
(<literal>fleet-linux-amd64</literal>).</para>
<para>Para usuários do Mac, existe um Homebrew Formulae <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>Navegue até o gabarito de instância do Fleet do gráfico <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>Crie um arquivo <literal>targets.yaml</literal> para instruir o Fleet sobre
os clusters em que ele deve implantar o gráfico Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>Converta o gráfico Helm <literal>Longhorn CRD</literal> do Fleet em um
recurso bundle usando <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>Implante os arquivos <literal>longhorn-bundle.yaml</literal> e
<literal>longhorn-crd-bundle.yaml</literal> no <literal>cluster de
gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>Siga estas etapas para garantir que o <literal>SUSE Storage</literal> seja
implantado em todos os clusters de gerenciamento especificados.</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>Gerenciar o gráfico Helm implantado</title>
<para>Após a implantação com o Fleet, para fazer upgrade dos gráficos Helm,
consulte a <xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>Quero fazer upgrade de um gráfico Helm gerenciado pelo Fleet</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine a versão para a qual você precisa fazer upgrade do seu gráfico
para que ele fique compatível com o lançamento desejado do Edge. Você
encontra a versão do gráfico Helm por lançamento do Edge nas Notas de
lançamento (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>No repositório Git monitorado pelo Fleet, edite o arquivo
<literal>fleet.yaml</literal> do gráfico Helm com a <emphasis
role="strong">versão</emphasis> e o <emphasis
role="strong">repositório</emphasis> corretos do gráfico conforme as Notas
de lançamento (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>Depois de confirmar e enviar as alterações ao repositório, será acionado um
upgrade do gráfico Helm desejado.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>Eu quero fazer upgrade de um gráfico Helm implantado pelo EIB</title>
<para>O <xref linkend="components-eib"/> implanta gráficos Helm criando um recurso
<literal>HelmChart</literal> e usando o <literal>helm-controller</literal>
introduzido pelo recurso de integração do Helm <link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>.</para>
<para>Para garantir que o upgrade do gráfico Helm implantado pelo
<literal>EIB</literal> seja feito com sucesso, os usuários precisam fazer
upgrade dos respectivos recursos <literal>HelmChart</literal>.</para>
<para>Veja a seguir informações sobre:</para>
<itemizedlist>
<listitem>
<para>A visão geral (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>)
do processo de upgrade.</para>
</listitem>
<listitem>
<para>As etapas de upgrade necessárias (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>).</para>
</listitem>
<listitem>
<para>Um exemplo (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>)
que demonstra o upgrade de um gráfico <link
xl:href="https://longhorn.io">Longhorn</link> usando o método explicado.</para>
</listitem>
<listitem>
<para>Como usar o processo de upgrade com uma ferramenta GitOps diferente (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>Visão geral</title>
<para>O upgrade dos gráficos Helm que são implantados pelo <literal>EIB</literal>
é feito por uma instância do <literal>Fleet</literal> chamada <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>.</para>
<para>Essa instância do <literal>Fleet</literal> processa os dados <emphasis
role="strong">fornecidos pelo usuário</emphasis> para <emphasis
role="strong">atualizar</emphasis> um conjunto específico de recursos
HelmChart.</para>
<para>A atualização desses recursos aciona o <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>,
que faz <emphasis role="strong">upgrade</emphasis> dos gráficos Helm
associados aos recursos <literal>HelmChart</literal> modificados.</para>
<para>O usuário precisa apenas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><link xl:href="https://helm.sh/docs/helm/helm_pull/">Obter</link> localmente
os arquivos para cada gráfico Helm que precisa de upgrade.</para>
</listitem>
<listitem>
<para>Especificar esses arquivos no script <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal>, que incluirá os dados dos
arquivos na instância do Fleet <literal>eib-charts-upgrader</literal>.</para>
</listitem>
<listitem>
<para>Implantar a instância do Fleet <literal>eib-charts-upgrader</literal> no
respectivo <literal>cluster de gerenciamento</literal>. Isso é feito por um
recurso <literal>GitRepo</literal> ou <literal>Bundle</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, o <literal>eib-charts-upgrader</literal>, com ajuda do
Fleet, enviará os recursos para o cluster de gerenciamento desejado.</para>
<para>Esses recursos incluem:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Um conjunto de <literal>segredos</literal> com os dados do gráfico Helm
<emphasis role="strong">fornecidos pelo usuário</emphasis>.</para>
</listitem>
<listitem>
<para>Um <literal>Kubernetes Job</literal> para implantar o <literal>pod</literal>
que vai montar os <literal>segredos</literal> já mencionados e, com base
neles, aplicar o <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">patch</link>
aos recursos HelmChart correspondentes.</para>
</listitem>
</orderedlist>
<para>Como já foi mencionado, isso acionará o <literal>helm-controller</literal>,
que faz upgrade do gráfico Helm real.</para>
<para>Veja a seguir um diagrama da descrição acima:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-management-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>upgrade eib helm gerenciamento dia2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>Etapas de upgrade</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clone o repositório <literal>suse-edge/fleet-examples</literal> da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>
da versão correta.</para>
</listitem>
<listitem>
<para>Crie um diretório para armazenar um ou mais arquivos dos gráficos Helm
enviados.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Dentro do diretório do arquivo recém-criado, <link
xl:href="https://helm.sh/docs/helm/helm_pull/">extraia</link> os arquivos
dos gráficos Helm dos quais você deseja fazer upgrade:</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>Em <emphasis role="strong">Assets</emphasis> (Ativos) da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag
da versão</link> desejada, faça download do script
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</listitem>
<listitem>
<para>Execute o script <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>Para cada arquivo de gráfico no diretório <literal>--archive-dir</literal>,
o script gera um arquivo <literal>YAML de segredo do Kubernetes</literal>
com os dados de upgrade do gráfico e o armazena no diretório
<literal>base/secrets</literal> da instância do Fleet especificada por
<literal>--fleet-path</literal>.</para>
<para>O script <literal>generate-chart-upgrade-data.sh</literal> também aplica
modificações adicionais à instância do Fleet para garantir que os arquivos
<literal>YAML de segredo do Kubernetes</literal> gerados sejam corretamente
usados pela carga de trabalho implantada pelo Fleet.</para>
<important>
<para>Os usuários não devem fazer alterações no conteúdo que é gerado pelo script
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</important>
</listitem>
</orderedlist>
<para>As etapas abaixo dependem do ambiente que está em execução:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Para um ambiente com suporte a GitOps (por exemplo, não air-gapped ou
air-gapped, mas com suporte a servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie a instância do Fleet
<literal>fleets/day2/eib-charts-upgrader</literal> para o repositório que
você vai usar com o GitOps.</para>
<note>
<para>Garanta que o Fleet inclua as alterações feitas pelo script
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</note>
</listitem>
<listitem>
<para>Configure o recurso <literal>GitRepo</literal> que será usado para enviar
todos os recursos da instância do Fleet
<literal>eib-charts-upgrader</literal>.</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Para configuração e implantação do <literal>GitRepo</literal> pela IU do
Rancher, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI</link> (Acessando o Fleet na IU do Racher).</para>
</listitem>
<listitem>
<para>Para configuração e implantação manuais do <literal>GitRepo</literal>,
consulte <link xl:href="https://fleet.rancher.io/tut-deployment">Creating a
Deployment</link> (Criando uma implantação).</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Para um ambiente sem suporte a GitOps (por exemplo, air-gapped que não
permite o uso de servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Faça download do binário <literal>fleet-cli</literal> da página da
<literal>rancher/fleet</literal> <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">versão</link>
(<literal>fleet-linux-amd64</literal> para Linux). Para usuários do Mac, é
possível usar o Homebrew Formulae disponível: <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Navegue até a instância do Fleet <literal>eib-charts-upgrader</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Crie um arquivo <literal>targets.yaml</literal> para instruir o Fleet sobre
onde implantar seus recursos:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To map the local(management) cluster
- clusterName: local
EOF</screen>
<note>
<para>Em alguns casos de uso, o cluster <literal>local</literal> pode ter um nome
diferente.</para>
<para>Para recuperar o nome do cluster <literal>local</literal>, execute o comando
abaixo:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Use <literal>fleet-cli</literal> para converter o Fleet em um recurso
<literal>Bundle</literal>:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>Isso cria um bundle (<literal>bundle.yaml</literal>) para armazenar todos os
recursos usados como gabaritos da instância do Fleet
<literal>eib-charts-upgrader</literal>.</para>
<para>Para obter mais informações sobre o comando <literal>fleet apply</literal>,
consulte <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>.</para>
<para>Para obter mais informações sobre como converter instâncias do Fleet em
bundles, consulte <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Converter um gráfico Helm em bundle).</para>
</listitem>
<listitem>
<para>Implante o <literal>Bundle</literal>. É possível fazer isso de uma destas
duas maneiras:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Pela IU do Rancher: navegue até <emphasis role="strong">Continuous Delivery
→ Advanced → Bundles → Create from YAML</emphasis> (Entrega contínua →
Avançado → Bundles → Criar do YAML) e cole o conteúdo do
<literal>bundle.yaml</literal> ou clique na opção <literal>Read from
File</literal> (Ler arquivo) e especifique o arquivo.</para>
</listitem>
<listitem>
<para>Manualmente: implante o arquivo <literal>bundle.yaml</literal> manualmente
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>A execução dessas etapas resulta na implantação bem-sucedida do recurso
<literal>GitRepo/Bundle</literal>. O Fleet seleciona o recurso, e o conteúdo
dele é implantado nos clusters de destino que o usuário especificou nas
etapas anteriores. Para obter uma visão geral do processo, consulte a <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>.</para>
<para>Para obter informações sobre como acompanhar o processo de upgrade, consulte
a <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>.</para>
<important>
<para>Após a verificação bem-sucedida do upgrade do gráfico, remova o recurso
<literal>Bundle/GitRepo</literal>.</para>
<para>Isso removerá os recursos de upgrade que não são mais necessários do cluster
de <literal>gerenciamento</literal>, garantindo que não haja conflitos com
versões futuras.</para>
</important>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>Exemplo</title>
<note>
<para>No exemplo a seguir, demonstramos como fazer upgrade de um gráfico Helm
implantado pelo <literal>EIB</literal> de uma versão para outra em um
cluster de <literal>gerenciamento</literal>. Observe que as versões usadas
neste exemplo <emphasis role="strong">não</emphasis> são recomendações. Para
recomendações de versão específicas a um lançamento do Edge, consulte as
Notas de lançamento (<xref linkend="release-notes"/>).</para>
</note>
<para><emphasis>Caso de uso:</emphasis></para>
<itemizedlist>
<listitem>
<para>Um cluster de <literal>gerenciamento</literal> que executa uma versão mais
antiga do <link xl:href="https://longhorn.io">Longhorn</link>.</para>
</listitem>
<listitem>
<para>O cluster foi implantado pelo EIB usando o seguinte
<emphasis>trecho</emphasis> de definição da imagem:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para>É necessário fazer upgrade do <literal>SUSE Storage</literal> para uma
versão compatível com o Edge 3.4, ou seja, para a versão
<literal>107.0.0+up1.9.1</literal>.</para>
</listitem>
<listitem>
<para>Presume-se que o <literal>cluster de gerenciamento</literal> seja <emphasis
role="strong">air-gapped</emphasis>, sem suporte a servidor Git local e com
uma instalação ativa do Rancher.</para>
</listitem>
</itemizedlist>
<para>Siga as etapas de upgrade (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>):</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clone o repositório <literal>suse-edge/fleet-example</literal> da tag
<literal>release-3.4.0</literal>.</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Crie um diretório para armazenar o arquivo de upgrade do
<literal>Longhorn</literal>.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Extraia a versão desejada do arquivo de gráficos do
<literal>Longhorn</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>Fora do diretório <literal>archives</literal>, faça download do script
<literal>generate-chart-upgrade-data.sh</literal> da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>
da versão do <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>A configuração do diretório deve ter uma aparência como esta:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>Execute o script <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>A estrutura de diretórios após a execução do script deve ser parecida com
esta:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Os arquivos alterados no Git devem ter aparência similar a esta:</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>Crie um <literal>Bundle</literal> para a instância do Fleet
<literal>eib-charts-upgrader</literal>:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Primeiro, navegue até o Fleet:</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Em seguida, crie um arquivo <literal>targets.yaml</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>Depois disso, use o binário <literal>fleet-cli</literal> para converter a
instância do Fleet em um bundle:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Implante o bundle usando a IU do Rancher:</para>
<figure>
<title>Implantar o bundle usando a IU do Rancher</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>exemplo 1 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject></figure>
<para>Agora selecione <emphasis role="strong">Read from File</emphasis> (Ler
arquivo) e encontre o arquivo <literal>bundle.yaml</literal> no sistema.</para>
<para>Isso preencherá automaticamente o <literal>Bundle</literal> na IU do
Rancher.</para>
<para>Selecione <emphasis role="strong">Create</emphasis> (Criar).</para>
</listitem>
<listitem>
<para>Após a implantação bem-sucedida, o bundle terá uma aparência similar a esta:</para>
<figure>
<title>Bundle implantado com sucesso</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>exemplo 2 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>Após a implantação bem-sucedida do <literal>Bundle</literal>, monitore o
processo de upgrade da seguinte maneira:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Consulte os registros do <literal>pod de upgrade</literal>:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_management.png" width="100%"/>
</imageobject>
<textobject><phrase>gerenciamento exemplo 3 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Agora consulte os registros do pod criado pelo helm-controller para o
upgrade:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>O nome do pod seguirá o seguinte gabarito:
<literal>helm-install-longhorn-&lt;sufixo-aleatório&gt;</literal></para>
</listitem>
<listitem>
<para>O pod estará no namespace em que o recurso <literal>HelmChart</literal> foi
implantado. No nosso caso, o namespace é <literal>kube-system</literal>.</para>
<figure>
<title>Registros do gráfico do Longhorn atualizado com sucesso</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_management.png" width="100%"/>
</imageobject>
<textobject><phrase>gerenciamento exemplo 4 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Verifique se a versão do <literal>HelmChart</literal> foi atualizada
navegando até a seção <literal>HelmCharts</literal> do Rancher
(<literal>More Resources → HelmCharts</literal> "Mais recursos →
HelmCharts"). Selecione o namespace em que o gráfico foi implantado; que,
neste exemplo, é <literal>kube-system</literal>.</para>
</listitem>
<listitem>
<para>Por fim, verifique se os pods do Longhorn estão em execução.</para>
</listitem>
</orderedlist>
<para>Depois de concluir as validações acima, é seguro considerar que o upgrade do
gráfico Helm do Longhorn foi feito para a versão
<literal>107.0.0+up1.9.1</literal>.</para>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>Upgrade do gráfico Helm usando uma ferramenta GitOps de terceiros</title>
<para>Em alguns casos de uso, os usuários talvez queiram seguir esse procedimento
de upgrade com um fluxo de trabalho do GitOps diferente do Fleet (por
exemplo, <literal>Flux</literal>).</para>
<para>Para gerar os recursos necessários ao procedimento de upgrade, você pode
usar o script <literal>generate-chart-upgrade-data.sh</literal> para
preencher a instância do Fleet <literal>eib-charts-upgrader</literal> com os
dados fornecidos pelo usuário. Para obter mais informações sobre como fazer
isso, consulte a <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
<para>Com a configuração completa, você pode usar o <link
xl:href="https://kustomize.io">kustomize</link> para gerar uma solução
totalmente funcional para ser implantada no seu cluster:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>Para incluir a solução no fluxo de trabalho do GitOps, remova o arquivo
<literal>fleet.yaml</literal> e use o que sobrou como configuração válida do
<literal>Kustomize</literal>. Lembre-se de executar primeiro o script
<literal>generate-chart-upgrade-data.sh</literal> para que ele possa
preencher a configuração do <literal>Kustomize</literal> com os dados dos
gráficos Helm para o qual você quer fazer upgrade.</para>
<para>Para saber qual é a finalidade de uso desse fluxo de trabalho, consulte a
<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>
e a <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="day2-downstream-clusters">
<title>Clusters downstream</title>
<important>
<para>As etapas a seguir não se aplicam a clusters <literal>downstream</literal>
gerenciados pelo SUSE Telco Cloud (<xref linkend="atip"/>). Para obter
orientação de como fazer upgrade desses clusters, consulte a <xref
linkend="atip-lifecycle-downstream"/>.</para>
</important>
<para>Esta seção apresenta as maneiras possíveis de executar operações de "dia 2"
em diferentes partes do cluster <literal>downstream</literal>.</para>
<section xml:id="downstream-day2-fleet">
<title>Fleet</title>
<para>Esta seção apresenta informações sobre como executar operações de "dia 2"
usando o componente Fleet (<xref linkend="components-fleet"/>).</para>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-components"/>: componentes padrão
usados para todas as operações de "dia 2".</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-determine-use-case"/>: apresenta uma
visão geral dos recursos personalizados do Fleet que serão usados e como se
adaptam aos diferentes casos de uso das operações de "dia 2".</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-upgrade-workflow"/>: fornece um guia de fluxo
de trabalho para execução de operações de "dia 2" com o Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade"/>: descreve como fazer
upgrades de sistema operacional com o Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade"/>: descreve como fazer
upgrades de versão do Kubernetes com o Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade"/>: descreve como fazer
upgrades de gráficos Helm com o Fleet.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-components">
<title>Componentes</title>
<para>Veja a seguir uma descrição dos componentes padrão que você deve configurar
no cluster <literal>downstream</literal> para realizar operações de "dia 2"
com sucesso pelo Fleet.</para>
<section xml:id="id-system-upgrade-controller-suc-2">
<title>System Upgrade Controller (SUC)</title>
<note>
<para><emphasis role="strong">Deve</emphasis> ser implantado em cada cluster
downstream.</para>
</note>
<para><emphasis role="strong">System Upgrade Controller</emphasis> é responsável
por executar tarefas em nós especificados com base nos dados de configuração
fornecidos por um recurso personalizado chamado <literal>plano</literal>.</para>
<para>O <emphasis role="strong">SUC</emphasis> é ativamente usado para fazer
upgrade do sistema operacional e da distribuição Kubernetes.</para>
<para>Para obter mais informações sobre o componente <emphasis
role="strong">SUC</emphasis> e como ele se adapta à pilha do Edge, consulte
o <xref linkend="components-system-upgrade-controller"/>.</para>
<para>Para obter informações sobre como implantar o <emphasis
role="strong">SUC</emphasis>, determine primeiro o seu caso de uso (<xref
linkend="downstream-day2-fleet-determine-use-case"/>) e depois consulte
Instalação do System Upgrade Controller – GitRepo (<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>) ou
Instalação do System Upgrade Controller – Bundle (<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>).</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-determine-use-case">
<title>Determinar seu caso de uso</title>
<para>O Fleet usa dois tipos de <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> para permitir o gerenciamento de recursos do
Kubernetes e do Helm.</para>
<para>Veja a seguir as informações sobre a finalidade desses recursos e para quais
casos de uso eles são mais adequados no contexto das operações de "dia 2".</para>
<section xml:id="id-gitrepo-2">
<title>GitRepo</title>
<para>O <literal>GitRepo</literal> é um recurso do Fleet (<xref
linkend="components-fleet"/>) que representa um repositório Git do qual o
<literal>Fleet</literal> pode criar <literal>Bundles</literal>. Cada
<literal>Bundle</literal> é criado com base nos caminhos de configuração
definidos no recurso <literal>GitRepo</literal>. Para obter mais
informações, consulte a documentação do <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>.</para>
<para>No contexto das operações de "dia 2", os recursos <literal>GitRepo</literal>
costumam ser usados para implantar o <literal>SUC</literal> ou os
<literal>planos do SUC</literal> em ambientes <emphasis role="strong">não
air-gapped</emphasis> que usam a abordagem <emphasis>GitOps do
Fleet</emphasis>.</para>
<para>Se preferir, use os recursos <literal>GitRepo</literal> para implantar o
<literal>SUC</literal> ou os <literal>planos do SUC</literal> em ambientes
<emphasis role="strong">air-gapped</emphasis>, <emphasis role="strong">desde
que você espelhe a configuração do seu repositório por um servidor Git
local</emphasis>.</para>
</section>
<section xml:id="id-bundle-2">
<title>Bundle</title>
<para>Os <literal>Bundles</literal> armazenam recursos <emphasis
role="strong">brutos</emphasis> do Kubernetes que serão implantados no
cluster de destino. Normalmente, eles são criados de um recurso
<literal>GitRepo</literal>, mas há casos de uso em que podem ser implantados
manualmente. Para obter mais informações, consulte a documentação do <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>.</para>
<para>No contexto das operações de "dia 2", os recursos <literal>Bundle</literal>
costumam ser usados para implantar o <literal>SUC</literal> ou
<literal>planos do SUC</literal> em ambientes <emphasis
role="strong">air-gapped</emphasis> que não usam nenhuma forma de
procedimento do <emphasis>GitOps local</emphasis> (por exemplo, um <emphasis
role="strong">servidor git local</emphasis>).</para>
<para>Se o seu caso de uso não possibilita um fluxo de trabalho
<emphasis>GitOps</emphasis> (por exemplo, usando um repositório Git), a
alternativa é usar os recursos <literal>Bundle</literal> para implantar o
<literal>SUC</literal> ou os <literal>planos do SUC</literal> em ambientes
<emphasis role="strong">não air-gapped</emphasis>.</para>
</section>
</section>
<section xml:id="downstream-day2-upgrade-workflow">
<title>Fluxo de trabalho de dia 2</title>
<para>Veja abaixo o fluxo de trabalho de "dia 2" que deve ser seguido para fazer
upgrading de um cluster downstream para uma versão específica do Edge.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Upgrade de sistema operacional (<xref
linkend="downstream-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Upgrade de versão do Kubernetes (<xref
linkend="downstream-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Upgrade de gráfico Helm (<xref
linkend="downstream-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade">
<title>Upgrade de sistema operacional</title>
<para>Esta seção descreve como fazer upgrade de um sistema operacional usando o
<xref linkend="components-fleet"/> e o <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-components"/>: componentes
adicionais usados pelo processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-overview"/>: visão geral do
processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-requirements"/>: requisitos
do processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment"/>:
informações de como implantar os <literal>planos do SUC</literal>,
responsáveis por acionar o processo de upgrade.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-os-upgrade-components">
<title>Componentes</title>
<para>Esta seção aborda os componentes personalizados que o processo de
<literal>upgrade de sistema operacional</literal> usa com os componentes de
"dia 2" padrão (<xref linkend="downstream-day2-fleet-components"/>).</para>
<section xml:id="downstream-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>O upgrade de sistema operacional em um nó específico é executado pelo <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>.</para>
<para>É criado um serviço diferente dependendo do tipo de upgrade que o sistema
operacional requer de uma versão do Edge para outra:</para>
<itemizedlist>
<listitem>
<para>Para versões do Edge que requerem a mesma versão de sistema operacional (por
exemplo, <literal>6.0</literal>), o <literal>os-pkg-update.service</literal>
é criado. Ele usa o <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para fazer <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">upgrade
de um pacote normal</link>.</para>
</listitem>
<listitem>
<para>Para versões do Edge que requerem a migração da versão de sistema
operacional (por exemplo, <literal>6.0</literal> → <literal>6.1</literal>),
o <literal>os-migration.service</literal> é criado. Ele usa o <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para fazer o seguinte:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>O <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">upgrade
de pacote normal</link>, que garante que todos os pacotes sejam atualizados
para mitigar falhas na migração relacionadas a versões antigas dos pacotes.</para>
</listitem>
<listitem>
<para>A migração de sistema operacional usando o comando <literal>zypper
migration</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>Os serviços mencionados acima são distribuídos para cada nó por meio do
<literal>plano do SUC</literal>, que deve estar localizado no cluster
downstream que precisa do upgrade de sistema operacional.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-overview">
<title>Visão geral</title>
<para>O upgrade do sistema operacional para nós de cluster downstream é feito pelo
<literal>Fleet</literal> e pelo <literal>System Upgrade Controller
(SUC)</literal>.</para>
<para>O <emphasis role="strong">Fleet</emphasis> é usado para implantar e
gerenciar os <literal>planos do SUC</literal> no cluster desejado.</para>
<note>
<para>Os <literal>planos do SUC</literal> são <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que descrevem as etapas que o <literal>SUC</literal>
precisa seguir para execução de uma tarefa específica em um conjunto de
nós. Para ver um exemplo de como é um <literal>plano do SUC</literal>,
consulte o <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositório
upstream</link>.</para>
</note>
<para>Os <literal>planos do SUC de sistema operacional</literal> são enviados a
cada cluster por meio da implantação de um recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> ou <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> em um <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espaço
de trabalho</link> do Fleet específico. O Fleet recupera o
<literal>GitRepo/Bundle</literal> implantado e implanta seu conteúdo (os
<literal>planos do SUC de sistema operacional</literal>) no(s) cluster(s)
desejado(s).</para>
<note>
<para>Os recursos <literal>GitRepo/Bundle</literal> sempre são implantados no
<literal>cluster de gerenciamento</literal>. O uso do recurso
<literal>GitRepo</literal> ou <literal>Bundle</literal> depende do seu caso
de uso. Consulte a <xref
linkend="downstream-day2-fleet-determine-use-case"/> para obter mais
informações.</para>
</note>
<para>Os <literal>planos do SUC de sistema operacional</literal> descrevem o
seguinte fluxo de trabalho:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Sempre use o comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
nos nós antes dos upgrades de sistema operacional.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade dos nós do <literal>control-plane</literal> antes dos
nós do <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade do cluster <emphasis role="strong">um</emphasis> nó de
cada vez.</para>
</listitem>
</orderedlist>
<para>Depois que os <literal>planos do SUC de sistema operacional</literal> forem
implantados, o fluxo de trabalho terá esta aparência:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O SUC reconcilia os <literal>planos do SUC de sistema operacional</literal>
implantados e cria um <literal>Kubernetes Job</literal> em <emphasis
role="strong">cada nó</emphasis>.</para>
</listitem>
<listitem>
<para>O <literal>Kubernetes Job</literal> cria um systemd.service (<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>)
para upgrade de pacote ou migração de sistema operacional.</para>
</listitem>
<listitem>
<para>O <literal>systemd.service</literal> criado aciona o processo de upgrade de
sistema operacional no nó específico.</para>
<important>
<para>Após o término do processo de upgrade do sistema operacional, o nó
correspondente será <literal>reinicializado</literal> para aplicar as
atualizações ao sistema.</para>
</important>
</listitem>
</orderedlist>
<para>Veja a seguir um diagrama da descrição acima:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>upgrade so downstream dia2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-requirements">
<title>Requisitos</title>
<para><emphasis>Geral:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Máquina registrada no SCC</emphasis>: todos os nós
do cluster downstream devem ser registrados no <literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal> para
que o respectivo <literal>systemd.service</literal> possa se conectar com
sucesso ao repositório RPM desejado.</para>
<important>
<para>Para lançamentos do Edge que exigem a migração da versão do sistema
operacional (por exemplo, <literal>6.0</literal> → <literal>6.1</literal>),
verifique se a chave do SCC oferece suporte à migração para a nova versão.</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">Garantir que as tolerâncias do plano do SUC
correspondam às do nó</emphasis>: se os nós do cluster Kubernetes têm
<emphasis role="strong">taints</emphasis> personalizados, adicione <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerâncias</link>
a esses taints nos <emphasis role="strong">planos do SUC</emphasis>. Por
padrão, os <emphasis role="strong">planos do SUC</emphasis> têm tolerâncias
apenas para nós do <emphasis role="strong">plano de controle</emphasis>. As
tolerâncias padrão incluem:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>As tolerâncias adicionais devem ser incluídas na seção
<literal>.spec.tolerations</literal> de cada plano. Os <emphasis
role="strong">planos do SUC</emphasis> relacionados a upgrade de sistema
operacional estão disponíveis no repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
em
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>.
<emphasis role="strong">Use os planos de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do repositório.</emphasis></para>
<para>Um exemplo de definição de tolerâncias personalizadas para o plano do SUC de
<emphasis role="strong">plano de controle</emphasis> tem esta aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>Air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Espelhar repositórios RPM do SUSE</emphasis>: os
repositórios RPM de sistema operacional devem ser espelhados localmente para
que o <literal>systemd.service</literal> tenha acesso a eles. Para isso, use
<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>
ou <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment">
<title>Upgrade de sistema operacional: implantação do plano do SUC</title>
<important>
<para>Em ambientes que já passaram por upgrade usando esse procedimento, os
usuários devem garantir que <emphasis role="strong">uma</emphasis> das
seguintes etapas seja concluída:</para>
<itemizedlist>
<listitem>
<para><literal>Remover os planos do SUC que já foram implantados relacionados a
versões de lançamento mais antigas do Edge do cluster downstream</literal>:
para fazer isso, remova o cluster desejado da <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuração
de destino</link> do <literal>GitRepo/Bundle</literal> existente ou remova o
recurso <literal>GitRepo/Bundle</literal> completamente.</para>
</listitem>
<listitem>
<para><literal>Reutilizar o recurso GitRepo/Bundle existente</literal>: para fazer
isso, aponte a revisão do recurso para uma nova tag que inclua as instâncias
do Fleet corretas para a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Isso é feito para evitar conflitos entre os <literal>planos do SUC</literal>
de versões de lançamento mais antigas do Edge.</para>
<para>Se os usuários tentarem fazer upgrade e já houver <literal>planos do
SUC</literal> no cluster downstream, eles verão o seguinte erro no Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Conforme mencionado na <xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>, os upgrades de
sistema operacional são feitos enviando os <literal>planos do SUC</literal>
ao cluster desejado de uma destas maneiras:</para>
<itemizedlist>
<listitem>
<para>Recurso <literal>GitRepo</literal> do Fleet: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>.</para>
</listitem>
<listitem>
<para>Recurso <literal>Bundle</literal> do Fleet: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle"/>.</para>
</listitem>
</itemizedlist>
<para>Para determinar o recurso que você deve usar, consulte a <xref
linkend="downstream-day2-fleet-determine-use-case"/>.</para>
<para>Para casos de uso de implantação dos <literal>planos do SUC de sistema
operacional</literal> de uma ferramenta GitOps de terceiros, consulte a
<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>Implantação do plano do SUC: recurso GitRepo</title>
<para>Um recurso <emphasis role="strong">GitRepo</emphasis>, que distribui os
<literal>planos do SUC de sistema operacional</literal> necessários, pode
ser implantado de uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do sistema
operacional dos nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>Criação do GitRepo: IU do Rancher</title>
<para>Para criar um recurso <literal>GitRepo</literal> usando a IU do Rancher,
consulte a <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentação</link>
oficial do produto.</para>
<para>A equipe do Edge mantém uma <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">instância
do Fleet</link> pronta para uso. Dependendo do seu ambiente, ela pode ser
usada diretamente ou como gabarito.</para>
<important>
<para>Use sempre essa instância do Fleet de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para casos de uso em que não há necessidade de incluir alterações
personalizadas nos <literal>planos do SUC</literal> que a instância do Fleet
envia, os usuários podem fazer referência à instância do Fleet de
<literal>os-upgrade</literal> do repositório
<literal>suse-edge/fleet-examples</literal>.</para>
<para>Nos casos em que as alterações personalizadas são necessárias (por exemplo,
para adicionar tolerâncias personalizadas), os usuários devem fazer
referência à instância do Fleet de <literal>os-upgrade</literal> de um
repositório separado, para que possam adicioná-las aos planos do SUC
conforme necessário.</para>
<para>Há um exemplo de como configurar um <literal>GitRepo</literal> para usar a
instância do Fleet do repositório
<literal>suse-edge/fleet-examples</literal> disponível <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">aqui</link>.</para>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>Criação do GitRepo: manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edite a configuração do <emphasis role="strong">GitRepo</emphasis> e
especifique a lista de destinos desejados em
<literal>spec.targets</literal>. Por padrão, os recursos
<literal>GitRepo</literal> de <literal>suse-edge/fleet-examples</literal>
<emphasis role="strong">NÃO</emphasis> são mapeados para clusters
downstream.</para>
<itemizedlist>
<listitem>
<para>Para corresponder todos os clusters, altere o <emphasis
role="strong">destino</emphasis> do <literal>GitRepo</literal> padrão para:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Se preferir uma seleção mais granular de clusters, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique o recurso <emphasis role="strong">GitRepo</emphasis> ao
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">GitRepo</emphasis> criado no
namespace <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>Implantação do plano do SUC – recurso Bundle</title>
<para>O recurso <emphasis role="strong">Bundle</emphasis>, que envia os
<literal>planos do SUC de sistema operacional</literal> necessários, pode
ser implantado de uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do sistema
operacional dos nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>Criação do bundle – IU do Rancher</title>
<para>A equipe do Edge mantém um <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle</link>
pronto para uso que pode ser usado nas etapas a seguir.</para>
<important>
<para>Sempre use esse bundle de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para criar um bundle pela IU do Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>No canto superior esquerdo, clique em <emphasis role="strong">☰ → Continuous
Delivery</emphasis> (Entrega contínua)</para>
</listitem>
<listitem>
<para>Vá para <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avançado > Bundles).</para>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create from YAML</emphasis> (Criar do
YAML).</para>
</listitem>
<listitem>
<para>Nesse local, você pode criar o bundle de uma das seguintes maneiras:</para>
<note>
<para>Há casos de uso em que você precisa incluir alterações personalizadas nos
<literal>planos do SUC</literal> que o bundle envia (por exemplo, para
adicionar tolerâncias personalizadas). Inclua essas alterações no bundle que
será gerado pelas etapas a seguir.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie manualmente o <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">conteúdo
do bundle</link> de <literal>suse-edge/fleet-examples</literal> para a
página <emphasis role="strong">Create from YAML</emphasis> (Criar do YAML).</para>
</listitem>
<listitem>
<para>Clone o repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
da tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada e selecione a opção <emphasis role="strong">Read from
File</emphasis> (Ler arquivo) na página <emphasis role="strong">Create from
YAML</emphasis> (Criar do YAML). Dessa página, navegue até o local do bundle
(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)
e selecione o arquivo do bundle. Esse procedimento preencherá
automaticamente a página <emphasis role="strong">Create from YAML</emphasis>
(Criar do YAML) com o conteúdo do bundle.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Altere os clusters de <emphasis role="strong">destino</emphasis> para o
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Para corresponder todos os clusters downstream, altere o bundle padrão
<literal>.spec.targets</literal> para:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Para mapeamentos mais granulares de clusters downstream, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create</emphasis> (Criar).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>Criação do bundle – manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o recurso <emphasis role="strong">Bundle</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edite as configurações de <emphasis role="strong">destino</emphasis> do
<literal>Bundle</literal>. Em <literal>spec.targets</literal>, insira a
lista dos destinos desejados. Por padrão, os recursos
<literal>Bundle</literal> de <literal>suse-edge/fleet-examples</literal>
<emphasis role="strong">NÃO</emphasis> são mapeados para clusters
downstream.</para>
<itemizedlist>
<listitem>
<para>Para corresponder todos os clusters, altere o <emphasis
role="strong">destino</emphasis> do <literal>Bundle</literal> padrão para:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Se preferir uma seleção mais granular de clusters, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique o recurso <emphasis role="strong">Bundle</emphasis> ao seu
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">Bundle</emphasis> criado no
namespace <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-default</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>Implantação do plano do SUC: fluxo de trabalho do GitOps de terceiros</title>
<para>Em alguns casos de uso, talvez você queira incorporar os <literal>planos do
SUC de sistema operacional</literal> ao fluxo de trabalho do GitOps de
terceiros (por exemplo, o <literal>Flux</literal>).</para>
<para>Para obter os recursos de upgrade de sistema operacional que você precisa,
primeiro determine a tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do Edge do repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
que deseja usar.</para>
<para>Depois disso, os recursos estarão disponíveis em
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>,
em que:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> é um recurso do plano do SUC para
nós do <emphasis role="strong">plano de controle</emphasis>.</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> é um recurso do plano do SUC para nós do
<emphasis role="strong">worker</emphasis>.</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> é um segredo que contém o script
<literal>upgrade.sh</literal>, responsável por criar o systemd.service
(<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>).</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> é um ConfigMap que armazena as
configurações consumidas pelo script <literal>upgrade.sh</literal>.</para>
</listitem>
</itemizedlist>
<important>
<para>Esses recursos do <literal>plano</literal> são interpretados pelo
<literal>System Upgrade Controller</literal> e devem ser implantados em cada
cluster downstream do qual você deseja fazer upgrade. Para obter informações
sobre a implantação do SUC, consulte a <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para entender melhor como usar o fluxo de trabalho do GitOps para implantar
os <emphasis role="strong">planos do SUC</emphasis> para upgrade de sistema
operacional, consulte a visão geral (<xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>).</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade">
<title>Upgrade da versão do Kubernetes</title>
<important>
<para>Esta seção aborda os upgrades do Kubernetes para clusters downstream que
<emphasis role="strong">NÃO</emphasis> foram criados por uma instância do
Rancher (<xref linkend="components-rancher"/>). Para obter informações de
como fazer upgrade da versão do Kubernetes dos clusters criados pelo
<literal>Rancher</literal>, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes</link> (Fazendo upgrade e rollback do
Kubernetes).</para>
</important>
<para>Esta seção descreve como fazer um upgrade do Kubernetes usando o <xref
linkend="components-fleet"/> e o <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>Os seguintes tópicos são abordados como parte desta seção:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-components"/>: componentes
adicionais usados pelo processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/>: visão geral do
processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-requirements"/>: requisitos
do processo de upgrade.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment"/>:
informações de como implantar os <literal>planos do SUC</literal>,
responsáveis por acionar o processo de upgrade.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components">
<title>Componentes</title>
<para>Esta seção aborda os componentes personalizados que o processo de
<literal>upgrade do K8s</literal> usa com os componentes de "dia 2" padrão
(<xref linkend="downstream-day2-fleet-components"/>).</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>Imagem do contêiner responsável por fazer upgrade da versão do RKE2 de um nó
específico.</para>
<para>Incluída em um pod criado pelo <emphasis role="strong">SUC</emphasis> com
base no <emphasis role="strong">plano do SUC</emphasis>. O plano deve estar
localizado em cada <emphasis role="strong">cluster</emphasis> que precisa de
upgrade do RKE2.</para>
<para>Para obter mais informações sobre como a imagem
<literal>rke2-upgrade</literal> faz o upgrade, consulte a documentação <link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">upstream</link>.</para>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Imagem do contêiner responsável por fazer upgrade da versão do K3s de um nó
específico.</para>
<para>Incluída em um pod criado pelo <emphasis role="strong">SUC</emphasis> com
base no <emphasis role="strong">plano do SUC</emphasis>. O plano deve estar
localizado em cada <emphasis role="strong">cluster</emphasis> que precisa de
upgrade do K3s.</para>
<para>Para obter mais informações sobre como a imagem
<literal>k3s-upgrade</literal> faz o upgrade, consulte a documentação <link
xl:href="https://github.com/k3s-io/k3s-upgrade">upstream</link>.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-overview">
<title>Visão geral</title>
<para>O upgrade de distribuições Kubernetes para nós de cluster downstream é feito
pelo <literal>Fleet</literal> e pelo <literal>System Upgrade Controller
(SUC)</literal>.</para>
<para>O <literal>Fleet</literal> é usado para implantar e gerenciar
<literal>planos do SUC</literal> no cluster desejado.</para>
<note>
<para>Os <literal>planos do SUC</literal> são <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que descrevem as etapas que o <emphasis
role="strong">SUC</emphasis> precisa seguir para execução de uma tarefa
específica em um conjunto de nós. Para ver um exemplo de como é um
<literal>plano do SUC</literal>, consulte o <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositório
upstream</link>.</para>
</note>
<para>Os <literal>planos do SUC do K8s</literal> são enviados a cada cluster por
meio da implantação de um recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> ou <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> em um <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espaço
de trabalho</link> do Fleet específico. O Fleet recupera o
<literal>GitRepo/Bundle</literal> implantado e implanta seu conteúdo (os
<literal>planos do SUC do K8s</literal>) no(s) cluster(s) desejado(s).</para>
<note>
<para>Os recursos <literal>GitRepo/Bundle</literal> sempre são implantados no
<literal>cluster de gerenciamento</literal>. O uso do recurso
<literal>GitRepo</literal> ou <literal>Bundle</literal> depende do seu caso
de uso. Consulte a <xref
linkend="downstream-day2-fleet-determine-use-case"/> para obter mais
informações.</para>
</note>
<para>Os <literal>planos do SUC do K8s</literal> descrevem o seguinte fluxo de
trabalho:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Sempre use o comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
nos nós antes dos upgrades do K8s.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade dos nós do <literal>control-plane</literal> antes dos
nós do <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Sempre faça upgrade dos nós do <literal>control-plane</literal> <emphasis
role="strong">um</emphasis> de cada vez, e dos nós do
<literal>worker</literal> <emphasis role="strong">dois</emphasis> nós de
cada vez.</para>
</listitem>
</orderedlist>
<para>Depois que os <literal>planos do SUC do K8s</literal> forem implantados, o
fluxo de trabalho terá esta aparência:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O SUC reconcilia os <literal>planos do SUC do K8s</literal> implantados e
cria um <literal>Kubernetes Job</literal> em <emphasis role="strong">cada
nó</emphasis>.</para>
</listitem>
<listitem>
<para>Dependendo da distribuição Kubernetes, o job cria um pod que executa a
imagem de contêiner do rke2-upgrade (<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>) ou do
k3s-upgrade (<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>).</para>
</listitem>
<listitem>
<para>O pod criado vai percorrer o seguinte fluxo de trabalho:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Substitua o binário <literal>rke2/k3s</literal> existente no nó pelo da
imagem <literal>rke2-upgrade/k3s-upgrade</literal>.</para>
</listitem>
<listitem>
<para>Cancele o processo do <literal>rke2/k3s</literal> em execução.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>O cancelamento do processo do <literal>rke2/k3s</literal> aciona a
reinicialização, o que inicia um novo processo que executa o binário
atualizado, resultando em uma versão atualizada da distribuição Kubernetes.</para>
</listitem>
</orderedlist>
<para>Veja a seguir um diagrama da descrição acima:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>upgrade k8s downstream dia2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-requirements">
<title>Requisitos</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Faça backup da distribuição Kubernetes:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Para <emphasis role="strong">clusters RKE2</emphasis>, consulte a
documentação sobre <link
xl:href="https://docs.rke2.io/datastore/backup_restore">backup e restauração
do RKE2</link>.</para>
</listitem>
<listitem>
<para>Para <emphasis role="strong">clusters K3s</emphasis>, consulte a
documentação sobre <link
xl:href="https://docs.k3s.io/datastore/backup-restore">backup e restauração
do K3s</link>.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Garantir que as tolerâncias do plano do SUC
correspondam às do nó</emphasis>: se os nós do cluster Kubernetes têm
<emphasis role="strong">taints</emphasis> personalizados, adicione <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerâncias</link>
a esses taints nos <emphasis role="strong">planos do SUC</emphasis>. Por
padrão, os <emphasis role="strong">planos do SUC</emphasis> têm tolerâncias
apenas para nós do <emphasis role="strong">plano de controle</emphasis>. As
tolerâncias padrão incluem:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>As tolerâncias adicionais devem ser incluídas na seção
<literal>.spec.tolerations</literal> de cada plano. Os <emphasis
role="strong">planos do SUC</emphasis> relacionados ao upgrade de versões do
Kubernetes estão disponíveis no repositório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
em:</para>
<itemizedlist>
<listitem>
<para>Para o <emphasis role="strong">RKE2</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>Para o <emphasis role="strong">K3s</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Use os planos de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do repositório.</emphasis></para>
<para>Um exemplo de definição de tolerâncias personalizadas para o plano do SUC de
<emphasis role="strong">plano de controle</emphasis> do RKE2 tem esta
aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment">
<title>Upgrade do K8s: implantação do plano do SUC</title>
<important>
<para>Em ambientes que já passaram por upgrade usando esse procedimento, os
usuários devem garantir que <emphasis role="strong">uma</emphasis> das
seguintes etapas seja concluída:</para>
<itemizedlist>
<listitem>
<para><literal>Remover os planos do SUC que já foram implantados relacionados a
versões de lançamento mais antigas do Edge do cluster downstream</literal>:
para fazer isso, remova o cluster desejado da <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuração
de destino</link> do <literal>GitRepo/Bundle</literal> existente ou remova o
recurso <literal>GitRepo/Bundle</literal> completamente.</para>
</listitem>
<listitem>
<para><literal>Reutilizar o recurso GitRepo/Bundle existente</literal>: para fazer
isso, aponte a revisão do recurso para uma nova tag que inclua as instâncias
do Fleet corretas para a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Isso é feito para evitar conflitos entre os <literal>planos do SUC</literal>
de versões de lançamento mais antigas do Edge.</para>
<para>Se os usuários tentarem fazer upgrade e já houver <literal>planos do
SUC</literal> no cluster downstream, eles verão o seguinte erro no Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Conforme mencionado na <xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>, os upgrades do
Kubernetes são feitos enviando os <literal>planos do SUC</literal> ao
cluster desejado de uma destas maneiras:</para>
<itemizedlist>
<listitem>
<para>Recurso GitRepo do Fleet (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Recurso Bundle do Fleet (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>Para determinar o recurso que você deve usar, consulte a <xref
linkend="downstream-day2-fleet-determine-use-case"/>.</para>
<para>Para casos de uso de implantação dos <literal>planos do SUC do K8s</literal>
de uma ferramenta GitOps de terceiros, consulte a <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>Implantação do plano do SUC: recurso GitRepo</title>
<para>Um recurso <emphasis role="strong">GitRepo</emphasis>, que distribui os
<literal>planos do SUC do K8s</literal> necessários, pode ser implantado de
uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do Kubernetes dos
nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>Criação do GitRepo: IU do Rancher</title>
<para>Para criar um recurso <literal>GitRepo</literal> usando a IU do Rancher,
consulte a <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentação</link>
oficial do produto.</para>
<para>A equipe do Edge mantém instâncias do Fleet prontas para uso para ambas as
distribuições Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>
e <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>.
Dependendo do seu ambiente, é possível usar a instância do Fleet diretamente
ou como gabarito.</para>
<important>
<para>Sempre use as instâncias do Fleet de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para casos de uso em que não há necessidade de incluir alterações
personalizadas nos <literal>planos do SUC</literal> que as instâncias do
Fleet envia, os usuários podem fazer referência a essas instâncias do Fleet
diretamente do repositório <literal>suse-edge/fleet-examples</literal>.</para>
<para>Nos casos em que as alterações personalizadas são necessárias (por exemplo,
para adicionar tolerâncias personalizadas), os usuários devem fazer
referência às instâncias do Fleet de um repositório separado, para que
possam adicionas as alterações aos planos do SUC conforme necessário.</para>
<para>Exemplos de configuração do recurso <literal>GitRepo</literal> usando as
instâncias do Fleet do repositório
<literal>suse-edge/fleet-examples</literal>:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>Criação do GitRepo: manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha o recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clusters <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para clusters <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite a configuração do <emphasis role="strong">GitRepo</emphasis> e
especifique a lista de destinos desejados em
<literal>spec.targets</literal>. Por padrão, os recursos
<literal>GitRepo</literal> de <literal>suse-edge/fleet-examples</literal>
<emphasis role="strong">NÃO</emphasis> são mapeados para clusters
downstream.</para>
<itemizedlist>
<listitem>
<para>Para corresponder todos os clusters, altere o <emphasis
role="strong">destino</emphasis> do <literal>GitRepo</literal> padrão para:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Se preferir uma seleção mais granular de clusters, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique os recursos <emphasis role="strong">GitRepo</emphasis> ao
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">GitRepo</emphasis> criado no
namespace <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>Implantação do plano do SUC – recurso Bundle</title>
<para>O recurso <emphasis role="strong">Bundle</emphasis>, que envia os
<literal>planos do SUC de upgrade do Kubernetes</literal> necessários, pode
ser implantado de uma das seguintes maneiras:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pela <literal>IU do Rancher</literal>: <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>
(quando o <literal>Rancher</literal> está disponível).</para>
</listitem>
<listitem>
<para>Pela implantação manual do recurso (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>)
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, para monitorar o processo de upgrade do Kubernetes dos
nós do cluster de destino, consulte a <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>Criação do bundle – IU do Rancher</title>
<para>A equipe do Edge mantém bundles prontos para uso para ambas as distribuições
Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>
e <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>.
Dependendo do seu ambiente, é possível usar os bundles diretamente ou como
gabaritos.</para>
<important>
<para>Sempre use esse bundle de uma tag de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
válida do Edge.</para>
</important>
<para>Para criar um bundle pela IU do Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>No canto superior esquerdo, clique em <emphasis role="strong">☰ → Continuous
Delivery</emphasis> (Entrega contínua)</para>
</listitem>
<listitem>
<para>Vá para <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avançado > Bundles).</para>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create from YAML</emphasis> (Criar do
YAML).</para>
</listitem>
<listitem>
<para>Nesse local, você pode criar o bundle de uma das seguintes maneiras:</para>
<note>
<para>Há casos de uso em que você precisa incluir alterações personalizadas nos
<literal>planos do SUC</literal> que o bundle envia (por exemplo, para
adicionar tolerâncias personalizadas). Inclua essas alterações no bundle que
será gerado pelas etapas a seguir.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie manualmente o conteúdo do bundle para <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>
ou <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>
de <literal>suse-edge/fleet-examples</literal> para a página <emphasis
role="strong">Create from YAML</emphasis> (Criar do YAML).</para>
</listitem>
<listitem>
<para>Clone o repositório <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
da tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
desejada e selecione a opção <emphasis role="strong">Read from
File</emphasis> (Ler arquivo) na página <emphasis role="strong">Create from
YAML</emphasis> (Criar do YAML). Dessa página, navegue até o bundle
necessário
(<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>
para RKE2 e
<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>
para K3s). Esse procedimento preencherá automaticamente a página <emphasis
role="strong">Create from YAML</emphasis> (Criar do YAML) com o conteúdo do
bundle.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Altere os clusters de <emphasis role="strong">destino</emphasis> para o
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Para corresponder todos os clusters downstream, altere o bundle padrão
<literal>.spec.targets</literal> para:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Para mapeamentos mais granulares de clusters downstream, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Selecione <emphasis role="strong">Create</emphasis> (Criar).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>Criação do bundle – manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenha os recursos <emphasis role="strong">Bundle</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clusters <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Para clusters <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite as configurações de <emphasis role="strong">destino</emphasis> do
<literal>Bundle</literal>. Em <literal>spec.targets</literal>, insira a
lista dos destinos desejados. Por padrão, os recursos
<literal>Bundle</literal> de <literal>suse-edge/fleet-examples</literal>
<emphasis role="strong">NÃO</emphasis> são mapeados para clusters
downstream.</para>
<itemizedlist>
<listitem>
<para>Para corresponder todos os clusters, altere o <emphasis
role="strong">destino</emphasis> do <literal>Bundle</literal> padrão para:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Se preferir uma seleção mais granular de clusters, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique os recursos <emphasis role="strong">Bundle</emphasis> ao
<literal>cluster de gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Visualize o recurso <emphasis role="strong">Bundle</emphasis> criado no
namespace <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>Implantação do plano do SUC: fluxo de trabalho do GitOps de terceiros</title>
<para>Em alguns casos de uso, talvez você queira incorporar os <literal>planos do
SUC de upgrade do Kubernetes</literal> ao fluxo de trabalho do GitOps de
terceiros (por exemplo, o <literal>Flux</literal>).</para>
<para>Para obter os recursos de upgrade do K8s upgrade que você precisa, primeiro
determine a tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do Edge do repositório <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
que deseja usar.</para>
<para>Depois disso, os recursos estarão em:</para>
<itemizedlist>
<listitem>
<para>Para ugrade do cluster RKE2:</para>
<itemizedlist>
<listitem>
<para>Para os nós do <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para os nós de <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Para upgrade do cluster K3s:</para>
<itemizedlist>
<listitem>
<para>Para os nós de <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para os nós de <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>Esses recursos do <literal>plano</literal> são interpretados pelo
<literal>System Upgrade Controller</literal> e devem ser implantados em cada
cluster downstream do qual você deseja fazer upgrade. Para obter informações
sobre a implantação do SUC, consulte a <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para entender melhor como usar o fluxo de trabalho do GitOps para implantar
os <emphasis role="strong">planos do SUC</emphasis> para upgrade de versão
do Kubernetes, consulte a visão geral (<xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>) do procedimento de
atualização com o <literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade">
<title>Upgrade do gráfico Helm</title>
<para>Esta seção aborda as seguintes partes:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-air-gap"/>: armazena
informações sobre como enviar gráficos e imagens OCI relacionados ao Edge
para seu registro particular.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure"/>: armazena
informações sobre diversos casos de uso de upgrade de gráficos Helm e o
respectivo procedimento de upgrade.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-air-gap">
<title>Preparação para ambientes air-gapped</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet-2">
<title>Garantir que você tenha acesso ao Fleet por gráfico Helm</title>
<para>Dependendo da compatibilidade do seu ambiente, você poderá seguir uma destas
opções:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Hospede os recursos do Fleet do seu gráfico em um servidor Git local que
possa ser acessado pelo <literal>cluster de gerenciamento</literal>.</para>
</listitem>
<listitem>
<para>Use a CLI do Fleet para <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">converter
um gráfico Helm em bundle</link> que você possa usar diretamente, sem
precisar hospedá-lo. É possível recuperar a CLI do Fleet da página da <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">versão</link>.
Para usuários do Mac, há um Homebrew Formulae <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version-2">
<title>Encontrar os ativos necessários à sua versão de lançamento do Edge</title>
<orderedlist numeration="arabic">
<listitem>
<para>Vá para a página de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versões</link>
do "dia 2" e localize a versão do Edge para a qual você quer fazer upgrade
do gráfico e clique em <emphasis role="strong">Assets</emphasis> (Ativos).</para>
</listitem>
<listitem>
<para>Na seção <emphasis role="strong">"Assets"</emphasis> (Ativos), faça download
dos seguintes arquivos:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Arquivo de versão</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Descrição</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Obtém as imagens especificadas no arquivo
<literal>edge-release-images.txt</literal> e as compacta em um arquivo
".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Obtém as imagens de gráfico OCI relacionadas à versão especifica do Edge e
as compacta em um arquivo ".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Carrega as imagens de um arquivo ".tar.gz", faz a remarcação e as envia a um
registro particular.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Detecta o diretório que contém os pacotes ".tgz" de gráficos OCI do Edge e
os carrega em um registro particular.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contém uma lista de imagens de gráfico OCI relacionadas a uma versão
específica do Edge.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contém uma lista de imagens relacionadas a uma versão específica do Edge.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive-2">
<title>Criar o arquivo de imagens da versão do Edge</title>
<para><emphasis>Em uma máquina com acesso à Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Torne o <literal>edge-save-images.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Gere o arquivo de imagens:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esse procedimento cria um arquivo pronto para carregamento chamado
<literal>edge-images.tar.gz</literal>.</para>
<note>
<para>Se a opção <literal>-i|--images</literal> foi especificada, o nome do
arquivo pode ser diferente.</para>
</note>
</listitem>
<listitem>
<para>Copie esse arquivo para sua máquina <emphasis
role="strong">air-gapped</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive-2">
<title>Criar o arquivo de imagens de gráfico OCI do Edge</title>
<para><emphasis>Em uma máquina com acesso à Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Torne o <literal>edge-save-oci-artefacts.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Gere o arquivo de imagens de gráfico OCI:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esse procedimento cria um arquivo chamado
<literal>oci-artefacts.tar.gz</literal>.</para>
<note>
<para>Se a opção <literal>-a|--archive</literal> foi especificada, o nome do
arquivo pode ser diferente.</para>
</note>
</listitem>
<listitem>
<para>Copie esse arquivo para sua máquina <emphasis
role="strong">air-gapped</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine-2">
<title>Carregar as imagens da versão do Edge para sua máquina air-gapped</title>
<para><emphasis>Na máquina air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Faça login no seu registro particular (se necessário):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Torne o <literal>edge-load-images.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Execute o script, especificando o arquivo já <emphasis
role="strong">copiado</emphasis> <literal>edge-images.tar.gz</literal>:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>Desse modo, todas as imagens do <literal>edge-images.tar.gz</literal> serão
carregadas, remarcadas e enviadas ao registro especificado na opção
<literal>--registry</literal>.</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine-2">
<title>Carregar as imagens de gráfico OCI do Edge em sua máquina air-gapped</title>
<para><emphasis>Na máquina air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Faça login no seu registro particular (se necessário):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Torne o <literal>edge-load-oci-artefacts.sh</literal> executável:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Descompacte o arquivo <literal>oci-artefacts.tar.gz</literal> copiado:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>Isso cria um diretório com o gabarito de nomenclatura
<literal>edge-release-oci-tgz-&lt;data&gt;</literal>.</para>
</listitem>
<listitem>
<para>Especifique esse diretório no script
<literal>edge-load-oci-artefacts.sh</literal> para carregar as imagens de
gráfico OCI do Edge em seu registro particular:</para>
<note>
<para>Esse script assume que a CLI <literal>helm</literal> foi pré-instalada em
seu ambiente. Para obter instruções de instalação do Helm, consulte <link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>
(Instalando o Helm).</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution-2">
<title>Configurar o registro particular na distribuição Kubernetes</title>
<para>Para RKE2, consulte <link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration</link> (Configuração de registro particular)</para>
<para>Para K3s, consulte <link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration</link> (Configuração de registro particular)</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure">
<title>Procedimento de upgrade</title>
<para>Esta seção tem como foco os seguintes casos de uso do procedimento de
upgrade do Helm:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>Não é possível fazer upgrade confiável de gráficos Helm implantados
manualmente. Sugerimos reimplantá-los usando o método da <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/>.</para>
</important>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>Eu tenho um novo cluster e desejo implantar e gerenciar um gráfico Helm do
Edge</title>
<para>Esta seção explica como:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>Preparar os recursos do Fleet para seu gráfico</title>
<orderedlist numeration="arabic">
<listitem>
<para>Adquira os recursos do Fleet do gráfico com base na tag da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versão</link>
do Edge que deseja usar.</para>
</listitem>
<listitem>
<para>Navegue até a instância do Fleet do gráfico Helm
(<literal>fleets/day2/chart-templates/&lt;gráfico&gt;</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Se você pretende usar um fluxo de trabalho do
GitOps</emphasis>, copie o diretório do Fleet do gráfico para o repositório
Git do qual você vai executar o GitOps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, se o gráfico Helm exigir
configurações em seus <emphasis role="strong">valores</emphasis>, edite a
configuração <literal>.helm.values</literal> no arquivo
<literal>fleet.yaml</literal> do diretório copiado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, pode haver casos de uso em
que você tenha que adicionar outros recursos à instância do Fleet do seu
gráfico para que se adapte melhor ao seu ambiente. Para obter informações de
como aprimorar o diretório do Fleet, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository
Contents</link> (Conteúdo do repositório Git).</para>
</listitem>
</orderedlist>
<note>
<para>Em alguns casos, o tempo limite padrão que o Fleet usa nas operações do Helm
pode ser insuficiente e resultar no seguinte erro:</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>Nesses casos, adicione a propriedade <link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>
à configuração <literal>helm</literal> do arquivo
<literal>fleet.yaml</literal>.</para>
</note>
<para>Esta é a aparência de um <emphasis role="strong">exemplo</emphasis> de
gráfico Helm do <literal>longhorn</literal>:</para>
<itemizedlist>
<listitem>
<para>Estrutura de repositórios Git do usuário:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>Conteúdo do <literal>fleet.yaml</literal> preenchido com os dados do
<literal>Longhorn</literal> do usuário:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>Estes são apenas valores de exemplo usados para ilustrar as configurações
comuns no gráfico do <literal>longhorn</literal>. Eles <emphasis
role="strong">NÃO</emphasis> devem ser considerados diretrizes de
implantação para o gráfico do <literal>longhorn</literal>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>Implantar o Fleet para seu gráfico</title>
<para>Você pode implantar a instância do Fleet para seu gráfico usando o GitRepo
(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>)
ou o bundle (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>).</para>
<note>
<para>Durante a implantação do Fleet, se você receber a mensagem
<literal>Modified</literal> (Modificado), adicione uma entrada
<literal>comparePatches</literal> correspondente à seção
<literal>diff</literal> do Fleet. Para obter mais informações, consulte
<link xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to
Ignore Modified GitRepos</link> (Gerando diffs para ignorar GitRepos
modificados).</para>
</note>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>O recurso <link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> do Fleet
armazena as informações sobre como acessar os recursos do Fleet do seu
gráfico e a quais clusters ele precisa aplicar esses recursos.</para>
<para>É possível implantar o recurso <literal>GitRepo</literal> usando a <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">IU
do Rancher</link> ou, manualmente, <link
xl:href="https://fleet.rancher.io/tut-deployment">implantando</link> o
recurso no <literal>cluster de gerenciamento</literal>.</para>
<para>Exemplo de recurso <literal>GitRepo</literal> do <emphasis
role="strong">Longhorn</emphasis> para implantação <emphasis
role="strong">manual</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>Bundle</title>
<para>Os recursos <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> armazenam os
recursos brutos do Kubernetes que o Fleet precisa implantar. Normalmente, a
recomendação é aplicar a abordagem do <literal>GitRepo</literal>, mas para
casos de uso em que o ambiente é air-gapped e não oferece suporte a um
servidor Git local, os <literal>Bundles</literal> podem ajudar na propagação
do Fleet do gráfico Helm aos clusters de destino.</para>
<para>É possível implantar um <literal>Bundle</literal> pela IU do Rancher
(<literal>Continuous Delivery → Advanced → Bundles → Create from
YAML</literal> (Entrega contínua → Avançado → Bundles → Criar do YAML)) ou
pela implantação manual do recurso <literal>Bundle</literal> no namespace
correto do Fleet. Para obter informações sobre os namespaces do Fleet,
consulte a <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentação</link>
upstream.</para>
<para>É possível criar <literal>Bundles</literal> para gráficos Helm do Edge
usando a abordagem <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Converter
um gráfico Helm em bundle</link> do Fleet.</para>
<para>Veja a seguir um exemplo de como criar um recurso <literal>Bundle</literal>
com base nos gabaritos de instância Fleet <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
e <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
do gráfico Helm e implantar manualmente esse bundle no <literal>cluster de
gerenciamento</literal>.</para>
<note>
<para>Para ilustrar o fluxo de trabalho, o exemplo a seguir usa a estrutura de
diretório <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Navegue até o gabarito de instância Fleet do gráfico <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>Crie um arquivo <literal>targets.yaml</literal> para instruir o Fleet sobre
os clusters em que ele deve implantar o gráfico Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>Para uma seleção mais granular de clusters downstream, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Mapeando para clusters downstream).</para>
</listitem>
<listitem>
<para>Converta o gráfico Helm <literal>Longhorn</literal> do Fleet em um recurso
bundle usando <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<note>
<para>É possível recuperar a CLI do Fleet da página <emphasis
role="strong">Assets</emphasis> (Ativos) da <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">versão</link>
(<literal>fleet-linux-amd64</literal>).</para>
<para>Para usuários do Mac, existe um Homebrew Formulae <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>Navegue até o gabarito de instância do Fleet do gráfico <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>Crie um arquivo <literal>targets.yaml</literal> para instruir o Fleet sobre
os clusters em que ele deve implantar o gráfico Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
</listitem>
<listitem>
<para>Converta o gráfico Helm <literal>Longhorn CRD</literal> do Fleet em um
recurso bundle usando <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>Implante os arquivos <literal>longhorn-bundle.yaml</literal> e
<literal>longhorn-crd-bundle.yaml</literal> no <literal>cluster de
gerenciamento</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>Siga estas etapas para garantir que o <literal>SUSE Storage</literal> seja
implantado em todos os clusters downstream especificados.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>Gerenciar o gráfico Helm implantado</title>
<para>Após a implantação com o Fleet, para fazer upgrade dos gráficos Helm,
consulte a <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>Quero fazer upgrade de um gráfico Helm gerenciado pelo Fleet</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine a versão para a qual você precisa fazer upgrade do seu gráfico
para que ele fique compatível com o lançamento desejado do Edge. Você
encontra a versão do gráfico Helm por lançamento do Edge nas Notas de
lançamento (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>No repositório Git monitorado pelo Fleet, edite o arquivo
<literal>fleet.yaml</literal> do gráfico Helm com a <emphasis
role="strong">versão</emphasis> e o <emphasis
role="strong">repositório</emphasis> corretos do gráfico conforme as Notas
de lançamento (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>Depois de confirmar e enviar as alterações ao repositório, será acionado um
upgrade do gráfico Helm desejado.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>Eu quero fazer upgrade de um gráfico Helm implantado pelo EIB</title>
<para>O <xref linkend="components-eib"/> implanta gráficos Helm criando um recurso
<literal>HelmChart</literal> e usando o <literal>helm-controller</literal>
introduzido pelo recurso de integração do Helm <link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>.</para>
<para>Para garantir que o upgrade do gráfico Helm implantado pelo
<literal>EIB</literal> seja feito com sucesso, os usuários precisam fazer
upgrade dos respectivos recursos <literal>HelmChart</literal>.</para>
<para>Veja a seguir informações sobre:</para>
<itemizedlist>
<listitem>
<para>A visão geral (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>)
do processo de upgrade.</para>
</listitem>
<listitem>
<para>As etapas de upgrade necessárias (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>).</para>
</listitem>
<listitem>
<para>Um exemplo (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>)
que demonstra o upgrade de um gráfico <link
xl:href="https://longhorn.io">Longhorn</link> usando o método explicado.</para>
</listitem>
<listitem>
<para>Como usar o processo de upgrade com uma ferramenta GitOps diferente (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>Visão geral</title>
<para>O upgrade dos gráficos Helm que são implantados pelo <literal>EIB</literal>
é feito por uma instância do <literal>Fleet</literal> chamada <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>.</para>
<para>Essa instância do <literal>Fleet</literal> processa os dados <emphasis
role="strong">fornecidos pelo usuário</emphasis> para <emphasis
role="strong">atualizar</emphasis> um conjunto específico de recursos
HelmChart.</para>
<para>A atualização desses recursos aciona o <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>,
que faz <emphasis role="strong">upgrade</emphasis> dos gráficos Helm
associados aos recursos <literal>HelmChart</literal> modificados.</para>
<para>O usuário precisa apenas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><link xl:href="https://helm.sh/docs/helm/helm_pull/">Obter</link> localmente
os arquivos para cada gráfico Helm que precisa de upgrade.</para>
</listitem>
<listitem>
<para>Especificar esses arquivos no script <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal>, que incluirá os dados dos
arquivos na instância do Fleet <literal>eib-charts-upgrader</literal>.</para>
</listitem>
<listitem>
<para>Implantar a instância do Fleet <literal>eib-charts-upgrader</literal> no
respectivo <literal>cluster de gerenciamento</literal>. Isso é feito por um
recurso <literal>GitRepo</literal> ou <literal>Bundle</literal>.</para>
</listitem>
</orderedlist>
<para>Após a implantação, o <literal>eib-charts-upgrader</literal>, com ajuda do
Fleet, enviará os recursos para o cluster downstream desejado.</para>
<para>Esses recursos incluem:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Um conjunto de <literal>segredos</literal> com os dados do gráfico Helm
<emphasis role="strong">fornecidos pelo usuário</emphasis>.</para>
</listitem>
<listitem>
<para>Um <literal>Kubernetes Job</literal> para implantar o <literal>pod</literal>
que vai montar os <literal>segredos</literal> já mencionados e, com base
neles, aplicar o <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">patch</link>
aos recursos HelmChart correspondentes.</para>
</listitem>
</orderedlist>
<para>Como já foi mencionado, isso acionará o <literal>helm-controller</literal>,
que faz upgrade do gráfico Helm real.</para>
<para>Veja a seguir um diagrama da descrição acima:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-downstream-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>upgrade eib helm downstream dia2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>Etapas de upgrade</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clone o repositório <literal>suse-edge/fleet-examples</literal> da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>
da versão correta.</para>
</listitem>
<listitem>
<para>Crie um diretório para armazenar um ou mais arquivos dos gráficos Helm
enviados.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Dentro do diretório do arquivo recém-criado, <link
xl:href="https://helm.sh/docs/helm/helm_pull/">extraia</link> os arquivos
dos gráficos Helm dos quais você deseja fazer upgrade:</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>Em <emphasis role="strong">Assets</emphasis> (Ativos) da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag
da versão</link> desejada, faça download do script
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</listitem>
<listitem>
<para>Execute o script <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>Para cada arquivo de gráfico no diretório <literal>--archive-dir</literal>,
o script gera um arquivo <literal>YAML de segredo do Kubernetes</literal>
com os dados de upgrade do gráfico e o armazena no diretório
<literal>base/secrets</literal> da instância do Fleet especificada por
<literal>--fleet-path</literal>.</para>
<para>O script <literal>generate-chart-upgrade-data.sh</literal> também aplica
modificações adicionais à instância do Fleet para garantir que os arquivos
<literal>YAML de segredo do Kubernetes</literal> gerados sejam corretamente
usados pela carga de trabalho implantada pelo Fleet.</para>
<important>
<para>Os usuários não devem fazer alterações no conteúdo que é gerado pelo script
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</important>
</listitem>
</orderedlist>
<para>As etapas abaixo dependem do ambiente que está em execução:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Para um ambiente com suporte a GitOps (por exemplo, não air-gapped ou
air-gapped, mas com suporte a servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie a instância do Fleet
<literal>fleets/day2/eib-charts-upgrader</literal> para o repositório que
você vai usar com o GitOps.</para>
<note>
<para>Garanta que o Fleet inclua as alterações feitas pelo script
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</note>
</listitem>
<listitem>
<para>Configure o recurso <literal>GitRepo</literal> que será usado para enviar
todos os recursos da instância do Fleet
<literal>eib-charts-upgrader</literal>.</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Para configuração e implantação do <literal>GitRepo</literal> pela IU do
Rancher, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI</link> (Acessando o Fleet na IU do Racher).</para>
</listitem>
<listitem>
<para>Para configuração e implantação manuais do <literal>GitRepo</literal>,
consulte <link xl:href="https://fleet.rancher.io/tut-deployment">Creating a
Deployment</link> (Criando uma implantação).</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Para um ambiente sem suporte a GitOps (por exemplo, air-gapped que não
permite o uso de servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Faça download do binário <literal>fleet-cli</literal> da página da
<literal>rancher/fleet</literal> <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">versão</link>
(<literal>fleet-linux-amd64</literal> para Linux). Para usuários do Mac, é
possível usar o Homebrew Formulae disponível: <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Navegue até a instância do Fleet <literal>eib-charts-upgrader</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Crie um arquivo <literal>targets.yaml</literal> para instruir o Fleet sobre
onde implantar seus recursos:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To match all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>Para obter informações sobre como mapear clusters de destino, consulte a
<link xl:href="https://fleet.rancher.io/gitrepo-targets">documentação</link>
upstream.</para>
</listitem>
<listitem>
<para>Use <literal>fleet-cli</literal> para converter o Fleet em um recurso
<literal>Bundle</literal>:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>Isso cria um bundle (<literal>bundle.yaml</literal>) para armazenar todos os
recursos usados como gabaritos da instância do Fleet
<literal>eib-charts-upgrader</literal>.</para>
<para>Para obter mais informações sobre o comando <literal>fleet apply</literal>,
consulte <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>.</para>
<para>Para obter mais informações sobre como converter instâncias do Fleet em
bundles, consulte <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Converter um gráfico Helm em bundle).</para>
</listitem>
<listitem>
<para>Implante o <literal>Bundle</literal>. É possível fazer isso de uma destas
duas maneiras:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Pela IU do Rancher: navegue até <emphasis role="strong">Continuous Delivery
→ Advanced → Bundles → Create from YAML</emphasis> (Entrega contínua →
Avançado → Bundles → Criar do YAML) e cole o conteúdo do
<literal>bundle.yaml</literal> ou clique na opção <literal>Read from
File</literal> (Ler arquivo) e especifique o arquivo.</para>
</listitem>
<listitem>
<para>Manualmente: implante o arquivo <literal>bundle.yaml</literal> manualmente
no <literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>A execução dessas etapas resulta na implantação bem-sucedida do recurso
<literal>GitRepo/Bundle</literal>. O Fleet seleciona o recurso, e o conteúdo
dele é implantado nos clusters de destino que o usuário especificou nas
etapas anteriores. Para obter uma visão geral do processo, consulte a <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>.</para>
<para>Para obter informações sobre como acompanhar o processo de upgrade, consulte
a <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>.</para>
<important>
<para>Após a verificação bem-sucedida do upgrade do gráfico, remova o recurso
<literal>Bundle/GitRepo</literal>.</para>
<para>Isso removerá os recursos de upgrade que não são mais necessários do cluster
<literal>downstream</literal>, garantindo que não haja conflitos com versões
futuras.</para>
</important>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>Exemplo</title>
<note>
<para>No exemplo a seguir, demonstramos como fazer upgrade de um gráfico Helm
implantado pelo <literal>EIB</literal> de uma versão para outra em um
cluster <literal>downstream</literal>. Observe que as versões usadas neste
exemplo <emphasis role="strong">não</emphasis> são recomendações. Para
recomendações de versão específicas a um lançamento do Edge, consulte as
Notas de lançamento (<xref linkend="release-notes"/>).</para>
</note>
<para><emphasis>Caso de uso:</emphasis></para>
<itemizedlist>
<listitem>
<para>Um cluster chamado <literal>doc-example</literal> executa uma versão mais
antiga do <link xl:href="https://longhorn.io">Longhorn</link>.</para>
</listitem>
<listitem>
<para>O cluster foi implantado pelo EIB usando o seguinte
<emphasis>trecho</emphasis> de definição da imagem:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para>É necessário fazer upgrade do <literal>SUSE Storage</literal> para uma
versão compatível com o Edge 3.4, ou seja, para a versão
<literal>107.0.0+up1.9.1</literal>.</para>
</listitem>
<listitem>
<para>Presume-se que o <literal>cluster de gerenciamento</literal> responsável
pelo gerenciamento do <literal>doc-example</literal> seja <emphasis
role="strong">air-gapped</emphasis>, sem suporte a servidor Git local e com
uma instalação ativa do Rancher.</para>
</listitem>
</itemizedlist>
<para>Siga as etapas de upgrade (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>):</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clone o repositório <literal>suse-edge/fleet-example</literal> da tag
<literal>release-3.4.0</literal>.</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Crie um diretório para armazenar o arquivo de upgrade do
<literal>Longhorn</literal>.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Extraia a versão desejada do arquivo de gráficos do
<literal>Longhorn</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>Fora do diretório <literal>archives</literal>, faça download do script
<literal>generate-chart-upgrade-data.sh</literal> da <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">tag</link>
da versão do <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>A configuração do diretório deve ter uma aparência como esta:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>Execute o script <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>A estrutura de diretórios após a execução do script deve ser parecida com
esta:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Os arquivos alterados no Git devem ter aparência similar a esta:</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>Crie um <literal>Bundle</literal> para a instância do Fleet
<literal>eib-charts-upgrader</literal>:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Primeiro, navegue até o Fleet:</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Em seguida, crie um arquivo <literal>targets.yaml</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: doc-example
EOF</screen>
</listitem>
<listitem>
<para>Depois disso, use o binário <literal>fleet-cli</literal> para converter a
instância do Fleet em um bundle:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
<listitem>
<para>Agora transfira o <literal>bundle.yaml</literal> para a máquina do
<literal>cluster de gerenciamento</literal>.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Implante o bundle usando a IU do Rancher:</para>
<figure>
<title>Implantar o bundle usando a IU do Rancher</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>exemplo 1 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject></figure>
<para>Agora selecione <emphasis role="strong">Read from File</emphasis> (Ler
arquivo) e encontre o arquivo <literal>bundle.yaml</literal> no sistema.</para>
<para>Isso preencherá automaticamente o <literal>Bundle</literal> na IU do
Rancher.</para>
<para>Selecione <emphasis role="strong">Create</emphasis> (Criar).</para>
</listitem>
<listitem>
<para>Após a implantação bem-sucedida, o bundle terá uma aparência similar a esta:</para>
<figure>
<title>Bundle implantado com sucesso</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>exemplo 2 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>Após a implantação bem-sucedida do <literal>Bundle</literal>, monitore o
processo de upgrade da seguinte maneira:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Consulte os registros do <literal>pod de upgrade</literal>:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>downstream exemplo 3 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Agora consulte os registros do pod criado pelo helm-controller para o
upgrade:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>O nome do pod seguirá o seguinte gabarito:
<literal>helm-install-longhorn-&lt;sufixo-aleatório&gt;</literal></para>
</listitem>
<listitem>
<para>O pod estará no namespace em que o recurso <literal>HelmChart</literal> foi
implantado. No nosso caso, o namespace é <literal>kube-system</literal>.</para>
<figure>
<title>Registros do gráfico do Longhorn atualizado com sucesso</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>downstream exemplo 4 upgrade gráfico helm dia2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Verifique se a versão do <literal>HelmChart</literal> foi atualizada
navegando até a seção <literal>HelmCharts</literal> do Rancher
(<literal>More Resources → HelmCharts</literal> "Mais recursos →
HelmCharts"). Selecione o namespace em que o gráfico foi implantado; que,
neste exemplo, é <literal>kube-system</literal>.</para>
</listitem>
<listitem>
<para>Por fim, verifique se os pods do Longhorn estão em execução.</para>
</listitem>
</orderedlist>
<para>Depois de concluir as validações acima, é seguro considerar que o upgrade do
gráfico Helm do Longhorn foi feito para a versão
<literal>107.0.0+up1.9.1</literal>.</para>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>Upgrade do gráfico Helm usando uma ferramenta GitOps de terceiros</title>
<para>Em alguns casos de uso, os usuários talvez queiram seguir esse procedimento
de upgrade com um fluxo de trabalho do GitOps diferente do Fleet (por
exemplo, <literal>Flux</literal>).</para>
<para>Para gerar os recursos necessários ao procedimento de upgrade, você pode
usar o script <literal>generate-chart-upgrade-data.sh</literal> para
preencher a instância do Fleet <literal>eib-charts-upgrader</literal> com os
dados fornecidos pelo usuário. Para obter mais informações sobre como fazer
isso, consulte a <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
<para>Com a configuração completa, você pode usar o <link
xl:href="https://kustomize.io">kustomize</link> para gerar uma solução
totalmente funcional para ser implantada no seu cluster:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>Para incluir a solução no fluxo de trabalho do GitOps, remova o arquivo
<literal>fleet.yaml</literal> e use o que sobrou como configuração válida do
<literal>Kustomize</literal>. Lembre-se de executar primeiro o script
<literal>generate-chart-upgrade-data.sh</literal> para que ele possa
preencher a configuração do <literal>Kustomize</literal> com os dados dos
gráficos Helm para o qual você quer fazer upgrade.</para>
<para>Para saber qual é a finalidade de uso desse fluxo de trabalho, consulte a
<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>
e a <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-suse-telco-cloud-documentation">
<title>Documentação do SUSE Telco Cloud</title>
<partintro>
<para>Encontre aqui a documentação do SUSE Telco Cloud</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Telco Cloud</title>
<para>O SUSE Telco Cloud (antigo SUSE Edge for Telco) é uma plataforma de
computação otimizada para telecomunicações que permite que operadores e
provedores de rede de telecomunicações inovem e acelerem a modernização de
suas redes.</para>
<para>O SUSE Telco Cloud é uma pilha completa, nativa de nuvem e habilitada para
telecomunicações capaz de hospedar CNFs, englobando todos os domínios de
telecomunicações: núcleo de pacotes, RAN, OSS e BSS.</para>
<itemizedlist>
<listitem>
<para>Automatiza a distribuição "zero touch" e o gerenciamento do ciclo de vida de
configurações complexas da pilha de borda de acordo com a escala de
telecomunicações.</para>
</listitem>
<listitem>
<para>Garanta a qualidade contínua do hardware voltado para telecomunicações
usando as configurações e cargas de trabalho específicas dessa área.</para>
</listitem>
<listitem>
<para>Consiste em componentes desenvolvidos especificamente para a borda e,
portanto, com menor pegada e maior desempenho por Watt.</para>
</listitem>
<listitem>
<para>Mantém uma estratégia de plataforma flexível com APIs independentes de
fornecedor e código 100% aberto.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>Conceito e arquitetura</title>
<para>O SUSE Telco Cloud é uma plataforma desenvolvida para hospedar em escala
aplicativos de telecomunicações modernos e nativos de nuvem, do núcleo à
borda.</para>
<para>Esta página explica a arquitetura e os componentes usados no SUSE Telco
Cloud.</para>
<section xml:id="id-suse-telco-cloud-architecture">
<title>Arquitetura do SUSE Telco Cloud</title>
<para>No seguinte diagrama, veja a arquitetura de alto nível do SUSE Telco Cloud:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.png"
width="100%"/> </imageobject>
<textobject><phrase>arquitetura1 produto atip</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-2">
<title>Componentes</title>
<para>Há dois blocos diferentes; pilha de gerenciamento e pilha de runtime:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Pilha de gerenciamento</emphasis>: essa parte do
SUSE Telco Cloud é usada para gerenciar a provisão e o ciclo de vida das
pilhas de runtime. Ela inclui os seguintes componentes:</para>
<itemizedlist>
<listitem>
<para>Gerenciamento multicluster em ambientes de nuvem pública e privada com o
Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Suporte a bare metal com os provedores de infraestrutura Metal3 (<xref
linkend="components-metal3"/>), MetalLB (<xref
linkend="components-metallb"/>) e <literal>CAPI</literal> (Cluster API)</para>
</listitem>
<listitem>
<para>Isolamento completo de locatários e integrações de <literal>IDP</literal>
(provedor de identidade)</para>
</listitem>
<listitem>
<para>Amplo mercado de integrações e extensões de terceiros</para>
</listitem>
<listitem>
<para>API independente de fornecedor e vasto ecossistema de provedores</para>
</listitem>
<listitem>
<para>Controle das atualizações transacionais do SUSE Linux Micro</para>
</listitem>
<listitem>
<para>GitOps Engine para gerenciar o ciclo de vida dos clusters usando
repositórios Git com o Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Pilha de runtime</emphasis>: essa parte do SUSE
Telco Cloud é usada para executar as cargas de trabalho.</para>
<itemizedlist>
<listitem>
<para>O RKE2 (<xref linkend="components-rke2"/>) funciona como uma distribuição
Kubernetes leve e de segurança reforçada, otimizada para ambientes de
telecomunicações de borda com foco na conformidade.</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>) para habilitar
recursos de segurança, como verificação de vulnerabilidades de imagem,
inspeção detalhada de pacotes e controle automático do tráfego intracluster.</para>
</listitem>
<listitem>
<para>Armazenamento em blocos com SUSE Storage (<xref
linkend="components-suse-storage"/>) para permitir um meio simples e fácil
de usar uma solução de armazenamento nativa de nuvem.</para>
</listitem>
<listitem>
<para>Sistema operacional otimizado com SUSE Linux Micro (<xref
linkend="components-slmicro"/>) para oferecer um sistema operacional seguro,
leve e imutável (sistema de arquivos transacional) para execução de
contêineres. O SUSE Linux Micro está disponível nas arquiteturas AArch64 e
AMD64/Intel 64 e oferece suporte ao <literal>kernel Real-Time</literal> para
casos de uso de telecomunicações e de borda.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>Fluxos de implantação de exemplo</title>
<para>Veja a seguir exemplos de alto nível de fluxos de trabalho para entender o
relacionamento entre os componentes de gerenciamento e de runtime.</para>
<para>O provisionamento de rede direcionado é o fluxo de trabalho que permite a
implantação de um novo cluster downstream com todos os componentes
pré-configurados e prontos para executar as cargas de trabalho sem
intervenção manual.</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>Exemplo 1: implantação de um novo cluster de gerenciamento com todos os
componentes instalados</title>
<para>Uso do Edge Image Builder (<xref linkend="components-eib"/>) para criar uma
nova imagem <literal>ISO</literal> com a pilha de gerenciamento
incluída. Você pode usar essa imagem <literal>ISO</literal> para instalar um
novo cluster de gerenciamento em VMs ou bare metal.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png"
width="100%"/> </imageobject>
<textobject><phrase>arquitetura2 produto atip</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Para obter mais informações sobre como implantar um novo cluster de
gerenciamento, consulte o guia referente ao cluster de gerenciamento do SUSE
Telco Cloud (<xref linkend="atip-management-cluster"/>).</para>
</note>
<note>
<para>Para obter mais informações sobre como usar o Edge Image Builder, consulte o
respectivo guia (<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>Exemplo 2: implantação de um cluster downstream de nó único com perfis de
telecomunicações para que ele possa executar cargas de trabalho de
telecomunicações</title>
<para>Quando o cluster de gerenciamento estiver em funcionamento, poderemos usá-lo
para implantar um cluster downstream de nó único com todos os recursos de
telecomunicações habilitados e configurados por meio do fluxo de trabalho de
provisionamento de rede direcionado.</para>
<para>O seguinte diagrama mostra o fluxo de trabalho de alto nível para
implantá-lo:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png"
width="100%"/> </imageobject>
<textobject><phrase>arquitetura3 produto atip</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Para obter mais informações sobre como implantar um cluster downstream,
consulte o guia referente ao provisionamento automatizado do SUSE Telco
Cloud. (<xref linkend="atip-automated-provisioning"/>)</para>
</note>
<note>
<para>Para obter mais informações sobre os recursos de telecomunicações, consulte
o guia referente aos recursos de telecomunicações do SUSE Telco
Cloud. (<xref linkend="atip-features"/>)</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>Exemplo 3: implantação de um cluster downstream de alta disponibilidade
usando o MetalLB como balanceador de carga</title>
<para>Depois que o cluster de gerenciamento estiver em funcionamento, poderemos
usá-lo para implantar um cluster downstream de alta disponibilidade com o
<literal>MetalLB</literal> como balanceador de carga, usando o fluxo de
trabalho de provisionamento de rede direcionado.</para>
<para>O seguinte diagrama mostra o fluxo de trabalho de alto nível para
implantá-lo:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png"
width="100%"/> </imageobject>
<textobject><phrase>arquitetura4 produto atip</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Para obter mais informações sobre como implantar um cluster downstream,
consulte o guia referente ao provisionamento automatizado do SUSE Telco
Cloud. (<xref linkend="atip-automated-provisioning"/>)</para>
</note>
<note>
<para>Para obter mais informações sobre o <literal>MetalLB</literal>, consulte:
(<xref linkend="components-metallb"/>)</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>Requisitos e suposições</title>
<section xml:id="id-hardware">
<title>Hardware</title>
<para>Consulte abaixo os requisitos de hardware para o SUSE Telco Cloud:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Cluster de gerenciamento</emphasis>: contém
componentes, como <literal>SUSE Linux Micro</literal>,
<literal>RKE2</literal>, <literal>SUSE Rancher Prime</literal> e
<literal>Metal<superscript>3</superscript></literal>, e é usado para
gerenciar vários clusters downstream. Dependendo do número de clusters
downstream que será gerenciado, os requisitos de hardware para o servidor
podem variar.</para>
<itemizedlist>
<listitem>
<para>Os requisitos mínimos para o servidor (<literal>VM</literal> ou
<literal>bare metal</literal>) são:</para>
<itemizedlist>
<listitem>
<para>RAM: mínimo de 8 GB (recomendamos pelo menos 16 GB)</para>
</listitem>
<listitem>
<para>CPU: mínimo de 2 (recomendamos pelo menos 4)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Clusters downstream</emphasis>: trata-se dos
clusters implantados para executar as cargas de trabalho de
telecomunicações. Requisitos específicos são necessários para habilitar
determinados recursos de telecomunicações, como <literal>SR-IOV</literal>,
<literal>otimização de desempenho da CPU</literal> etc.</para>
<itemizedlist>
<listitem>
<para>SR-IOV: para anexar funções virtuais (VFs, Virtual Functions) no modo de
passagem a CNFs/VNFs, a NIC deve permitir que SR-IOV e VT-d/AMD-Vi sejam
habilitados no BIOS.</para>
</listitem>
<listitem>
<para>Processadores de CPU: para executar cargas de trabalho de telecomunicações,
o modelo de processador da CPU deve ser adaptado para disponibilizar a maior
parte dos recursos nesta tabela de referência (<xref
linkend="atip-features"/>).</para>
</listitem>
<listitem>
<para>Requisitos de firmware para instalação com mídia virtual:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Hardware de servidor</entry>
<entry align="left" valign="top">Modelo de BMC</entry>
<entry align="left" valign="top">Gerenciamento</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>Hardware Dell</para></entry>
<entry align="left" valign="top"><para>15ª geração</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Hardware Supermicro</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC – redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Hardware HPE</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-network">
<title>Rede</title>
<para>Como referência à arquitetura de rede, o seguinte diagrama mostra uma
arquitetura de rede típica para um ambiente de telecomunicações:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirements1.svg"
width="100%"/> </imageobject>
<textobject><phrase>requisitos1 produto atip</phrase></textobject>
</mediaobject>
</informalfigure>
<para>A arquitetura de rede baseia-se nos seguintes componentes:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Rede de gerenciamento</emphasis>: essa rede é usada
para gerenciamento de nós do cluster downstream. Ela é usada para
gerenciamento fora da banda. Em geral, essa rede também é conectada a um
comutador de gerenciamento separado, mas pode ser conectada ao mesmo
comutador de serviço usando VLANs para isolar o tráfego.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Rede do plano de controle</emphasis>: essa rede é
usada para comunicação entre os nós do cluster downstream e os serviços
executados neles. Ela também é usada para comunicação entre os nós e os
serviços externos, como servidores <literal>DHCP</literal> ou
<literal>DNS</literal>. Em alguns casos, o comutador/roteador processa o
tráfego pela Internet nos ambientes conectados.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Outras redes</emphasis>: em alguns casos, os nós
podem se conectar a outras redes para finalidades específicas.</para>
</listitem>
</itemizedlist>
<note>
<para>Para usar o fluxo de trabalho de provisionamento de rede direcionado, o
cluster de gerenciamento deve ter conectividade de rede com o Baseboard
Management Controller (BMC) do servidor de cluster downstream para que seja
possível automatizar a preparação e o provisionamento do host.</para>
</note>
</section>
<section xml:id="id-port-requirements">
<title>Requisitos de porta</title>
<para>Para operar de maneira apropriada, a implantação do SUSE Telco Cloud requer
várias portas acessíveis nos nós de cluster Kubernetes tanto de
gerenciamento quanto downstream.</para>
<note>
<para>A lista exata depende dos componentes opcionais implantados e das opções de
implantação selecionadas (por exemplo, plug-in de CNI).</para>
</note>
<section xml:id="id-management-nodes">
<title>Nós de gerenciamento</title>
<para>A seguinte tabela lista as portas abertas nos nós que executam o cluster de
gerenciamento:</para>
<note>
<para>Para as portas relacionadas ao plug-in de CNI, consulte os requisitos de
porta específicos da CNI (<xref linkend="cni-specific-port-requirements"/>).</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-nodes" frame="all" rowsep="1" colsep="1">
<title>Regras de rede de entrada para nós de gerenciamento</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocolo</entry>
<entry align="left" valign="top">Porta</entry>
<entry align="left" valign="top">Fonte</entry>
<entry align="left" valign="top">Descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte que exija acesso SSH</para></entry>
<entry align="left" valign="top"><para>Acesso SSH aos nós do cluster de gerenciamento</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Balanceador de carga/proxy que faça terminação TLS externa</para></entry>
<entry align="left" valign="top"><para>IU do Rancher/API quando a terminação TLS externa é usada</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte que exija acesso TLS à IU do Rancher/API</para></entry>
<entry align="left" valign="top"><para>Agente do Rancher, IU do Rancher/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>Nós do servidor RKE2 (cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>Porta do cliente <literal>etcd</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>Nós do servidor RKE2 (cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>Porta do peer <literal>etcd</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6180</para></entry>
<entry align="left" valign="top"><para>Um BMC<superscript>(1)</superscript> que já recebeu instruções do
<literal>Metal<superscript>3</superscript>/ironic</literal> para extrair uma
imagem ramdisk do IPA<superscript>(2)</superscript> desta porta exposta (não
TLS)</para></entry>
<entry align="left" valign="top"><para>Servidor web não TLS httpd <literal>Ironic</literal> para exibir imagens ISO
do IPA<superscript>(2)</superscript> para inicialização com base em mídia
virtual <?asciidoc-br?> <?asciidoc-br?> Se essa porta estiver habilitada, a
funcionalidade equivalente, exceto aquela habilitada por TLS (veja abaixo),
não será aberta</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6185</para></entry>
<entry align="left" valign="top"><para>Um BMC<superscript>(1)</superscript> que já recebeu instruções do
<literal>Metal<superscript>3</superscript>/ironic</literal> para extrair uma
imagem ramdisk do IPA<superscript>(2)</superscript> dessa porta exposta
(TLS)</para></entry>
<entry align="left" valign="top"><para>Servidor web habilitado para TLS httpd <literal>Ironic</literal> para exibir
imagens ISO do IPA<superscript>(2)</superscript> para inicialização com base
em mídia virtual <?asciidoc-br?> <?asciidoc-br?> Se essa porta estiver
habilitada, a funcionalidade equivalente, exceto aquela desabilitada para
TLS (veja abaixo), não será aberta</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6385</para></entry>
<entry align="left" valign="top"><para>Uma imagem ramdisk do IPA<superscript>(1)</superscript> do
<literal>Metal<superscript>3</superscript>/ironic</literal> implantada e em
execução na instância <literal>BareMetalHost</literal> "registrada"</para></entry>
<entry align="left" valign="top"><para>API Ironic</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>Qualquer nó do cluster de gerenciamento; qualquer cliente Kubernetes externo
(ao cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>API Kubernetes</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6545</para></entry>
<entry align="left" valign="top"><para>Qualquer nó do cluster de gerenciamento</para></entry>
<entry align="left" valign="top"><para>Extrair artefatos do registro compatível com OCI (Hauler)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>Nós do agente e servidor RKE2 (cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>API de supervisor RKE2 para registro de nós (porta aberta em todos os nós do
servidor RKE2)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>Qualquer nó do cluster de gerenciamento</para></entry>
<entry align="left" valign="top"><para>Métricas <literal>kubelet</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000-32767</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte externa (ao cluster de gerenciamento) que acessa um serviço
exposto na rede principal por um <link
xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">objeto
de API de serviço</link> <literal>spec.type: NodePort</literal> ou
<literal>spec.type: LoadBalancer</literal></para></entry>
<entry align="left" valign="top"><para>Intervalo de portas <literal>NodePort</literal></para></entry>
</row>
</tbody>
</tgroup>
</table>
<para><superscript>(1)</superscript> BMC: Baseboard Management
Controller<?asciidoc-br?> <superscript>(2)</superscript> IPA: Ironic Python
Agent</para>
</section>
<section xml:id="id-downstream-nodes">
<title>Nós downstream</title>
<para>No SUSE Telco Cloud, antes de um servidor (downstream) fazer parte de um
cluster Kubernetes downstream em execução (ou ser executado em um cluster
Kubernetes downstream de nó único), ele deve passar por alguns dos <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/baremetalhost-states.md">estados
de provisionamento do BaremetalHost</link>.</para>
<itemizedlist>
<listitem>
<para>O Baseboard Management Controller (BMC) de um servidor downstream
recém-declarado deve ser acessível pela rede fora da banda. O BMC recebe
instruções (do serviço Ironic executado no cluster de gerenciamento) sobre
as etapas iniciais que deve seguir:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Extrair e carregar a imagem ramdisk indicada do IPA na <literal>mídia
virtual</literal> oferecida pelo BMC.</para>
</listitem>
<listitem>
<para>Ligar o servidor.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>A exposição das seguintes portas é esperada do BMC (podem variar de acordo
com o hardware exato):</para>
<table xml:id="table-inbound-network-rules-for-baseboard-management-controllers" frame="all" rowsep="1" colsep="1">
<title>Regras da rede de entrada para Baseboard Management Controllers</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocolo</entry>
<entry align="left" valign="top">Porta</entry>
<entry align="left" valign="top">Fonte</entry>
<entry align="left" valign="top">Descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Condutor do Ironic (do cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>Acesso à API Redfish (HTTP)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Condutor do Ironic (do cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>Acesso à API Redfish (HTTPS)</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>Depois que a imagem ramdisk do IPA carregada na <literal>mídia
virtual</literal> do BMC for usada para inicializar a imagem do servidor
downstream, a fase de inspeção do hardware será iniciada. A seguinte tabela
lista as portas expostas pela imagem ramdisk do IPA em execução:</para>
</listitem>
</itemizedlist>
<table xml:id="table-inbound-network-rules-for-downstream-nodes-provisioning-phase" frame="all" rowsep="1" colsep="1">
<title>Regras da rede de entrada para nós downstream – Fase de provisionamento do
<literal>Metal<superscript>3</superscript>/Ironic</literal></title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocolo</entry>
<entry align="left" valign="top">Porta</entry>
<entry align="left" valign="top">Fonte</entry>
<entry align="left" valign="top">Descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte que exija acesso SSH à imagem ramdisk do IPA</para></entry>
<entry align="left" valign="top"><para>Acesso SSH a um nó do cluster downstream que está sendo inspecionado</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9999</para></entry>
<entry align="left" valign="top"><para>Condutor do Ironic (do cluster de gerenciamento)</para></entry>
<entry align="left" valign="top"><para>Comandos do Ironic para a imagem ramdisk em execução</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>Depois que o host bare metal for devidamente provisionado e fizer parte de
um cluster Kubernetes downstream, ele vai expor as seguintes portas:</para>
</listitem>
</itemizedlist>
<note>
<para>Para as portas relacionadas ao plug-in de CNI, consulte os requisitos de
porta específicos da CNI (<xref linkend="cni-specific-port-requirements"/>).</para>
</note>
<table xml:id="table-inbound-network-rules-for-downstream-nodes" frame="all" rowsep="1" colsep="1">
<title>Regras da rede de entrada para nós downstream</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocolo</entry>
<entry align="left" valign="top">Porta</entry>
<entry align="left" valign="top">Fonte</entry>
<entry align="left" valign="top">Descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte que exija acesso SSH</para></entry>
<entry align="left" valign="top"><para>Acesso SSH aos nós do cluster downstream</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Balanceador de carga/proxy que faça terminação TLS externa</para></entry>
<entry align="left" valign="top"><para>IU do Rancher/API quando a terminação TLS externa é usada</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte que exija acesso TLS à IU do Rancher/API</para></entry>
<entry align="left" valign="top"><para>Agente do Rancher, IU do Rancher/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>Nós do servidor RKE2 (cluster downstream)</para></entry>
<entry align="left" valign="top"><para>Porta do cliente <literal>etcd</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>Nós do servidor RKE2 (cluster downstream)</para></entry>
<entry align="left" valign="top"><para>Porta do peer <literal>etcd</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>Qualquer nó do cluster downstream; qualquer cliente Kubernetes externo (ao
cluster downstream)</para></entry>
<entry align="left" valign="top"><para>API Kubernetes</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>Nós do agente e servidor RKE2 (cluster downstream)</para></entry>
<entry align="left" valign="top"><para>API de supervisor RKE2 para registro de nós (porta aberta em todos os nós do
servidor RKE2)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>Qualquer nó do cluster downstream</para></entry>
<entry align="left" valign="top"><para>Métricas <literal>kubelet</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10255</para></entry>
<entry align="left" valign="top"><para>Qualquer nó do cluster downstream</para></entry>
<entry align="left" valign="top"><para>Acesso somente leitura ao <literal>kubelet</literal></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000-32767</para></entry>
<entry align="left" valign="top"><para>Qualquer fonte externa (ao cluster downstream) que acessa um serviço exposto
na rede principal por um <link
xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">objeto
de API de serviço</link> <literal>spec.type: NodePort</literal> ou
<literal>spec.type: LoadBalancer</literal></para></entry>
<entry align="left" valign="top"><para>Intervalo de portas <literal>NodePort</literal></para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="cni-specific-port-requirements">
<title>Requisitos de porta específicos da CNI</title>
<para>Cada variante da CNI com suporte tem o próprio conjunto de requisitos de
porta. Para obter mais detalhes, consulte <link
xl:href="https://docs.rke2.io/install/requirements#cni-specific-inbound-network-rules">CNI
Specific Inbound Network Rules</link> (Regras da rede de entrada específicas
da CNI) na documentação do RKE2.</para>
<para>Quando <literal>cilium</literal> é definido como plug-in de CNI
padrão/principal, a porta TCP a seguir também é exposta quando a carga de
trabalho <literal>cilium-operator</literal> está configurada para expor as
métricas fora do cluster Kubernetes no qual foi implantada. Isso garante que
uma instância externa do servidor <literal>Prometheus</literal> em execução
fora do cluster Kubernetes ainda possa coletar essas métricas.</para>
<note>
<para>Trata-se da opção padrão quando o <literal>cilium</literal> é implantado
pelo gráfico Helm rke2-cilium.</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-downstream-nodes-external-metrics-cilium-operator" frame="all" rowsep="1" colsep="1">
<title>Regras da rede de entrada para nós de gerenciamento/downstream – exposição
de métricas externa do <literal>cilium-operator</literal> habilitada</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Protocolo</entry>
<entry align="left" valign="top">Porta</entry>
<entry align="left" valign="top">Fonte</entry>
<entry align="left" valign="top">Descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9963</para></entry>
<entry align="left" valign="top"><para>Coletor de métricas externo (ao cluster Kubernetes)</para></entry>
<entry align="left" valign="top"><para>Exposição de métricas do cilium-operator</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>Serviços (DHCP, DNS etc.)</title>
<para>Alguns serviços externos, como <literal>DHCP</literal>,
<literal>DNS</literal> etc., podem ser necessários, dependendo do tipo de
ambiente em que estão implantados:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Ambiente conectado</emphasis>: nesse caso, os nós
são conectados à Internet (por protocolos de roteamento L3), e o cliente
providencia os serviços externos.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Ambiente desconectado/air-gapped</emphasis>: nesse
caso, os nós não têm conectividade IP de Internet e são exigidos serviços
adicionais para espelhar localmente o conteúdo necessário pelo fluxo de
trabalho de provisionamento de rede direcionado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Servidor de arquivos</emphasis>: usado para
armazenar as imagens de sistema operacional que são provisionadas nos nós do
cluster downstream durante o fluxo de trabalho de provisionamento de rede
direcionado. O gráfico Helm do
<literal>Metal<superscript>3</superscript></literal> pode implantar um
servidor de mídia para armazenar as imagens de sistema operacional. Consulte
a seguinte seção (<xref linkend="metal3-media-server"/>), mas também é
possível usar um servidor web local existente.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-systemd-services">
<title>Desabilitando os serviços systemd</title>
<para>Para cargas de trabalho de telecomunicações, é importante desabilitar ou
configurar apropriadamente alguns dos serviços executados nos nós para
evitar qualquer impacto no desempenho das cargas de trabalho em execução nos
nós (latência).</para>
<itemizedlist>
<listitem>
<para><literal>rebootmgr</literal> é um serviço que permite configurar uma
estratégia de reinicialização quando o sistema tem atualizações
pendentes. Para cargas de trabalho de telecomunicações, é realmente
importante desabilitar ou configurar apropriadamente o serviço
<literal>rebootmgr</literal> para evitar a reinicialização dos nós em caso
de atualizações programadas pelo sistema, a fim de evitar qualquer impacto
nos serviços executados nos nós.</para>
</listitem>
</itemizedlist>
<note>
<para>Para obter mais informações sobre o <literal>rebootmgr</literal>, consulte o
<link xl:href="https://github.com/SUSE/rebootmgr">repositório rebootmgr do
GitHub</link>.</para>
</note>
<para>Verifique a estratégia usada executando este comando:</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>e você pode desabilitá-la executando:</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>ou usando o comando <literal>rebootmgrctl</literal>:</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para>É possível automatizar essa configuração para definir a estratégia
<literal>rebootmgr</literal> usando o fluxo de trabalho de provisionamento
de rede direcionado. Para obter mais informações, consulte a documentação
sobre provisionamento automatizado (<xref
linkend="atip-automated-provisioning"/>).</para>
</note>
<itemizedlist>
<listitem>
<para><literal>transactional-update</literal> é um serviço que permite que o
sistema controle as atualizações automáticas. Para cargas de trabalho de
telecomunicações, é importante desabilitar as atualizações automáticas para
evitar qualquer impacto nos serviços executados nos nós.</para>
</listitem>
</itemizedlist>
<para>Para desabilitar as atualizações automáticas, execute:</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable transactional-update.timer
systemctl --now disable transactional-update-cleanup.timer</screen>
<itemizedlist>
<listitem>
<para><literal>fstrim</literal> é um serviço que permite cortar os sistemas de
arquivos automaticamente toda semana. Para cargas de trabalho de
telecomunicações, é importante desabilitar o corte automático para evitar
qualquer impacto nos serviços executados nos nós.</para>
</listitem>
</itemizedlist>
<para>Para desabilitar o corte automático, execute:</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable fstrim.timer</screen>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>Configurando o cluster de gerenciamento</title>
<section xml:id="id-introduction-2">
<title>Introdução</title>
<para>O cluster de gerenciamento faz parte do SUSE Telco Cloud usado para
gerenciar a provisão e o ciclo de vida das pilhas de runtime. De um ponto de
vista técnico, o cluster de gerenciamento contém os seguintes componentes:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro</literal> como sistema operacional. Dependendo do
caso de uso, é possível personalizar algumas configurações, como rede,
armazenamento, usuários e argumentos do kernel.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> como cluster Kubernetes. Dependendo do caso de uso,
é possível configurá-lo para usar plug-ins de CNI específicos, como
<literal>Multus</literal>, <literal>Cilium</literal>,
<literal>Calico</literal> etc.</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> como plataforma de gerenciamento para gerenciar o
ciclo de vida dos clusters.</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal> como componente para
gerenciar o ciclo de vida dos nós bare metal.</para>
</listitem>
<listitem>
<para><literal>CAPI</literal> como componente para gerenciar o ciclo de vida dos
clusters Kubernetes (downstream). O <literal>provedor CAPI RKE2</literal> é
usado para gerenciar o ciclo de vida dos clusters RKE2.</para>
</listitem>
</itemizedlist>
<para>Com todos os componentes mencionados acima, o cluster de gerenciamento pode
gerenciar o ciclo de vida dos clusters downstream, aplicando uma abordagem
declarativa para gerenciar a infraestrutura e os aplicativos.</para>
<note>
<para>Para obter mais informações sobre o <literal>SUSE Linux Micro</literal>,
consulte: SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
<para>Para obter mais informações sobre o <literal>RKE2</literal>, consulte: RKE2
(<xref linkend="components-rke2"/>)</para>
<para>Para obter mais informações sobre o <literal>Rancher</literal>, consulte:
Rancher (<xref linkend="components-rancher"/>)</para>
<para>Para obter mais informações sobre o
<literal>Metal<superscript>3</superscript></literal>, consulte: Metal3
(<xref linkend="components-metal3"/>)</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>Etapas para configurar o cluster de gerenciamento</title>
<para>As seguintes etapas são necessárias para configurar o cluster de
gerenciamento (de nó único):</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png"
width="100%"/> </imageobject>
<textobject><phrase>clustergrcmt1 produto atip</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Veja a seguir as etapas principais para configurar o cluster de
gerenciamento com uma abordagem declarativa:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Preparação da imagem para ambientes conectados
(<xref linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>: a
primeira etapa é preparar o manifesto e os arquivos com todas as
configurações necessárias aos ambientes conectados.</para>
<itemizedlist>
<listitem>
<para>Estrutura de diretórios para ambientes conectados (<xref
linkend="mgmt-cluster-directory-structure"/>): esta etapa cria uma estrutura
de diretórios que o Edge Image Builder usará para armazenar os arquivos de
configuração e a própria imagem.</para>
</listitem>
<listitem>
<para>Arquivo de definição do cluster de gerenciamento (<xref
linkend="mgmt-cluster-image-definition-file"/>): o
<literal>mgmt-cluster.yaml</literal> é o principal arquivo de definição do
cluster de gerenciamento. Ele inclui as seguintes informações sobre a imagem
que será criada:</para>
<itemizedlist>
<listitem>
<para>Informações da imagem: as informações relacionadas à imagem que será criada
usando a imagem base.</para>
</listitem>
<listitem>
<para>Sistema operacional: a configuração do sistema operacional que será usada na
imagem.</para>
</listitem>
<listitem>
<para>Kubernetes: repositórios e gráficos Helm, versão do kubernetes, configuração
de rede e os nós que serão usados no cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Pasta custom (<xref linkend="mgmt-cluster-custom-folder"/>): a pasta
<literal>custom</literal> contém os arquivos de configuração e os scripts
que o Edge Image Builder usará para implantar um cluster totalmente
funcional.</para>
<itemizedlist>
<listitem>
<para>Arquivos: contêm os arquivos de configuração que serão usados pelo cluster
de gerenciamento.</para>
</listitem>
<listitem>
<para>Scripts: contêm os scripts que serão usados pelo cluster de gerenciamento.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Pasta kubernetes (<xref linkend="mgmt-cluster-kubernetes-folder"/>): a pasta
<literal>kubernetes</literal> contém os arquivos de configuração que serão
usados pelo cluster de gerenciamento.</para>
<itemizedlist>
<listitem>
<para>Manifestos: contêm os manifestos que serão usados pelo cluster de
gerenciamento.</para>
</listitem>
<listitem>
<para>Helm: contém os arquivos de valores do Helm que serão usados pelo cluster de
gerenciamento.</para>
</listitem>
<listitem>
<para>Config: contém os arquivos de configuração que serão usados pelo cluster de
gerenciamento.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Pasta network (<xref linkend="mgmt-cluster-network-folder"/>): a pasta
<literal>network</literal> contém os arquivos de configuração de rede que
serão usados pelos nós do cluster de gerenciamento.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Preparação da imagem para ambientes air-gapped
(<xref linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>: a
etapa mostra as diferenças para preparar os manifestos e os arquivos que
serão usados em um cenário air-gapped.</para>
<itemizedlist>
<listitem>
<para>Modificações no arquivo de definição (<xref
linkend="mgmt-cluster-image-definition-file-airgap"/>): o arquivo
<literal>mgmt-cluster.yaml</literal> deve ser modificado para incluir a
seção <literal>embeddedArtifactRegistry</literal> com o campo
<literal>images</literal> definido para que todas as imagens do contêiner
sejam incluídas na imagem de saída do EIB.</para>
</listitem>
<listitem>
<para>Modificações na pasta custom (<xref
linkend="mgmt-cluster-custom-folder-airgap"/>): a pasta
<literal>custom</literal> deve ser modificada para incluir os recursos
necessários para executar o cluster de gerenciamento em um ambiente
air-gapped.</para>
<itemizedlist>
<listitem>
<para>Script de registro: o script
<literal>custom/scripts/99-register.sh</literal> deve ser removido quando
você usa um ambiente air-gapped.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Modificações na pasta de valores do Helm (<xref
linkend="mgmt-cluster-helm-values-folder-airgap"/>): a pasta
<literal>helm/values</literal> deve ser modificada para incluir a
configuração necessária para executar o cluster de gerenciamento em um
ambiente air-gapped.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Criação da imagem (<xref
linkend="mgmt-cluster-image-creation"/>)</emphasis>: esta etapa abrange a
criação da imagem com a ferramenta Edge Image Builder (para os cenários
tanto conectados quanto air-gapped). Consulte os pré-requisitos (<xref
linkend="components-eib"/>) para executar a ferramenta Edge Image Builder no
seu sistema.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Provisionamento do cluster de gerenciamento (<xref
linkend="mgmt-cluster-provision"/>)</emphasis>: esta etapa aborda o
provisionamento do cluster de gerenciamento usando a imagem criada na etapa
anterior (para os cenários tanto conectados quanto air-gapped). Para
executá-la, é possível usar um laptop, um servidor, uma VM ou qualquer outro
sistema AMD64/Intel 64 com uma porta USB.</para>
</listitem>
</orderedlist>
<note>
<para>Para obter mais informações sobre o Edge Image Builder, consulte o
respectivo guia (<xref linkend="components-eib"/>) e o Guia de Início Rápido
(<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>Preparação da imagem para ambientes conectados</title>
<para>O Edge Image Builder é usado para criar a imagem do cluster de
gerenciamento. Neste documento, explicamos a configuração mínima necessária
para configurar o cluster de gerenciamento.</para>
<para>O Edge Image Builder é executado dentro de um contêiner, portanto, um tempo
de execução do contêiner é necessário, como <link
xl:href="https://podman.io">Podman</link> ou <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>. Neste guia,
consideramos que o podman esteja disponível.</para>
<para>Também como pré-requisito para implantar um cluster de gerenciamento
altamente disponível, você precisa reservar três IPs na sua rede:</para>
<itemizedlist>
<listitem>
<para><literal>apiVIP</literal> para o endereço VIP da API (usado para acessar o
servidor da API Kubernetes).</para>
</listitem>
<listitem>
<para><literal>ingressVIP</literal> para o endereço VIP de entrada (consumido pela
IU do Rancher, por exemplo).</para>
</listitem>
<listitem>
<para><literal>metal3VIP</literal> para o endereço VIP do Metal3.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure">
<title>Estrutura de diretórios</title>
<para>Ao executar o EIB, um diretório é montado do host, portanto, a primeira
etapa é criar uma estrutura de diretórios para o EIB usar para armazenar os
arquivos de configuração e a própria imagem. Esse diretório tem a seguinte
estrutura:</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── os-files
│ └── var
│   └── lib
│     └── rancher
│       └── rke2
│         └── server
│           └── manifests
│             └── rke2-ingress-config.yaml
├── kubernetes
│ ├── manifests
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── longhorn.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>É necessário fazer download da imagem
<literal>SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso</literal> do
<link xl:href="https://scc.suse.com/">SUSE Customer Center</link> ou da
<link xl:href="https://www.suse.com/download/sle-micro/">página de downloads
da SUSE</link> e salvá-la na pasta <literal>base-images</literal>.</para>
<para>Confira o checksum SHA256 da imagem para garantir que ela não tenha sido
adulterada. O checksum está no mesmo local em que a imagem foi baixada.</para>
<para>Há um exemplo da estrutura de diretórios disponível no <link
xl:href="https://github.com/suse-edge/atip">repositório SUSE Edge do GitHub
na pasta "telco-examples"</link>.</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>Arquivo de definição do cluster de gerenciamento</title>
<para>O <literal>mgmt-cluster.yaml</literal> é o principal arquivo de definição do
cluster de gerenciamento. Ele contém as seguintes informações:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    - open-iscsi
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: $API_HOST
    apiVIP: $API_VIP
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>Para explicar os campos e os valores no arquivo de definição
<literal>mgmt-cluster.yaml</literal>, dividimos esse arquivo nas seções a
seguir.</para>
<itemizedlist>
<listitem>
<para>Seção da imagem (arquivo de definição):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>em que <literal>baseImage</literal> é a imagem original que você baixou do
SUSE Customer Center ou da página de downloads da
SUSE. <literal>outputImageName</literal> é o nome da nova imagem que será
usada para provisionar o cluster de gerenciamento.</para>
<itemizedlist>
<listitem>
<para>Seção do sistema operacional (arquivo de definição):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>em que <literal>installDevice</literal> é o dispositivo usado para instalar
o sistema operacional, <literal>username</literal> e
<literal>encryptedPassword</literal> são as credenciais usadas para acessar
o sistema, <literal>packageList</literal> é a lista dos pacotes que devem
ser instalados (<literal>jq</literal> é obrigatório internamente durante o
processo de instalação) e <literal>sccRegistrationCode</literal> é o código
de registro usado para obter os pacotes e as dependências no momento da
criação e pode ser recuperado do SUSE Customer Center. É possível gerar a
senha criptografada usando o comando <literal>openssl</literal> da seguinte
maneira:</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>A saída é similar a esta:</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Seção do Kubernetes (arquivo de definição):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>A seção <literal>helm</literal> contém a lista de gráficos Helm para
instalação, os repositórios que serão usados e a configuração da versão de
todos eles.</para>
<para>A seção <literal>network</literal> contém a configuração da rede, como o
<literal>apiHost</literal> e o <literal>apiVIP</literal> que o componente
<literal>RKE2</literal> usará. O <literal>apiVIP</literal> deve ser um
endereço IP não usado na rede e que não faça parte do pool DHCP (se DHCP for
usado). Além disso, quando usamos o <literal>apiVIP</literal> em um cluster
de vários nós, ele é usado para acessar o servidor da API Kubernetes. O
<literal>apiHost</literal> é a resolução de nome para o
<literal>apiVIP</literal> que o componente <literal>RKE2</literal> usará.</para>
<para>A seção <literal>nodes</literal> contém a lista dos nós usados no
cluster. Neste exemplo, usamos um cluster de nó único, mas é possível
estendê-lo para um cluster de vários nós adicionando nós à lista (removendo
o marcador de comentário das linhas).</para>
<note>
<itemizedlist>
<listitem>
<para>Os nomes dos nós devem ser exclusivos no cluster.</para>
</listitem>
<listitem>
<para>Se preferir, use o campo <literal>initializer</literal> para especificar o
host de inicialização; do contrário, ele será o primeiro nó da lista.</para>
</listitem>
<listitem>
<para>Os nomes dos nós devem ser iguais aos nomes de host definidos na pasta de
rede (<xref linkend="mgmt-cluster-network-folder"/>) quando a configuração
de rede é obrigatória.</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Pasta custom</title>
<para>A pasta <literal>custom</literal> contém as seguintes subpastas:</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para>A pasta <literal>custom/files</literal> contém os arquivos de configuração
usados pelo cluster de gerenciamento.</para>
</listitem>
<listitem>
<para>A pasta <literal>custom/scripts</literal> contém os scripts usados pelo
cluster de gerenciamento.</para>
</listitem>
</itemizedlist>
<para>A pasta <literal>custom/files</literal> contém os seguintes arquivos:</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>: inclui os parâmetros de configuração para
<literal>Metal<superscript>3</superscript></literal>,
<literal>Rancher</literal> e <literal>MetalLB</literal>. Modifique esse
arquivo apenas para alterar os namespaces usados.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>: inclui a configuração para o componente
<literal>Metal<superscript>3</superscript></literal> usado (sem necessidade
de modificação). Em versões futuras, esse script será usado no lugar do
<literal>Rancher Turtles</literal> para facilitar o uso.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
        EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
        EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
        apiVersion: management.cattle.io/v3
        kind: Feature
        metadata:
          name: embedded-cluster-api
        spec:
          value: false
        EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>: inclui a configuração para o componente
<literal>Rancher</literal> usado (sem necessidade de modificação).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>: contém a configuração para
criar o serviço systemd para executar os scripts durante a primeira
inicialização (nenhuma modificação é necessária).</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single one
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>A pasta <literal>custom/scripts</literal> contém os seguintes arquivos:</para>
<itemizedlist>
<listitem>
<para>Script <literal>99-alias.sh</literal>: contém o álias que o cluster de
gerenciamento usa para carregar o arquivo kubeconfig na primeira
inicialização (sem necessidade de modificação).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para>Script <literal>99-mgmt-setup.sh</literal>: contém a configuração para
copiar os scripts durante a primeira inicialização (sem necessidade de
modificação).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
        cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para>Script <literal>99-register.sh</literal>: contém a configuração para
registrar o sistema usando o código de registro do SCC. O
<literal>${SCC_ACCOUNT_EMAIL}</literal> e o
<literal>${SCC_REGISTRATION_CODE}</literal> devem ser devidamente definidos
para registrar o sistema com a sua conta.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
        zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Pasta kubernetes</title>
<para>A pasta <literal>kubernetes</literal> contém as seguintes subpastas:</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para>A pasta <literal>kubernetes/config</literal> contém os seguintes arquivos:</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>: por padrão, o plug-in de
<literal>CNI</literal> instalado é o <literal>Cilium</literal>, portanto,
você não precisa criar a pasta nem o arquivo. Caso seja necessário
personalizar o plug-in de <literal>CNI</literal>, use o arquivo
<literal>server.yaml</literal> na pasta
<literal>kubernetes/config</literal>, que contém as seguintes informações:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
system-default-registry: registry.rancher.com</screen>
</listitem>
</itemizedlist>
<note>
<para>Esse é um arquivo opcional para definir personalizações do Kubernetes, como
os plug-ins de CNI usados ou várias opções que você pode conferir na <link
xl:href="https://docs.rke2.io/install/configuration">documentação
oficial</link>.</para>
</note>
<para>A pasta <literal>os-files/var/lib/rancher/rke2/server/manifests</literal>
contém o seguinte arquivo:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>: contém a configuração para
criar o serviço <literal>Ingress</literal> para o cluster de gerenciamento
(sem necessidade de modificação).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
</itemizedlist>
<note>
<para>É necessário incluir <literal>HelmChartConfig</literal> por meio de
<literal>os-files</literal> no diretório
<literal>/var/lib/rancher/rke2/server/manifests</literal>, e não por meio de
<literal>kubernetes/manifests</literal> conforme descrito nas versões
anteriores.</para>
</note>
<para>A pasta <literal>kubernetes/manifests</literal> contém os seguintes
arquivos:</para>
<itemizedlist>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>: contém a configuração para
criar o namespace <literal>NeuVector</literal> (sem necessidade de
modificação).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>: contém a configuração para criar o
<literal>L2Advertisement</literal> para o componente
<literal>MetalLB</literal> (sem necessidade de modificação).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>: contém a configuração para criar o
<literal>IPAddressPool</literal> para o componente
<literal>rke2-ingress-nginx</literal>. O <literal>${INGRESS_VIP}</literal>
deve ser definido apropriadamente para especificar o endereço IP reservado
que o componente <literal>rke2-ingress-nginx</literal> usará.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para>A pasta <literal>kubernetes/helm/values</literal> contém os seguintes
arquivos:</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>: contém a configuração para criar o
componente <literal>Rancher</literal>. É necessário definir apropriadamente
o <literal>${INGRESS_VIP}</literal> para especificar o endereço IP que o
componente <literal>Rancher</literal> consumirá. O URL para acessar o
componente <literal>Rancher</literal> será
<literal>https://rancher-${INGRESS_VIP}.sslip.io</literal>.</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>: contém a configuração para criar o
componente <literal>NeuVector</literal> (sem necessidade de modificação).</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
registry: "registry.rancher.com"
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>longhorn.yaml</literal>: inclui a configuração para criar o
componente <literal>Longhorn</literal> (sem necessidade de modificação).</para>
<screen language="yaml" linenumbering="unnumbered">global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>: contém a configuração para criar o
componente <literal>Metal<superscript>3</superscript></literal>. É
necessário definir apropriadamente o <literal>${METAL3_VIP}</literal> para
especificar o endereço IP que o componente
<literal>Metal<superscript>3</superscript></literal> consumirá.</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>O servidor de mídia é um recurso opcional incluído no
Metal<superscript>3</superscript> (que está desabilitado por padrão). Para
usar o recurso Metal3, configure-o no manifesto anterior. Para usar o
servidor de mídia Metal<superscript>3</superscript>, especifique a seguinte
variável:</para>
<itemizedlist>
<listitem>
<para>adicione <literal>enable_metal3_media_server</literal> a
<literal>true</literal> para habilitar o recurso de servidor de mídia na
seção global.</para>
</listitem>
<listitem>
<para>inclua a seguinte configuração sobre o servidor de mídia, em que
${MEDIA_VOLUME_PATH} é o caminho para o volume da mídia (por exemplo,
<literal>/home/metal3/bmh-image-cache</literal>).</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>É possível usar um servidor de mídia externo para armazenar as imagens e,
caso você queira usá-lo com TLS, precisará modificar as seguintes
configurações:</para>
<itemizedlist>
<listitem>
<para>defina <literal>additionalTrustedCAs</literal> como <literal>true</literal>
no arquivo <literal>metal3.yaml</literal> anterior para permitir as CAs
confiáveis adicionais do servidor de mídia externo.</para>
</listitem>
<listitem>
<para>inclua a seguinte configuração de segredo na pasta
<literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal> para
armazenar o certificado CA do servidor de mídia externo.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para>O <literal>additional_ca_cert</literal> é o certificado CA codificado em
base64 do servidor de mídia externo. É possível usar o seguinte comando para
codificar o certificado e gerar o segredo manualmente:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>: contém a configuração para criar o
componente <literal>Cert-Manager</literal> (sem necessidade de modificação).</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: true</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>Pasta de rede</title>
<para>A pasta <literal>network</literal> contém o mesmo número de arquivos e nós
no cluster de gerenciamento. Em nosso caso, temos apenas um nó, portanto,
temos somente um arquivo chamado
<literal>mgmt-cluster-node1.yaml</literal>. O nome do arquivo deve ser igual
ao nome de host especificado no arquivo de definição
<literal>mgmt-cluster.yaml</literal> incluído na seção network/node descrita
acima.</para>
<para>Se você precisa personalizar a configuração de rede, por exemplo, para usar
um endereço IP estático específico (cenário sem DHCP), use o arquivo
<literal>mgmt-cluster-node1.yaml</literal> na pasta
<literal>network</literal>, que contém as seguintes informações:</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: o endereço IP do gateway.</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: o endereço IP do servidor DNS.</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: o endereço MAC da interface de rede.</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: o endereço IP do cluster de
gerenciamento.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>Para usar DHCP para obter o endereço IP, você pode definir a seguinte
configuração (o endereço <literal>MAC</literal> deve ser devidamente
definido por meio da variável <literal>${MGMT_MAC}</literal>):</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>Dependendo do número de nós no cluster de gerenciamento, você poderá criar
mais arquivos, como <literal>mgmt-cluster-node2.yaml</literal>,
<literal>mgmt-cluster-node3.yaml</literal> etc., para configurar o restante
dos nós.</para>
</listitem>
<listitem>
<para>A seção <literal>routes</literal> é usada para definir a tabela de
roteamento para o cluster de gerenciamento.</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>Preparação da imagem para ambientes air-gapped</title>
<para>Esta seção descreve como preparar a imagem para ambientes air-gapped
mostrando apenas as diferenças das seções anteriores. As seguintes
alterações na seção anterior (Preparação da imagem para ambientes conectados
(<xref linkend="mgmt-cluster-image-preparation-connected"/>)) são
necessárias para preparar a imagem para os ambientes air-gapped:</para>
<itemizedlist>
<listitem>
<para>É necessário modificar o arquivo <literal>mgmt-cluster.yaml</literal> para
incluir a seção <literal>embeddedArtifactRegistry</literal> com o campo
<literal>images</literal> definido para que todas as imagens do contêiner
sejam incluídas na imagem de saída do EIB.</para>
</listitem>
<listitem>
<para>É necessário modificar o arquivo <literal>mgmt-cluster.yaml</literal> para
incluir o gráfico Helm <literal>rancher-turtles-airgap-resources</literal>.</para>
</listitem>
<listitem>
<para>É necessário remover o script
<literal>custom/scripts/99-register.sh</literal> ao usar um ambiente
air-gapped.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>Modificações no arquivo de definição</title>
<para>É necessário modificar o arquivo <literal>mgmt-cluster.yaml</literal> para
incluir a seção <literal>embeddedArtifactRegistry</literal>. Nessa seção, o
campo <literal>images</literal> deve conter a lista de todas as imagens do
contêiner que serão incluídas na imagem de saída.</para>
<note>
<para>Veja a seguir um exemplo do arquivo <literal>mgmt-cluster.yaml</literal> com
a seção <literal>embeddedArtifactRegistry</literal> incluída. Garanta que as
imagens listadas contenham as versões dos componentes necessárias.</para>
</note>
<para>É necessário também adicionar o gráfico Helm
<literal>rancher-turtles-airgap-resources</literal> para criar os recursos
conforme descrito na <link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/v0.19/en/getting-started/air-gapped-environment.html">documentação
do Rancher Turtles sobre ambientes air-gapped</link>. Isso também requer um
arquivo de valores turtles.yaml para que o gráfico rancher-turtles
especifique a configuração necessária.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/mirrored-cilium-cilium:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-cilium-operator-generic:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0
    - name: registry.rancher.com/rancher/neuvector-compliance-config:1.0.6
    - name: registry.rancher.com/rancher/neuvector-controller:5.4.5
    - name: registry.rancher.com/rancher/neuvector-enforcer:5.4.5
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.suse.com/rancher/cluster-api-addon-provider-fleet:v0.11.0
    - name: registry.rancher.com/rancher/cluster-api-operator:v0.18.1
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.7-build20250425
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher/turtles:v0.24.0
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/shell:v0.4.1
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.suse.com/rancher/cluster-api-controller:v1.10.5
    - name: registry.suse.com/rancher/cluster-api-provider-metal3:v1.10.2
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1
    - name: registry.suse.com/rancher/hardened-sriov-network-operator:v1.5.0-build20250425
    - name: registry.suse.com/rancher/ip-address-manager:v1.10.2
    - name: registry.rancher.com/rancher/kubectl:v1.32.2
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.9.5</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>Modificações na pasta custom</title>
<itemizedlist>
<listitem>
<para>É necessário remover o script
<literal>custom/scripts/99-register.sh</literal> ao usar um ambiente
air-gapped. Conforme você pode observar na estrutura de diretórios, o script
<literal>99-register.sh</literal> não está incluído na pasta
<literal>custom/scripts</literal>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-helm-values-folder-airgap">
<title>Modificações na pasta de valores do Helm</title>
<itemizedlist>
<listitem>
<para>O <literal>turtles.yaml</literal>: contém a configuração necessária para
especificar a operação air-gapped para o Rancher Turtles. Observe que isso
depende da instalação do gráfico rancher-turtles-airgap-resources.</para>
<screen language="yaml" linenumbering="unnumbered">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"</screen>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>Criação de imagem</title>
<para>Depois que a estrutura de diretórios foi preparada de acordo com as seções
anteriores (para os cenários tanto conectados quanto air-gapped), execute o
comando abaixo para criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>Isso cria o arquivo de imagem ISO de saída que, com base na definição da
imagem descrita acima, no nosso caso é
<literal>eib-mgmt-cluster-image.iso</literal>.</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>Provisionar o cluster de gerenciamento</title>
<para>A imagem anterior contém todos os componentes explicados acima e pode ser
usada para provisionar o cluster de gerenciamento usando uma máquina virtual
ou um servidor bare metal (com o recurso de mídia virtual).</para>
</section>
<section xml:id="mgmt-cluster-dualstack">
<title>Considerações e configuração da pilha dupla</title>
<para>Os exemplos apresentados nas seções anteriores servem para orientar como
configurar um cluster de gerenciamento IPv4 de pilha única. Esse tipo de
cluster de gerenciamento é independente do status operacional dos clusters
downstream, que podem ser configurados individualmente para operação em uma
configuração de pilha única ou de pilha dupla IPv4/IPv6 após a
implantação. No entanto, a maneira como o cluster de gerenciamento é
configurado tem um impacto direto sobre os protocolos de comunicação que
podem ser usados durante a fase de provisionamento, em que as comunicações
tanto dentro quanto fora da banda devem acontecer de acordo com os
protocolos compatíveis com o cluster de gerenciamento e o host
downstream. Caso alguns ou todos os BMCs e/ou nós do cluster downstream
devam usar IPv6, uma configuração de pilha dupla será necessária para o
cluster de gerenciamento.</para>
<note>
<para>Os clusters de gerenciamento IPv6 de pilha única ainda não são suportados.</para>
</note>
<para>Para alcançar a funcionalidade de pilha dupla, o Kubernetes deve ser
fornecido com CIDRs IPv4 e IPv6 para PODs e serviços. No entanto, outros
componentes também exigem ajustes específicos antes de criar a imagem do
cluster de gerenciamento com EIB. É possível configurar os serviços de
provisionamento do Metal<superscript>3</superscript> (Ironic) de várias
maneiras, dependendo da sua infraestrutura ou dos seus requisitos:</para>
<itemizedlist>
<listitem>
<para>É possível configurar os serviços do Ironic para escutar em todas as
interfaces no sistema, em vez de um único endereço IP. Portanto, desde que
os hosts do cluster de gerenciamento tenham endereços tanto IPv4 quanto IPv6
atribuídos à interface relevante, qualquer um deles poderá ser usado durante
o provisionamento. Neste ponto, observe que é possível selecionar apenas um
desses endereços para geração do URL (para que outros serviços consumam, por
exemplo, Baremetal Operator, BMCs etc.). Como consequência, para habilitar a
comunicação IPv6 com os BMCs, é possível instruir o Baremetal Operator a
expor e especificar o URL IPv6 ao processar as definições de BMH, incluindo
um endereço IPv6. Em outras palavras, quando um BMC é identificado como
compatível com IPv6, o provisionamento é realizado apenas por IPv6, e por
IPv4 em todos os outros casos.</para>
</listitem>
<listitem>
<para>O Metal<superscript>3</superscript> pode usar um único nome de host, que
resolve tanto IPv4 quanto IPv6, para permitir que o Ironic use esses
endereços para vinculação e criação de URL. Essa abordagem permite uma
configuração fácil e um comportamento flexível (ambos IPv4 e IPv6 permanecem
viáveis em cada etapa de provisionamento), mas requer uma infraestrutura com
servidores DNS preexistentes, alocações de IP e registros já disponíveis.</para>
</listitem>
</itemizedlist>
<para>Nos dois casos, o Kubernetes precisa saber quais CIDRs usar para ambos IPv4
e IPv6, de modo que você possa adicionar as seguintes linhas ao
<literal>kubernetes/config/server.yaml</literal> no diretório do EIB,
assegurando que o IPv4 seja listado primeiro:</para>
<screen language="yaml" linenumbering="unnumbered">service-cidr: 10.96.0.0/12,fd12:4567:789c::/112
cluster-cidr: 193.168.0.0/18,fd12:4567:789b::/48</screen>
<para>Alguns contêineres usam a rede do host, portanto, modifique a configuração
de rede dos hosts no diretório <literal>network</literal> para habilitar a
conectividade IPv6:</para>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    next-hop-address: ${MGMT_GATEWAY_V4}
    next-hop-interface: eth0
  - destination: ::/0
    next-hop-address: ${MGMT_GATEWAY_V6}
    next-hop-interface: eth0
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
    - 2001:4860:4860::8888
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP_V4}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    address:
    - ip: ${MGMT_CLUSTER_IP_V6}
      prefix-length: 128
    dhcp: false
    autoconf: false
    enabled: true</screen>
<para>Substitua os espaços reservados pelos endereços IP do gateway, pelo servidor
DNS adicional (se necessário), pelo endereço MAC da interface de rede e pelo
endereço IP do cluster de gerenciamento. Em vez disso, se a preferência for
a configuração automática de endereços, consulte o seguinte trecho e defina
apenas a variável <literal>${MGMT_MAC}</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    enabled: true
    dhcp: true
  ipv6:
    enabled: false
    dhcp: true
    autoconf: true</screen>
<para>Agora podemos definir os arquivos restantes com uma configuração de nó
único, a partir da primeira opção, criando o
<literal>kubernetes/helm/values/metal3.yaml</literal> como:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${MGMT_CLUSTER_IP_V4}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  listenOnAll: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort
metal3-baremetal-operator:
  baremetaloperator:
    externalHttpIPv6: ${MGMT_CLUSTER_IP_V6}</screen>
<para>e o <literal>kubernetes/helm/values/rancher.yaml</literal> como:</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_IP_V4}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>em que <literal>${MGMT_CLUSTER_IP_V4}</literal> e
<literal>${MGMT_CLUSTER_IP_V6}</literal> são os endereços IP já atribuídos
ao host.</para>
<para>Se preferir usar o nome de host no lugar dos endereços IP, modifique o
<literal>kubernetes/helm/values/metal3.yaml</literal> para:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  provisioningHostname: `${MGMT_CLUSTER_HOSTNAME}`
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort</screen>
<para>e o <literal>kubernetes/helm/values/rancher.yaml</literal> para:</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_HOSTNAME}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>em que <literal>${MGMT_CLUSTER_HOSTNAME}</literal> deve ser um nome de
domínio completo e qualificado resolvido para seus endereços IP de host.</para>
<para>Para obter mais informações, visite <link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/mgmt-cluster/dual-stack">Repositório
GitHub do SUSE Edge na pasta "dual-stack"</link>, em que você encontra uma
estrutura de diretórios de exemplo.</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>Configuração dos recursos de telecomunicações</title>
<para>Esta seção documenta e explica a configuração de recursos específicos do
Telco em clusters implantados por meio do SUSE Telco Cloud.</para>
<para>O método de implantação de provisionamento de rede direcionado é usado,
conforme descrito na seção sobre provisionamento automatizado (<xref
linkend="atip-automated-provisioning"/>).</para>
<para>Os seguintes tópicos são abordados nesta seção:</para>
<itemizedlist>
<listitem>
<para>Imagem do kernel para tempo real (<xref
linkend="kernel-image-for-real-time"/>): que será usada pelo kernel
Real-Time.</para>
</listitem>
<listitem>
<para>Argumentos do kernel para baixa latência e alto desempenho (<xref
linkend="kernel-args"/>): usados pelo kernel Real-Time para máximo
desempenho e baixa latência na execução de cargas de trabalho de
telecomunicações.</para>
</listitem>
<listitem>
<para>Fixação de CPU via TuneD e argumentos do kernel (<xref
linkend="cpu-tuned-configuration"/>): isolamento de CPUs por meio de
argumentos do kernel e perfil do TuneD.</para>
</listitem>
<listitem>
<para>Configuração da CNI (<xref linkend="cni-configuration"/>): usada pelo
cluster Kubernetes.</para>
</listitem>
<listitem>
<para>Configuração do SR-IOV (<xref linkend="sriov"/>): usada pelas cargas de
trabalho Kubernetes.</para>
</listitem>
<listitem>
<para>Configuração do DPDK (<xref linkend="dpdk"/>): usada pelo sistema.</para>
</listitem>
<listitem>
<para>Placa aceleradora vRAN (<xref linkend="acceleration"/>): configuração da
placa aceleradora usada pelas cargas de trabalho Kubernetes.</para>
</listitem>
<listitem>
<para>HugePages (<xref linkend="huge-pages"/>): configuração do HugePages usada
pelas cargas de trabalho Kubernetes.</para>
</listitem>
<listitem>
<para>Fixação de CPU no Kubernetes (<xref linkend="cpu-pinning-kubernetes"/>):
configuração do Kubernetes e de aplicativos para aproveitar a fixação de
CPU.</para>
</listitem>
<listitem>
<para>Configuração da programação com reconhecimento de NUMA (<xref
linkend="numa-aware-scheduling"/>): usada pelas cargas de trabalho
Kubernetes.</para>
</listitem>
<listitem>
<para>Configuração do Metal LB (<xref linkend="metal-lb-configuration"/>): usada
pelas cargas de trabalho Kubernetes.</para>
</listitem>
<listitem>
<para>Configuração do registro particular (<xref linkend="private-registry"/>):
usada pelas cargas de trabalho Kubernetes.</para>
</listitem>
<listitem>
<para>Configuração do Precision Time Protocol (<xref
linkend="ptp-configuration"/>): arquivos de configuração para execução de
perfis de telecomunicações PTP.</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>Imagem do kernel para tempo real</title>
<para>A imagem do kernel Real-Time não é necessariamente melhor do que o kernel
padrão. Trata-se de um kernel diferente adaptado a um caso de uso
específico. O kernel Real-Time é adaptado para latência mais baixa em
detrimento da taxa de transferência. Ele não é recomendado para fins de uso
geral; mas, no nosso caso, esse é o kernel recomendado para cargas de
trabalho de telecomunicações em que a latência é um fator importante.</para>
<para>Há quatro recursos principais:</para>
<itemizedlist>
<listitem>
<para>Execução determinística:</para>
<para>Aumente a previsibilidade: garanta que os processos de negócios críticos
sejam sempre concluídos dentro do prazo e ofereçam um serviço de alta
qualidade, mesmo com cargas pesadas do sistema. Ao proteger os recursos
importantes do sistema nos processos de alta prioridade, você garante maior
previsibilidade para aplicativos urgentes.</para>
</listitem>
<listitem>
<para>Baixa instabilidade:</para>
<para>A baixa instabilidade decorrente da tecnologia altamente determinística
ajuda a manter a sincronização dos aplicativos com o mundo real. Isso ajuda
os serviços que precisam de cálculo contínuo e repetido.</para>
</listitem>
<listitem>
<para>Herança de prioridade:</para>
<para>A herança de prioridade refere-se à capacidade de um processo de prioridade
mais baixa assumir a prioridade mais alta quando há um processo de maior
prioridade que requer a conclusão do processo de menor prioridade para então
finalizar sua tarefa. O SUSE Linux Enterprise Real Time resolve esses
problemas de inversão de prioridade para processos de extrema importância.</para>
</listitem>
<listitem>
<para>Interrupções de threads:</para>
<para>Os processos executados no modo de interrupção em um sistema operacional de
uso geral não são preemptíveis. Com o SUSE Linux Enterprise Real Time, essas
interrupções foram encapsuladas pelos threads do kernel, que são
interrompíveis e permitem a preempção de interrupções fixas e flexíveis por
processos de prioridade mais alta definidos pelo usuário.</para>
<para>No nosso caso, se você instalou uma imagem em tempo real, como <literal>SUSE
Linux Micro RT</literal>, o kernel Real-Time já está instalado. Você pode
fazer download da imagem do kernel Real-Time pelo <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>.</para>
<note>
<para>Para obter mais informações sobre o kernel Real-Time, visite <link
xl:href="https://www.suse.com/products/realtime/">SUSE Real Time</link>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="kernel-args">
<title>Argumentos do kernel para baixa latência e alto desempenho</title>
<para>É importante configurar os argumentos do kernel para que o kernel Real-Time
funcione corretamente, apresentando o melhor desempenho e a baixa latência
para executar as cargas de trabalho de telecomunicações. Há alguns conceitos
importantes para manter em mente na hora de configurar os argumentos do
kernel para este caso de uso:</para>
<itemizedlist>
<listitem>
<para>Remova o <literal>kthread_cpus</literal> ao usar o kernel Real-Time da
SUSE. Esse parâmetro controla em quais CPUs os threads do kernel serão
criados. Ele também controla quais CPUs têm permissão para PID 1 e para
carregar módulos do kernel (o auxiliar de espaço do usuário kmod). Esse
parâmetro não é reconhecido e não tem nenhum efeito.</para>
</listitem>
<listitem>
<para>Isole os núcleos da CPU usando <literal>isolcpus</literal>,
<literal>nohz_full</literal>, <literal>rcu_nocbs</literal> e
<literal>irqaffinity</literal>. Para acessar a lista completa de técnicas de
fixação de CPU, consulte o capítulo Fixação de CPU via TuneD e argumentos do
kernel (<xref linkend="cpu-tuned-configuration"/>).</para>
</listitem>
<listitem>
<para>Adicione os sinalizadores <literal>domain,nohz,managed_irq</literal> ao
argumento do kernel <literal>isolcpus</literal>. Sem os sinalizadores, o
<literal>isolcpus</literal> equivale a especificar apenas o sinalizador
<literal>domain</literal>. Isso isola as CPUs especificadas da programação,
incluindo as tarefas do kernel. O sinalizador <literal>nohz</literal>
interrompe o tick do programador nas CPUs especificadas (se apenas uma
tarefa for executável em determinada CPU), e o sinalizador
<literal>managed_irq</literal> evita o roteamento de interrupções externas
gerenciadas (dispositivos) nas CPUs especificadas. Observe que as linhas da
IRQ dos dispositivos NVMe são totalmente gerenciadas pelo kernel e serão
roteadas para os núcleos não isolados (manutenção) como consequência. Por
exemplo, a linha de comando inserida no fim desta seção resultará apenas em
quatro filas (mais uma fila de admin/controle) alocadas no sistema:</para>
<screen language="shell" linenumbering="unnumbered">for I in $(grep nvme0 /proc/interrupts | cut -d ':' -f1); do cat /proc/irq/${I}/effective_affinity_list; done | column
39      0       19      20      39</screen>
<para>Esse comportamento impede interrupções causadas por E/S do disco em qualquer
aplicativo urgente executado nos núcleos isolados, mas pode exigir atenção e
definição cuidadosa para cargas de trabalho com foco em armazenamento.</para>
</listitem>
<listitem>
<para>Ajuste os ticks (interrupções periódicas do temporizador do kernel):</para>
<itemizedlist>
<listitem>
<para><literal>skew_tick=1</literal>: os ticks às vezes podem ocorrer ao mesmo
tempo. Em vez de todas as CPUs receberem o tick do temporizador exatamente
no mesmo momento, o <literal>skew_tick=1</literal> faz com que ele ocorra em
horários um pouco diferentes. Isso ajuda a reduzir a instabilidade do
sistema, resultando em tempos de resposta mais consistentes e com menos
interrupções (um requisito essencial para aplicativos sensíveis à latência).</para>
</listitem>
<listitem>
<para><literal>nohz=on</literal>: interrompe o tick do temporizador periódico em
CPUs ociosas.</para>
</listitem>
<listitem>
<para><literal>nohz_full=&lt;cpu-cores&gt;</literal>: interrompe o tick periódico
do temporizador nas CPUs especificadas que são dedicadas a aplicativos em
tempo real.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Desabilite o processamento de exceção de verificação de máquina (MCE,
Machine Check Exception) especificando <literal>mce=off</literal>. As MCEs
são erros do hardware detectados pelo processador, e sua desabilitação pode
evitar registros com muito ruído.</para>
</listitem>
<listitem>
<para>Adicione <literal>nowatchdog</literal> para desabilitar o watchdog de
bloqueio flexível que é implementado como um temporizador em execução no
contexto de interrupção fixa do temporizador. Quando ele expira (ou seja, um
bloqueio flexível é detectado), um aviso é exibido (no contexto de
interrupção fixa), executando os destinos de latência. Mesmo que nunca
expire, ele é incluído na lista do temporizador, o que aumenta levemente a
sobrecarga de cada interrupção do temporizador. Essa opção também desabilita
o watchdog de NMI, assim as NMIs não podem interferir.</para>
</listitem>
<listitem>
<para><literal>nmi_watchdog=0</literal> desabilita o watchdog de NMI (interrupção
não mascarável). Para omiti-lo, use <literal>nowatchdog</literal>.</para>
</listitem>
<listitem>
<para>RCU (Read-Copy-Update, Ler-Copiar-Atualizar) é um mecanismo que permite o
acesso simultâneo e sem bloqueio de vários leitores aos dados
compartilhados. O retorno de chamada RCU, função acionada após um "período
extra", garante que todos os leitores anteriores tenham finalizado para que
os dados antigos possam ser recuperados com segurança. Ajustamos o RCU,
especificamente para cargas de trabalho confidenciais, para descarregar
esses retornos de chamada das CPUs dedicadas (fixadas), evitando que as
operações do kernel interfiram em tarefas críticas e urgentes.</para>
<itemizedlist>
<listitem>
<para>Especifique as CPUs fixadas em <literal>rcu_nocbs</literal> para que os
retornos de chamada RCU não sejam executados nelas. Isso ajuda a reduzir a
instabilidade e a latência para cargas de trabalho em tempo real.</para>
</listitem>
<listitem>
<para>O <literal>rcu_nocb_poll</literal> faz com que as CPUs sem retorno de
chamada realizem sondagens regulares para ver se há necessidade de gerenciar
retornos de chamadas. Isso pode reduzir a sobrecarga de interrupções.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_cpu_stall_suppress=1</literal> suprime os avisos de
parada de RCU da CPU, que às vezes podem ser falsos positivos nos sistemas
em tempo real com carga elevada.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_expedited=1</literal> acelera o período extra das
operações RCU, o que torna as seções críticas do lado da leitura mais
responsivas.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_normal_after_boot=1</literal>, quando usado com
rcu_expedited, permite que o RCU volte à operação normal (não acelerada)
após a inicialização do sistema.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_task_stall_timeout=0</literal> desabilita o detector
de paradas de tarefas do RCU, evitando possíveis avisos ou paralisações do
sistema provocadas por tarefas do RCU de longa duração.</para>
</listitem>
<listitem>
<para><literal>rcutree.kthread_prio=99</literal> define a prioridade do thread do
kernel de retorno de chamada RCU como a mais alta possível (99), garantindo
que ele seja programado e processe os retornos de chamada RCU prontamente,
quando necessário.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Adicione o <literal>ignition.platform.id=openstack</literal> para que o
Metal3 e a Cluster API provisionem/desprovisionem o cluster com sucesso. Ele
é usado pelo agente Metal3 Python, que teve sua origem no Openstack Ironic.</para>
</listitem>
<listitem>
<para>Remova <literal>intel_pstate=passive</literal>. Essa opção configura o
<literal>intel_pstate</literal> para operar com controladores cpufreq
genéricos. No entanto, para que isso funcione, ele desabilita os estados P
gerenciados pelo hardware (<literal>HWP</literal>) como efeito
colateral. Para reduzir a latência do hardware, essa opção não é recomendada
para cargas de trabalho em tempo real.</para>
</listitem>
<listitem>
<para>Substitua <literal>intel_idle.max_cstate=0 processor.max_cstate=1</literal>
por <literal>idle=poll</literal>. Para evitar transições de estado C, a
opção <literal>idle=poll</literal> é usada para desabilitar essas transições
e manter a CPU no estado C mais alto. A opção
<literal>intel_idle.max_cstate=0</literal> desabilita
<literal>intel_idle</literal> para que <literal>acpi_idle</literal> seja
usado e, em seguida, o <literal>acpi_idle.max_cstate=1</literal> define o
estado C máximo para acpi_idle. Nas arquiteturas AMD64/Intel 64, o primeiro
estado C da ACPI sempre é <literal>POLL</literal>, mas ela usa uma função
<literal>poll_idle()</literal>, que pode gerar uma pequena latência com a
leitura periódica do relógio e a reinicialização do loop principal em
<literal>do_idle()</literal> após o tempo limite (isso também envolve limpar
e definir o sinalizador da tarefa <literal>TIF_POLL</literal>). Por outro
lado, <literal>idle=poll</literal> é executado em um loop restrito,
mantendo-se em espera ocupada até que uma tarefa seja reprogramada. Isso
minimiza a latência de sair do estado ocioso, mas à custa de manter a CPU em
execução na máxima velocidade no thread ocioso.</para>
</listitem>
<listitem>
<para>Desabilite C1E no BIOS. Essa opção é importante para desabilitar o estado
C1E no BIOS para evitar que a CPU entre no estado C1E quando estiver
ociosa. C1E é um estado de baixo consumo que pode gerar latência quando a
CPU está ociosa.</para>
</listitem>
</itemizedlist>
<para>O restante desta documentação aborda parâmetros adicionais, incluindo
HugePages e IOMMU.</para>
<para>Este é um exemplo de argumentos do kernel para um servidor Intel de 32
núcleos, incluindo os ajustes já mencionados:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
<para>Este é outro exemplo de configuração para um servidor AMD de 64
núcleos. Dentre os 128 processadores lógicos (<literal>0-127</literal>), os
primeiros 8 núcleos (<literal>0-7</literal>) são destinados à manutenção,
enquanto os 120 núcleos restantes (<literal>8-127</literal>) são fixados
para os aplicativos:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=575291cf-74e8-42cf-8f2c-408a20dc00b8 skew_tick=1 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack amd_iommu=on iommu=pt irqaffinity=0-7 isolcpus=domain,nohz,managed_irq,8-127 nohz_full=8-127 rcu_nocbs=8-127 mce=off nohz=on net.ifnames=0 nowatchdog nmi_watchdog=0 nosoftlockup quiet rcu_nocb_poll rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
</section>
<section xml:id="cpu-tuned-configuration">
<title>Fixação de CPU via TuneD e argumentos do kernel</title>
<para><literal>tuned</literal> é uma ferramenta de ajuste de sistema que monitora
as condições do sistema para otimizar o desempenho usando vários perfis
predefinidos. Um recurso importante é sua capacidade de isolar os núcleos da
CPU para cargas de trabalho específicas, como os aplicativos em tempo
real. Isso impede que o sistema operacional use esses núcleos e,
possivelmente, aumente a latência.</para>
<para>Para habilitar e configurar esse recurso, a primeira etapa é criar um perfil
para os núcleos da CPU que desejamos isolar. Neste exemplo, dos 64 núcleos,
dedicamos 60 (<literal>1-30,33-62</literal>) para o aplicativo e os 4
restantes são usados para manutenção. Observe que o design das CPUs isoladas
depende significativamente dos aplicativos em tempo real.</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>Na sequência, precisamos modificar a opção GRUB para isolar os núcleos da
CPU e outros parâmetros importantes para uso da CPU. É importante
personalizar as seguintes opções com suas especificações de hardware atuais:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parâmetro</entry>
<entry align="left" valign="top">valor</entry>
<entry align="left" valign="top">descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,&#x200B;managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Isolar os núcleos 1-30 e 33-62. <literal>domain</literal> indica que as CPUs
fazem parte do domínio de isolamento. <literal>nohz</literal> permite a
operação sem ticks nessas CPUs isoladas quando estão ociosas, para reduzir
interrupções. <literal>managed_irq</literal> isola as CPUs fixadas para que
não sejam destinos das IRQs. Isso contempla o
<literal>irqaffinity=0-7</literal>, que já direciona a maiorias das IRQs
para os núcleos de manutenção.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Essa opção permite que o kernel distribua as interrupções do temporizador
entre as CPUs isoladas.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>em</para></entry>
<entry align="left" valign="top"><para>Quando habilitada, a interrupção periódica do temporizador do kernel
("tick") vai parar em qualquer núcleo da CPU que esteja ocioso. Isso
beneficia principalmente as CPUs de manutenção
(<literal>0,31,32,63</literal>), além de economizar energia e reduzir
ativações desnecessárias nos núcleos de uso geral.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Para os núcleos isolados, esse processo interrompe o tick, mesmo quando a
CPU está executando uma única tarefa ativa. Dessa forma, ele faz com que a
CPU seja executada no modo sem ticks total (ou "dyntick"). O kernel apenas
enviará interrupções do temporizador quando forem de fato necessárias.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Essa opção descarrega o processamento de retorno de chamada RCU dos núcleos
especificados da CPU.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocb_poll</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Quando essa opção é definida, as CPUs sem retorno de chamada RCU fazem uma
sondagem regular para ver se o processamento de retornos de chamada é
necessário, em vez de ser explicitamente ativadas por outras CPUs. Isso pode
reduzir a sobrecarga das interrupções.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Essa opção permite que o kernel execute as interrupções nos núcleos de
manutenção.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>Isso minimiza a latência de sair do estado ocioso, mas à custa de manter a
CPU em execução na velocidade máxima no thread ocioso.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>Essa opção desabilita apenas o watchdog de NMI. Para omiti-la, defina
<literal>nowatchdog</literal>.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Essa opção desabilita o watchdog de bloqueio flexível, que é implementado
como um temporizador executado no contexto de interrupção fixa do
temporizador.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Os seguintes comandos modificam a configuração do GRUB e aplicam as
alterações mencionadas acima para que estejam presentes na próxima
inicialização:</para>
<para>Edite o arquivo <literal>/etc/default/grub</literal> com os parâmetros
acima, e ele terá esta aparência:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Atualize a configuração do GRUB:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>Para validar a aplicação dos parâmetros após a reinicialização, é possível
usar o seguinte comando para verificar a linha de comando do kernel:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<para>Existe outro script que pode ser usado para ajustar a configuração da CPU
que, basicamente, executa as seguintes etapas:</para>
<itemizedlist>
<listitem>
<para>Definir o controlador da CPU como <literal>performance</literal>.</para>
</listitem>
<listitem>
<para>Cancelar a definição da migração do temporizador para as CPUs isoladas.</para>
</listitem>
<listitem>
<para>Migrar os threads kdaemon para as CPUs de manutenção.</para>
</listitem>
<listitem>
<para>Definir a latência das CPUs isoladas como o valor mais baixo possível.</para>
</listitem>
<listitem>
<para>Atrasar as atualizações de vmstat para 300 segundos.</para>
</listitem>
</itemizedlist>
<para>O script está disponível no <link
xl:href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.4/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh">repositório
de exemplos do SUSE Telco Cloud</link>.</para>
</section>
<section xml:id="cni-configuration">
<title>Configuração da CNI</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal> é o plug-in de CNI padrão para o SUSE Telco
Cloud. Para habilitar o Cilium no cluster RKE2 como o plug-in padrão, a
seguinte configuração é necessária no arquivo
<literal>/etc/rancher/rke2/config.yaml</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>Também é possível especificar isso com argumentos de linha de comando, ou
seja, <literal>--cni=cilium</literal> na linha do servidor no arquivo
<literal>/etc/systemd/system/rke2-server</literal>.</para>
<para>Para usar o operador de rede <literal>SR-IOV</literal> descrito na próxima
seção (<xref linkend="option2-sriov-helm"/>), use o
<literal>Multus</literal> com outro plug-in de CNI, como
<literal>Cilium</literal> ou <literal>Calico</literal>, como o plug-in
secundário.</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</section>
<section xml:id="id-calico">
<title>Calico</title>
<para><literal>Calico</literal> é outro plug-in de CNI para o SUSE Edge for
Telco. Para habilitar o Calico no cluster RKE2 como o plug-in padrão, a
seguinte configuração é necessária no arquivo
<literal>/etc/rancher/rke2/config.yaml</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- calico</screen>
<para>Também é possível especificá-lo com argumentos de linha de comando, ou seja,
<literal>--cni=calico</literal> na linha do servidor no arquivo
<literal>/etc/systemd/system/rke2-server</literal>.</para>
<para>Para usar o operador de rede <literal>SR-IOV</literal> descrito na próxima
seção (<xref linkend="option2-sriov-helm"/>), use o
<literal>Multus</literal> com outro plug-in de CNI, como
<literal>Cilium</literal> ou <literal>Calico</literal>, como o plug-in
secundário.</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- calico</screen>
<note>
<para>Para obter mais informações sobre os plug-ins de CNI, consulte <link
xl:href="https://docs.rke2.io/install/network_options">Network
Options</link> (Opções de rede).</para>
</note>
</section>
<section xml:id="id-bond-cni">
<title>CNI de vínculo</title>
<para>Em termos gerais, a vinculação oferece um meio de agregar várias interfaces
de rede a uma única interface lógica "vinculada". Normalmente, ela é usada
para aumentar a disponibilidade dos serviços com a inclusão de caminhos de
rede redundantes, mas também pode ser usada para aumentar a largura de banda
com determinados modos de vínculo. Os seguintes plug-ins de CNI são
compatíveis com o plug-in de CNI de vínculo em conjunto com o multus:</para>
<itemizedlist>
<listitem>
<para>MACVLAN</para>
</listitem>
<listitem>
<para>Dispositivo host</para>
</listitem>
<listitem>
<para>SR-IOV</para>
</listitem>
</itemizedlist>
<section xml:id="id-bond-cni-with-macvlan">
<title>CNI de vínculo com MACVLAN</title>
<para>Para usar o plug-in de CNI de vínculo com MACVLAN, são necessárias duas
interfaces livres no contêiner. O seguinte exemplo usa "enp8s0" e
"enp9s0". Para começar, crie as definições de conexão de rede para elas:</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp8s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp9s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para>Depois disso, adicione uma definição de conexão de rede para o próprio
vínculo.</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>A atribuição de endereço IP aqui é estática e define o endereço do vínculo
como "192.168.200.100" na rede /24, com um gateway que reside no primeiro
endereço disponível da rede. Na conexão de rede do vínculo, também definimos
o tipo de vínculo desejado. Neste caso, é active-backup.</para>
<para>Para usar esse vínculo, o pod precisa conhecer todas as interfaces. Uma
definição de pod de exemplo tem esta aparência:</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-conf",
"interface": "net1"
},
{"name": "enp9s0-conf",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
<para>Observe como a anotação faz referência a todas as redes e define o
mapeamento entre as interfaces "enp8s0 → net1" e "enp9s0→net2".</para>
</section>
<section xml:id="id-bond-cni-with-host-device">
<title>CNI de vínculo com dispositivo host</title>
<para>Para usar o plug-in de CNI de vínculo com dispositivo host, são necessárias
duas interfaces livres no host. Essas interfaces são mapeadas pelo
contêiner. O seguinte exemplo usa "enp8s0" e "enp9s0". Para começar, crie as
definições de conexão de rede para elas:</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp8s0",
          "ipam": {}
        }]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp9s0",
          "ipam": {}
        }]
    }'</screen>
<para>Depois disso, adicione a definição da conexão de rede ao próprio
vínculo. Isso é similar ao caso de uso do MACVLAN.</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>A atribuição de endereço IP aqui é estática e define o endereço do vínculo
como "192.168.200.100" na rede /24, com um gateway que reside no primeiro
endereço disponível da rede. Na conexão de rede do vínculo, defina o tipo de
vínculo. Neste caso, é active-backup.</para>
<para>Para usar esse vínculo, o pod precisa conhecer todas as interfaces. Uma
definição de pod de exemplo para vínculo com dispositivos host pode ter esta
aparência:</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-hostdev",
"interface": "net1"
},
{"name": "enp9s0-hostdev",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
</section>
<section xml:id="id-bond-cni-with-sr-iov">
<title>CNI de vínculo com SR-IOV</title>
<para>É bem simples usar a CNI de vínculo com o SR-IOV. Para obter mais detalhes
de como configurar o SR-IOV, consulte a <xref linkend="sriov"/>. Conforme
descrito nela, você deve criar <literal>SriovNetworkNodePolicies</literal>
com a definição de <literal>resourceNames</literal> e várias funções
virtuais. Os <literal>resourceNames</literal> são usados pela rede
<literal>SriovNetwork</literal>, que é usada como interface na definição de
pod. A definição do vínculo é exatamente a mesma dos casos de uso do MACVLAN
e dispositivo host.</para>
<note>
<para>A CNI de vínculo com SR-IOV é aplicável apenas às funções virtuais (VF,
Virtual Functions) SRIOV que usam o driver de kernel. As VFs de driver de
espaço de usuário, como aquelas usadas nas cargas de trabalho DPDK, não
podem ser vinculadas à CNI de vínculo.</para>
</note>
</section>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>O SR-IOV permite que um dispositivo, por exemplo, adaptador de rede, separe
o acesso a seus recursos entre várias funções de hardware
<literal>PCIe</literal>. Há diversas maneiras de implantar o
<literal>SR-IOV</literal> e, neste documento, mostramos duas opções
diferentes:</para>
<itemizedlist>
<listitem>
<para>Opção 1: usar os plug-ins de dispositivo CNI <literal>SR-IOV</literal> e um
mapa de configuração para configurá-lo de maneira apropriada.</para>
</listitem>
<listitem>
<para>Opção 2 (recomendada): usar o gráfico Helm do <literal>SR-IOV</literal> do
Rancher Prime para facilitar a implantação.</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">Opção 1 – Instalação dos plug-ins de dispositivo CNI
SR-IOV e um mapa de configuração para configurá-los de maneira
apropriada</emphasis></para>
<itemizedlist>
<listitem>
<para>Preparar o mapa de configuração para o plug-in de dispositivo</para>
</listitem>
</itemizedlist>
<para>Obtenha as informações para preencher o mapa de configuração executando o
comando <literal>lspci</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>O mapa de configuração consiste em um arquivo <literal>JSON</literal> que
descreve os dispositivos usando filtros para descobri-los e cria grupos para
as interfaces. O principal é entender os filtros e os grupos. Os filtros são
usados para descobrir os dispositivos, e os grupos para criar as interfaces.</para>
<para>É possível definir os filtros desta maneira:</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (placa aceleradora)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (driver)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (nome da interface física)</para>
</listitem>
</itemizedlist>
<para>É possível também definir os filtros para corresponder à uma sintaxe de
interface mais complexa, por exemplo:</para>
<itemizedlist>
<listitem>
<para>pfNames: <literal>["eth1#1,2,3,4,5,6"]</literal> ou
<literal>[eth1#1-6]</literal> (nome da interface física)</para>
</listitem>
</itemizedlist>
<para>Em relação aos grupos, podemos criar um para a placa <literal>FEC</literal>
e outro para a placa <literal>Intel</literal>, e até criar um prefixo
dependendo do nosso caso de uso:</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>Há inúmeras combinações para descobrir e criar o grupo de recursos para
alocar algumas <literal>VFs</literal> aos pods.</para>
<note>
<para>Para obter mais informações sobre filtros e grupos, visite <link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sr-iov
network device plug-in</link> (Plug-in de dispositivo de rede sr-iov).</para>
</note>
<para>Depois de definir os filtros e os grupos para corresponder as interfaces,
dependendo do hardware e do caso de uso, o seguinte mapa de configuração
mostrará um exemplo para ser usado:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para>Preparar o arquivo <literal>daemonset</literal> para implantar o plug-in de
dispositivo</para>
</listitem>
</itemizedlist>
<para>O plug-in de dispositivo oferece suporte a várias arquiteturas
(<literal>arm</literal>, <literal>amd</literal>,
<literal>ppc64le</literal>), portanto, é possível usar o mesmo arquivo para
arquiteturas diferentes ao implantar vários <literal>daemonset</literal>
para cada arquitetura.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>Depois de aplicar o mapa de configuração e o <literal>daemonset</literal>, o
plug-in de dispositivo será implantado, e as interfaces serão descobertas e
estarão disponíveis para os pods.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Verifique as interfaces descobertas e disponíveis nos nós usados pelos pods:</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para>O <literal>FEC</literal> é <literal>intel.com/intel_fec_5g</literal> e o
valor é 1.</para>
</listitem>
<listitem>
<para>A <literal>VF</literal> é <literal>intel.com/intel_sriov_odu</literal> ou
<literal>intel.com/intel_sriov_oru</literal>, se você a implantar com um
plug-in de dispositivo e um mapa de configuração sem gráficos Helm.</para>
</listitem>
</itemizedlist>
<important>
<para>Se não há interfaces neste ponto, não faz muito sentido continuar porque a
interface não estará disponível para os pods. Revise o mapa de configuração
e os filtros para resolver o problema primeiro.</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">Opção 2 (recomendada) – Instalação usando o Rancher
com gráficos Helm para plug-ins de dispositivo CNI SR-IOV</emphasis></para>
<itemizedlist>
<listitem>
<para>Obtenha o Helm se não estiver presente:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>Instale o SR-IOV.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">helm install sriov-crd oci://registry.suse.com/edge/charts/sriov-crd -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/charts/sriov-network-operator -n sriov-network-operator</screen>
<itemizedlist>
<listitem>
<para>Verifique os recursos crd e os pods implantados:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>Verifique o rótulo nos nós.</para>
</listitem>
</itemizedlist>
<para>Com todos os recursos em execução, o rótulo aparecerá automaticamente em seu
nó:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para>Revise o <literal>daemonset</literal> para ver os novos
<literal>sriov-network-config-daemon</literal> e
<literal>sriov-rancher-nfd-worker</literal> já ativos e prontos:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>Em poucos minutos (a atualização pode levar até 10 minutos), os nós são
detectados e configurados com os recursos do <literal>SR-IOV</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>Verifique as interfaces detectadas.</para>
</listitem>
</itemizedlist>
<para>As interfaces descobertas devem ser o endereço PCI do dispositivo de
rede. Verifique essa informação com o comando <literal>lspci</literal> no
host.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>Se a sua interface não foi detectada neste momento, verifique se ela está
presente no próximo mapa de configuração:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>Se o seu dispositivo não estiver presente lá, edite o mapa de configuração
adicionando os valores corretos a serem descobertos (deve ser necessário
reiniciar o daemonset <literal>sriov-network-config-daemon</literal>).</para>
</note>
<itemizedlist>
<listitem>
<para>Crie a <literal>política NetworkNode</literal> para configurar as
<literal>VFs</literal>.</para>
</listitem>
</itemizedlist>
<para>Serão criadas algumas <literal>VFs</literal> (<literal>numVfs</literal>) do
dispositivo (<literal>rootDevices</literal>), e ela será configurada com o
<literal>deviceType</literal> do driver e a <literal>MTU</literal>:</para>
<note>
<para>O campo <literal>resourceName</literal> não deve conter caracteres especiais
e deve ser exclusivo em todo o cluster. O exemplo usa <literal>deviceType:
vfio-pci</literal> porque <literal>dpdk</literal> será usado em conjunto com
<literal>sr-iov</literal>. Se você não usar <literal>dpdk</literal>, o
deviceType deverá ser <literal>deviceType: netdevice</literal> (valor
padrão).</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>Valide as configurações:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>Crie a rede sr-iov (opcional, caso seja necessária uma rede diferente):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>Verifique a rede criada:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal> (Data Plane Development Kit) é um conjunto de
bibliotecas e drivers para processamento rápido de pacotes. Ele é usado para
acelerar as cargas de trabalho de processamento de pacotes executadas em uma
ampla variedade de arquiteturas de CPU. O DPDK inclui as bibliotecas de
plano de controle e os drivers otimizados de placa de interface de rede
(<literal>NIC</literal>) para o seguinte:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Um gerenciador de fila que implementa filas sem bloqueio.</para>
</listitem>
<listitem>
<para>Um gerenciador de buffer que pré-aloca buffers de tamanho fixo.</para>
</listitem>
<listitem>
<para>Um gerenciador de memória que aloca pools de objetos na memória e usa um
anel para armazenar objetos livres; garante que os objetos sejam igualmente
distribuídos por todos os canais <literal>DRAM</literal>.</para>
</listitem>
<listitem>
<para>Drivers de modo de sondagem (<literal>PMD</literal>) desenvolvidos para
operar sem notificações assíncronas, reduzindo a sobrecarga.</para>
</listitem>
<listitem>
<para>Uma estrutura de pacotes como um conjunto de bibliotecas auxiliares para
desenvolver o processamento de pacotes.</para>
</listitem>
</orderedlist>
<para>As seguintes etapas mostram como habilitar o <literal>DPDK</literal> e criar
<literal>VFs</literal> das <literal>NICs</literal> usadas pelas interfaces
do <literal>DPDK</literal>:</para>
<itemizedlist>
<listitem>
<para>Instale o pacote <literal>DPDK</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Parâmetros do kernel:</para>
</listitem>
</itemizedlist>
<para>Para usar o DPDK, aplique alguns drivers para habilitar determinados
parâmetros no kernel:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parâmetro</entry>
<entry align="left" valign="top">valor</entry>
<entry align="left" valign="top">descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Essa opção permite usar o driver <literal>vfio</literal> para as interfaces
do DPDK.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu ou amd_iommu</para></entry>
<entry align="left" valign="top"><para>em</para></entry>
<entry align="left" valign="top"><para>Essa opção permite usar o <literal>vfio</literal> para
<literal>VFs</literal>.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Para habilitar os parâmetros, adicione-os ao arquivo
<literal>/etc/default/grub</literal>:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Atualize a configuração do GRUB e reinicialize o sistema para aplicar as
alterações:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Carregue o módulo do kernel <literal>vfio-pci</literal> e habilite o
<literal>SR-IOV</literal> nas <literal>NICs</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Crie algumas funções virtuais (<literal>VFs</literal>) das
<literal>NICs</literal>.</para>
</listitem>
</itemizedlist>
<para>Por exemplo, para criar <literal>VFs</literal> para duas
<literal>NICs</literal> diferentes, os seguintes comandos são necessários:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>Vincule as novas VFs ao driver <literal>vfio-pci</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>Verifique se a configuração foi aplicada corretamente:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>Aceleração vRAN (<literal>Intel ACC100/ACC200</literal>)</title>
<para>À medida que os provedores de serviços de comunicação migram da rede 4G para
5G, muitos deles estão adotando as arquiteturas de rede de acesso por rádio
virtualizada (<literal>vRAN</literal>) para maior capacidade dos canais e
implantação mais fácil dos serviços e aplicativos de borda. As soluções vRAN
estão no local ideal para oferecer serviços de baixa latência com
flexibilidade para aumentar ou reduzir a capacidade de acordo com o volume
do tráfego e da demanda em tempo real na rede.</para>
<para>Uma das cargas de trabalho 4G e 5G com uso mais intenso de recursos é a
<literal>FEC</literal> da RAN de camada 1 (<literal>L1</literal>), que
resolve os erros de transmissão de dados por meio de canais de comunicação
incertos ou ruidosos. A tecnologia <literal>FEC</literal> detecta e corrige
um número limitado de erros em dados 4G ou 5G, eliminando a necessidade de
retransmissão. Como a transação de aceleração da <literal>FEC</literal> não
contém informações de estado da célula, é possível virtualizá-la com
facilidade para aproveitar os benefícios dos agrupamentos e facilitar a
migração de células.</para>
<itemizedlist>
<listitem>
<para>Parâmetros do kernel</para>
</listitem>
</itemizedlist>
<para>Para habilitar a aceleração da <literal>vRAN</literal>, precisamos habilitar
os seguintes parâmetros do kernel (se ainda não estiverem presentes):</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parâmetro</entry>
<entry align="left" valign="top">valor</entry>
<entry align="left" valign="top">descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Essa opção permite usar vfio paras as interfaces do DPDK.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu ou amd_iommu</para></entry>
<entry align="left" valign="top"><para>em</para></entry>
<entry align="left" valign="top"><para>Essa opção permite usar vfio para VFs.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modifique o arquivo GRUB <literal>/etc/default/grub</literal> para
adicioná-los à linha de comando do kernel:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Atualize a configuração do GRUB e reinicialize o sistema para aplicar as
alterações:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>Para verificar se os parâmetros foram aplicados após a reinicialização,
consulte a linha de comando:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Carregue os módulos do kernel vfio-pci para habilitar a aceleração da
<literal>vRAN</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Obtenha as informações da interface Acc100:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>Vincule a interface física (<literal>PF</literal>) ao driver
<literal>vfio-pci</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>Crie as funções virtuais (<literal>VFs</literal>) da interface física
(<literal>PF</literal>).</para>
</listitem>
</itemizedlist>
<para>Crie 2 <literal>VFs</literal> da <literal>PF</literal> e vincule a elas o
<literal>vfio-pci</literal> seguindo estas etapas:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>Configure a acc100 com o arquivo de configuração proposto:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>Verifique as novas VFs criadas da PF FEC:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>HugePages</title>
<para>Quando um processo usa <literal>RAM</literal>, a <literal>CPU</literal> a
marca como usada por esse processo. Para manter a eficiência, a
<literal>CPU</literal> aloca a <literal>RAM</literal> em blocos de
<literal>4K</literal> bytes, que é o valor padrão em muitas
plataformas. Esses blocos são chamados de páginas. As páginas podem ser
substituídas por discos, entre outros.</para>
<para>Como o espaço do endereço do processo é virtual, a <literal>CPU</literal> e
o sistema operacional precisam memorizar quais páginas pertencem a qual
processo, e onde cada página é armazenada. Quanto maior o número de páginas,
mais longa a pesquisa de mapeamento de memória. Quando um processo usa
<literal>1 GB</literal> de memória, isso equivale a 262144 entradas para
pesquisa (<literal>1 GB</literal>/<literal>4 K</literal>). Se uma entrada da
tabela de páginas consome 8 bytes, isso equivale a <literal>2 MB</literal>
(262144 * 8) para pesquisa.</para>
<para>As arquiteturas de <literal>CPU</literal> mais atuais oferecem suporte às
páginas maiores que o padrão, o que reduz o número de entradas para a
<literal>CPU/SO</literal> pesquisar.</para>
<itemizedlist>
<listitem>
<para>Parâmetros do kernel</para>
</listitem>
</itemizedlist>
<para>Para habilitar o HugePages, devemos adicionar os seguintes parâmetros do
kernel. Neste exemplo, configuramos 40 páginas de 1G, portanto, o tamanho e
o número exato de páginas enormes devem ser adaptados aos requisitos de
memória do seu aplicativo:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parâmetro</entry>
<entry align="left" valign="top">valor</entry>
<entry align="left" valign="top">descrição</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Essa opção permite definir o tamanho das páginas enormes como 1 G</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>Esse é o número de páginas enormes definido antes</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Esse é o valor padrão para obter as páginas enormes</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modifique o arquivo GRUB <literal>/etc/default/grub</literal> para adicionar
esses parâmetros a <literal>GRUB_CMDLINE_LINUX</literal>:</para>
<screen language="shell" linenumbering="unnumbered">default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0</screen>
<para>Atualize a configuração do GRUB e reinicialize o sistema para aplicar as
alterações:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>Para validar se os parâmetros foram aplicados após a reinicialização,
verifique a linha de comando:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Usando o HugePages</para>
</listitem>
</itemizedlist>
<para>Para usar o HugePages, precisamos montá-lo:</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Implante a carga de trabalho Kubernetes criando os recursos e os volumes:</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-kubernetes">
<title>Fixação da CPU em Kubernetes</title>
<section xml:id="id-prerequisite">
<title>Pré-requisito</title>
<para>A <literal>CPU</literal> deve estar ajustada de acordo com o perfil de
desempenho abordado nesta seção (<xref linkend="cpu-tuned-configuration"/>).</para>
</section>
<section xml:id="id-configure-kubernetes-for-cpu-pinning">
<title>Configurar o Kubernetes para fixação da CPU</title>
<para>Configure os argumentos de kubelet para implementar o gerenciamento da CPU
no cluster <literal>RKE2</literal>. Adicione o seguinte bloco de
configuração, como no exemplo abaixo, ao arquivo
<literal>/etc/rancher/rke2/config.yaml</literal>. Especifique os núcleos da
CPU de manutenção nos argumentos <literal>kubelet-reserved</literal> e
<literal>system-reserved</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=0,31,32,63"
- "system-reserved=cpu=0,31,32,63"</screen>
</section>
<section xml:id="id-leveraging-pinned-cpus-for-workloads">
<title>Aproveitando as CPUs fixadas para as cargas de trabalho</title>
<para>Há três maneiras de usar este recurso com a <literal>política
estática</literal> definida no kubelet, dependendo das solicitações e dos
limites definidos em sua carga de trabalho:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Classe de QoS <literal>BestEffort</literal>: se você não definir uma
solicitação ou um limite para a <literal>CPU</literal>, o pod será
programado na primeira <literal>CPU</literal> disponível no sistema.</para>
<para>Um exemplo de uso da classe de QoS <literal>BestEffort</literal> é:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para>Classe de QoS <literal>Burstable</literal>: se você definir uma solicitação
de CPU que não é igual aos limites, ou se não houver solicitações de CPU.</para>
<para>Alguns exemplos de uso da classe de QoS <literal>Burstable</literal> são:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>ou</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para>Classe de QoS <literal>Guaranteed</literal>: se você definir uma solicitação
de CPU igual aos limites.</para>
<para>Um exemplo de uso da classe de QoS <literal>Guaranteed</literal> é:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="numa-aware-scheduling">
<title>Programação com reconhecimento de NUMA</title>
<para>Acesso não uniforme à memória ou arquitetura não uniforme de acesso à
memória (<literal>NUMA</literal>, Non-Uniform Memory Access ou Non-Uniform
Memory Architecture) é um projeto de memória física usado na arquitetura
<literal>SMP</literal> (multiprocessadores), em que o tempo de acesso à
memória depende do local da memória relativo ao processador. No
<literal>NUMA</literal>, um processador pode acessar a própria memória local
com mais rapidez do que a memória não local, ou seja, a memória local de
outro processador ou a memória compartilhada entre processadores.</para>
<section xml:id="id-identifying-numa-nodes">
<title>Identificando os nós NUMA</title>
<para>Para identificar os nós <literal>NUMA</literal>, execute o seguinte comando
em seu sistema:</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>Para este exemplo, temos apenas um nó <literal>NUMA</literal> com 64
<literal>CPUs</literal>.</para>
<para>É necessário habilitar o <literal>NUMA</literal> no
<literal>BIOS</literal>. Se o <literal>dmesg</literal> não tem registros de
inicialização do NUMA durante o bootup, as mensagens relacionadas ao
<literal>NUMA</literal> no buffer de anel do kernel podem ter sido
substituídas.</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>MetalLB</title>
<para><literal>MetalLB</literal> é uma implementação de balanceador de carga para
clusters Kubernetes bare metal, que usa os protocolos de roteamento padrão,
como <literal>L2</literal> e <literal>BGP</literal>, como protocolos de
anúncio. Trata-se de um balanceador de carga de rede que pode ser usado para
expor serviços em um cluster Kubernetes ao ambiente externo por causa da
necessidade de usar serviços do Kubernetes do tipo
<literal>LoadBalancer</literal> com bare metal.</para>
<para>Para habilitar o <literal>MetalLB</literal> no cluster
<literal>RKE2</literal>, são necessárias as seguintes etapas:</para>
<itemizedlist>
<listitem>
<para>Instale o <literal>MetalLB</literal> usando o seguinte comando:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/metallb
  targetNamespace: metallb-system
  version: 304.0.0+up0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
  version: 304.0.1+up0.3.0
  createNamespace: true
EOF</screen>
<itemizedlist>
<listitem>
<para>Crie a configuração de <literal>IpAddressPool</literal> e
<literal>L2advertisement</literal>:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para>Crie o serviço de endpoint para expor o <literal>VIP</literal>:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para>Verifique se o <literal>VIP</literal> foi criado e se os pods do
<literal>MetalLB</literal> estão em execução:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>Configuração do registro particular</title>
<para>É possível configurar o <literal>Containerd</literal> para conexão com os
registros particulares e usá-los para extrair as imagens particulares de
cada nó.</para>
<para>Na inicialização, o <literal>RKE2</literal> verifica se existe um arquivo
<literal>registries.yaml</literal> em <literal>/etc/rancher/rke2/</literal>
e instrui o <literal>containerd</literal> a usar os registros definidos no
arquivo. Para usar um registro particular, crie esse arquivo como raiz em
cada nó que usará o registro.</para>
<para>Para adicionar o registro particular, crie o arquivo
<literal>/etc/rancher/rke2/registries.yaml</literal> com o seguinte
conteúdo:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>ou sem autenticação:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>Para que as alterações no registro entrem em vigor, você precisa configurar
esse arquivo antes de iniciar o RKE2 no nó ou reiniciar o RKE2 em cada nó
configurado.</para>
<note>
<para>Para obter mais informações sobre isso, acesse a <link
xl:href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html#_registries_configuration_file">configuração
de registro containerd para RKE2</link>.</para>
</note>
</section>
<section xml:id="ptp-configuration">
<title>Precision Time Protocol</title>
<para>Precision Time Protocol (PTP) é um protocolo de rede desenvolvido pelo
Institute of Electrical and Electronics Engineers (IEEE) para permitir a
sincronização de tempo em submicrossegundos em uma rede de
computadores. Desde a sua origem e durante as duas últimas décadas, o PTP
tem sido usado em diversos setores. Recentemente, observamos uma crescente
adoção nas redes de telecomunicações como elemento essencial a redes
5G. Apesar de ser um protocolo relativamente simples, sua configuração pode
mudar bastante de acordo com o aplicativo. Por essa razão, foram definidos e
padronizados vários perfis.</para>
<para>Nesta seção, apenas os perfis específicos de telecomunicações serão
apresentados. Portanto, vamos considerar o recurso de marcação de data e
hora e um relógio de hardware PTP (PHC, PTP Hardware Clock) na NIC. Hoje em
dia, todos os adaptadores de rede para telecomunicações contam com suporte a
PTP no hardware, mas você pode verificar esses recursos com o seguinte
comando:</para>
<screen language="console" linenumbering="unnumbered"># ethtool -T p1p1
Time stamping parameters for p1p1:
Capabilities:
        hardware-transmit
        software-transmit
        hardware-receive
        software-receive
        software-system-clock
        hardware-raw-clock
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
        off
        on
Hardware Receive Filter Modes:
        none
        all</screen>
<para>Substitua <literal>p1p1</literal> pelo nome da interface usada para PTP.</para>
<para>As seções a seguir orientam na instalação e na configuração do PTP
especificamente no SUSE Telco Cloud, mas é esperada alguma familiaridade com
os conceitos básicos do PTP. Para uma breve visão geral do PTP e da
implementação incluída no SUSE Telco Cloud, consulte <link
xl:href="https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html">https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html</link>.</para>
<section xml:id="id-install-ptp-software-components">
<title>Instalar os componentes de software PTP</title>
<para>No SUSE Telco Cloud, a implementação do PTP é fornecida pelo pacote
<literal>linuxptp</literal>, que inclui dois componentes:</para>
<itemizedlist>
<listitem>
<para><literal>ptp4l</literal>: um daemon que controla o PHC na NIC e executa o
protocolo PTP</para>
</listitem>
<listitem>
<para><literal>phc2sys</literal>: um daemon que mantém a sincronização do relógio
do sistema com o PHC sincronizado por PTP na NIC</para>
</listitem>
</itemizedlist>
<para>Os dois daemons são necessários para que a sincronização do sistema funcione
por completo e devem ser definidos de maneira correta com a sua
configuração, o que foi abordado na <xref linkend="ptp-telco-config"/>.</para>
<para>A maneira melhor e mais fácil de integrar o PTP ao cluster downstream é
adicionar o pacote <literal>linuxptp</literal> em
<literal>packageList</literal> ao arquivo de definição do Edge Image Builder
(EIB). Desse modo, o software de plano de controle PTP será instalado
automaticamente durante o provisionamento do cluster. Consulte a
documentação do EIB (<xref linkend="eib-configuring-rpm-packages"/>) para
obter mais informações sobre como instalar os pacotes.</para>
<para>Veja a seguir um manifesto do EIB de amostra com
<literal>linuxptp</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: {micro-base-rt-image-raw}
  outputImageName: eibimage-slmicrort-telco.raw
operatingSystem:
  time:
    timezone: America/New_York
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
    enable:
      - ptp4l
      - phc2sys
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
      - open-iscsi
      - tuned
      - cpupower
      - linuxptp
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<note>
<para>Por padrão, o pacote <literal>linuxptp</literal> incluído no SUSE Telco
Cloud não habilita o <literal>ptp4l</literal> e o
<literal>phc2sys</literal>. Se os arquivos de configuração específicos do
sistema forem implantados junto com o provisionamento (consulte a <xref
linkend="ptp-capi"/>), eles deverão estar habilitados. Para isso,
adicione-os à seção <literal>systemd</literal> do manifesto, como no exemplo
acima.</para>
</note>
<para>Siga o processo normal para criar a imagem conforme descrito na documentação
do EIB (<xref linkend="eib-how-to-build-image"/>) e use-a para implantar o
cluster. Se você não tem experiência com o EIB, comece pelo <xref
linkend="components-eib"/>.</para>
</section>
<section xml:id="ptp-telco-config">
<title>Configurar o PTP para implantações de telecomunicações</title>
<para>Muitos aplicativos de telecomunicações exigem uma sincronização rígida de
fase e de tempo com pouca variação, o que resultou na definição de dois
perfis orientados a telecomunicações: ITU-T G.8275.1 e ITU-T G.8275.2. Os
dois têm alta taxa de mensagens de sincronização e outros aspectos
diferenciados, como uso de um algoritmo BMCA (Best Master Clock Algorithm)
alternativo. Esse comportamento exige definições específicas no arquivo de
configuração consumido pelo <literal>ptp4l</literal>, apresentadas nas
seções a seguir como referência.</para>
<note>
<itemizedlist>
<listitem>
<para>As duas seções abordam apenas o caso de um relógio comum na configuração de
receptor de tempo.</para>
</listitem>
<listitem>
<para>Esse tipo de perfil deve ser usado em uma infraestrutura PTP bem planejada.</para>
</listitem>
<listitem>
<para>Sua rede PTP específica pode exigir um ajuste de configuração
adicional. Revise os exemplos apresentados e adapte-os se necessário.</para>
</listitem>
</itemizedlist>
</note>
<section xml:id="id-ptp-profile-itu-t-g-8275-1">
<title>Perfil de PTP ITU-T G.8275.1</title>
<para>O perfil G.8275.1 tem as seguintes especificações:</para>
<itemizedlist>
<listitem>
<para>Executado diretamente em Ethernet e requer suporte completo à rede
(nós/comutadores adjacentes devem dar suporte a PTP).</para>
</listitem>
<listitem>
<para>A configuração de domínio padrão é 24.</para>
</listitem>
<listitem>
<para>A comparação de conjunto de dados baseia-se no algoritmo G.8275.x e nos
valores <literal>localPriority</literal> depois de
<literal>priority2</literal>.</para>
</listitem>
</itemizedlist>
<para>Copie o seguinte conteúdo para um arquivo chamado
<literal>/etc/ptp4l-G.8275.1.conf</literal>:</para>
<screen linenumbering="unnumbered"># Telecom G.8275.1 example configuration
[global]
domainNumber                    24
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             -3
logSyncInterval                 -4
logMinDelayReqInterval          -4
announceReceiptTimeout          3
serverOnly                      0
ptp_dst_mac                     01:80:C2:00:00:0E
network_transport               L2</screen>
<para>Após a criação do arquivo, faça referência a ele em
<literal>/etc/sysconfig/ptp4l</literal> para o daemon ser iniciado
corretamente. Para fazer isso, altere a linha <literal>OPTIONS=</literal>:</para>
<screen linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"</screen>
<para>Mais precisamente:</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> requer o nome do arquivo de configuração que será
usado; neste caso, <literal>/etc/ptp4l-G.8275.1.conf</literal>.</para>
</listitem>
<listitem>
<para><literal>-i</literal> requer o nome da interface que será usada. Substitua
<literal>$IFNAME</literal> pelo nome da interface real.</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> permite identificar melhor a saída de ptp4l
nos registros do sistema e é opcional.</para>
</listitem>
</itemizedlist>
<para>Após a conclusão das etapas acima, o daemon <literal>ptp4l</literal> deverá
ser (re)iniciado:</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>Verifique o status da sincronização observando os registros com:</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-ptp-profile-itu-t-g-8275-2">
<title>Perfil de PTP ITU-T G.8275.2</title>
<para>O perfil G.8275.2 tem as seguintes especificações:</para>
<itemizedlist>
<listitem>
<para>Executado em IP e não requer suporte total à rede (nós/comutadores
adjacentes podem não dar suporte a PTP).</para>
</listitem>
<listitem>
<para>A configuração de domínio padrão é 44.</para>
</listitem>
<listitem>
<para>A comparação de conjunto de dados baseia-se no algoritmo G.8275.x e nos
valores <literal>localPriority</literal> depois de
<literal>priority2</literal>.</para>
</listitem>
</itemizedlist>
<para>Copie o seguinte conteúdo para o arquivo chamado
<literal>/etc/ptp4l-G.8275.2.conf</literal>:</para>
<screen linenumbering="unnumbered"># Telecom G.8275.2 example configuration
[global]
domainNumber                    44
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             0
serverOnly                      0
hybrid_e2e                      1
inhibit_multicast_service       1
unicast_listen                  1
unicast_req_duration            60
logSyncInterval                 -5
logMinDelayReqInterval          -4
announceReceiptTimeout          2
#
# Customize the following for slave operation:
#
[unicast_master_table]
table_id                        1
logQueryInterval                2
UDPv4                           $PEER_IP_ADDRESS
[$IFNAME]
unicast_master_table            1</screen>
<para>Substitua os seguintes espaços reservados:</para>
<itemizedlist>
<listitem>
<para><literal>$PEER_IP_ADDRESS</literal>: o endereço IP do nó PTP seguinte com o
qual se comunicar, como o relógio mestre ou de limite que fornecerá a
sincronização.</para>
</listitem>
<listitem>
<para><literal>$IFNAME</literal>: instrui o <literal>ptp4l</literal> sobre qual
interface usar para PTP.</para>
</listitem>
</itemizedlist>
<para>Após a criação do arquivo, faça referência a ele, junto com o nome da
interface para PTP, em <literal>/etc/sysconfig/ptp4l</literal> para que o
daemon seja iniciado corretamente. Para fazer isso, altere a linha
<literal>OPTIONS=</literal> para:</para>
<screen language="shell" linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.2.conf --message_tag ptp-8275.2"</screen>
<para>Mais precisamente:</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> requer o nome do arquivo de configuração que será
usado; neste caso, <literal>/etc/ptp4l-G.8275.2.conf</literal>.</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> permite identificar melhor a saída de ptp4l
nos registros do sistema e é opcional.</para>
</listitem>
</itemizedlist>
<para>Após a conclusão das etapas acima, o daemon <literal>ptp4l</literal> deverá
ser (re)iniciado:</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>Verifique o status da sincronização observando os registros com:</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-configuration-of-phc2sys">
<title>Configuração do phc2sys</title>
<para>Embora não seja obrigatório, é recomendado completar toda a configuração de
<literal>ptp4l</literal> antes de passar para o
<literal>phc2sys</literal>. O <literal>phc2sys</literal> não requer um
arquivo de configuração, e seus parâmetros de execução podem ser controlados
unicamente pela variável <literal>OPTIONS=</literal> presente em
<literal>/etc/sysconfig/ptp4l</literal>, de maneira similar a
<literal>ptp4l</literal>:</para>
<screen linenumbering="unnumbered">OPTIONS="-s $IFNAME -w"</screen>
<para>Em que <literal>$IFNAME</literal> é o nome da interface já configurada em
ptp4l, que será usada como fonte para o relógio do sistema. Isso é usado
para identificar o PHC de origem.</para>
</section>
</section>
<section xml:id="ptp-capi">
<title>Integração da Cluster API</title>
<para>Sempre que um cluster é implantado por meio de um cluster de gerenciamento e
do provisionamento de rede direto, tanto o arquivo de configuração quanto as
duas variáveis de configuração em <literal>/etc/sysconfig</literal> podem
ser implantados no host junto com o provisionamento. Veja abaixo um trecho
da definição de um cluster, com foco no objeto
<literal>RKE2ControlPlane</literal> modificado que implanta o mesmo arquivo
de configuração G.8275.1 em todos os hosts:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  serverConfig:
    cni: canal
  agentConfig:
    format: ignition
    cisProfile: cis
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /etc/ptp4l-G.8275.1.conf
              overwrite: true
              contents:
                inline: |
                  # Telecom G.8275.1 example configuration
                  [global]
                  domainNumber                    24
                  priority2                       255
                  dataset_comparison              G.8275.x
                  G.8275.portDS.localPriority     128
                  G.8275.defaultDS.localPriority  128
                  maxStepsRemoved                 255
                  logAnnounceInterval             -3
                  logSyncInterval                 -4
                  logMinDelayReqInterval          -4
                  announceReceiptTimeout          3
                  serverOnly                      0
                  ptp_dst_mac                     01:80:C2:00:00:0E
                  network_transport               L2
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/ptp4l
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): ptp4l settings
                  ## Type:           string
                  ## Default:        "-i eth0 -f /etc/ptp4l.conf"
                  ## ServiceRestart: ptp4l
                  #
                  # Arguments when starting ptp4l(8).
                  #
                  OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/phc2sys
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): phc2sys settings
                  ## Type:           string
                  ## Default:        "-s eth0 -w"
                  ## ServiceRestart: phc2sys
                  #
                  # Arguments when starting phc2sys(8).
                  #
                  OPTIONS="-s $IFNAME -w"
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Além de outras variáveis, é necessário preencher a definição acima com o
nome da interface e os outros objetos da Cluster API, conforme descrito no
<xref linkend="atip-automated-provisioning"/>.</para>
<note>
<itemizedlist>
<listitem>
<para>Essa é uma abordagem prática apenas quando o hardware no cluster é uniforme
e a mesma configuração é necessária em todos os hosts, inclusive o nome da
interface.</para>
</listitem>
<listitem>
<para>Há outras abordagens possíveis que serão explicadas em versões futuras.</para>
</listitem>
</itemizedlist>
</note>
<para>Neste ponto, os hosts devem ter uma pilha PTP em funcionamento e começarão a
negociar sua função de PTP.</para>
</section>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>Provisionamento de rede direcionado totalmente automatizado</title>
<section xml:id="id-introduction-3">
<title>Introdução</title>
<para>O provisionamento de rede direcionado é um recurso que permite automatizar o
provisionamento de clusters downstream. Ele é útil quando você tem que
provisionar muitos clusters downstream e deseja automatizar o processo.</para>
<para>Um cluster de gerenciamento (<xref linkend="atip-management-cluster"/>)
automatiza a implantação dos seguintes componentes:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro RT</literal> como sistema operacional. Dependendo
do caso de uso, é possível personalizar configurações como rede,
armazenamento, usuários e argumentos do kernel.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> como cluster Kubernetes. O plug-in de
<literal>CNI</literal> padrão é <literal>Cilium</literal>. Dependendo do
caso de uso, é possível usar determinados plug-ins de
<literal>CNI</literal>, como <literal>Cilium+Multus</literal>.</para>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal></para>
</listitem>
<listitem>
<para><literal>SUSE Security</literal></para>
</listitem>
<listitem>
<para>É possível usar o <literal>MetalLB</literal> como balanceador de carga para
clusters de vários nós altamente disponíveis.</para>
</listitem>
</itemizedlist>
<note>
<para>Para obter mais informações sobre o <literal>SUSE Linux Micro</literal>,
consulte o <xref linkend="components-slmicro"/>. Para obter mais informações
sobre o <literal>RKE2</literal>, consulte o <xref
linkend="components-rke2"/>. Para obter mais informações sobre o
<literal>SUSE Storage</literal>, consulte o <xref
linkend="components-suse-storage"/>. Para obter mais informações sobre o
<literal>SUSE Security</literal>, consulte o <xref
linkend="components-suse-security"/>.</para>
</note>
<para>As seguintes seções descrevem os diferentes fluxos de trabalho de
provisionamento de rede direcionado e alguns recursos adicionais que podem
ser incluídos no processo de provisionamento:</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
<note>
<para>As seções a seguir mostram como preparar os diversos cenários para o fluxo
de trabalho de provisionamento de rede direcionado usando o SUSE Telco
Cloud. Para ver exemplos de opções de configurações diferentes para
implantação (incluindo ambientes air-gapped, redes DHCP e sem DHCP,
registros de contêiner particulares etc.), consulte o <link
xl:href="https://github.com/suse-edge/atip/tree/release-3.3/telco-examples/edge-clusters">repositório
do SUSE Telco Cloud</link>.</para>
</note>
</section>
<section xml:id="eib-edge-image-connected">
<title>Preparar uma imagem de cluster downstream para cenários conectados</title>
<para>O Edge Image Builder (<xref linkend="components-eib"/>) é usado para
preparar uma imagem base do SLEMicro modificada que será provisionada aos
hosts de cluster downstream.</para>
<para>Grande parte da configuração é possível pelo Edge Image Builder, mas neste
guia, abordamos as configurações mínimas necessárias para preparar o cluster
downstream.</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>Pré-requisitos para cenários conectados</title>
<itemizedlist>
<listitem>
<para>É necessário um tempo de execução do contêiner, como <link
xl:href="https://podman.io">Podman</link> ou <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>, para executar o
Edge Image Builder.</para>
</listitem>
<listitem>
<para>A imagem base será criada conforme este guia <xref
linkend="guides-kiwi-builder-images"/> com o perfil
<literal>Base-SelfInstall</literal> (ou
<literal>Base-RT-SelfInstall</literal> para o kernel Real-Time). O processo
é o mesmo para as duas arquiteturas (x86-64 e aarch64).</para>
</listitem>
</itemizedlist>
<note>
<para>É necessário usar um host de build com a mesma arquitetura das imagens que
estão sendo criadas. Em outras palavras, para criar uma imagem
<literal>aarch64</literal>, é necessário usar um host de build
<literal>aarch64</literal>, e vice-versa para <literal>x86-64</literal> (não
há suporte para builds cruzados no momento).</para>
</note>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>Configuração da imagem para cenários conectados</title>
<para>Ao executar o Edge Image Builder, um diretório é montado com base no host,
portanto, é necessário criar uma estrutura de diretórios para armazenar os
arquivos de configuração usados para definir a imagem de destino.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> é o arquivo de definição
da imagem. Consulte o <xref linkend="quickstart-eib"/> para obter mais
detalhes.</para>
</listitem>
<listitem>
<para>A pasta da imagem base inclui a imagem bruta de saída gerada conforme o guia
<xref linkend="guides-kiwi-builder-images"/> com o perfil
<literal>Base-SelfInstall</literal> (ou
<literal>Base-RT-SelfInstall</literal> para o kernel Real-Time) e deve ser
copiada/movida para a pasta <literal>base-images</literal>.</para>
</listitem>
<listitem>
<para>A pasta <literal>network</literal> é opcional. Consulte a <xref
linkend="add-network-eib"/> para obter mais detalhes.</para>
</listitem>
<listitem>
<para>O diretório <literal>custom/scripts</literal> contém scripts para execução
na primeira inicialização:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O script <literal>01-fix-growfs.sh</literal> é necessário para redimensionar
a partição raiz do sistema operacional na implantação.</para>
</listitem>
<listitem>
<para>O script <literal>02-performance.sh</literal> é opcional e pode ser usado
para configurar o sistema para ajuste de desempenho.</para>
</listitem>
<listitem>
<para>O script <literal>03-sriov.sh</literal> é opcional e pode ser usado para
configurar o sistema para SR-IOV.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>O diretório <literal>custom/files</literal> contém os arquivos
<literal>performance-settings.sh</literal> e
<literal>sriov-auto-filler.sh</literal> para copiar na imagem durante o
processo de criação da imagem.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>Arquivo de definição da imagem do cluster downstream</title>
<para>O <literal>downstream-cluster-config.yaml</literal> é o arquivo de
configuração principal para a imagem do cluster downstream. Veja a seguir um
exemplo mínimo de implantação por meio do Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Em que <literal>$SCC_REGISTRATION_CODE</literal> é o código de registro
copiado do <link xl:href="https://scc.suse.com/">SUSE Customer
Center</link>, e a lista de pacotes inclui o <literal>jq</literal>, que é
obrigatório.</para>
<para><literal>$ROOT_PASSWORD</literal> é a senha criptografada do usuário root,
que pode ser útil para teste/depuração. É possível gerá-la com o comando
<literal>openssl passwd -6 PASSWORD</literal>.</para>
<para>Para os ambientes de produção, a recomendação é usar as chaves SSH que podem
ser adicionadas ao bloco de usuários substituindo a
<literal>$USERKEY1</literal> pelas chaves SSH reais.</para>
<note>
<para><literal>arch: x86_64</literal> é a arquitetura da imagem. Para arm64, use
<literal>arch: aarch64</literal>.</para>
<para>O <literal>net.ifnames=1</literal> habilita a <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Nomenclatura
de interface de rede previsível</link>.</para>
<para>Isso corresponde à configuração padrão para o gráfico metal3, mas a
configuração deve corresponder ao valor
<literal>predictableNicNames</literal> do gráfico configurado.</para>
<para>Veja também que o <literal>ignition.platform.id=openstack</literal> é
obrigatório. Sem esse argumento, a configuração do SLEMicro pelo ignition
vai falhar no fluxo automatizado do Metal<superscript>3</superscript>.</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Script growfs</title>
<para>Atualmente, é necessário um script personalizado
(<literal>custom/scripts/01-fix-growfs.sh</literal>) para expandir o sistema
de arquivos de acordo com o tamanho do disco na primeira inicialização após
o provisionamento. O script <literal>01-fix-growfs.sh</literal> contém as
seguintes informações:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="add-custom-script-performance">
<title>Script de desempenho</title>
<para>O seguinte script opcional
(<literal>custom/scripts/02-performance.sh</literal>) pode ser usado para
configurar o sistema para ajuste de desempenho:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para>O conteúdo do <literal>custom/files/performance-settings.sh</literal> é um
script que pode ser usado para configurar o sistema para ajuste de
desempenho e pode ser baixado acessando este <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">link</link>.</para>
</section>
<section xml:id="add-custom-script-sriov">
<title>Script SR-IOV</title>
<para>O seguinte script opcional (<literal>custom/scripts/03-sriov.sh</literal>)
pode ser usado para configurar o sistema para SR-IOV:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para>O conteúdo do <literal>custom/files/sriov-auto-filler.sh</literal> é um
script que pode ser usado para configurar o sistema para SR-IOV e pode ser
baixado acessando este <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">link</link>.</para>
<note>
<para>Adicione seus próprios scripts personalizados para execução durante o
processo de provisionamento usando a mesma abordagem. Para obter mais
informações, consulte o <xref linkend="quickstart-eib"/>.</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>Configuração adicional para cargas de trabalho de telecomunicações</title>
<para>Para habilitar recursos de telecomunicações, como <literal>dpdk</literal>,
<literal>sr-iov</literal> ou <literal>FEC</literal>, podem ser necessários
pacotes adicionais, conforme mostrado no exemplo a seguir.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $user1Key1
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Em que <literal>$SCC_REGISTRATION_CODE</literal> é o código de registro
copiado do <link xl:href="https://scc.suse.com/">SUSE Customer
Center</link>, e a lista de pacotes inclui os pacotes mínimos usados nos
perfis de telecomunicações.</para>
<note>
<para><literal>arch: x86_64</literal> é a arquitetura da imagem. Para arm64, use
<literal>arch: aarch64</literal>.</para>
</note>
</section>
<section xml:id="add-network-eib">
<title>Script adicional para configuração de rede avançada</title>
<para>Se você precisa configurar IPs estáticos ou cenários de rede mais avançados,
conforme descrito na <xref linkend="advanced-network-configuration"/>, a
configuração adicional abaixo é necessária.</para>
<para>Na pasta <literal>network</literal>, crie o seguinte arquivo
<literal>configure-network.sh</literal>, que consome os dados da unidade de
configuração na primeira inicialização e configura a rede do host usando a
<link xl:href="https://github.com/suse-edge/nm-configurator">ferramenta NM
Configurator</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>Criação de imagem</title>
<para>Depois que a estrutura de diretórios for preparada de acordo com as seções
anteriores, execute o seguinte comando para criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>Esse procedimento cria o arquivo da imagem ISO de saída chamado
<literal>eibimage-output-telco.raw</literal>, com base na definição descrita
acima.</para>
<para>Em seguida, disponibilize a imagem de saída por um servidor web, ou o
contêiner de servidor de mídia habilitado de acordo com as instruções na
documentação sobre o cluster de gerenciamento (<xref
linkend="metal3-media-server"/>) ou algum outro servidor acessível
localmente. Nos exemplos abaixo, esse servidor é mencionado como
<literal>imagecache.local:8080</literal>.</para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>Preparar uma imagem de cluster downstream para cenários air-gapped</title>
<para>O Edge Image Builder (<xref linkend="components-eib"/>) é usado para
preparar uma imagem base do SLEMicro modificada que será provisionada aos
hosts de cluster downstream.</para>
<para>Grande parte da configuração é possível com o Edge Image Builder, mas neste
guia, abordamos as configurações mínimas necessárias para preparar o cluster
downstream em cenários air-gapped.</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>Pré-requisitos para cenários air-gapped</title>
<itemizedlist>
<listitem>
<para>É necessário um tempo de execução do contêiner, como <link
xl:href="https://podman.io">Podman</link> ou <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>, para executar o
Edge Image Builder.</para>
</listitem>
<listitem>
<para>A imagem base será criada conforme este guia <xref
linkend="guides-kiwi-builder-images"/> com o perfil
<literal>Base-SelfInstall</literal> (ou
<literal>Base-RT-SelfInstall</literal> para o kernel Real-Time). O processo
é o mesmo para as duas arquiteturas (x86-64 e aarch64).</para>
</listitem>
<listitem>
<para>Para usar SR-IOV ou qualquer outra carga de trabalho que exija uma imagem de
contêiner, é necessário implantar e já configurar um registro particular
local (com/sem TLS e/ou autenticação). Esse registro será usado para
armazenar as imagens comuns e as imagens OCI de gráfico Helm.</para>
</listitem>
</itemizedlist>
<note>
<para>É necessário usar um host de build com a mesma arquitetura das imagens que
estão sendo criadas. Em outras palavras, para criar uma imagem
<literal>aarch64</literal>, é necessário usar um host de build
<literal>aarch64</literal>, e vice-versa para <literal>x86-64</literal> (não
há suporte para builds cruzados no momento).</para>
</note>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>Configuração da imagem para cenários air-gapped</title>
<para>Ao executar o Edge Image Builder, um diretório é montado com base no host,
portanto, é necessário criar uma estrutura de diretórios para armazenar os
arquivos de configuração usados para definir a imagem de destino.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal> é o arquivo de
definição da imagem, consulte o <xref linkend="quickstart-eib"/> para obter
mais detalhes.</para>
</listitem>
<listitem>
<para>A pasta da imagem base inclui a imagem bruta de saída gerada conforme o guia
<xref linkend="guides-kiwi-builder-images"/> com o perfil
<literal>Base-SelfInstall</literal> (ou
<literal>Base-RT-SelfInstall</literal> para o kernel Real-Time) e deve ser
copiada/movida para a pasta <literal>base-images</literal>.</para>
</listitem>
<listitem>
<para>A pasta <literal>network</literal> é opcional. Consulte a <xref
linkend="add-network-eib"/> para obter mais detalhes.</para>
</listitem>
<listitem>
<para>O diretório <literal>custom/scripts</literal> contém scripts para execução
na primeira inicialização:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O script <literal>01-fix-growfs.sh</literal> é necessário para redimensionar
a partição raiz do sistema operacional na implantação.</para>
</listitem>
<listitem>
<para>O script <literal>02-airgap.sh</literal> é necessário para copiar as imagens
no local certo durante o processo de criação da imagem para ambientes
air-gapped.</para>
</listitem>
<listitem>
<para>O script <literal>03-performance.sh</literal> é opcional e pode ser usado
para configurar o sistema para ajuste de desempenho.</para>
</listitem>
<listitem>
<para>O script <literal>04-sriov.sh</literal> é opcional e pode ser usado para
configurar o sistema para SR-IOV.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>O diretório <literal>custom/files</literal> contém as imagens
<literal>rke2</literal> e <literal>cni</literal> para copiar na imagem
durante o processo de criação da imagem. É possível também incluir os
arquivos opcionais <literal>performance-settings.sh</literal> e
<literal>sriov-auto-filler.sh</literal>.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>Arquivo de definição da imagem do cluster downstream</title>
<para>O <literal>downstream-cluster-airgap-config.yaml</literal> é o principal
arquivo de configuração da imagem do cluster downstream, e o conteúdo foi
descrito na seção anterior (<xref linkend="add-telco-feature-eib"/>).</para>
</section>
<section xml:id="id-growfs-script">
<title>Script growfs</title>
<para>Atualmente, é necessário um script personalizado
(<literal>custom/scripts/01-fix-growfs.sh</literal>) para expandir o sistema
de arquivos de acordo com o tamanho do disco na primeira inicialização após
o provisionamento. O script <literal>01-fix-growfs.sh</literal> contém as
seguintes informações:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>Script air-gap</title>
<para>O seguinte script (<literal>custom/scripts/02-airgap.sh</literal>) é
necessário para copiar as imagens no local certo durante o processo de
criação da imagem:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="add-custom-script-performance2">
<title>Script de desempenho</title>
<para>O seguinte script opcional
(<literal>custom/scripts/03-performance.sh</literal>) pode ser usado para
configurar o sistema para ajuste de desempenho:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para>O conteúdo do <literal>custom/files/performance-settings.sh</literal> é um
script que pode ser usado para configurar o sistema para ajuste de
desempenho e pode ser baixado acessando este <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">link</link>.</para>
</section>
<section xml:id="add-custom-script-sriov2">
<title>Script SR-IOV</title>
<para>O seguinte script opcional (<literal>custom/scripts/04-sriov.sh</literal>)
pode ser usado para configurar o sistema para SR-IOV:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para>O conteúdo do <literal>custom/files/sriov-auto-filler.sh</literal> é um
script que pode ser usado para configurar o sistema para SR-IOV e pode ser
baixado acessando este <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">link</link>.</para>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>Arquivos personalizados para cenários air-gapped</title>
<para>O diretório <literal>custom/files</literal> contém as imagens do
<literal>rke2</literal> e <literal>cni</literal> que devem ser copiadas para
a imagem durante o processo de criação dela. Para gerar as imagens com
facilidade, prepare-as no local usando o seguinte link <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">script</link>
e a lista de imagens <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-release-rke2-images.txt">aqui</link>
para gerar os artefatos necessários para incluir em
<literal>custom/files</literal>. Você também pode fazer download do script
<literal>rke2-install</literal> mais recente <link
xl:href="https://get.rke2.io/">aqui</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>Após o download das imagens, a estrutura de diretórios deve ter esta
aparência:</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>Pré-carregar seu registro particular com as imagens necessárias para
cenários air-gapped e SR-IOV (opcional)</title>
<para>Para usar SR-IOV em seu cenário air-gapped ou qualquer outra imagem de carga
de trabalho, você deve pré-carregar o registro particular local com as
imagens seguindo as próximas etapas:</para>
<itemizedlist>
<listitem>
<para>Faça download, extraia e envie as imagens OCI de gráfico Helm ao registro
particular</para>
</listitem>
<listitem>
<para>Faça download, extraia e envie o restante das imagens necessárias ao
registro particular</para>
</listitem>
</itemizedlist>
<para>É possível usar os seguintes scripts para fazer download, extrair e enviar
as imagens ao registro particular. Vamos mostrar um exemplo para
pré-carregar as imagens SR-IOV, mas você também pode usar a mesma abordagem
para pré-carregar qualquer outra imagem personalizada:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pré-carregamento com imagens OCI de gráfico Helm para SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Você deve criar uma lista com as imagens OCI de gráfico Helm necessárias:</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:304.0.2+up1.5.0
edge/sriov-crd-chart:304.0.2+up1.5.0
EOF</screen>
</listitem>
<listitem>
<para>Gere um arquivo tarball local usando o seguinte <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-oci-artefacts.sh">script</link>
e a lista criada acima:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/charts/sriov-network-operator:304.0.2+up1.5.0
Pulled: registry.suse.com/edge/charts/sriov-crd:304.0.2+up1.5.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-304.0.2+up1.5.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-304.0.2+up1.5.0.tgz</screen>
</listitem>
<listitem>
<para>Faça upload do arquivo tarball para seu registro particular (por exemplo,
<literal>myregistry:5000</literal>) usando o seguinte <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-oci-artefacts.sh">script</link>
para pré-carregar o registro com as imagens OCI do gráfico Helm baixadas na
etapa anterior:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Pré-carregamento com o restante das imagens necessárias para SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Nesse caso, devemos incluir as imagens do contêiner `sr-iov para cargas de
trabalho de telecomunicações (por exemplo, como referência, você pode
obtê-las dos <link
xl:href="https://github.com/suse-edge/charts/blob/main/charts/sriov-network-operator/1.5.0/values.yaml">valores
de gráfico Helm</link>)</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</screen>
</listitem>
<listitem>
<para>Usando o seguinte <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">script</link>
e a lista criada acima, você deve gerar localmente o arquivo tarball com as
imagens necessárias:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</screen>
</listitem>
<listitem>
<para>Faça upload do arquivo tarball para seu registro particular (por exemplo,
<literal>myregistry:5000</literal>) usando o seguinte <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-images.sh">script</link>
para pré-carregar o registro particular com as imagens baixadas na etapa
anterior:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>Criação da imagem para cenários air-gapped</title>
<para>Depois que a estrutura de diretórios for preparada de acordo com as seções
anteriores, execute o seguinte comando para criar a imagem:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>Esse procedimento cria o arquivo da imagem ISO de saída chamado
<literal>eibimage-output-telco.raw</literal>, com base na definição descrita
acima.</para>
<para>Em seguida, disponibilize a imagem de saída por um servidor web, ou o
contêiner de servidor de mídia habilitado de acordo com as instruções na
documentação sobre o cluster de gerenciamento (<xref
linkend="metal3-media-server"/>) ou algum outro servidor acessível
localmente. Nos exemplos abaixo, esse servidor é mencionado como
<literal>imagecache.local:8080</literal>.</para>
</section>
</section>
<section xml:id="single-node">
<title>Provisionamento de cluster downstream com provisionamento de rede
direcionado (nó único)</title>
<para>Esta seção descreve o fluxo de trabalho usado para automatizar o
provisionamento de um cluster downstream de nó único usando o
provisionamento de rede direcionado. Essa é a maneira mais fácil de
automatizar o provisionamento de um cluster downstream.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>A imagem gerada por meio do <literal>EIB</literal>, conforme descrito na
seção anterior (<xref linkend="eib-edge-image-connected"/>), com a
configuração mínima para definir o cluster downstream, deve estar localizada
no cluster de gerenciamento exatamente no caminho configurado nesta seção
(<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>O servidor de gerenciamento criado e disponível para uso nas seções a
seguir. Para obter mais informações, consulte a seção sobre o cluster de
gerenciamento <xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Fluxo de trabalho</emphasis></para>
<para>O diagrama a seguir mostra o fluxo de trabalho usado para automatizar o
provisionamento de um cluster downstream de nó único por meio do
provisionamento de rede direcionado:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png"
width="100%"/> </imageobject>
<textobject><phrase>nóúnico1 automatizado atip</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Há duas etapas diferentes para automatizar o provisionamento de um cluster
downstream de nó único por meio do provisionamento de rede direcionado:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Registre o host bare metal para disponibilizá-lo ao processo de
provisionamento.</para>
</listitem>
<listitem>
<para>Provisione o host bare metal para instalar e configurar o sistema
operacional e o cluster Kubernetes.</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">Registrar o host bare metal</emphasis></para>
<para>A primeira etapa é registrar o novo host bare metal no cluster de
gerenciamento para disponibilizá-lo para provisionamento. Para fazer isso, é
necessário criar este arquivo (<literal>bmh-example.yaml</literal>) no
cluster de gerenciamento, para especificar as credenciais que serão usadas
do <literal>BMC</literal> e o objeto <literal>BaremetalHost</literal> para
registro:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>em que:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal>: o nome de usuário para o
<literal>BMC</literal> do novo host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal>: a senha para o <literal>BMC</literal> do
novo host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal>: o endereço <literal>MAC</literal> do novo
host bare metal que será usado.</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal>: o <literal>URL</literal> do host bare
metal <literal>BMC</literal> (por exemplo,
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>).
Para saber mais sobre as diversas opções disponíveis de acordo com o seu
provedor de hardware, acesse este <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">link</link>.</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>A arquitetura deve ser <literal>x86_64</literal> ou
<literal>aarch64</literal>, dependendo da arquitetura do host bare metal que
será registrado.</para>
</listitem>
<listitem>
<para>Se não foi especificada uma configuração de rede para o host, seja no
momento da criação da imagem, seja pela definição
<literal>BareMetalHost</literal>, é usado um mecanismo de configuração
automática (DHCP, DHCPv6, SLAAC). Para obter mais detalhes ou ver
configurações mais complexas, consulte a <xref
linkend="advanced-network-configuration"/>.</para>
</listitem>
</itemizedlist>
</note>
<para>Depois que o arquivo for criado, execute o seguinte comando no cluster de
gerenciamento para iniciar o registro do novo host bare metal:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>O novo objeto host bare metal será registrado, o que altera seu estado de
"registrando" para "inspecionando" e "disponível". Para verificar as
alterações, use o seguinte comando:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para>O objeto <literal>BaremetalHost</literal> fica no estado
<literal>registering</literal> até a validação das credenciais do
<literal>BMC</literal>. Depois disso, o estado do objeto
<literal>BaremetalHost</literal> muda para <literal>inspecting</literal>, e
essa etapa pode levar algum tempo dependendo do hardware (até 20
minutos). Durante a fase de inspeção, as informações do hardware são
recuperadas e o objeto Kubernetes é atualizado. Para verificar as
informações, use o seguinte comando: <literal>kubectl get bmh -o
yaml</literal>.</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">Etapa de provisionamento</emphasis></para>
<para>Depois que o host bare metal for registrado e estiver disponível, a próxima
etapa será provisioná-lo para instalar e configurar o sistema operacional e
o cluster Kubernetes. Para fazer isso, é necessário criar este arquivo
(<literal>capi-provisioning-example.yaml</literal>) no cluster de
gerenciamento com as seguintes informações (é possível gerar o
<literal>capi-provisioning-example.yaml</literal> unindo os blocos abaixo).</para>
<note>
<para>Apenas os valores entre <literal>$\{…​\}</literal> devem ser substituídos
pelos valores reais.</para>
</note>
<para>O bloco a seguir é a definição do cluster, em que a rede pode ser
configurada usando os blocos <literal>pods</literal> e
<literal>services</literal>. Ele também contém as referências aos objetos de
plano de controle e infraestrutura (usando o provedor
<literal>Metal<superscript>3</superscript></literal>) a serem usados.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:bad:cafe::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:bad:bad:cafe::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<note>
<itemizedlist>
<listitem>
<para>As duas implantações de pilha única e de pilha dupla são possíveis. Remova
os CIDRs IPv6 da definição acima no caso de um cluster apenas IPv4.</para>
</listitem>
<listitem>
<para>As implantações IPv6 de pilha única estão no status de prévia de tecnologia
e ainda não contam com suporte oficial.</para>
</listitem>
</itemizedlist>
</note>
<para>O objeto <literal>Metal3Cluster</literal> especifica o endpoint do plano de
controle (substituindo o
<literal>${DOWNSTREAM_CONTROL_PLANE_IPV4}</literal>) que será configurado e
o <literal>noCloudProvider</literal> porque não é usado um nó bare metal.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para>O objeto <literal>RKE2ControlPlane</literal> especifica a configuração do
plano de controle e o objeto <literal>Metal3MachineTemplate</literal>
especifica a imagem do plano de controle que serão usadas. Ele também contém
as informações sobre o número de réplicas (neste caso, uma) e o plug-in de
<literal>CNI</literal> (neste caso, <literal>Cilium</literal>) que serão
usados. O bloco agentConfig contém o formato do <literal>Ignition</literal>
e o <literal>additionalUserData</literal> a serem usados para configurar o
nó <literal>RKE2</literal>, com informações como um systemd chamado
<literal>rke2-preinstall.service</literal> para substituir automaticamente o
<literal>BAREMETALHOST_UUID</literal> e o <literal>node-name</literal>
durante o processo de provisionamento usando as informações do Ironic. Para
permitir o multus com o cilium, um arquivo é criado no diretório de
manifestos do servidor <literal>rke2</literal> chamado
<literal>rke2-cilium-config.yaml</literal> com a configuração que será
usada. O último bloco de informações inclui a versão do Kubernetes a ser
usada. <literal>${RKE2_VERSION}</literal> é a versão do
<literal>RKE2</literal> que substituirá esse valor (por exemplo,
<literal>v1.33.3+rke2r1</literal>).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>O objeto <literal>Metal3MachineTemplate</literal> especifica as seguintes
informações:</para>
<itemizedlist>
<listitem>
<para>O <literal>dataTemplate</literal> para usar como referência para o gabarito.</para>
</listitem>
<listitem>
<para>O <literal>hostSelector</literal> a ser usado correspondente ao rótulo
criado durante o processo de registro.</para>
</listitem>
<listitem>
<para>A <literal>image</literal> que será usada como referência à imagem gerada
pelo <literal>EIB</literal> na seção anterior (<xref
linkend="eib-edge-image-connected"/>), e o <literal>checksum</literal> e o
<literal>checksumType</literal> usados para validar a imagem.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para>O objeto <literal>Metal3DataTemplate</literal> especifica o
<literal>metaData</literal> para o cluster downstream.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Após a criação do arquivo unindo os blocos anteriores, execute o seguinte
comando no cluster de gerenciamento para iniciar o provisionamento do novo
host bare metal:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>Provisionamento de cluster downstream com provisionamento de rede
direcionado (vários nós)</title>
<para>Esta seção descreve o fluxo de trabalho usado para automatizar o
provisionamento de um cluster downstream de vários nós usando o
provisionamento de rede direcionado e o <literal>MetalLB</literal> como
estratégia de balanceador de carga. Essa é a maneira mais simples de
automatizar o provisionamento de clusters downstream. O diagrama abaixo
mostra o fluxo de trabalho usado para automatizar o provisionamento de um
cluster downstream de vários nós usando o provisionamento de rede
direcionado e o <literal>MetalLB</literal>.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>A imagem gerada por meio do <literal>EIB</literal>, conforme descrito na
seção anterior (<xref linkend="eib-edge-image-connected"/>), com a
configuração mínima para definir o cluster downstream, deve estar localizada
no cluster de gerenciamento exatamente no caminho configurado nesta seção
(<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>O servidor de gerenciamento criado e disponível para uso nas seções a
seguir. Para obter mais informações, consulte a seção sobre o cluster de
gerenciamento: <xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Fluxo de trabalho</emphasis></para>
<para>O diagrama a seguir mostra o fluxo de trabalho usado para automatizar o
provisionamento de um cluster downstream de vários nós por meio do
provisionamento de rede direcionado:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png"
width="100%"/> </imageobject>
<textobject><phrase>váriosnós1 automatizar atip</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>Registre os três hosts bare metal para disponibilizá-los ao processo de
provisionamento.</para>
</listitem>
<listitem>
<para>Provisione os três hosts bare metal para instalar e configurar o sistema
operacional e o cluster Kubernetes usando o <literal>MetalLB</literal>.</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">Registrar os hosts bare metal</emphasis></para>
<para>A primeira etapa é registrar os três hosts bare metal no cluster de
gerenciamento para disponibilizá-los para provisionamento. Para fazer isso,
é necessário criar estes arquivos
(<literal>bmh-example-node1.yaml</literal>,
<literal>bmh-example-node2.yaml</literal> e
<literal>bmh-example-node3.yaml</literal>) no cluster de gerenciamento, para
especificar as credenciais que serão usadas do <literal>BMC</literal> e o
objeto <literal>BaremetalHost</literal> para registro no cluster de
gerenciamento.</para>
<note>
<itemizedlist>
<listitem>
<para>Apenas os valores entre <literal>$\{…​\}</literal> devem ser substituídos
pelos valores reais.</para>
</listitem>
<listitem>
<para>Vamos orientar você apenas pelo processo de um host. As mesmas etapas são
válidas para os outros dois nós.</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>Em que:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal>: o nome de usuário para o BMC do
primeiro host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal>: a senha para o BMC do primeiro
host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal>: o endereço MAC do primeiro host bare
metal que será usado.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal>: o URL do primeiro BMC de host bare
metal (por exemplo,
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>).
A parte do URL do host pode ser um endereço IP (v4 ou v6) ou um nome de
domínio, quando a infraestrutura existente permite. Para saber mais sobre as
diferentes opções disponíveis de acordo com o seu provedor de hardware,
acesse este <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">link</link>.</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>Se não foi especificada uma configuração de rede para o host, seja no
momento da criação da imagem, seja pela definição
<literal>BareMetalHost</literal>, é usado um mecanismo de configuração
automática (DHCP, DHCPv6, SLAAC). Para obter mais detalhes ou ver
configurações mais complexas, consulte a <xref
linkend="advanced-network-configuration"/>.</para>
</listitem>
<listitem>
<para>Os clusters IPv6 de pilha única estão no status de prévia de tecnologia e
ainda não contam com suporte oficial.</para>
</listitem>
<listitem>
<para>A arquitetura deve ser <literal>x86_64</literal> ou
<literal>aarch64</literal>, dependendo da arquitetura do host bare metal que
será registrado.</para>
</listitem>
<listitem>
<para>Todos os servidores modernos vêm com um BMC compatível com pilha dupla, no
entanto, o suporte a IPv6 (e, possivelmente, a opção para usar nomes de host
com o recurso VirtualMedia) deve ser verificado antes do uso em produção no
ambiente de pilha dupla.</para>
</listitem>
</itemizedlist>
</note>
<para>Depois que o arquivo for criado, execute o seguinte comando no cluster de
gerenciamento para iniciar o registro dos hosts bare metal:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>Os novos objetos host bare metal serão registrados, o que altera o estado
deles de "registrando" para "inspecionando" e "disponíveis". Para verificar
as alterações, use o seguinte comando:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para>O objeto <literal>BaremetalHost</literal> fica no estado
<literal>registering</literal> até a validação das credenciais do
<literal>BMC</literal>. Depois disso, o estado do objeto
<literal>BaremetalHost</literal> muda para <literal>inspecting</literal>, e
essa etapa pode levar algum tempo dependendo do hardware (até 20
minutos). Durante a fase de inspeção, as informações do hardware são
recuperadas e o objeto Kubernetes é atualizado. Para verificar as
informações, use o seguinte comando: <literal>kubectl get bmh -o
yaml</literal>.</para>
</note>
<para><emphasis role="strong">Etapa de provisionamento</emphasis></para>
<para>Depois que os três hosts bare metal forem registrados e estiverem
disponíveis, a próxima etapa será provisioná-los para instalar e configurar
o sistema operacional e o cluster Kubernetes, criando um balanceador de
carga para gerenciá-los. Para fazer isso, é necessário criar este arquivo
(<literal>capi-provisioning-example.yaml</literal>) no cluster de
gerenciamento com as seguintes informações (é possível gerar o
`capi-provisioning-example.yaml unindo os blocos abaixo).</para>
<note>
<itemizedlist>
<listitem>
<para>Apenas os valores entre <literal>$\{…​\}</literal> devem ser substituídos
pelos valores reais.</para>
</listitem>
<listitem>
<para>O endereço <literal>VIP</literal> é um endereço IP reservado não atribuído a
nenhum nó e usado para configurar o balanceador de carga. Em um cluster de
pilha dupla, é possível especificar ambos IPv4 e IPv6; mas, nos exemplos a
seguir, a prioridade será dada ao endereço IPv4.</para>
</listitem>
</itemizedlist>
</note>
<para>Veja a seguir a definição do cluster, em que a rede do cluster pode ser
configurada usando os blocos <literal>pods</literal> e
<literal>services</literal>. Ela também contém as referências aos objetos de
plano de controle e infraestrutura (usando o provedor
<literal>Metal<superscript>3</superscript></literal>) a serem usados.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:1234:4321::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:5678:8765:4321::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<note>
<para>As duas implantações de pilha única e de pilha dupla são possíveis. Remova
os CIDRs IPv6 e os endereços VIP IPv6 (das seções seguintes) para um cluster
apenas IPv4.</para>
</note>
<para>O objeto <literal>Metal3Cluster</literal> especifica o endpoint de plano de
controle que usa o endereço <literal>VIP</literal> já reservado
(substituindo o <literal>${EDGE_VIP_ADDRESS_IPV4}</literal>) que será
configurado e o <literal>noCloudProvider</literal>, porque são usados os
três nós bare metal.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para>O objeto <literal>RKE2ControlPlane</literal> especifica a configuração do
plano de controle e o objeto <literal>Metal3MachineTemplate</literal>
especifica a imagem do plano de controle que serão usadas.</para>
<itemizedlist>
<listitem>
<para>O número de réplicas a ser usado (neste caso, três).</para>
</listitem>
<listitem>
<para>O modo de anúncio usado pelo balanceador de carga
(<literal>address</literal> usa a implementação L2) e o endereço que será
usado (substituindo o <literal>${EDGE_VIP_ADDRESS}</literal> pelo endereço
<literal>VIP</literal>).</para>
</listitem>
<listitem>
<para>O <literal>serverConfig</literal> com o plug-in de <literal>CNI</literal>
que será usado (neste caso, <literal>Cilium</literal>) e os endereços
<literal>VIP</literal> e os nomes adicionais para listar em
<literal>tlsSan</literal>.</para>
</listitem>
<listitem>
<para>O bloco agentConfig inclui o formato usado do <literal>Ignition</literal> e
o <literal>additionalUserData</literal> usado para configurar o nó
<literal>RKE2</literal> com informações como:</para>
<itemizedlist>
<listitem>
<para>O serviço systemd chamado <literal>rke2-preinstall.service</literal> para
substituir automaticamente o <literal>BAREMETALHOST_UUID</literal> e o
<literal>node-name</literal> durante o processo de provisionamento com as
informações do Ironic.</para>
</listitem>
<listitem>
<para>O bloco <literal>storage</literal> que contém os gráficos Helm que serão
usados para instalar o <literal>MetalLB</literal> e o
<literal>endpoint-copier-operator</literal>.</para>
</listitem>
<listitem>
<para>O arquivo de recurso personalizado <literal>metalLB</literal> com o
<literal>IPaddressPool</literal> e o <literal>L2Advertisement</literal> que
serão usados (substituindo <literal>${EDGE_VIP_ADDRESS_IPV4}</literal> pelo
endereço <literal>VIP</literal>).</para>
</listitem>
<listitem>
<para>O arquivo <literal>endpoint-svc.yaml</literal> que será usado para
configurar o serviço <literal>kubernetes-vip</literal> usado pelo
<literal>MetalLB</literal> para gerenciar o endereço <literal>VIP</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>O último bloco de informações contém a versão do Kubernetes que deve ser
usada. <literal>${RKE2_VERSION}</literal> é a versão do
<literal>RKE2</literal> que vai substituir esse valor (por exemplo,
<literal>v1.33.3+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS_IPV4}
      - ${EDGE_VIP_ADDRESS_IPV6}
      - https://${EDGE_VIP_ADDRESS_IPV4}.sslip.io
      - https://${EDGE_VIP_ADDRESS_IPV6}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS_IPV4}/32
                      - ${EDGE_VIP_ADDRESS_IPV6}/128
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ipFamilyPolicy: PreferDualStack
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "Node-multinode-cluster"</screen>
<para>O objeto <literal>Metal3MachineTemplate</literal> especifica as seguintes
informações:</para>
<itemizedlist>
<listitem>
<para>O <literal>dataTemplate</literal> para usar como referência para o gabarito.</para>
</listitem>
<listitem>
<para>O <literal>hostSelector</literal> a ser usado correspondente ao rótulo
criado durante o processo de registro.</para>
</listitem>
<listitem>
<para>A <literal>image</literal> que será usada como referência à imagem gerada
pelo <literal>EIB</literal> na seção anterior (<xref
linkend="eib-edge-image-connected"/>), e o <literal>checksum</literal> e o
<literal>checksumType</literal> usados para validar a imagem.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para>O objeto <literal>Metal3DataTemplate</literal> especifica o
<literal>metaData</literal> para o cluster downstream.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Os arquivos yaml a seguir são uma configuração de exemplo de nós do worker.</para>
<para><literal>MachineDeployment</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: multinode-cluster
    nodepool: nodepool-0
  name: multinode-cluster-workers
  namespace: default
spec:
  clusterName: multinode-cluster
  replicas: 3
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: multinode-cluster
      nodepool: nodepool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: multinode-cluster
        nodepool: nodepool-0
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: RKE2ConfigTemplate
          name: multinode-cluster-workers
      clusterName: multinode-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: multinode-cluster-workers
      nodeDrainTimeout: 0s
      version: ${RKE2_VERSION}</screen>
<para>O objeto RKE2ConfigTemplate especifica o gabarito de configuração que será
usado para nós do worker de um cluster de vários nós.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: RKE2ConfigTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "Node-multinode-cluster-worker"
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target</screen>
<para>O objeto <literal>Metal3MachineTemplate</literal> contém referências a
<literal>dataTemplate</literal>, <literal>hostSelector</literal> e
<literal>image</literal> para os nós do worker:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw</screen>
<para>O objeto <literal>Metal3DataTemplate</literal> especifica
<literal>metaData</literal> para o cluster downstream para os nós do worker:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-workers-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Depois que o arquivo é criado unindo os blocos anteriores, execute o
seguinte comando no cluster de gerenciamento para iniciar o provisionamento
dos três hosts bare metal:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>Configuração avançada de rede</title>
<para>O fluxo de trabalho de provisionamento de rede direcionado permite fazer
configurações de rede específicas nos clusters downstream, como IPs
estáticos, vínculo, VLANs, IPv6 etc.</para>
<para>As seções a seguir descrevem as etapas adicionais necessárias para permitir
o provisionamento de clusters downstream usando a configuração de rede
avançada.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>A imagem gerada pelo <literal>EIB</literal> deve incluir a pasta de rede e o
script, de acordo com as instruções nesta seção (<xref
linkend="add-network-eib"/>).</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuração</emphasis></para>
<para>Antes de continuar, consulte uma das seções a seguir para obter orientação
sobre as etapas necessárias para registrar e provisionar os hosts:</para>
<itemizedlist>
<listitem>
<para>Provisionamento de cluster downstream com provisionamento de rede
direcionado (nó único) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Provisionamento de cluster downstream com provisionamento de rede
direcionado (vários nós) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>É necessário aplicar uma configuração de rede avançada no momento do
registro por meio de uma definição do host <literal>BareMetalHost</literal>
e um segredo associado que contém um bloco <literal>networkData</literal> no
formato <literal>nmstate</literal>. O arquivo de exemplo a seguir define um
segredo com o <literal>networkData</literal> necessário que solicita um
<literal>IP</literal> estático e a <literal>VLAN</literal> para o host do
cluster downstream:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      identifier: mac-address
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>Como você pode ver, o exemplo mostra a configuração para habilitar a
interface com os IPs estáticos e a VLAN usando a interface base, depois que
as seguintes variáveis são substituídas pelos valores reais, de acordo com a
sua infraestrutura:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal>: a interface do plano de
controle que será usada para o cluster de borda (por exemplo,
<literal>eth0</literal>). Ao incluir <literal>identifier:
mac-address</literal>, a nomenclatura é inspecionada automaticamente pelo
endereço MAC, portanto qualquer nome de interface pode ser usado.</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP}</literal>: o endereço IP que será usado como
endpoint do cluster de borda (deve ser igual ao endpoint kubeapi-server).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX}</literal>: o CIDR que será usado para o
cluster de borda (por exemplo, <literal>24</literal> para
<literal>/24</literal> ou <literal>255.255.255.0</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY}</literal>: o gateway que será usado para o
cluster de borda (por exemplo, <literal>192.168.100.1</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_MAC}</literal>: o endereço MAC que será usado para a
interface do plano de controle (por exemplo,
<literal>00:0c:29:3e:3e:3e</literal>).</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal>: o DNS usado para o cluster de borda (por
exemplo, <literal>192.168.100.2</literal>).</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal>: o ID da VLAN usado para o cluster de borda
(por exemplo, <literal>100</literal>).</para>
</listitem>
</itemizedlist>
<para>É possível usar qualquer outra definição adequada a
<literal>nmstate</literal> para configurar a rede do cluster downstream de
acordo com os requisitos específicos. Por exemplo, é possível especificar
uma configuração de pilha dupla estática:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: ${CONTROLPLANE_MAC}
      ipv4:
        enabled: true
        dhcp: false
        address:
        - ip: ${CONTROLPLANE_IP_V4}
          prefix-length: ${CONTROLPLANE_PREFIX_V4}
      ipv6:
        enabled: true
        dhcp: false
        autoconf: false
        address:
        - ip: ${CONTROLPLANE_IP_V6}
          prefix-length: ${CONTROLPLANE_PREFIX_V6}
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V4}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
      - destination: ::/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V6}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
    dns-resolver:
      config:
        server:
        - ${DNS_SERVER_V4}
        - ${DNS_SERVER_V6}</screen>
<para>Assim como no exemplo anterior, substitua as seguintes variáveis pelos
valores reais, de acordo com a sua infraestrutura:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_IP_V4}</literal>: o endereço IPv4 para atribuir ao
host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V4}</literal>: o prefixo IPv4 da rede à qual
pertence o IP do host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP_V6}</literal>: o endereço IPv6 para atribuir ao
host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V6}</literal>: o prefixo IPv6 da rede à qual
pertence o IP do host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V4}</literal>: o endereço IPv4 do gateway
para o tráfego correspondente à rota padrão</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V6}</literal>: o endereço IPv6 do gateway
para o tráfego correspondente à rota padrão</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal>: o nome da interface à qual
atribuir os endereços e que será usada para o tráfego de saída
correspondente à rota padrão, para ambos IPv4 e IPv6</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER_V4}</literal> e/ou
<literal>${DNS_SERVER_V6}</literal>: os endereços IP dos servidores DNS, que
podem ser especificados como uma única ou várias entradas. Há suporte para
os endereços tanto IPv4 quanto IPv6</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>Consulte o <link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/edge-clusters">repositório
de exemplos do SUSE Telco Cloud</link> para ver exemplos mais complexos,
incluindo as configurações apenas IPv6 e de pilha dupla.</para>
</listitem>
<listitem>
<para>As implantações IPv6 de pilha única estão no status de prévia de tecnologia
e ainda não contam com suporte oficial.</para>
</listitem>
</itemizedlist>
</note>
<para>Por fim, sejam quais forem os detalhes da configuração de rede, garanta que
o segredo seja referenciado anexando
<literal>preprovisioningNetworkDataName</literal> ao objeto
<literal>BaremetalHost</literal> para registrar o host no cluster de
gerenciamento com sucesso.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<itemizedlist>
<listitem>
<para>Se você precisa implantar um cluster de vários nós, siga o mesmo processo
para cada nó.</para>
</listitem>
<listitem>
<para>Atualmente, o <literal>Metal3DataTemplate</literal>, o
<literal>networkData</literal> e o <literal>Metal3 IPAM</literal> não são
suportados. Apenas a configuração por meio de segredos estáticos conta com
suporte total.</para>
</listitem>
<listitem>
<para>A arquitetura deve ser <literal>x86_64</literal> ou
<literal>aarch64</literal>, dependendo da arquitetura do host bare metal que
será registrado.</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="add-telco">
<title>Recursos de telecomunicações (DPDK, SR-IOV, isolamento de CPU, HugePages,
NUMA etc.)</title>
<para>O fluxo de trabalho de provisionamento de rede direcionado permite
automatizar os recursos de telecomunicações que serão usados nos clusters
downstream para executar as cargas de trabalho de telecomunicações nesses
servidores.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>A imagem gerada por meio do <literal>EIB</literal>, conforme descrito na
seção anterior (<xref linkend="eib-edge-image-connected"/>), deve estar
localizada no cluster de gerenciamento exatamente no caminho configurado
nesta seção (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>A imagem gerada por meio do <literal>EIB</literal> deve incluir os pacotes
de telecomunicações específicos de acordo com as instruções nesta seção
(<xref linkend="add-telco-feature-eib"/>).</para>
</listitem>
<listitem>
<para>O servidor de gerenciamento criado e disponível para uso nas seções a
seguir. Para obter mais informações, consulte a seção sobre o cluster de
gerenciamento: <xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuração</emphasis></para>
<para>Use as duas seções a seguir como base para registrar e provisionar os hosts:</para>
<itemizedlist>
<listitem>
<para>Provisionamento de cluster downstream com provisionamento de rede
direcionado (nó único) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Provisionamento de cluster downstream com provisionamento de rede
direcionado (vários nós) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>Os recursos de telecomunicações abordados nesta seção são:</para>
<itemizedlist>
<listitem>
<para>DPDK e criação de VFs</para>
</listitem>
<listitem>
<para>Alocação de SR-IOV e VFs para as cargas de trabalho usarem</para>
</listitem>
<listitem>
<para>Isolamento de CPU e ajuste de desempenho</para>
</listitem>
<listitem>
<para>Configuração do HugePages</para>
</listitem>
<listitem>
<para>Ajuste de parâmetros do kernel</para>
</listitem>
</itemizedlist>
<note>
<para>Para obter mais informações sobre os recursos de telecomunicações, consulte
o <xref linkend="atip-features"/>.</para>
</note>
<para>As alterações necessárias para habilitar os recursos de telecomunicações
mostrados acima estão todas no bloco <literal>RKE2ControlPlane</literal> do
arquivo de provisionamento
<literal>capi-provisioning-example.yaml</literal>. O restante das
informações no arquivo <literal>capi-provisioning-example.yaml</literal> é o
mesmo que as informações apresentadas na seção de provisionamento (<xref
linkend="single-node-provision"/>).</para>
<para>Para esclarecer o processo, as alterações necessárias neste bloco
(<literal>RKE2ControlPlane</literal>) para habilitar os recursos de
telecomunicações são:</para>
<itemizedlist>
<listitem>
<para>Os <literal>preRKE2Commands</literal> usados para executar os comandos antes
do processo de instalação do <literal>RKE2</literal>. Neste caso, use o
comando <literal>modprobe</literal> para habilitar os módulos do kernel
<literal>vfio-pci</literal> e <literal>SR-IOV</literal>.</para>
</listitem>
<listitem>
<para>O arquivo do Ignition
<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>
que será usado para definir as interfaces, os drivers e o número de
<literal>VFs</literal> que será criado e exposto às cargas de trabalho.</para>
<itemizedlist>
<listitem>
<para>Os valores no mapa de configuração
<literal>sriov-custom-auto-config</literal> são os únicos que precisam ser
substituídos pelos valores reais.</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal>: o nome do recurso usado para a
primeira interface <literal>PF</literal> (por exemplo,
<literal>sriov-resource-du1</literal>). Ele é adicionado ao prefixo
<literal>rancher.io</literal> usado como o rótulo que as cargas de trabalho
usarão (por exemplo, <literal>rancher.io/sriov-resource-du1</literal>).</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal>: o nome da primeira interface
<literal>PF</literal> que será usada (por exemplo, <literal>eth0</literal>).</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal>: o nome da primeira função física
<literal>PF</literal> que será usada. Gere filtros mais complexos usando
este comando (por exemplo, <literal>eth0#2-5</literal>).</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal>: o nome do driver que será usado para a
primeira interface <literal>VF</literal> (por exemplo,
<literal>vfio-pci</literal>).</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal>: o número de <literal>VFs</literal> que será
criado para a primeira interface <literal>PF</literal> (por exemplo,
<literal>8</literal>).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>O <literal>/var/sriov-auto-filler.sh</literal> usado como conversor entre o
mapa de configuração de alto nível
<literal>sriov-custom-auto-config</literal> e o
<literal>sriovnetworknodepolicy</literal>, que contém as informações de
hardware de baixo nível. Esse script foi criado para eliminar a complexidade
do usuário em ter que saber com antecedência as informações do hardware. Não
há necessidade de alterações no arquivo, mas ele deverá estar presente se
precisarmos habilitar <literal>sr-iov</literal> e criar
<literal>VFs</literal>.</para>
</listitem>
<listitem>
<para>Os argumentos do kernel que serão usados para habilitar os seguintes
recursos:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Parâmetro</para></entry>
<entry align="left" valign="top"><para>Valor</para></entry>
<entry align="left" valign="top"><para>Descrição</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,&#x200B;managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Isolar os núcleos 1-30 e 33-62.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Permite que o kernel distribua as interrupções do temporizador entre as CPUs
isoladas.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>em</para></entry>
<entry align="left" valign="top"><para>Permite que o kernel execute o tick do temporizador em uma única CPU quando
o sistema está ocioso.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>O parâmetro de inicialização do kernel é a interface principal atual para
configurar dynticks completos junto com o isolamento de CPU.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Permite que o kernel execute os retornos de chamada RCU em uma única CPU
quando o sistema está ocioso.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Permite que o kernel execute as interrupções em uma única CPU quando o
sistema está ocioso.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>Minimiza a latência de sair do estado ocioso.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Permite usar vfio para as interfaces dpdk.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>em</para></entry>
<entry align="left" valign="top"><para>Permite usar vfio para VFs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Permite definir o tamanho das páginas enormes como 1 G.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>O número de páginas enormes definido antes.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>O valor padrão para habilitar o HugePages.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Desabilita o watchdog.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>Desabilita o watchdog de NMI.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>Os serviços systemd abaixo são usados para habilitar o seguinte:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> para substituir automaticamente o
<literal>BAREMETALHOST_UUID</literal> e o <literal>node-name</literal>
durante o processo de provisionamento com as informações do Ironic.</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> para habilitar os núcleos de
isolamento da <literal>CPU</literal> (por exemplo,
<literal>1-30,33-62</literal>).</para>
</listitem>
<listitem>
<para><literal>performance-settings.service</literal> para habilitar o ajuste de
desempenho da CPU.</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> para instalar o gráfico
Helm <literal>sriov</literal>. Aguarde a criação dos recursos personalizados
e execute o <literal>/var/sriov-auto-filler.sh</literal> para substituir os
valores no mapa de configuração <literal>sriov-custom-auto-config</literal>
e criar o <literal>sriovnetworknodepolicy</literal> que as cargas de
trabalho usarão.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>${RKE2_VERSION}</literal> é a versão do <literal>RKE2</literal> que
vai substituir esse valor (por exemplo, <literal>v1.33.3+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<para>Com todas essas alterações mencionadas, o bloco
<literal>RKE2ControlPlane</literal> no
<literal>capi-provisioning-example.yaml</literal> terá a seguinte aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-crd
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-network-operator
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Após a criação do arquivo unindo os blocos anteriores, execute o seguinte
comando no cluster de gerenciamento para iniciar o provisionamento do novo
cluster downstream usando os recursos de telecomunicações:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>Registro particular</title>
<para>É possível configurar um registro particular como espelho para as imagens
que as cargas de trabalho usam.</para>
<para>Para isso, criamos o segredo com as informações sobre o registro particular
que será usado pelo cluster downstream.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para>O <literal>tls.crt</literal>, <literal>tls.key</literal> e
<literal>ca.crt</literal> são os certificados usados para autenticar o
registro particular. O <literal>username</literal> e a
<literal>password</literal> são as credenciais usadas para autenticar o
registro particular.</para>
<note>
<para>O <literal>tls.crt</literal>, <literal>tls.key</literal>,
<literal>ca.crt</literal> , <literal>username</literal> e
<literal>password</literal> devem ser codificados no formato base64 antes de
serem usados no segredo.</para>
</note>
<para>Com todas essas alterações mencionadas, o bloco
<literal>RKE2ControlPlane</literal> no
<literal>capi-provisioning-example.yaml</literal> terá a seguinte aparência:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Em que <literal>registry.example.com</literal> é o nome de exemplo do
registro particular usado pelo cluster downstream e deve ser substituído
pelos valores reais.</para>
</section>
<section xml:id="airgap-deployment">
<title>Provisionamento de cluster downstream em cenários air-gapped</title>
<para>O fluxo de trabalho de provisionamento de rede direcionado permite
automatizar o provisionamento de clusters downstream nos cenários
air-gapped.</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>Requisitos para cenários air-gapped</title>
<orderedlist numeration="arabic">
<listitem>
<para>A imagem <literal>bruta</literal> gerada pelo <literal>EIB</literal> deve
incluir as imagens do contêiner específicas (imagens OCI de gráfico Helm e
do contêiner) necessárias para executar o cluster downstream em um cenário
air-gapped. Para obter mais informações, consulte esta seção (<xref
linkend="eib-edge-image-airgap"/>).</para>
</listitem>
<listitem>
<para>No caso de uso de SR-IOV ou qualquer outra carga de trabalho personalizada,
as imagens necessárias para executar as cargas de trabalho devem ser
pré-carregadas no registro particular, seguindo a seção sobre
pré-carregamento no registro particular (<xref
linkend="preload-private-registry"/>).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>Registrar hosts bare metal em cenários air-gapped</title>
<para>O processo de registro de hosts bare metal no cluster de gerenciamento é o
mesmo descrito na seção anterior (<xref linkend="enroll-bare-metal-host"/>).</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>Provisionar o cluster downstream em cenários air-gapped</title>
<para>Há algumas alterações importantes necessárias para provisionar o cluster
downstream em cenários air-gapped:</para>
<orderedlist numeration="arabic">
<listitem>
<para>O bloco <literal>RKE2ControlPlane</literal> no arquivo
<literal>capi-provisioning-example.yaml</literal> deve incluir a diretiva
<literal>spec.agentConfig.airGapped: true</literal>.</para>
</listitem>
<listitem>
<para>É necessário incluir a configuração do registro particular no bloco
<literal>RKE2ControlPlane</literal> do arquivo
<literal>capi-provisioning-airgap-example.yaml</literal> de acordo com a
seção de registro particular (<xref linkend="atip-private-registry"/>).</para>
</listitem>
<listitem>
<para>Se você usa SR-IOV ou qualquer outra configuração
<literal>AdditionalUserData</literal> (script combustion) que requer a
instalação de gráfico Helm, deve modificar o conteúdo para fazer referência
ao registro particular, em vez de usar o registro público.</para>
</listitem>
</orderedlist>
<para>O exemplo a seguir mostra a configuração de SR-IOV no bloco
<literal>AdditionalUserData</literal> do arquivo
<literal>capi-provisioning-airgap-example.yaml</literal> com as modificações
necessárias para fazer referência ao registro particular.</para>
<itemizedlist>
<listitem>
<para>Referências de segredos do registro particular</para>
</listitem>
<listitem>
<para>Definição do gráfico Helm usando o registro particular em vez das imagens
OCI públicas</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=1800
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --timeout=30m --all ; do sleep 10 ; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>Ações de ciclo de vida</title>
<para>Esta seção aborda as ações de gerenciamento de ciclo de vida para clusters
implantados pelo SUSE Telco Cloud.</para>
<section xml:id="id-management-cluster-upgrades">
<title>Upgrades de cluster de gerenciamento</title>
<para>O upgrade do cluster de gerenciamento envolve vários componentes. Para ver a
lista dos componentes gerais que exigem upgrade, consulte a documentação do
cluster de gerenciamento de <literal>dia 2</literal> (<xref
linkend="day2-mgmt-cluster"/>).</para>
<para>O procedimento de upgrade dos componentes específicos desta configuração
está descrito abaixo.</para>
<para><emphasis role="strong">Fazendo upgrade do
Metal<superscript>3</superscript></emphasis></para>
<para>Para fazer upgrade do <literal>Metal<superscript>3</superscript></literal>,
use o seguinte comando para atualizar o cache do repositório Helm e buscar o
gráfico mais recente para instalar o
<literal>Metal<superscript>3</superscript></literal> do repositório de
gráficos Helm:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>Depois disso, a maneira fácil de fazer upgrade é exportar suas configurações
atuais para um arquivo e fazer upgrade da versão do
<literal>Metal<superscript>3</superscript></literal> usando o arquivo
anterior. Se for necessário alterar alguma coisa na nova versão, edite o
arquivo antes do upgrade.</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=304.0.16+up0.12.6</screen>
</section>
<section xml:id="atip-lifecycle-downstream">
<title>Upgrades de cluster downstream</title>
<para>O upgrade de clusters downstream envolve a atualização de vários
componentes. As seções a seguir abordam o processo de upgrade de cada um
deles.</para>
<para><emphasis role="strong">Fazendo upgrade do sistema operacional</emphasis></para>
<para>Para este processo, consulte a seguinte referência (<xref
linkend="eib-edge-image-connected"/>) para criar uma imagem com uma nova
versão do sistema operacional. Com essa imagem gerada pelo
<literal>EIB</literal>, a próxima fase de provisionamento usará a nova
versão do sistema operacional fornecida. Na etapa seguinte, a nova imagem
será usada para fazer upgrade dos nós.</para>
<para><emphasis role="strong">Fazendo upgrade do cluster RKE2</emphasis></para>
<para>As alterações necessárias para fazer upgrade do cluster
<literal>RKE2</literal> usando o fluxo de trabalho automatizado são as
seguintes:</para>
<itemizedlist>
<listitem>
<para>Altere o bloco <literal>RKE2ControlPlane</literal> no
<literal>capi-provisioning-example.yaml</literal> mostrado na seguinte seção
(<xref linkend="single-node-provision"/>):</para>
<itemizedlist>
<listitem>
<para>Especifique a <literal>rolloutStrategy</literal> desejada.</para>
</listitem>
<listitem>
<para>Altere a versão do cluster <literal>RKE2</literal> para a nova versão,
substituindo <literal>${RKE2_NEW_VERSION}</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  version: ${RKE2_NEW_VERSION}
  replicas: 1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>Altere o bloco <literal>Metal3MachineTemplate</literal> no
<literal>capi-provisioning-example.yaml</literal> mostrado na seguinte seção
(<xref linkend="single-node-provision"/>):</para>
<itemizedlist>
<listitem>
<para>Altere o nome e o checksum da imagem para a nova versão gerada na etapa
anterior.</para>
</listitem>
<listitem>
<para>Adicione a diretiva <literal>nodeReuse</literal> a <literal>true</literal>
para evitar a criação de um novo nó.</para>
</listitem>
<listitem>
<para>Adicione a diretiva <literal>automatedCleaningMode</literal> a
<literal>metadata</literal> para habilitar a limpeza automatizada do nó.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>Depois de fazer essas alterações, o arquivo
<literal>capi-provisioning-example.yaml</literal> poderá ser aplicado ao
cluster usando o seguinte comando:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-troubleshooting-3">
<title>Resolução de Problemas</title>
<partintro>
<para>Esta seção orienta no diagnóstico e na solução dos problemas comuns às
implantações e operações do SUSE Edge. Ela aborda vários tópicos, incluindo
as etapas de solução de problemas específicas do componente, as ferramentas
importantes e os locais relevantes dos registros.</para>
</partintro>
<chapter xml:id="general-troubleshooting-principles">
<title>Princípios gerais da solução de problemas</title>
<para>Antes de entrar nos detalhes dos problemas específicos dos componentes,
avalie estes princípios gerais:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Consultar os registros</emphasis>: os registros são
a principal fonte de informações. Na maior parte das vezes, os erros são
autoexplicativos e contêm dicas sobre o motivo da falha.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verificar os relógios</emphasis>: as diferenças
entre os relógios dos sistemas podem provocar diversos tipos de
erros. Assegure a sincronização dos relógios. É possível instruir o EIB a
forçar a sincronização dos relógios no momento da inicialização. Consulte
Configurando a hora do sistema operacional (<xref
linkend="quickstart-eib"/>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas na inicialização</emphasis>: se o sistema
travar durante a inicialização, anote as últimas mensagens exibidas. Acesse
o console (físico ou via BMC) para observar as mensagens de inicialização.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas de rede</emphasis>: verifique a
configuração da interface de rede (<literal>ip a</literal>) e a tabela de
roteamento (<literal>ip route</literal>), teste a conectividade com outros
nós e serviços externos (<literal>ping</literal>, <literal>nc</literal>) e
garanta que as regras de firewall não bloqueiem as portas necessárias.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verificar o status dos componentes</emphasis>: use
<literal>kubectl get</literal> e <literal>kubectl describe</literal> para os
recursos do Kubernetes. Use <literal>kubectl get events
--sort-by='.lastTimestamp' -n &lt;namespace&gt;</literal> para ver os
eventos em um determinado namespace do Kubernetes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verificar o status dos serviços</emphasis>: use
<literal>systemctl status &lt;serviço&gt;</literal> para os serviços
systemd.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verificar a sintaxe</emphasis>: o software espera
uma determinada estrutura e sintaxe nos arquivos de configuração. Para os
arquivos yaml, por exemplo, use <literal>yamllint</literal> ou ferramentas
similares para verificar a sintaxe adequada.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Isolar o problema</emphasis>: tente restringir o
problema a um componente ou uma camada específica (por exemplo, rede,
armazenamento, sistema operacional, Kubernetes,
Metal<superscript>3</superscript>, Ironic etc.).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Documentação</emphasis>: consulte sempre a <link
xl:href="https://documentation.suse.com/suse-edge/">SUSE Edge
documentation</link> and also upstream documentation for detailed
information.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Versões</emphasis>: o SUSE Edge é uma versão
consistente e completamente testada de diferentes componentes SUSE. Consulte
as versões de cada componente por lançamento do SUSE Edge na <link
xl:href="https://documentation.suse.com/suse-edge/support-matrix/html/support-matrix/index.html">matriz
de suporte do SUSE Edge</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas conhecidos</emphasis>: para cada versão do
SUSE Edge existe uma seção de "problemas conhecidos" nas Notas de
lançamento, com informações dos problemas que serão corrigidos em versões
futuras, mas que podem afetar a versão atual.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="troubleshooting-kiwi">
<title>Solução de problemas do Kiwi</title>
<para>O Kiwi é usado para gerar as imagens atualizadas do SUSE Linux Micro que
serão usadas com o Edge Image Builder.</para>
<itemizedlist>
<title>Problemas comuns</title>
<listitem>
<para><emphasis role="strong">Incompatibilidade de versões do SL Micro</emphasis>:
a versão do sistema operacional do host de build deve ser compatível com a
versão do sistema operacional que está sendo criada (host do SL Micro 6.0 →
imagem do SL Micro 6.0).</para>
</listitem>
<listitem>
<para><emphasis role="strong">SELinux no estado de imposição</emphasis>: devido a
algumas limitações, é necessário desabilitar o SELinux temporariamente para
criar imagens com o Kiwi. Verifique o status do SElinux com o comando
<literal>getenforce</literal> e desabilite-o antes de executar o processo de
criação com <literal>setenforce 0</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Host de build não registrado</emphasis>: o processo
de build usa os registros dos hosts de build para obter os pacotes do SUSE
SCC. Se o host não foi registrado, o processo falha.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Falha no teste do dispositivo em loop</emphasis>: na
primeira execução do processo de criação do Kiwi, ocorre uma falha logo
depois que ele é iniciado com o erro: "ERROR: Early loop device test failed,
please retry the container run." (ERRO: Falha no teste do dispositivo no
ciclo inicial. Tente executar o contêiner novamente.). Trata-se de um
sintoma em que os dispositivos que são criados em loop no sistema host
subjacente não ficam imediatamente visíveis dentro da imagem do
contêiner. Execute novamente o processo de criação do Kiwi, e ele deverá
prosseguir sem problemas.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Permissões ausentes</emphasis>: o processo de build
espera sua execução como usuário root (ou via sudo).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Privilégios incorretos</emphasis>: o processo de
build espera o sinalizador <literal>--privileged</literal> ao executar o
contêiner. Certifique-se de que ele esteja presente.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros do contêiner de build</emphasis>: consulte
os registros do contêiner de build. Os registros são gerados no diretório
que foi usado para armazenar os artefatos. Consulte também os registros do
docker ou do podman para ver as informações necessárias.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Diretórios temporários de build</emphasis>: o Kiwi
cria diretórios temporários durante o processo de build. Consulte-os para
acessar os registros ou os artefatos intermediários se a saída principal não
for suficiente.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Etapas da solução de problemas</title>
<listitem>
<para><emphasis role="strong">Revise a saída de
<literal>build-image</literal></emphasis>: a mensagem de erro na saída do
console costuma ser muito clara.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique o ambiente do build</emphasis>: garanta
que todos os pré-requisitos para o próprio Kiwi (por exemplo, docker/podman,
SElinux, espaço em disco suficiente) sejam atendidos na máquina que executa
o Kiwi.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Analise os registros do contêiner de
build</emphasis>: revise os registros do contêiner com falha para verificar
os erros em mais detalhes (veja acima).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique o arquivo de definição</emphasis>: se você
usa um arquivo de definição de imagem do Kiwi personalizado, revise-o para
garantir que não haja erros de sintaxe ou ortografia.</para>
</listitem>
</orderedlist>
<note>
<para>Consulte o <link
xl:href="https://documentation.suse.com/appliance/kiwi-9/html/kiwi/troubleshooting.html">Guia
de Solução de Problemas do Kiwi</link> (em inglês).</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-image-builder">
<title>Solucionando problemas no Edge Image Builder (EIB)</title>
<para>O EIB é usado para criar imagens personalizadas do SUSE Edge.</para>
<itemizedlist>
<title>Problemas comuns</title>
<listitem>
<para><emphasis role="strong">Código incorreto do SCC</emphasis>: assegure que o
código do SCC usado no arquivo de definição do EIB corresponda à versão e
arquitetura do SL Micro.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Dependências ausentes</emphasis>: garanta que não
falte nenhum pacote ou ferramenta no ambiente do build.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Tamanho da imagem incorreto</emphasis>: para as
imagens brutas, o parâmetro <literal>diskSize</literal> é obrigatório e
depende significativamente de que as imagens, os RPMs e outros artefatos
sejam incluídos na imagem.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Permissões</emphasis>: se você armazena um script no
diretório personalizado/de arquivos, assegure que ele tenha permissões de
execução, já que esses arquivos apenas estão disponíveis no momento da
combustão, mas o EIB não faz nenhuma alteração.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Dependências de grupo do sistema
operacional</emphasis>: ao criar uma imagem com usuários e grupos
personalizados, os grupos definidos como "<literal>primaryGroup</literal>"
devem ser explicitamente criados.</para>
</listitem>
<listitem>
<para><emphasis role="strong">As chaves SSH dos usuários do sistema operacional
requerem uma pasta pessoal</emphasis>: ao criar uma imagem com usuários que
têm chaves SSH, também é necessário criar a pasta pessoal com
<literal>createHomeDir=true</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas no Combustion</emphasis>: o EIB usa o
Combustion para a personalização do sistema operacional e da implantação de
todos os outros componentes do SUSE Edge, o que também inclui os scripts
personalizados substituídos na pasta custom/scripts. Observe que o processo
do Combustion é executado com o <literal>initrd</literal>, portanto, o
sistema não é completamente inicializado quando os scripts são executados.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Tamanho da máquina podman</emphasis>: conforme
explicado na seção de dicas e truques do EIB (<xref
linkend="tips-and-tricks"/>), verifique se a máquina podman tem CPU/memória
suficiente para executar o contêiner do EIB em sistemas operacionais não
Linux.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Saída do EIB</emphasis>: a saída do console do
comando <literal>eib build</literal> é fundamental.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do contêiner de build</emphasis>: consulte
os registros do contêiner de build. Eles são gerados no diretório que foi
usado para armazenar os artefatos. Consulte também os <literal>docker
logs</literal> ou os <literal>podman logs</literal> para obter as
informações necessárias.</para>
<note>
<para>Para obter mais informações, consulte <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Debugging</link>
(Depuração).</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Diretórios temporários de build</emphasis>: o EIB
cria diretórios temporários durante o processo de build. Consulte-os para
acessar os registros ou os artefatos intermediários se a saída principal não
for suficiente.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do Combustion</emphasis>: se a imagem que
está sendo criada com o EIB não for inicializada por algum motivo, um shell
root estará disponível. Conecte-se ao console do host (fisicamente, via BMC
etc.) e consulte os registros do Combustion com <literal>journalctl -u
combustion</literal> e, em geral, todos os registros do sistema operacional
com <literal>journalctl</literal>, para encontrar a causa raiz da falha.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Etapas da solução de problemas</title>
<listitem>
<para><emphasis role="strong">Revise a saída de
<literal>eib-build</literal></emphasis>: a mensagem de erro na saída do
console costuma ser bem clara.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique o ambiente do build</emphasis>: garanta
que todos os pré-requisitos para o próprio EIB (por exemplo, docker/podman,
espaço em disco suficiente) sejam atendidos na máquina que executa o EIB.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Analise os registros do contêiner de
build</emphasis>: revise os registros do contêiner com falha para verificar
os erros em mais detalhes (veja acima).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique a configuração do
<literal>eib</literal></emphasis>: revise o arquivo de configuração do
<literal>eib</literal> para verificar se não há erros de ortografia ou
caminhos incorretos para os arquivos de origem ou scripts de build.</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Teste os componentes individualmente</emphasis>: se
o build do EIB envolver scripts ou estágios personalizados, execute-os
separadamente para isolar falhas.</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>Consulte <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Edge
Image Builder Debugging</link> (Depuração do Edge Image Builder).</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-networking">
<title>Solução de problemas da rede de borda (NMC)</title>
<para>O NMC é injetado nas imagens EIB do SL Micro para configurar a rede dos
hosts do Edge no momento da inicialização pelo Combustion. Ele também é
executado no fluxo de trabalho do Metal3 como parte do processo de
inspeção. Os problemas podem ocorrer quando o host é inicializado pela
primeira vez ou no processo de inspeção do Metal3.</para>
<itemizedlist>
<title>Problemas comuns</title>
<listitem>
<para><emphasis role="strong">O host não é inicializado corretamente na primeira
vez</emphasis>: arquivos de definição de rede mal elaborados podem provocar
falha na fase de combustão e, dessa forma, o host perde o shell root.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Os arquivos não são gerados de maneira
apropriada</emphasis>: garanta que os arquivos de rede sigam o formato <link
xl:href="https://nmstate.io/examples.html">NMState</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">As interfaces de rede não são configuradas
corretamente</emphasis>: garanta que os endereços MAC correspondam às
interfaces usadas no host.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Incompatibilidade entre nomes de
interface</emphasis>: o argumento do kernel <literal>net.ifnames=1</literal>
habilita o <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Esquema
de nomenclatura previsível para interfaces de rede</link>, portanto, não
existe mais o <literal>eth0</literal>, porém, outro esquema de nomenclatura,
como o <literal>enp2s0</literal>.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros do Combustion</emphasis>: como o NMC é
usado no momento da execução do Combustion, consulte os registros do
Combustion com o comando <literal>journalctl -u combustion</literal> no host
que está sendo provisionado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do NetworkManager</emphasis>: no fluxo de
trabalho de implantação do Metal<superscript>3</superscript>, o NMC faz
parte da execução do IPA e é executado como uma dependência do serviço
NetworkManager, usando a funcionalidade ExecStartPre do systemd. Consulte os
registros do NetworkManager no host IPA como <literal>journalctl -u
NetworkManager</literal> (consulte a seção de solução de problemas de
provisionamento de rede direcionado (<xref
linkend="troubleshooting-directed-network-provisioning"/>) para saber como
acessar o host quando inicializado com o IPA).</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Etapas da solução de problemas</title>
<listitem>
<para><emphasis role="strong">Verifique a sintaxe do yaml</emphasis>: os arquivos
de configuração do NMC são yaml. Verifique a sintaxe adequada com
<literal>yamllint</literal> ou ferramentas similares.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Execute o NMC manualmente</emphasis>: como o NMC faz
parte do contêiner EIB, é possível usar o comando podman local para depurar
problemas.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Crie uma pasta temporária para armazenar os arquivos nmc.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p ${HOME}/tmp/foo</screen>
</listitem>
<listitem>
<para>Salve os arquivos nmc nesse local.</para>
<screen language="shell" linenumbering="unnumbered">❯ tree --noreport ${HOME}/tmp/foo
/Users/johndoe/tmp/foo
├── host1.example.com.yaml
└── host2.example.com.yaml</screen>
</listitem>
<listitem>
<para>Execute o contêiner EIB com o NMC como ponto de entrada e o comando generate
para executar as mesmas tarefas que o NMC faz no momento da execução do
Combustion:</para>
<screen language="shell" linenumbering="unnumbered">podman run -it --rm -v ${HOME}/tmp/foo:/tmp/foo:Z --entrypoint=/usr/bin/nmc registry.suse.com/edge/3.3/edge-image-builder:1.2.0 generate --config-dir /tmp/foo --output-dir /tmp/foo/

[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host2.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host1.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc] Successfully generated and stored network config</screen>
</listitem>
<listitem>
<para>Observe os registros e os arquivos que são gerados na pasta temporária.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-phone-home-scenarios">
<title>Solucionando problemas em cenários "phone home"</title>
<para>Os cenários "phone home" envolvem o uso do Elemental para conectar-se com o
cluster de gerenciamento e o EIB criar uma imagem de sistema operacional que
inclua os bits do elemental-registration. Pode haver problemas quando o host
é inicializado pela primeira vez, durante o processo de criação do EIB ou na
tentativa de registro no cluster de gerenciamento.</para>
<itemizedlist>
<title>Problemas comuns</title>
<listitem>
<para><emphasis role="strong">Falha no registro do sistema</emphasis>: o nó não é
registrado na IU. Assegure que o host seja inicializado de maneira
apropriada e possa se comunicar com o Rancher, o relógio esteja sincronizado
e os serviços do Elemental estejam ativos.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Falha no provisionamento do sistema</emphasis>: o nó
é registrado, mas não é provisionado. Assegure que o host possa se comunicar
com o Rancher, o relógio esteja sincronizado e os serviços do Elemental
estejam ativos.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros do sistema</emphasis>:
<literal>journalctl</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do elemental-system-agent</emphasis>:
<literal>journalctl -u elemental-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do K3s/RKE2</emphasis>:
<literal>journalctl -u k3s ou journalctl -u rke2-server</literal> (ou
<literal>rke2-agent</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">Pod do operador Elemental</emphasis>:
<literal>kubectl logs -n cattle-elemental-system -l
app=elemental-operator</literal></para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Etapas da solução de problemas</title>
<listitem>
<para><emphasis role="strong">Revise os registros</emphasis>: consulte os
registros do pod do operador Elemental para ver se há problemas. Verifique
os registros do host se o nó for inicializado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique o MachineRegistration e o TPM</emphasis>:
por padrão, o TPM é usado para <link
xl:href="https://elemental.docs.rancher.com/authentication/">autenticação</link>,
mas há alternativas para hosts sem o TPM.</para>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-directed-network-provisioning">
<title>Solução de problemas de provisionamento de rede direcionado</title>
<para>Os cenários de provisionamento de rede direcionado envolvem o uso de
elementos do Metal<superscript>3</superscript> e da CAPI para provisionar o
cluster downstream. O EIB também é incluído para criar uma imagem do sistema
operacional. Pode haver problemas quando o host é inicializado pela primeira
vez ou durante os processos de inspeção ou provisionamento.</para>
<itemizedlist>
<title>Problemas comuns</title>
<listitem>
<para><emphasis role="strong">Firmware antigo</emphasis>: garanta que qualquer
firmware diferente usado nos hosts físicos esteja atualizado. Isso inclui o
firmware BMC, já que às vezes o Metal<superscript>3</superscript> <link
xl:href="https://book.metal3.io/bmo/supported_hardware#redfish-and-its-variants">requer
um específico/atualizado</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Falha no provisionamento com erros de
SSL</emphasis>: se o servidor web que envia as imagens usa https, o
Metal<superscript>3</superscript> precisa ser configurado para injetar e
confiar no certificado da imagem do IPA. Consulte a pasta do Kubernetes
(<xref linkend="mgmt-cluster-kubernetes-folder"/>) para saber como incluir
um arquivo <literal>ca-additional.crt</literal> no gráfico do
Metal<superscript>3</superscript>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas no certificado ao inicializar os hosts com
IPA</emphasis>: alguns fornecedores de servidor verificam a conexão SSL ao
anexar imagens ISO de mídia virtual ao BMC, o que pode causar um problema
porque os certificados gerados para a implantação do Metal3 são
autoassinados. É possível que o host seja inicializado, mas acabe entrando
em um shell UEFI. Consulte Desabilitando TLS para anexo de ISO de mídia
virtual (<xref linkend="disabling-tls-for-virtualmedia-iso-attachment"/>)
para saber como corrigir isso.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Referência a nome ou rótulo incorreto</emphasis>: se
o cluster faz referência ao nome ou rótulo incorreto de um nó, ele aparece
como implantado, mas o BMH continua como "Disponível". Revise as referências
nos objetos envolvidos dos BMHs.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas na comunicação com o BMC</emphasis>:
certifique-se de que os pods do Metal<superscript>3</superscript> executados
no cluster de gerenciamento possam acessar o BMC dos hosts que estão sendo
provisionados (normalmente, a rede do BMC é muito restrita).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Estado do host bare metal incorreto</emphasis>: o
objeto BMH passa por diferentes estados (inspeção, preparação,
provisionamento etc.) durante seu ciclo de vida <link
xl:href="https://book.metal3.io/bmo/state_machine">Ciclo de vida da máquina
de estado</link> (em inglês). Se for detectado um estado incorreto,
verifique o campo <literal>status</literal> do objeto BMH, pois ele contém
mais informações, como <literal>kubectl get bmh &lt;nome&gt; -o
jsonpath=’{.status}’| jq</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">O host não é provisionado</emphasis>: em caso de
falha no desprovisionamento de um host, é possível tentar removê-lo depois
de adicionar a anotação "detached" ao objeto BMH desta forma:
<literal>kubectl annotate bmh/&lt;BMH&gt;
baremetalhost.metal3.io/detached=””</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Erros na imagem</emphasis>: verifique se a imagem
que está sendo criada com o EIB para o cluster downstream está disponível,
tem um checksum apropriado e não é grande demais para descompactar ou para o
disco.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Incompatibilidade de tamanho de disco</emphasis>:
por padrão, o disco não expande até se preencher por completo. Conforme
explicado na seção do script growfs (<xref linkend="growfs-script"/>), é
necessário incluir um script growfs na imagem que está sendo criada com o
EIB para os hosts de cluster downstream.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Processo de limpeza travado</emphasis>: o processo
de limpeza é repetido várias vezes. Se a limpeza não for mais possível
devido a um problema com o host, desabilite primeiro a limpeza definindo o
campo <literal>automatedCleanMode</literal> como <literal>disabled</literal>
no objeto BMH.</para>
<warning>
<para>Não é recomendado remover manualmente o finalizador quando o processo de
limpeza está falhando ou levando mais tempo do que o desejado. Se você fizer
isso, o registro do host será removido do Kubernetes, mas ficará no
Ironic. A ação executada no momento continua em segundo plano, e a tentativa
de adicionar o host novamente pode falhar devido ao conflito.</para>
</warning>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas nos pods do Metal3/Rancher
Turtles/CAPI</emphasis>: o fluxo de implantação de todos os componentes
obrigatórios é:</para>
<itemizedlist>
<listitem>
<para>O controlador Rancher Turtles implanta o controlador do operador CAPI.</para>
</listitem>
<listitem>
<para>O controlador do operador CAPI depois implanta os controladores do provedor
(núcleo CAPI, CAPM3 e plano de controle/inicialização RKE2).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Verifique se todos os pods estão sendo executados corretamente e consulte os
registos caso não estejam.</para>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros do
Metal<superscript>3</superscript></emphasis>: consulte os registros dos
diferentes pods.</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n metal3-system -l app.kubernetes.io/component=baremetal-operator
kubectl logs -n metal3-system -l app.kubernetes.io/component=ironic</screen>
<note>
<para>O pod metal3-ironic contém pelo menos 4 contêineres diferentes
(<literal>ironic-httpd</literal>,` ironic-log-watch`,
<literal>ironic</literal> &amp; <literal>ironic-ipa-downloader</literal>
(init)) no mesmo pod. Use o sinalizador <literal>-c</literal> ao usar o
comando <literal>kubectl logs</literal> para verificar os registros de cada
um dos contêineres.</para>
</note>
<note>
<para>O contêiner <literal>ironic-log-watch</literal> expõe os registros do
console dos hosts após a inspeção/provisionamento, desde que a conectividade
de rede permita o envio desses registros de volta ao cluster de
gerenciamento. Isso pode ser útil quando há erros de provisionamento, mas
você não tem acesso direto aos registros do console do BMC.</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do Rancher Turtles</emphasis>: consulte os
registros dos diferentes pods.</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n rancher-turtles-system -l control-plane=controller-manager
kubectl logs -n rancher-turtles-system -l app.kubernetes.io/name=cluster-api-operator
kubectl logs -n rke2-bootstrap-system -l cluster.x-k8s.io/provider=bootstrap-rke2
kubectl logs -n rke2-control-plane-system -l cluster.x-k8s.io/provider=control-plane-rke2
kubectl logs -n capi-system -l cluster.x-k8s.io/provider=cluster-api
kubectl logs -n capm3-system -l cluster.x-k8s.io/provider=infrastructure-metal3</screen>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do BMC</emphasis>: em geral, os BMCs têm
uma IU em que é possível realizar grande parte da interação. Normalmente, há
uma seção "registros" na qual observar possíveis problemas (imagem
inacessível, falhas de hardware etc.).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros do console</emphasis>: conecte-se ao
console BMC (pela IU da web do BMC, série etc.) e verifique se há erros nos
registros gravados.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Etapas da solução de problemas</title>
<listitem>
<para><emphasis role="strong">Verifique o status de
<literal>BareMetalHost</literal></emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>kubectl get bmh -A</literal> mostra o estado atual. Procure por
<literal>provisioning</literal>, <literal>ready</literal>,
<literal>error</literal>, <literal>registering</literal>.</para>
</listitem>
<listitem>
<para><literal>kubectl describe bmh -n &lt;namespace&gt;
&lt;nome_do_bmh&gt;</literal> retorna os eventos detalhados e as condições
que explicam o motivo da paralisação do BMH.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Teste a conectividade do RedFish</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Use <literal>curl</literal> no plano de controle do
Metal<superscript>3</superscript> para testar a conectividade com os BMCs
pelo Redfish.</para>
</listitem>
<listitem>
<para>Garanta que as credenciais corretas do BMC sejam fornecidas na definição de
<literal>BareMetalHost-Secret</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique o status dos pods
turtles/CAPI/metal3</emphasis>: garanta que os contêineres no cluster de
gerenciamento estejam em funcionamento: <literal>kubectl get pods -n
metal3-system</literal> e <literal>kubectl get pods -n
rancher-turtles-system</literal> (veja também
<literal>capi-system</literal>, <literal>capm3-system</literal>,
<literal>rke2-bootstrap-system</literal> e
<literal>rke2-control-plane-system</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique se o endpoint do Ironic é acessível ao
host provisionado</emphasis>: o host provisionado precisa acessar o endpoint
do Ironic para retornar os relatórios ao
Metal<superscript>3</superscript>. Verifique o IP com <literal>kubectl get
svc -n metal3-system metal3-metal3-ironic</literal> e tente acessá-lo via
<literal>curl/nc</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique se a imagem do IPA é acessível ao
BMC</emphasis>: o IPA é fornecido pelo endpoint do Ironic e precisa ser
acessível ao BMC porque é usado como CD virtual.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique se a imagem do sistema operacional é
acessível ao host provisionado</emphasis>: a imagem usada para provisionar o
host precisa ser acessível ao próprio host (ao executar o IPA) já que ela
será baixada temporariamente e gravada no disco.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Examine os registros do componente
Metal<superscript>3</superscript></emphasis>: veja acima.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Acione novamente a inspeção do BMH</emphasis>: se
houver falha em uma inspeção ou se o hardware de um host disponível for
modificado, um novo processo de inspeção poderá ser acionado anotando o
objeto BMH com <literal>inspect.metal3.io: ""</literal>. Consulte o guia
<link
xl:href="https://book.metal3.io/bmo/inspect_annotation">Metal<superscript>3</superscript>
Controlling inspection</link> (Inspeção de controle do Metal³) para obter
mais informações.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Console do IPA bare metal</emphasis>: para
solucionar problemas do IPA, há algumas alternativas:</para>
<itemizedlist>
<listitem>
<para>Habilite o "login automático". Isso habilita o login automático do usuário
root ao se conectar ao console do IPA.</para>
<warning>
<para>Isso é apenas para fins de depuração, já que concede acesso total ao host.</para>
</warning>
<para>Para habilitar o login automático, o valor
<literal>global.ironicKernelParams</literal> do Helm do Metal3 deve ser
assim: <literal>console=ttyS0 suse.autologin=ttyS0</literal> (dependendo do
console, <literal>ttyS0</literal> pode ser alterado). Em seguida, é
necessário reimplantar o gráfico do
Metal<superscript>3</superscript>. (Observe que <literal>ttyS0</literal> é
um exemplo, ele deve corresponder ao terminal real, por exemplo, em muitos
casos, pode ser <literal>tty1</literal> em bare metal. Para confirmar isso,
observe a saída do console do ramdisk IPA na inicialização, em que
<literal>/etc/issue</literal> mostra o nome do console).</para>
<para>Outra maneira de fazer isso é alterar o parâmetro
<literal>IRONIC_KERNEL_PARAMS</literal> no mapa de configuração
<literal>ironic-bmo</literal> do namespace
<literal>metal3-system</literal>. Isso pode ser mais fácil porque é feito
pela edição de <literal>kubectl</literal>, mas será substituído ao atualizar
o gráfico. Depois disso, o pod do Metal<superscript>3</superscript>
precisará ser reiniciado com <literal>kubectl delete pod -n metal3-system -l
app.kubernetes.io/component=ironic</literal>.</para>
</listitem>
<listitem>
<para>Injete uma chave SSH para o usuário root no IPA.</para>
<warning>
<para>Isso é apenas para fins de depuração, já que concede acesso total ao host.</para>
</warning>
<para>Para injetar a chave SSH para o usuário root, o valor
<literal>debug.ironicRamdiskSshKey</literal> do Helm do
Metal<superscript>3</superscript> deve ser usado. Em seguida, reimplante o
gráfico do Metal<superscript>3</superscript>.</para>
<para>Outra maneira de fazer isso é alterar o parâmetro
<literal>IRONIC_RAMDISK_SSH_KEY</literal> no <literal>ironic-bmo
configmap</literal> do namespace <literal>metal3-system</literal>. Isso pode
ser mais fácil porque é feito pela edição de <literal>kubectl</literal>, mas
será substituído durante a atualização do gráfico. Em seguida, o pod do
Metal<superscript>3</superscript> precisa ser reiniciado com
<literal>kubectl delete pod -n metal3-system -l
app.kubernetes.io/component=ironic</literal></para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>Consulte os guias de <link
xl:href="https://cluster-api.sigs.k8s.io/user/troubleshooting">solução de
problemas da CAPI</link> e de <link
xl:href="https://book.metal3.io/troubleshooting">solução de problemas do
Metal<superscript>3</superscript></link>.</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-other-components">
<title>Solução de problemas de outros componentes</title>
<para>Consulte os guias de solução de problemas de outros componentes do SUSE Edge
na respectiva documentação oficial:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://documentation.suse.com/smart/micro-clouds/html/SLE-Micro-5.5-admin/index.html#id-1.10">Solução
de problemas do SUSE Linux Micro</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.rke2.io/known_issues">Problemas conhecidos do
RKE2</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.k3s.io/known-issues">Problemas conhecidos do
K3s</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/troubleshooting/general-troubleshooting">Solução
de problemas gerais do Rancher</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://documentation.suse.com/multi-linux-manager/5.1/en/docs/administration/troubleshooting/tshoot-intro.html">Solução
de problemas do SUSE Multi-Linux Manager</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://elemental.docs.rancher.com/troubleshooting-support/">Suporte
do Elemental</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://turtles.docs.rancher.com/turtles/stable/en/troubleshooting/troubleshooting.html">Solução
de problemas do Rancher Turtles</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://longhorn.io/docs/1.9.1/troubleshoot/troubleshooting/">Solução
de problemas do Longhorn</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://open-docs.neuvector.com/next/troubleshooting/troubleshooting/">Solução
de problemas do Neuvector</link></para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/troubleshooting">Solução de
problemas do Fleet</link></para>
</listitem>
</itemizedlist>
<para>Você também pode consultar o <link
xl:href="https://www.suse.com/support/kb/">SUSE Knowledgebase</link>.</para>
</chapter>
<chapter xml:id="collecting-diagnostics-for-support">
<title>Coletando diagnósticos para o suporte</title>
<para>Ao contatar o suporte da SUSE, é essencial passar todas as informações de
diagnóstico.</para>
<itemizedlist>
<title>Informações essenciais para coletar</title>
<listitem>
<para><emphasis role="strong">Descrição detalhada do problema</emphasis>: o que
aconteceu, quando aconteceu, o que você estava fazendo, qual era o
comportamento esperado e qual foi o comportamento real?</para>
</listitem>
<listitem>
<para><emphasis role="strong">Etapas de reprodução</emphasis>: você pode
reproduzir o problema de maneira confiável? Se puder, faça uma lista das
etapas exatas.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Versões dos componentes</emphasis>: as versões do
SUSE Edge e dos componentes (RKE2/K3, EIB,
Metal<superscript>3</superscript>, Elemental etc.).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros relevantes</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Saída do <literal>journalctl</literal> (filtrada por serviço, se possível,
ou os registros completos de inicialização)</para>
</listitem>
<listitem>
<para>Registros do pod Kubernetes (registros do kubectl)</para>
</listitem>
<listitem>
<para>Registros do componente Metal³/Elemental</para>
</listitem>
<listitem>
<para>Registros do build do EIB e outros registros</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Informações do sistema</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>uname -a</literal></para>
</listitem>
<listitem>
<para><literal>df -h</literal></para>
</listitem>
<listitem>
<para><literal>ip a</literal></para>
</listitem>
<listitem>
<para><literal>/etc/os-release</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Arquivos de configuração</emphasis>: arquivos de
configuração relevantes para Elemental, Metal<superscript>3</superscript> e
EIB, como valores de gráficos Helm, mapas de configuração etc.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Informações do Kubernetes</emphasis>: nós, serviços,
implantações etc.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Objetos do Kubernetes afetados</emphasis>: BMH,
MachineRegistration etc.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Como fazer a coleta</title>
<listitem>
<para><emphasis role="strong">Dos registros</emphasis>: redirecione a saída do
comando para os arquivos (por exemplo, <literal>journalctl -u k3s &gt;
k3s_logs.txt</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Dos recursos do Kubernetes</emphasis>: use
<literal>kubectl get &lt;recurso&gt; -o yaml &gt;
&lt;nome_do_recurso&gt;.yaml</literal> para obter as definições detalhadas
do YAML.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Das informações do sistema</emphasis>: colete a
saída dos comandos listados acima.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Do SL Micro</emphasis>: consulte a documentação do
<link
xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-adm-support-slemicro.html">Guia
de Solução de Problemas do SUSE Linux Micro</link> sobre como reunir as
informações do sistema para o suporte com <literal>supportconfig</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Do RKE2/Rancher</emphasis>: consulte o artigo <link
xl:href="https://www.suse.com/support/kb/doc/?id=000020191">The Rancher v2.x
Linux log collector script</link> para executar o script do coletor de
registros do Linux para Rancher v2.x.</para>
</listitem>
</itemizedlist>
<formalpara>
<title>Contatar o suporte</title>
<para>Leia o artigo disponível em <link
xl:href="https://www.suse.com/support/kb/doc/?id=000019452">How-to
effectively work with SUSE Technical Support</link> (Como trabalhar de
maneira eficiente com o suporte técnico da SUSE) e o manual de suporte
localizado em <link xl:href="https://www.suse.com/support/handbook/">SUSE
Technical Support Handbook</link> (Manual de Suporte Técnico da SUSE) para
obter mais detalhes sobre como contatar o suporte da SUSE.</para>
</formalpara>
</chapter>
</part>
<part xml:id="id-appendix">
<title>Apêndice</title>
<chapter xml:id="id-release-notes">
<title>Notas de lançamento</title>
<section xml:id="release-notes">
<title>Resumo</title>
<para>O SUSE Edge 3.4 é uma solução completa, de estreita integração e amplamente
validada, capaz de abordar os desafios exclusivos da implantação da
infraestrutura e dos aplicativos nativos de nuvem na borda. Seu foco
principal é oferecer uma plataforma consistente, porém altamente flexível,
escalável e segura, que envolve a criação de imagem da implantação inicial,
o provisionamento e a integração de nós, a implantação de aplicativos, a
observabilidade e o gerenciamento do ciclo de vida.</para>
<para>A solução foi criada com a ideia de que não existe uma plataforma de borda
do tipo "tamanho único" porque os requisitos e as expectativas dos clientes
variam de maneira significativa. As implantações de borda nos levam a
resolver (e sempre aprimorar) alguns dos problemas mais desafiadores, como
escalabilidade massiva, disponibilidade de rede restrita, limitações de
espaço físico, novas ameaças à segurança e vetores de ataque, variações na
arquitetura de hardware e nos recursos de sistema, a necessidade de
implantar e estabelecer interface com infraestruturas e aplicativos legados
e soluções de clientes com durações estendidas.</para>
<para>O SUSE Edge foi criado do zero, com base no melhor software de código
aberto, e condiz com os nossos 30 anos de história como provedores de
plataformas SUSE Linux seguras, estáveis e certificadas e com a nossa
experiência em oferecer gerenciamento Kubernetes altamente escalável e
repleto de recursos com o nosso portfólio Rancher. O SUSE Edge é
fundamentado nesses recursos para entregar funcionalidades que atendem a
inúmeros segmentos de mercado, como varejo, medicina, transporte, logística,
telecomunicações, manufatura inteligente e IoT industrial.</para>
<para>Para obter mais informações sobre as atualizações do ciclo de vida de
suporte do produto para o SUSE Edge, consulte <link
xl:href="https://www.suse.com/lifecycle/#suse-edge-33">Ciclo de vida de
suporte do produto</link>.</para>
<note>
<para>O SUSE Telco Cloud (antes conhecido como SUSE Edge for Telco) deriva do SUSE
Edge, com otimizações e componentes adicionais que possibilitam que a
plataforma atenda aos requisitos constantes nos casos de uso de
telecomunicações. A menos que claramente indicado, todas as Notas de
lançamento são aplicáveis ao SUSE Edge 3.4 e ao SUSE Telco Cloud 3.4.</para>
</note>
</section>
<section xml:id="id-about">
<title>Sobre</title>
<para>As Notas de lançamento, a menos que claramente especificado e explicado, são
idênticas em todas as arquiteturas; e a versão mais recente, junto com as
Notas de lançamento de todos os outros produtos SUSE, está sempre disponível
online em <link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>.</para>
<para>As entradas são listadas apenas uma vez, mas é possível fazer referência a
elas em vários locais, se forem importantes e pertencerem a mais de uma
seção. Normalmente, as Notas de lançamento listam somente as alterações
feitas entre dois lançamentos subsequentes. Determinadas entradas
importantes das Notas de lançamento de versões anteriores dos produtos podem
se repetir. Para facilitar a identificação dessas entradas, elas incluem uma
observação a esse respeito.</para>
<para>No entanto, as entradas repetidas são fornecidas apenas como
conveniência. Portanto, se você pular um ou mais lançamentos, consulte as
Notas de lançamento também dos lançamentos que forem pulados. Se você
estiver apenas lendo as Notas de lançamento referentes ao lançamento atual,
poderá perder alterações importantes que podem afetar o comportamento do
sistema. As versões do SUSE Edge são definidas como x.y.z, em que "x" indica
a versão principal, "y" indica a versão secundária e "z" indica a versão do
patch, também conhecida como "z-stream". Os ciclos de vida do produto SUSE
Edge são definidos com base na versão secundária especificada, por exemplo,
"3.4", mas acompanham as atualizações de patch subsequentes ao longo do seu
ciclo de vida, por exemplo, "3.4.1".</para>
<note>
<para>As versões z-stream do SUSE Edge são estreitamente integradas e foram
testadas na íntegra como uma pilha com controle de versão. O upgrade de
qualquer componente individual para versões diferentes das que estão
listadas acima pode resultar em tempo de inatividade do sistema. É possível
executar clusters Edge em configurações não testadas, mas não é recomendado
e pode levar mais tempo para conseguir uma resolução por meio dos canais de
suporte.</para>
</note>
</section>
<section xml:id="release-notes-3-4-0">
<title>Versão 3.4.0</title>
<para>Data da disponibilidade: 24 de setembro de 2025</para>
<para>Data de encerramento total do suporte: 20 de março de 2026</para>
<para>Data de encerramento do suporte de manutenção: 20 de setembro de 2027</para>
<para>Fim do serviço: 21 de setembro de 2027</para>
<para>Resumo: SUSE Edge 3.4.0 é a primeira versão no fluxo de versões do SUSE Edge
3.4.</para>
<section xml:id="id-new-features">
<title>Novos recursos</title>
<itemizedlist>
<listitem>
<para>Atualizado para o Kubernetes 1.33 e o Rancher Prime 2.12</para>
</listitem>
<listitem>
<para>Versões atualizadas do Rancher Turtles, da Cluster API e do Metal3/Ironic</para>
</listitem>
<listitem>
<para>Atualizado para o SUSE Storage (Longhorn) 1.9.1 <link
xl:href="https://longhorn.io/docs/1.9.1/">Notas de lançamento</link></para>
</listitem>
<listitem>
<para>Agora é possível uma implantação mais flexível dos clusters downstream
AArch64 por meio do fluxo de provisionamento de rede direcionado. Consulte o
<xref linkend="atip-automated-provisioning"/> para obter mais detalhes.</para>
</listitem>
<listitem>
<para>Agora há suporte completo para implantação de clusters de pilha dupla (ipv6
de pilha única continua na <xref linkend="tech-previews"/>)</para>
</listitem>
<listitem>
<para>O modo BGP para MetalLB agora está disponível como prévia de
tecnologia. Consulte a <xref linkend="tech-previews"/> e o <xref
linkend="guides-metallb-k3s-l3"/> para obter mais detalhes.</para>
</listitem>
<listitem>
<para>O Edge Image Builder foi atualizado para 1.3.0. Consulte as <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/RELEASE_NOTES.md">Notas
de lançamento upstream</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>Correções de bugs e de segurança</title>
<itemizedlist>
<listitem>
<para>O Rancher Prime 2.12 contém várias correções de bugs. <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">Notas de
lançamento upstream do Rancher</link></para>
</listitem>
<listitem>
<para>O Rancher Prime 2.12 contém uma correção de problemas relacionados à
AppVersion ao determinar a disponibilidade de upgrade de extensão, que
afetava os gráficos do Edge. <link
xl:href="https://github.com/rancher/dashboard/issues/14204">Problema
upstream</link></para>
</listitem>
<listitem>
<para>O SUSE Storage (Longhorn) 1.9.1 contém várias correções de bugs. <link
xl:href="https://github.com/longhorn/longhorn/releases/tag/v1.9.1">Correções
de bugs upstream do Longhorn</link></para>
</listitem>
<listitem>
<para>O gráfico atualizado do Metal<superscript>3</superscript> corrige um
problema em que o MAC incorreto poderia ser coletado para interfaces
vinculadas durante a inspeção. <link
xl:href="https://bugs.launchpad.net/ironic-python-agent/+bug/2103450">Problema
upstream do IPA</link></para>
</listitem>
<listitem>
<para>O gráfico atualizado do Metal<superscript>3</superscript> corrige um
problema em que a implantação poderia não ser reiniciada corretamente nas
atualizações de ConfigMap. <link
xl:href="https://github.com/suse-edge/charts/issues/219">Problema
upstream</link></para>
</listitem>
<listitem>
<para>A atualização do Rancher Turtles inclui uma correção que resolve o problema
em que o provedor CAPI RKE2 não aplicava as ownerReferences de
MachineTemplate. <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/500">Problema
upstream</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-known-issues-8">
<title>Problemas conhecidos</title>
<warning>
<para>Se você está implantando novos clusters, siga o <xref
linkend="guides-kiwi-builder-images"/> para primeiro criar as novas imagens,
já que esta é a primeira etapa necessária para criar clusters para ambas as
arquiteturas AMD64/Intel 64 e AArch64, além dos clusters de gerenciamento e
downstream.</para>
</warning>
<itemizedlist>
<listitem>
<para>Na implantação por meio do Edge Image Builder, poderá haver falha nos
manifestos <literal>HelmChartConfigs</literal> se eles forem colocados no
diretório de configuração <literal>kubernetes/manifests</literal>. Em vez
disso, a recomendação é colocar os <literal>HelmChartConfigs</literal> em
<literal>/var/lib/rancher/{rke2/k3s}/server/manifests/</literal> usando a
interface os-files do EIB. Consulte a <xref
linkend="mgmt-cluster-directory-structure"/> para ver um exemplo. Se isso
não for feito, os nós poderão ficar no estado <literal>NotReady</literal> na
inicialização inicial, conforme abordado no <link
xl:href="https://github.com/rancher/rke2/issues/8357">problema #8357 do
RKE2</link></para>
</listitem>
<listitem>
<para>Nas versões do RKE2/K3s 1.31, 1.32 e 1.33, o diretório
<literal>/etc/cni</literal> usado para armazenar as configurações de CNI
podem não acionar uma notificação sobre os arquivos que estão sendo gravados
nele para o <literal>containerd</literal> devido a determinadas condições
relacionadas ao <literal>overlayfs</literal> (consulte o <link
xl:href="https://github.com/rancher/rke2/issues/8356">problema #8356 do
RKE2</link>). Isso, por sua vez, acaba travando a implantação do RKE2/K3s na
espera pela inicialização da CNI e os nós do RKE2/K3s no estado
<literal>NotReady</literal>. Isso pode ser observado no nível do nó com
<literal>kubectl describe node &lt;nó_afetado&gt;</literal>:</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Conditions:
  Type   Status  LastHeartbeatTime                LastTransitionTime               Reason           Message
  ----   ------  -----------------                ------------------               ------           -------
  Ready  False   Thu, 05 Jun 2025 17:41:28 +0000  Thu, 05 Jun 2025 14:38:16 +0000  KubeletNotReady  container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized</screen>
<para>Como solução alternativa, é possível montar um volume tmpfs no diretório
<literal>/etc/cni</literal> antes que o RKE2 seja iniciado. Isso evita o uso
do overlayfs, que faz com que o containerd perca notificações e que as
configurações sejam regravadas sempre que o nó é reiniciado e os
initcontainers dos pods são novamente executados. Se você usa o EIB, isso
pode ser um script <literal>04-tmpfs-cni.sh</literal> no diretório
<literal>custom/scripts</literal> (conforme explicado aqui [<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom">https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom</link>])
com esta aparência:</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash
mkdir -p /etc/cni
mount -t tmpfs -o mode=0700,size=5M tmpfs /etc/cni
echo "tmpfs /etc/cni tmpfs defaults,size=5M,mode=0700 0 0" &gt;&gt; /etc/fstab</screen>
<itemizedlist>
<listitem>
<para>Na integração de hosts remotos usando o Elemental, pode haver uma condição
de corrida entre <literal>dbus.service</literal> e
<literal>elemental-system-agent.service</literal>, o que resulta em falha
para iniciar o <literal>rancher-system-agent.service</literal> no host
remoto, com erros similares ao mostrado abaixo. (Consulte o <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/784">problema
#784 do Edge Image Builder</link> para obter detalhes.)</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Sep 19 19:38:07 elementalvm elemental-system-agent[3671]: time="2025-09-19T19:38:07Z" level=info msg="[6b20fe64c854da2639804884b34129bb8f718eb59578111da58d9de1509c24db_1:stderr]: Failed to restart rancher-system-agent.service: Message recipient disconnected from message bus without replying"</screen>
<para>Como solução alternativa, é possível criar um arquivo de substituição do
systemd como este:</para>
<screen language="bash" linenumbering="unnumbered">[Unit]
Wants=dbus.service network-online.target
After=dbus.service network-online.target time-sync.target

[Service]
ExecStartPre=/bin/bash -c 'echo "Waiting for dbus to become active..." | systemd-cat -p info -t elemental-system-agent; sleep 15; timeout 300 bash -c "while ! systemctl is-active --quiet dbus.service; do sleep 15; done"'</screen>
<para>E é possível usar um script personalizado chamado
<literal>30a-copy-elemental-system-agent-override.sh</literal> para inserir
a substituição em
<literal>/etc/systemd/system/elemental-system-agent.service.d</literal>
antes da execução do script <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/pkg/combustion/templates/31-elemental-register.sh.tpl">31-elemental-register.sh</link>
do EIB durante a fase de combustão.</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash

/bin/mkdir -p /etc/systemd/system/elemental-system-agent.service.d
/bin/cp -f elemental-system-agent-override.conf /etc/systemd/system/elemental-system-agent.service.d/override.conf</screen>
</section>
<section xml:id="id-component-versions">
<title>Versões dos componentes</title>
<para>A tabela a seguir descreve os componentes individuais que compõem o
lançamento 3.4.0, incluindo a versão, a versão do gráfico Helm (se
aplicável) e de onde obter o artefato lançado no formato binário. Consulte a
documentação associada para ver exemplos de uso e implantação.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Nome</para></entry>
<entry align="left" valign="top"><para>Versão</para></entry>
<entry align="left" valign="top"><para>Versão do gráfico Helm</para></entry>
<entry align="left" valign="top"><para>Local do artefato (URL/imagem)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Linux Micro</para></entry>
<entry align="left" valign="top"><para>6.1 (mais recente)</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">Página de downloads
do SUSE Linux Micro</link><?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso (sha256
70b9be28f2d92bc3b228412e4fc2b1d5026e691874b728e530b8063522158854)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-SelfInstall-GM.install.iso (sha256
9ce83e4545d4b36c7c6a44f7841dc3d9c6926fe32dbff694832e0fbd7c496e9d)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-GM.raw.xz (sha256
36e3efa55822113840dd76fdf6914e933a7b7e88a1dce5cb20c424ccf2fb4430)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-GM.raw.xz (sha256
2ee66735da3e1da107b4878e73ae68f5fb7309f5ec02b5dfdb94e254fda8415e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Multi-Linux Manager</para></entry>
<entry align="left" valign="top"><para>5.0.5</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">Página de
downloads do SUSE Multi-Linux Manager</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.33.3%2Bk3s1">Versão
upstream do K3s</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.33.3%2Brke2r1">Versão
upstream do RKE2</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://charts.rancher.com/server-charts/prime/index.yaml">Repositório
Helm do Rancher Prime</link><?asciidoc-br?> <link
xl:href="https://github.com/rancher/rancher/releases/download/v2.12.1/rancher-images.txt">Imagens
do contêiner do Rancher 2.12.1</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Storage (Longhorn)</para></entry>
<entry align="left" valign="top"><para>1.9.1</para></entry>
<entry align="left" valign="top"><para>107.0.0+up1.9.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositório de gráficos
Helm do Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.61<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Security</para></entry>
<entry align="left" valign="top"><para>5.4.5</para></entry>
<entry align="left" valign="top"><para>107.0.0+up2.8.7</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositório de gráficos
Helm do Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/neuvector-controller:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-enforcer:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-manager:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-compliance-config:1.0.6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-registry-adapter:0.1.8<?asciidoc-br?>
registry.suse.com/rancher/neuvector-scanner:6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-updater:0.0.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles (CAPI)</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles:304.0.6_up0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-controller:v1.10.2<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-ipam-controller:v1.10.2<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-controller:v1.10.5<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Recursos de isolamento do Rancher Turtles</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles-airgap-resources:304.0.6_up0.24.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>0.11.5</para></entry>
<entry align="left" valign="top"><para>304.0.16+up0.12.6</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metal3:304.0.16_up0.12.6<?asciidoc-br?>
registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic:29.0.4.3<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic-ipa-downloader:3.0.9<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metallb:304.0.0_up0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-controller:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-speaker:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.4/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.7.3</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Extensão de dashboard do Elemental</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/4.0.0/charts/elemental/3.0.1">Gráfico
Helm da extensão do Elemental</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/edge-image-builder:1.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.3">Versão
upstream do NMConfigurator</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.5.2</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-operator:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-api:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-controller:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-handler:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Extensão de dashboard KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.3.2</para></entry>
<entry align="left" valign="top"><para>304.0.3+up1.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt-dashboard-extension:304.0.3_up1.3.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.62.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/cdi:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-operator:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.3.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.3.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/endpoint-copier-operator:304.0.1_up0.3.0<?asciidoc-br?>
registry.suse.com/edge/3.4/endpoint-copier-operator:0.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (descontinuado)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/akri:304.0.0_up0.12.20<?asciidoc-br?>
registry.suse.com/edge/charts/akri-dashboard-extension:304.0.0_up1.3.1<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SR-IOV Network Operator</para></entry>
<entry align="left" valign="top"><para>1.5.0</para></entry>
<entry align="left" valign="top"><para>304.0.2+up1.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/sriov-network-operator:304.0.2_up1.5.0<?asciidoc-br?>
registry.suse.com/edge/charts/sriov-crd:304.0.2_up1.5.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.16.0</para></entry>
<entry align="left" valign="top"><para>107.0.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositório de gráficos
Helm do Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.16.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Controller de upgrade</para></entry>
<entry align="left" valign="top"><para>0.1.1</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.1.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/upgrade-controller:304.0.1_up0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/upgrade-controller:0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/kubectl:1.33.4<?asciidoc-br?>
registry.suse.com/edge/3.4/release-manifest:3.4.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Construtor Kiwi</para></entry>
<entry align="left" valign="top"><para>10.2.12.0</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-deprecated-features">
<title>Recursos descontinuados</title>
<para>Exceto se especificado de outra forma, elas são aplicáveis à versão 3.4.0 e
a todas as versões z-stream subsequentes.</para>
<itemizedlist>
<listitem>
<para>Akri é uma oferta de prévia de tecnologia nas versões anteriores do Edge e
agora foi descontinuada. Sua remoção está planejada para uma versão futura.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="tech-previews">
<title>Prévias de tecnologia</title>
<para>Exceto se especificado de outra forma, elas são aplicáveis à versão 3.4.0 e
a todas as versões z-stream subsequentes.</para>
<itemizedlist>
<listitem>
<para>As implantações IPv6 de pilha única são uma oferta de prévia de tecnologia e
não estão sujeitas ao escopo padrão de suporte.</para>
</listitem>
<listitem>
<para>O Precision Time Protocol (PTP) em implantações downstream é uma oferta de
prévia de tecnologia e não está sujeito ao escopo padrão de suporte.</para>
</listitem>
<listitem>
<para>O modo BGP para MetalLB é uma oferta de prévia de tecnologia e não está
sujeito ao escopo padrão de suporte.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-component-verification">
<title>Verificação de componentes</title>
<para>Os componentes mencionados acima podem ser verificados usando os dados do
Software Bill Of Materials (SBOM), por exemplo, com o comando
<literal>cosign</literal> conforme descrito abaixo:</para>
<para>Faça download da chave pública do SUSE Edge Container da <link
xl:href="https://www.suse.com/support/security/keys/">fonte de chaves de
assinatura da SUSE</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>Verifique o hash da imagem do contêiner, usando, por exemplo, o
<literal>crane</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1 --platform linux/amd64
sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374</screen>
<note>
<para>Para imagens de várias arquiteturas, também é necessário especificar uma
plataforma ao obter o resumo, por exemplo, <literal>--platform
linux/amd64</literal> ou <literal>--platform linux/arm64</literal>. Se isso
não for feito, um erro será retornado na etapa seguinte (<literal>Error: no
matching attestations</literal>, "Erro: não há atestados correspondentes").</para>
</note>
<para>Faça a verificação com o comando <literal>cosign</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 &gt; /dev/null
#
Verification for registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The signatures were verified against the specified public key</screen>
<para>Extraia os dados do SBOM conforme descrito na <link
xl:href="https://www.suse.com/support/security/sbom/">documentação do SBOM
da SUSE</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>Etapas de upgrade</title>
<para>Consulte a <xref linkend="day-2-operations"/> para obter detalhes sobre como
fazer upgrade para uma nova versão.</para>
</section>
<section xml:id="id-product-support-lifecycle">
<title>Ciclo de vida de suporte do produto</title>
<para>O SUSE Edge conta com o suporte reconhecido da SUSE, líder consagrada em
tecnologia com um histórico comprovado de prestação de serviços de suporte
de qualidade empresarial. Para obter mais informações, visite <link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>
e a página da política de suporte em <link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>.
Se você tiver dúvidas sobre como criar um caso de suporte, como a SUSE
classifica os níveis de gravidade ou sobre o escopo do suporte, consulte o
Manual de Suporte Técnico em <link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>.</para>
<para>O SUSE Edge "3.4" tem suporte de produção de 24 meses, com os primeiros 6
meses de "suporte completo", seguidos de 18 meses de "suporte de
manutenção". Após essas fases de suporte, o produto chega ao "fim do
serviço" (EOL, End of Life) e não é mais suportado. Encontre mais
informações sobre as fases do ciclo de vida na tabela abaixo:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Suporte completo (6 meses)</emphasis></para></entry>
<entry align="left" valign="top"><para>Correções de bugs urgentes e de alta prioridade selecionadas serão lançadas
durante a janela de suporte completo, e todos os outros patches (não
urgentes, melhorias, novos recursos) serão lançados de acordo com a
programação de lançamento regular.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Suporte de manutenção (18 meses)</emphasis></para></entry>
<entry align="left" valign="top"><para>Durante esse período, apenas correções críticas serão lançadas por meio de
patches. Outras correções de bugs poderão ser lançadas a critério da SUSE,
mas não devem ser esperadas.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Fim do serviço (EOL, End of Life)</emphasis></para></entry>
<entry align="left" valign="top"><para>Quando um produto chega à data de fim do serviço, o cliente ainda pode
usá-lo nos termos do contrato de licenciamento do produto. Os planos de
suporte da SUSE não são válidos para versões de produtos que ultrapassaram a
data EOL.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Exceto se for claramente especificado, todos os componentes listados são
considerados em disponibilidade geral (GA, Generally Available) e cobertos
pelo escopo padrão de suporte da SUSE. Alguns componentes podem constar na
lista como "prévia de tecnologia", em que a SUSE concede aos clientes acesso
antecipado a recursos e funcionalidades pré-GA para avaliação, mas que não
estão sujeitos às políticas de suporte padrão e não são recomendados em
casos de uso de produção. A SUSE tem grande consideração pelo feedback e
pelas sugestões sobre melhorias que possam ser feitas nos componentes em
prévia de tecnologia, mas reserva-se o direito de descontinuar um recurso em
prévia de tecnologia antes de lançá-lo em disponibilidade geral, caso ele
não atenda às necessidades de nossos clientes ou não atinja o estado de
maturidade exigido pela SUSE.</para>
<para>Observe que a SUSE deve, ocasionalmente, descontinuar recursos ou alterar
especificações de APIs. Alguns motivos para descontinuar um recurso ou
alterar uma API são a atualização do recurso ou sua substituição por uma
nova implementação, um novo conjunto de recursos, uma tecnologia upstream
que não está mais disponível ou a comunidade upstream que fez alterações
incompatíveis. Não há intenção de que isso aconteça dentro de uma
determinada versão secundária (x.z) e, portanto, todas as versões z-stream
manterão a compatibilidade com as APIs e as funcionalidades dos recursos. A
SUSE se empenhará em enviar avisos sobre descontinuação com muito tempo de
antecedência como parte das Notas de lançamento, junto com soluções
alternativas, sugestões e mitigações para minimizar interrupções de
serviços.</para>
<para>A equipe do SUSE Edge também agradece o feedback da comunidade, em que é
possível relatar problemas dentro do respectivo repositório de códigos em
<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>.</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>Obtendo o código-fonte</title>
<para>Este produto SUSE inclui materiais licenciados à SUSE de acordo com a
Licença Pública Geral (GPL, General Public License) GNU e várias outras
licenças de código aberto. A GPL exige que a SUSE providencie o código-fonte
em conformidade com o material licenciado sob a GPL, e a SUSE cumpre todos
os outros requisitos de licença de código aberto. Sendo assim, a SUSE
disponibiliza todo o código-fonte que, de modo geral, está armazenado no
repositório SUSE Edge do GitHub (<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>),
no repositório SUSE Rancher do GitHub (<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)
para componentes dependentes, e especificamente para o SUSE Linux Micro, o
código-fonte está disponível para download em <link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>
na "mídia 2".</para>
</section>
<section xml:id="id-legal-notices">
<title>Avisos legais</title>
<para>A SUSE não faz representações ou garantias no que se refere ao conteúdo ou
ao uso desta documentação e, especificamente, isenta-se de quaisquer
garantias expressas ou implícitas de comercialização ou adequação para uma
finalidade específica. Além disso, a SUSE reserva o direito de revisar esta
publicação e fazer alterações em seu conteúdo, a qualquer momento, sem a
obrigação de notificar qualquer pessoa ou entidade de tais revisões ou
alterações.</para>
<para>A SUSE não faz representações ou garantias em relação a nenhum software e,
especificamente, isenta-se de quaisquer garantias expressas ou implícitas de
comercialização ou adequação a uma finalidade específica. Além disso, a SUSE
reserva o direito de fazer alterações em qualquer parte do software SUSE, a
qualquer momento, sem a obrigação de notificar qualquer pessoa ou entidade
de tais alterações.</para>
<para>Os produtos ou as informações técnicas constantes neste Contrato podem estar
sujeitos aos controles de exportação dos EUA e à legislação comercial de
outros países. Você concorda em cumprir todas as normas de controle de
exportação e obter as licenças ou as classificações necessárias para
exportar, reexportar ou importar produtos entregáveis. Você concorda em não
exportar ou reexportar para entidades que façam parte das listas atuais de
exclusão de exportação dos EUA nem para países embargados ou terroristas,
conforme especificado nas leis de exportação dos EUA. Você concorda em não
usar os produtos entregáveis para fins proibidos como armas nucleares,
mísseis ou armamento químico/biológico. Visite <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>
para obter mais informações sobre exportação do software SUSE. A SUSE não
assumirá nenhuma responsabilidade se você não obtiver as aprovações de
exportação necessárias.</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>Este documento de Notas de lançamento é protegido por uma licença Creative
Commons Atribuição-NãoComercial-SemDerivações 4.0 Internacional
(CC-BY-ND-4.0). Você deve ter recebido uma cópia da licença junto com este
documento. Do contrário, acesse <link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>.</para>
<para>A SUSE tem direitos de propriedade intelectual pertinentes à tecnologia
incorporada ao produto descrito neste documento. Em particular, e sem
limitação, esses direitos de propriedade intelectual podem incluir uma ou
mais patentes nos EUA, relacionadas em <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>,
e uma ou mais patentes adicionais ou solicitações de patentes pendentes nos
EUA e em outros países.</para>
<para>Para conhecer as marcas registradas da SUSE, consulte a lista de marcas
registradas e marcas de serviço da SUSE (<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>).
Todas as marcas registradas de terceiros pertencem aos respectivos
proprietários. Para saber as informações e os requisitos de uso da marca
SUSE, consulte as diretrizes publicadas em <link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link>.</para>
</section>
</chapter>
</part>
</book>
