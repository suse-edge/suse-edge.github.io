<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude">
<info>
<title>SUSE Edgeドキュメント</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>2024年11月15日</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="id-suse-edge-documentation">
<title>SUSE Edgeドキュメント</title>
<para>『SUSE
Edgeドキュメント』をお読みいただきありがとうございます。このドキュメントには、高レベルアーキテクチャの概要、クイックスタートガイド、検証済みの設計、コンポーネントの使用に関するガイダンス、サードパーティ統合、エッジコンピューティングインフラストラクチャとワークロードを管理するためのベストプラクティスが記載されています。</para>
<section xml:id="id-what-is-suse-edge">
<title>SUSE Edgeとは</title>
<para>SUSE
Edgeは、インフラストラクチャとクラウドネイティブなアプリケーションをエッジにデプロイするという独自の課題に対処することに特化した、緊密に統合されて包括的に検証されたエンドツーエンドのソリューションです。SUSE
Edgeが重点を置いているのは、独創的でありながら高い柔軟性とスケーラビリティを持つセキュアなプラットフォームを提供し、初期デプロイメントイメージの構築からノードのプロビジョニングとオンボーディング、アプリケーションのデプロイメント、可観測性、ライフサイクル全体の運用にまで対応することです。このプラットフォームは、最良のオープンソースソフトウェアに基づいてゼロから構築されており、SUSEが持つ、30年にわたってセキュアで安定した定評あるSUSE
Linuxプラットフォームを提供してきた歴史と、Rancherポートフォリオによって拡張性に優れ機能豊富なKubernetes管理を提供してきた経験の両方に合致するものです。SUSE
Edgeは、これらの機能の上に構築されており、小売、医療、輸送、物流、通信、スマート製造、産業用IoTなど、さまざまな市場セグメントに対応できる機能を提供します。</para>
</section>
<section xml:id="id-design-philosophy">
<title>設計理念</title>
<para>このソリューションは、顧客の要件や期待はさまざまであるため「万能」なエッジプラットフォームは存在しないという考え方に基づいて設計されています。エッジデプロイメントにより、実に困難な問題を解決し、継続的に進化させることが要求されます。たとえば、大規模なスケーラビリティ、ネットワークの可用性の制限、物理的なスペースの制約、新たなセキュリティの脅威と攻撃ベクトル、ハードウェアアーキテクチャとシステムリソースのバリエーション、レガシインフラストラクチャやレガシアプリケーションのデプロイとインタフェースの要件、耐用年数を延長している顧客ソリューションといった課題があります。こうした課題の多くは、従来の考え方(たとえば、データセンター内やパブリッククラウドへのインフラストラクチャやアプリケーションのデプロイメント)とは異なるため、はるかに細かく設計を検討し、一般的な前提の多くを再検討する必要があります。</para>
<para>たとえば、SUSEはミニマリズム、モジュール性、操作のしやすさに価値を見出しています。システムは複雑化するほど故障しやすくなるため、エッジ環境ではミニマリズムが重要です。数百、数十万カ所に及ぶとなると、複雑なシステムは複雑な故障が発生します。また、SUSEのソリューションはモジュール性を備えているため、ユーザの選択肢を増やしながら、デプロイしたプラットフォームが不必要に複雑になることを解消できます。さらに、ミニマリズムおよびモジュール性と、操作のしやすさとのバランスを取ることも必要です。人間はプロセスを何千回も繰り返すとミスを犯す可能性があるため、プラットフォーム側で潜在的なミスを確実に回復し、技術者が現場に出向かなくても済むようにすると同時に、一貫性と標準化を実現するよう努める必要もあります。</para>
</section>
<section xml:id="id-high-level-architecture">
<title>高レベルアーキテクチャ</title>
<para>SUSE
Edgeの高レベルシステムアーキテクチャは、「管理」クラスタと「ダウンストリーム」クラスタの2つのコアカテゴリに分けられます。管理クラスタは1つまたは複数のダウンストリームクラスタのリモート管理を担当しますが、特定の状況下では、ダウンストリームクラスタはリモート管理なしで動作する必要があります。たとえば、エッジサイトに外部接続がない場合や独立して動作する必要がある場合などです。SUSE
Edgeでは、管理クラスタとダウンストリームクラスタの両方の動作に利用される技術コンポーネントは大部分が共通していますが、システム仕様と最上位に位置するアプリケーションが異なる場合があります。すなわち管理クラスタはシステム管理とライフサイクル操作を有効にするアプリケーションを実行しますが、ダウンストリームクラスタはユーザアプリケーションを提供するための要件を満たします。</para>
<section xml:id="id-components-used-in-suse-edge">
<title>SUSE Edgeで使用されるコンポーネント</title>
<para>SUSE
Edgeは、既存のSUSEとRancherのコンポーネントと、エッジコンピューティングに必要な制約や複雑さに対応できるようにEdgeチームが構築した追加機能とコンポーネントで構成されています。管理クラスタとダウンストリームクラスタの両方で使用されるコンポーネントは、簡略化された高レベルなアーキテクチャ図とともに以下に説明しますが、これは網羅的なリストではないことに注意してください。</para>
<section xml:id="id-management-cluster">
<title>管理クラスタ</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-management-cluster.png"
width=""/> </imageobject>
<textobject><phrase>SUSE Edge管理クラスタ</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理</emphasis>:
これは、接続されたダウンストリームクラスタのプロビジョニングとライフサイクルの管理に使用されるSUSE
Edgeの中核です。管理クラスタには通常、以下のコンポーネントが含まれます。</para>
<itemizedlist>
<listitem>
<para>Rancher Prime (<xref
linkend="components-rancher"/>)によるマルチクラスタ管理により、ダウンストリームクラスタのオンボーディングとインフラストラクチャおよびアプリケーションの継続的なライフサイクル管理のための共通ダッシュボードが可能になり、包括的なテナント分離と<literal>IDP</literal>
(アイデンティティプロバイダ)統合、サードパーティの統合および拡張のための大規模なマーケットプレイス、ベンダーニュートラルなAPIも提供されます。</para>
</listitem>
<listitem>
<para>SUSE
Managerを使用したLinuxシステム管理により、ダウンストリームクラスタ上で実行される基礎となるLinuxオペレーティングシステム(*SLE
Micro (<xref
linkend="components-slmicro"/>))の自動的なLinuxパッチおよび設定管理が可能になります。このコンポーネントはコンテナ化されていますが、現時点では他の管理コンポーネントとは別のシステムで実行する必要があるため、上の図では「Linux管理」とラベル付けされています。</para>
</listitem>
<listitem>
<para>特定のSUSE Edgeリリースへの管理クラスタコンポーネントのアップグレードを処理する専用のライフサイクル管理(<xref
linkend="components-upgrade-controller"/>)コントローラ。</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)を使用したRancher
Primeへのリモートシステムのオンボーディングにより、接続されたエッジノードを目的のKubernetesクラスタに遅延バインディングしたり、GitOps経由でアプリケーションをデプロイメントしたりできます。</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>)、MetalLB (<xref
linkend="components-metallb"/>)、および<literal>CAPI</literal> (Cluster
API)インフラストラクチャプロバイダによるオプションの完全なベアメタルライフサイクルおよび管理サポートにより、リモート管理機能を備えたベアメタルシステムの完全なエンドツーエンドのプロビジョニングが可能になります。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタとそれらに存在するアプリケーションのプロビジョニングとライフサイクルの管理のためのFleet (<xref
linkend="components-fleet"/>)と呼ばれるオプションのGitOpsエンジン。</para>
</listitem>
<listitem>
<para>管理クラスタ自体を支えるのは、ベースオペレーティングシステムとしてのSLE Micro (<xref
linkend="components-slmicro"/>)と、管理クラスタアプリケーションをサポートするKubernetesディストリビューションとしてのRKE2
(<xref linkend="components-rke2"/>)です。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-downstream-clusters">
<title>ダウンストリームクラスタ</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-downstream-cluster.png"
width=""/> </imageobject>
<textobject><phrase>SUSE Edgeダウンストリームクラスタ</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ダウンストリーム</emphasis>:
これは、エッジでユーザワークロードを実行するために使用されるSUSE
Edgeの分散部分です。つまり、エッジの場所自体で実行されるソフトウェアであり、通常は次のコンポーネントで構成されます。</para>
<itemizedlist>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)やRKE2 (<xref
linkend="components-rke2"/>)などのセキュアで軽量なディストリビューションを含む、Kubernetesディストリビューションの選択肢(<literal>RKE2</literal>は、政府機関や規制産業での使用に耐えるように強化、認定、最適化されています）。</para>
</listitem>
<listitem>
<para>NeuVector (<xref
linkend="components-neuvector"/>)を使用するとイメージ脆弱性スキャン、ディープパケットインスペクション、リアルタイム脅威および脆弱性保護のようなセキュリティ機能が有効になります。</para>
</listitem>
<listitem>
<para>Longhorn (<xref
linkend="components-longhorn"/>)によるソフトウェアブロックストレージにより、軽量で永続的、弾力性があり、拡張可能なブロックストレージが可能になります。</para>
</listitem>
<listitem>
<para>SLE Micro (<xref
linkend="components-slmicro"/>)を搭載した、軽量でコンテナに最適化された堅牢なLinuxオペレーティングシステムで、エッジでのコンテナや仮想マシンの実行に不変で耐障害性に優れたOSを提供します。SLE
Microは、<literal>aarch64</literal>および<literal>x86_64</literal>アーキテクチャの両方で使用でき、レイテンシの影響を受けやすいアプリケーション(通信事業者のユースケースなど)向けの<literal>リアルタイムカーネル</literal>もサポートしています。</para>
</listitem>
<listitem>
<para>接続されたクラスタ(つまり、管理クラスタに接続しているクラスタ)には、Rancher Primeへの接続を管理するためのRancher System
Agentと、SUSE
Managerからの指示を受けてLinuxソフトウェアの更新を適用するためのvenv-salt-minionの2つのエージェントがデプロイされます。これらのエージェントは、切断されたクラスタの管理には必要ありません。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-connectivity">
<title>接続</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-connected-architecture.png"
width=""/> </imageobject>
<textobject><phrase>SUSE Edge接続アーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
<para>上記のイメージは、<emphasis
role="strong">接続された</emphasis>ダウンストリームクラスタと、それらの管理クラスタへの接続に関する高レベルアーキテクチャの概要を示しています。管理クラスタは、ダウンストリームクラスタとターゲット管理クラスタとの間のネットワーキングの可用性に応じて、オンプレミスとクラウドの両方の容量で、さまざまな基礎となるインフラストラクチャプラットフォーム上にデプロイできます。これが機能するための唯一の要件は、ダウンストリームクラスタノードを管理インフラストラクチャに接続するネットワークでアクセス可能なAPIとコールバックURLです。</para>
<para>この接続が確立されるメカニズムは、ダウンストリームクラスタのデプロイメントのメカニズムとは異なるものであることを認識することが重要です。この詳細については、次のセクションでさらに詳しく説明しますが、基本的な理解を深めるために、接続されたダウンストリームクラスタが「管理」クラスタとして確立される主なメカニズムは3つあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ダウンストリームクラスタはまず「切断された」容量でデプロイされ(Edge Image Builder (<xref
linkend="components-eib"/>)経由)、接続が許可されると、管理クラスタにインポートされます。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタは、組み込みオンボーディングメカニズム(たとえばElemental (<xref
linkend="components-elemental"/>)経由)を使用するように設定され、初回ブート時に管理クラスタに自動的に登録されるため、クラスタ設定の遅延バインディングが許可されます。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタにはベアメタル管理機能 (CAPI +
Metal^3)がプロビジョニングされており、クラスタがデプロイされ、設定されると(Rancher
Turtlesオペレータ経由)管理クラスタに自動的にインポートされます。</para>
</listitem>
</orderedlist>
<note>
<para>大規模なデプロイメントの規模に対応し、地理的に分散した環境における帯域幅とレイテンシの問題を最適化し、停止時や管理クラスタのアップグレード時の混乱を最小限に抑えるために、複数の管理クラスタを実装することが推奨されます。現在の管理クラスタのスケーラビリティの限界とシステム要件については、
<link
xl:href="https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/installation-requirements">こちら</link>をご覧ください。</para>
</note>
</section>
</section>
<section xml:id="id-common-edge-deployment-patterns">
<title>一般的なEdge デプロイメントパターン</title>
<para>動作環境とライフサイクル要件はさまざまであるため、SUSEでは、SUSE
Edgeを運用する市場セグメントやユースケースに大まかに一致する別個のデプロイメントパターンを多数サポートしています。また、これらの各デプロイメントパターンに対応するクイックスタートガイドを作成し、ユーザのニーズに基づいてSUSE
Edgeプラットフォームに習熟できるようにしています。以下に、現在サポートされている3つのデプロイメントパターンを、各クイックスタートページへのリンクとともに説明します。</para>
<section xml:id="id-directed-network-provisioning">
<title>ダイレクトネットワークプロビジョニング</title>
<para>ダイレクトネットワークプロビジョニングでは、デプロイ先のハードウェアの詳細がわかっている場合に、アウトオブバンド管理インタフェースに直接アクセスして、プロビジョニングプロセス全体をオーケストレーションして自動化します。このシナリオで顧客が期待するソリューションとは、エッジサイトを一元的な場所から完全に自動化してプロビジョニングすることができ、ブートイメージの作成をはるかに上回る機能を備えていて、エッジロケーションでの手動操作を最小限に抑えられるソリューションです。つまり、ラックに搭載して電源をオンにし、必要なネットワークを物理ハードウェアに接続するだけで、自動化プロセスによってアウトオブバンド管理(Redfish
APIなど)を介してマシンの電源が投入され、ユーザの介入なしにインフラストラクチャのプロビジョニング、オンボーディング、デプロイメントが処理されるソリューションです。これが機能するための鍵は、管理者がシステムを把握している、つまりどのハードウェアがどこにあるかを管理者が把握していることと、デプロイメントが中央で処理されることが想定されていることです。</para>
<para>このソリューションは最も堅牢です。管理者がハードウェアの管理インタフェースを直接操作して既知のハードウェアを扱うことに加え、ネットワークの利用可否に対する制約が少ないためです。機能面では、このソリューションは、Cluster
APIとMetal<superscript>3</superscript>を広範に使用して、ベアメタルからオペレーティングシステム、Kubernetes、階層化アプリケーションまでを自動プロビジョニングし、デプロイメント後にSUSE
Edgeの他の一般的なライフサイクル管理機能にリンクする機能を提供します。このソリューションのクイックスタートについては、<xref
linkend="quickstart-metal3"/>を参照してください。</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>「Phone Home」ネットワークプロビジョニング</title>
<para>場合によっては、中央管理クラスタでハードウェアを直接管理できない環境で運用することがあります(たとえば、リモートネットワークがファイアウォールの背後にある場合や、アウトオブバンド管理インタフェースがない場合などがあり、エッジでよく見られる「PC」タイプのハードウェアで一般的です)。このシナリオの場合のために、SUSEでは、ハードウェアのブートストラップ時にその配置先がわかっていなくても、クラスタとそのワークロードをリモートでプロビジョニングできるツールを提供しています。エッジコンピューティングについて考える場合、ほとんどの人はこう考えます。エッジコンピューティングとは、不明な部分がある数千あるいは数万台のシステムがエッジロケーションで起動し、安全にPhone
Home通信を行い、そのシステムの身元を検証し、実行すべき処理についての指示を受信することです。ここで要件として期待されるのは、工場でマシンを事前イメージングしたり、USBなどでブートイメージをアタッチしたりする以外には、ユーザがほとんど介入しなくてもプロビジョニングとライフサイクル管理ができることです。この領域での主な課題は、こうしたデバイスの規模、一貫性、セキュリティ、ライフサイクルに対処することです。</para>
<para>このソリューションでは、非常に柔軟で一貫性のある方法でシステムをプロビジョニングおよびオンボーディングできます。システムの場所、タイプや仕様、初回電源投入日時などは問いません。SUSE
Edgeでは、Edge Image
Builderを使用してシステムを非常に柔軟にカスタマイズできます。また、ノードのオンボーディングとKubernetesのプロビジョニングにはRancherのElementalが提供する登録機能を活用するとともに、オペレーティングシステムへのパッチの適用にはSUSE
Managerを活用します。このソリューションのクイックスタートについては、<xref
linkend="quickstart-elemental"/>を参照してください。</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>イメージベースのプロビジョニング</title>
<para>スタンドアロン環境、エアギャップ環境、またはネットワークが制限された環境で運用する必要があるお客様向けに、SUSE
Edgeでは、必要なデプロイメントアーティファクトがすべて含まれる、完全にカスタマイズされたインストールメディアを生成できるソリューションを提供しています。これにより、シングルノードとマルチノード両方の高可用性Kubernetesクラスタを、必要なワークロードと追加の階層化コンポーネントを含めてエッジに設定できます。これはすべて、外部とのネットワーク接続や集中管理プラットフォームの介入なしに行うことができます。ユーザエクスペリエンスは、インストールメディアをターゲットシステムに提供するという点では「Phone
Home」ソリューションによく似ていますが、このソリューションは「インプレースでブートストラップ」する点が異なります。このシナリオでは、生成されたクラスタをRancherに接続して継続的に管理する(つまり、大幅な再設定や再デプロイメントなしに、「非接続」動作モードから「接続」動作モードに移行する)ことも、分離した状態のまま動作を続行することもできます。どちらの場合も、一貫した同じメカニズムを適用してライフサイクル操作を自動化できることに注意してください。</para>
<para>さらに、このソリューションを使用すると、「ダイレクトネットワークプロビジョニング」モデルと「Phone
Homeネットワークプロビジョニング」モデルの両方をサポートする集中型インフラストラクチャをホストできる管理クラスタを迅速に作成することもできます。この方法では、あらゆるタイプのエッジインフラストラクチャを最も迅速・簡単にプロビジョニングできます。このソリューションでは、SUSE
Edge Image
Builderの機能を多用して、完全にカスタマイズされた無人インストールメディアを作成します。クイックスタートについては、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</section>
</section>
<section xml:id="id-suse-edge-stack-validation">
<title>SUSE Edge Stack Validation</title>
<para>すべてのSUSe Edgeリリースは、緊密に統合され、徹底的に検証されたコンポーネントで構成されており、1
つのバージョンとして管理されています。コンポーネント間の統合をテストするだけでなく、強制的な障害シナリオ下でシステムが期待通りに動作することを保証する継続的な統合とスタック検証の一環として、SUSE
Edgeチームはすべてのテスト実行と結果を公開しています。結果とすべての入力パラメータは<link
xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link>でご確認いただけます。</para>
</section>
<section xml:id="id-full-component-list">
<title>コンポーネントの全リスト</title>
<para>コンポーネントの全リストと、各コンポーネントの概要説明へのリンク、およびSUSE Edgeでの使用方法については、以下をご覧ください。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Rancher Dashboard拡張機能(<xref
linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>SUSE Manager</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SLE Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>Akri (<xref linkend="components-akri"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>Longhorn (<xref linkend="components-longhorn"/>)</para>
</listitem>
<listitem>
<para>NeuVector (<xref linkend="components-neuvector"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>)</para>
</listitem>
<listitem>
<para>Upgrade Controller (<xref linkend="components-upgrade-controller"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>クイックスタート</title>
<partintro>
<para>クイックスタートはこちら</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>Metal<superscript>3</superscript>を使用したBMCの自動デプロイメント</title>
<para>Metal<superscript>3</superscript>は、Kubernetesにベアメタルインフラストラクチャ管理機能を提供する<link
xl:href="https://metal3.io/">CNCFプロジェクト</link>です。</para>
<para>Metal<superscript>3</superscript>は、<link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>などのアウトオブバンドプロトコルを介した管理をサポートするベアメタルサーバのライフサイクルを管理するためのKubernetesネイティブリソースを提供します。</para>
<para>また、<link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link>も十分にサポートされており、広く採用されているベンダニュートラルなAPIを使用して、複数のインフラストラクチャプロバイダにわたってインフラストラクチャリソースを管理できます。</para>
<section xml:id="id-why-use-this-method">
<title>この方法を使用する理由</title>
<para>この方法は、ターゲットハードウェアがアウトオブバンド管理をサポートしていて、完全に自動化されたインフラストラクチャ管理フローが望まれるシナリオで役立ちます。</para>
<para>管理クラスタは宣言型APIを提供するように設定されており、このAPIによってダウンストリームクラスタのベアメタルサーバのインベントリと状態を管理できます。これには、自動検査、クリーニング、プロビジョニング/プロビジョニング解除も含まれます。</para>
</section>
<section xml:id="id-high-level-architecture-2">
<title>アーキテクチャの概要</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.png"
width=""/> </imageobject>
<textobject><phrase>クイックスタートmetal3アーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>前提条件</title>
<para>ダウンストリームクラスタのサーバハードウェアとネットワーキングに関連する固有の制約がいくつかあります。</para>
<itemizedlist>
<listitem>
<para>管理クラスタ</para>
<itemizedlist>
<listitem>
<para>ターゲットサーバ管理/BMC APIへのネットワーク接続が必要</para>
</listitem>
<listitem>
<para>ターゲットサーバのコントロールプレーンネットワークへのネットワーク接続が必要</para>
</listitem>
<listitem>
<para>マルチノード管理クラスタの場合、追加の予約済みIPアドレスが必要</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>制御対象ホスト</para>
<itemizedlist>
<listitem>
<para>Redfish、iDRAC、またはiLOのインタフェースを介したアウトオブバンド管理のサポートが必要</para>
</listitem>
<listitem>
<para>仮想メディアを使用したデプロイメントのサポートが必要(PXEは現在未サポート)</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>プロビジョニングAPIにアクセスするために管理クラスタへのネットワーク接続が必要</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>ツールがいくつか必要です。ツールは管理クラスタにインストールするか、管理クラスタにアクセス可能なホストにインストールできます。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://helm.sh">Helm</link>、および<link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para><link xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナ ランタイム</para>
</listitem>
</itemizedlist>
<para><literal>SL-Micro.x86_64-6.0-Base-GM2.raw.xz</literal> OSイメージファイルは<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
<section xml:id="id-setup-management-cluster">
<title>管理クラスタのセットアップ</title>
<para>管理クラスタをインストールし、Metal<superscript>3</superscript>を使用する基本的な手順は次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>RKE2管理クラスタをインストールします。</para>
</listitem>
<listitem>
<para>Rancherのインストール</para>
</listitem>
<listitem>
<para>ストレージプロバイダをインストールします。</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>の依存関係をインストールします。</para>
</listitem>
<listitem>
<para>Rancher Turtles経由でCAPIの依存関係をインストールします。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタホスト用のSLEMicro OSイメージを構築します。</para>
</listitem>
<listitem>
<para>BareMetalHost CRを登録し、ベアメタルのインベントリを定義します。</para>
</listitem>
<listitem>
<para>CAPIリソースを定義して、ダウンストリームクラスタを作成します。</para>
</listitem>
</orderedlist>
<para>このガイドでは、既存のRKE2クラスタとRancher (cert-managerを含む)が、たとえばEdge Image Builder (<xref
linkend="components-eib"/>)を使用してインストールされていることを前提としています。</para>
<tip>
<para>ここでの手順は、ATIP管理クラスタのドキュメント(<xref
linkend="atip-management-cluster"/>)で説明されているように、完全に自動化することもできます。</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Metal<superscript>3</superscript>の依存関係のインストール</title>
<para>cert-managerがまだRancherのインストールの一部としてインストールされていない場合は、cert-managerをインストールして実行する必要があります。</para>
<para>永続ストレージプロバイダをインストールする必要があります。Longhornを推奨しますが、開発/PoC環境ではローカルパスを使用することもできます。以下の手順は、StorageClassが<link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">デフォルトとしてマーク</link>されていることを前提としています。マークされていない場合は、Metal<superscript>3</superscript>チャートに追加の設定が必要です。</para>
<para>追加のIPが必要です。このIPは<link
xl:href="https://metallb.universe.tf/">MetalLB</link>によって管理され、Metal<superscript>3</superscript>管理サービスに一貫したエンドポイントを提供します。このIPは、コントロールプレーンサブネットに属していて、静的設定用に予約されている必要があります(どのDHCPプールにも属していてはなりません)。</para>
<tip>
<para>管理クラスタがシングルノードである場合、MetalLBを介して管理されるフローティングIPを追加する必要はありません。「シングルノード設定」(<xref
linkend="id-single-node-configuration"/>)を参照してください。</para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>まず、MetalLBをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/3.1/metallb-chart \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>続いて、次のように、予約済みIPを使用して<literal>IPAddressPool</literal>と<literal>L2Advertisment</literal>を定義し、<literal>STATIC_IRONIC_IP</literal>として定義します。</para>
<screen language="yaml" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>これでMetal<superscript>3</superscript>をインストールできます。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/3.1/metal3-chart \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="${STATIC_IRONIC_IP}"</screen>
</listitem>
<listitem>
<para>initContainerがこのデプロイメントで実行されるまでに2分ほどかかる場合があります。そのため、Podがすべて実行されていることを確認してから次に進んでください。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para><literal>metal3-system</literal>ネームスペースのすべてのPodが実行されるまで、次の手順に進まないでください。</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Cluster APIの依存関係のインストール</title>
<para>Cluster APIの依存関係は、Rancher Turtles Helmチャートで管理されます。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/3.1/rancher-turtles-chart \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</screen>
<para>しばらくすると、コントローラPodが<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>、および<literal>rke2-control-plane-system</literal>の各ネームスペースで実行されているはずです。</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>ダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroベースイメージを準備します。</para>
<para>このガイドではダウンストリームクラスタをデプロイするために必要な最小限の設定について説明します。</para>
<section xml:id="id-image-configuration">
<title>イメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたベースイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="metal3-add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>custom/scriptsディレクトリには、初回ブート時に実行するスクリプトが含まれます。現在、デプロイメントのOSルートパーティションのサイズを変更するには、<literal>01-fix-growfs.sh</literal>スクリプトが必要です。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.0-Base-GM2.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-config.yaml</literal>ファイルは、ダウンストリームクラスタイメージの主要な設定ファイルです。次に、Metal<superscript>3</superscript>を介したデプロイメントの最小例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.0-Base-GM2.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</screen>
<para><literal>${ROOT_PASSWORD}</literal>はルートユーザの暗号化パスワードで、テスト/デバッグに役立ちます。このパスワードは、<literal>openssl
passwd -6 PASSWORD</literal>コマンドで生成できます。</para>
<para>運用環境では、<literal>${USERKEY1}</literal>を実際のSSHキーに置き換えて、usersブロックに追加できるSSHキーを使用することをお勧めします。</para>
<note>
<para><literal>net.ifnames=1</literal>は、<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link>を有効にします。</para>
<para>これはmetal3チャートのデフォルト設定と一致しますが、この設定は、設定されたチャートの<literal>predictableNicNames</literal>の値と一致する必要があります。</para>
<para>また、<literal>ignition.platform.id=openstack</literal>は必須であり、この引数がないと、Metal<superscript>3</superscript>の自動化フローでIgnitionによるSLEMicroの設定が失敗することにも注意してください。</para>
</note>
</section>
<section xml:id="id-growfs-script">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト<literal>custom/scripts/01-fix-growfs.sh</literal>が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>同じアプローチを使用して、プロビジョニングプロセス中に実行する独自のカスタムスクリプトを追加します。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>イメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>これにより、上記の定義に基づいて、<literal>SLE-Micro-eib-output.raw</literal>という名前の出力イメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、Metal3チャートを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>BareMetalHostインベントリの追加</title>
<para>自動デプロイメント用にベアメタルサーバを登録するには、リソースを2つ作成する必要があります。BMCアクセス資格情報を保存するシークレットと、BMC接続とその他の詳細を定義するMetal<superscript>3</superscript>
BareMetalHostリソースです。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>次の点に注意してください。</para>
<itemizedlist>
<listitem>
<para>シークレットのユーザ名/パスワードはbase64でエンコードされている必要があります。また、末尾に改行を含めないでください(たとえば、単なる<literal>echo</literal>ではなく、<literal>echo
‑n</literal>を使用してください)。</para>
</listitem>
<listitem>
<para><literal>cluster-role</literal>ラベルは、この時点で設定することも、後でクラスタの作成時に設定することもできます。以下の例では、<literal>control-plane</literal>または<literal>worker</literal>を想定しています。</para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal>は、ホストのコントロールプレーンNICに一致する有効なMACである必要があります。</para>
</listitem>
<listitem>
<para><literal>bmc</literal>のアドレスはBMC管理APIへの接続です。次のアドレスがサポートされています。</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/&lt;SYSTEM ID&gt;</literal>:
Redfish仮想メディア(たとえば、SuperMicro)</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/System.Embedded.1</literal>: Dell iDRAC</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>BareMetalHost APIの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">アップストリームのAPIドキュメント</link>を参照してください。</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>静的IPの設定</title>
<para>上記のBareMetalHostの例では、DHCPでコントロールプレーンネットワークの設定を提供することを想定していますが、静的IPなどの手動設定が必要なシナリオでは、以下に説明するように追加の設定を指定できます。</para>
<section xml:id="metal3-add-network-eib">
<title>静的ネットワーク設定用の追加スクリプト</title>
<para>Edge Image
Builderでゴールデンイメージを作成する際には、<literal>network</literal>フォルダ内に次の<literal>configure-network.sh</literal>ファイルを作成します。</para>
<para>このファイルにより、初回ブート時に設定ドライブのデータを使用して、<link
xl:href="https://github.com/suse-edge/nm-configurator">NM
Configuratorツール</link>を使ってホストネットワーキングを設定します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>ホストネットワーク設定の追加シークレット</title>
<para>NM Configurator (<xref linkend="components-nmc"/>)でサポートされている<link
xl:href="https://nmstate.io/">nmstate</link>形式のデータを含む追加シークレットをホストごとに定義できます。</para>
<para>その後、このシークレットは、<literal>BareMetalHost</literal>リソースで<literal>preprovisioningNetworkDataName</literal>の指定フィールドを使用して参照されます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>状況によってはmac-addressを省略できますが、<literal>configure-network.sh</literal>スクリプトは上記の<literal>_all.yaml</literal>ファイル名を使用して、nm-configuratorで統合ノード設定(<xref
linkend="networking-unified"/>)を有効にする必要があります。</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>BareMetalHostの準備</title>
<para>上記の説明に従ってBareMetalHostリソースと関連するシークレットを作成すると、次のようにホスト準備ワークフローがトリガされます。</para>
<itemizedlist>
<listitem>
<para>ターゲットホストのBMCに接続された仮想メディアによってramdiskイメージがブートされる</para>
</listitem>
<listitem>
<para>ramdiskがハードウェア詳細を検査し、ホストをプロビジョニング用に準備する(たとえば、ディスクから以前のデータを消去する)</para>
</listitem>
<listitem>
<para>このプロセスが完了すると、BareMetalHostの<literal>status.hardware</literal>フィールドのハードウェア詳細が更新され、検証可能になる</para>
</listitem>
</itemizedlist>
<para>このプロセスには数分かかる場合がありますが、完了すると、BareMetalHostの状態が<literal>available</literal>になります。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>ダウンストリームクラスタの作成</title>
<para>続いて、ダウンストリームクラスタを定義するCluster
APIリソースと、BareMetalHostリソースをプロビジョニングしてからブートストラップを実行してRKE2クラスタを形成するマシンリソースを作成します。</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>コントロールプレーンのデプロイメント</title>
<para>コントロールプレーンをデプロイするために、以下のリソースを含む次のようなyamlマニフェストを定義します。</para>
<itemizedlist>
<listitem>
<para>クラスタリソースでは、クラスタ名、ネットワーク、およびコントロールプレーン/インフラストラクチャプロバイダのタイプ(この場合はRKE2/Metal3)を定義します。</para>
</listitem>
<listitem>
<para>Metal3Clusterでは、コントロールプレーンのエンドポイント(シングルノードの場合はホストIP、マルチノードの場合はLoadBalancerエンドポイント。この例ではシングルノードを想定)を定義します。</para>
</listitem>
<listitem>
<para>RKE2ControlPlaneでは、RKE2のバージョンと、クラスタのブートストラップ時に必要な追加設定を定義します。</para>
</listitem>
<listitem>
<para>Metal3MachineTemplateではBareMetalHostリソースに適用するOSイメージを定義し、hostSelectorでは使用するBareMetalHostを定義します。</para>
</listitem>
<listitem>
<para>Metal3DataTemplateでは、BareMetalHostに渡す追加のメタデータを定義します(networkDataは現在のところEdgeソリューションではサポートされていないことに注意してください)。</para>
</listitem>
</itemizedlist>
<para>シンプルにするために、この例では、BareMetalHostにIP
<literal>192.168.125.200</literal>が設定されたシングルノードのコントロールプレーンを想定しています。より高度なマルチノードの例については、ATIPのドキュメント(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    version: v1.30.5+rke2r1
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>上記の例をコピーし、自身の環境に合わせて調整したら、<literal>kubectl</literal>を使用して適用し、<literal>clusterctl</literal>でクラスタのステータスを監視できます。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned - status can be checked via clusterctl
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>ワーカー/コンピュートのデプロイメント</title>
<para>コントロールプレーンと同様に、次のリソースを含むyamlマニフェストを定義します。</para>
<itemizedlist>
<listitem>
<para>MachineDeploymentでは、レプリカ(ホスト)の数とブートストラップ/インフラストラクチャプロバイダ(この場合はRKE2/Metal3)を定義します。</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplateでは、エージェントホストのブートストラップ用のRKE2のバージョンと初回ブート設定を記述します。</para>
</listitem>
<listitem>
<para>Metal3MachineTemplateではBareMetalHostリソースに適用するOSイメージを定義し、hostSelectorでは使用するBareMetalHostを定義します。</para>
</listitem>
<listitem>
<para>Metal3DataTemplateでは、BareMetalHostに渡す追加のメタデータを定義します(networkDataは現在のところEdgeソリューションではサポートされていないことに注意してください)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.30.5+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.30.5+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>上記の例をコピーし、自身の環境に合わせて調整したら、<literal>kubectl</literal>を使用して適用し、<literal>clusterctl</literal>でクラスタのステータスを監視できます。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait some time for the compute/agent hosts to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>クラスタのプロビジョニング解除</title>
<para>ダウンストリームクラスタをプロビジョニング解除するには、上記の作成手順で適用したリソースを削除します。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>これにより、BareMetalHostリソースのプロビジョニング解除がトリガされます。これには数分かかることがあり、その後リソースは再び利用可能な状態になります。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現在、アップストリームの<link
xl:href="https://github.com/metal3-io/ip-address-manager">IPアドレス管理コントローラ</link>はサポートされていません。このコントローラには、SLEMicroで選択されているネットワーク設定ツールと初回ブートツールチェーンとの互換性がまだないためです。</para>
</listitem>
<listitem>
<para>関連して、 IPAMリソースと、Metal3DataTemplateのnetworkDataフィールドは現在のところサポートされていません。</para>
</listitem>
<listitem>
<para>redfish-virtualmediaを介したデプロイメントのみが現在サポートされています。</para>
</listitem>
<listitem>
<para>デプロイされたクラスタは現在、Rancherにインポートされません。</para>
</listitem>
<listitem>
<para>Rancherの組み込みCAPIコントローラが無効化されるため、上記の方法でMetal<superscript>3</superscript>用に設定した管理クラスタを、Elemental
(<xref linkend="components-elemental"/>)などの他のクラスタプロビジョニング方法でも使用することはできません。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-planned-changes">
<title>予定されている変更</title>
<itemizedlist>
<listitem>
<para>Rancherへのデプロイされたクラスタのインポート。これは今後、<link
xl:href="https://turtles.docs.rancher.com/">Rancher
Turtles</link>を介してインポートできるようになる予定です。</para>
</listitem>
<listitem>
<para>Rancher
Turtlesとの連携。これにより、Rancherの組み込みCAPIを無効にする必要がなくなるため、管理クラスタを介して他のクラスタ方法も使用できるようになります。</para>
</listitem>
<listitem>
<para>networkDataフィールドを使用した、IPAMリソースと設定のサポートの有効化。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>追加のリソース</title>
<para>ATIPのドキュメント(<xref
linkend="atip"/>)に、通信事業者のユースケースにおけるMetal<superscript>3</superscript>のより高度な使用例が記載されています。</para>
<section xml:id="id-single-node-configuration">
<title>シングルノード設定</title>
<para>管理クラスタがシングルノードであるテスト/PoC環境では、MetalLBを介して管理されるフローティングIPを追加する必要はありません。</para>
<para>このモードでは、管理クラスタAPIのエンドポイントが管理クラスタのIPになるため、DHCPを使用している場合はそのIPを予約するか、管理クラスタのIPが変更されないように静的に設定する必要があります(以下では<literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal>と表記しています)。</para>
<para>このシナリオを有効にするために必要なmetal3チャートの値は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="id-disabling-tls-for-virtualmedia-iso-attachment">
<title>仮想メディアISOをアタッチするためのTLSの無効化</title>
<para>一部のサーバベンダは、仮想メディアISOイメージをBMCにアタッチする際にSSL接続を検証しますが、Metal3のデプロイメント用に生成された証明書は自己署名されているため、問題が発生する可能性があります。この問題を回避するには、次のようなmetal3チャートの値を使用して、仮想メディアディスクをアタッチする場合にのみTLSを無効にすることができます。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>別の解決策は、CA証明書を使用してBMCを設定することです。この場合、<literal>kubectl</literal>を使用してクラスタから証明書を読み込むことができます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>これにより、証明書をサーバのBMCコンソールで設定できますが、そのプロセスはベンダ固有です(すべてのベンダで可能というわけではなく、可能でない場合は<literal>enable_vmedia_tls</literal>フラグが必要なことがあります)。</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Elementalを使用したリモートホストのオンボーディング</title>
<para>このセクションでは、SUSE Edgeの一部としての「Phone
Homeネットワークプロビジョニング」ソリューションについて説明します。このソリューションは、Elementalを使用してノードのオンボーディングを支援します。Elementalは、Kubernetesを使用してリモートホスト登録と一元化された完全なクラウドネイティブOS管理を可能にするソフトウェアスタックです。SUSE
Edgeスタックでは、Elementalの登録機能を使用して、リモートホストをRancherにオンボーディングできます。これにより、ホストを集中管理プラットフォームに統合し、そこからKubernetesクラスタに加えて、階層化コンポーネント、アプリケーション、およびそのライフサイクルをすべて共通の場所からデプロイおよび管理できるようになります。</para>
<para>このアプローチが役立つシナリオとしては、制御するデバイスがアップストリームクラスタと同じネットワーク上にないか、アウトオブバンド管理コントローラが搭載されておらず、より直接的に制御できない場合や、さまざまな「不明」なシステムをエッジで多数ブートしており、それらを安全にオンボーディングして大規模に管理する必要がある場合が考えられます。これは、小売や産業用IoTなど、デバイスが設置されるネットワークをほとんど制御できない分野のユースケースによく見られるシナリオです。</para>
<section xml:id="id-high-level-architecture-3">
<title>アーキテクチャの概要</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.png"
width=""/> </imageobject>
<textobject><phrase>クイックスタートElementalアーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>必要なリソース</title>
<para>このクイックスタートを実行するためのシステムと環境の最小要件を次に示します。</para>
<itemizedlist>
<listitem>
<para>集中管理クラスタ(RancherとElementalをホストするクラスタ)用のホスト:</para>
<itemizedlist>
<listitem>
<para>開発またはテスト用の場合、最小8GBのRAMと20GBのディスク容量 (運用環境での使用については<link
xl:href="https://ranchermanager.docs.rancher.com/pages-for-subheaders/installation-requirements#hardware-requirements">こちら</link>
を参照)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>プロビジョニングするターゲットノード、すなわちエッジデバイス(デモまたはテストの場合は仮想マシンを使用可能)</para>
<itemizedlist>
<listitem>
<para>最小4GBのRAM、2 CPUコア、20GBのディスク</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>管理クラスタの解決可能なホスト名、またはsslip.ioなどのサービスで使用する静的IPアドレス</para>
</listitem>
<listitem>
<para>Edge Image Builderでインストールメディアを構築するためのホスト</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP6、openSUSE Leap 15.6、またはPodmanをサポートする他の互換性のあるオペレーティング システムを実行している</para>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://podman.io">Podman</link>、および<link
xl:href="https://helm.sh">Helm</link>がインストールされていること</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ブート用のUSBフラッシュ ドライブ(物理ハードウェアを使用する場合)</para>
</listitem>
<listitem>
<para>最新のSLE Micro 6.0 SelfInstall「GM2」ISOイメージのダウンロードコピーは、<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>でご覧いただけます。</para>
</listitem>
</itemizedlist>
<note>
<para>ターゲットマシンにある既存のデータはこのプロセスの一環として上書きされます。ターゲットデプロイメントノードに接続されているUSBストレージデバイスやディスク上のデータは、必ずバックアップしてください。</para>
</note>
<para>このガイドは、アップストリームクラスタをホストするためにDigital Oceanドロップレットを使用し、ダウンストリームデバイスとしてIntel
NUCを使用して作成されています。インストールメディアの構築には、SUSE Linux Enterprise Serverを使用しています。</para>
</section>
<section xml:id="build-bootstrap-cluster">
<title>ブートストラップクラスタの構築</title>
<para>まず、RancherとElementalをホストできるクラスタを作成します。このクラスタは、ダウンストリームノードが接続されているネットワークからルーティングできる必要があります。</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Kubernetesクラスタの作成</title>
<para>ハイパースケーラ(Azure、AWS、Google
Cloudなど)を使用している場合、クラスタを設定する最も簡単な方法は、ハイパースケーラのビルトインツールを使用することです。このガイドでは、簡潔にするために、これらの各オプションのプロセスについては詳述しません。</para>
<para>ベアメタルや別のホスティングサービスにインストールしようとしていて、Kubernetesディストリビューションそのものも用意する必要がある場合は、<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>を使用することをお勧めします。</para>
</section>
<section xml:id="id-set-up-dns">
<title>DNSの設定</title>
<para>続行する前に、クラスタへのアクセスを設定する必要があります。クラスタ自体のセットアップと同様に、DNSの設定方法は、クラスタがホストされている場所によって異なります。</para>
<tip>
<para>DNSレコードの設定を扱わない場合(たとえば、これが一時的なテストサーバである場合)、代わりに<link
xl:href="https://sslip.io">sslip.io</link>などのサービスを使用できます。このサービスを使用すると、<literal>&lt;address&gt;.sslip.io</literal>を使用して任意のIPアドレスを解決できます。</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Rancherのインストール</title>
<para>Rancherをインストールするには、作成したクラスタのKubernetes
APIにアクセスする必要があります。これは、使用しているKubernetesのディストリビューションによって異なります。</para>
<para>RKE2の場合、kubeconfigファイルは<literal>/etc/rancher/rke2/rke2.yaml</literal>に書き込まれます。このファイルをローカルシステムに<literal>~/.kube/config</literal>として保存します。このファイルを編集して、外部にルーティング可能な正しいIPアドレスまたはホスト名を含めなければならない場合があります。</para>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster">Rancherのドキュメント</link>に記載されているコマンドを使用して、Rancherを簡単にインストールできます。</para>
<para><link xl:href="https://cert-manager.io">cert-manager</link>をインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true</screen>
<para>次に、Rancher自体をインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt; \
  --version 2.9.3</screen>
<note>
<para>これを運用システムにする予定の場合は、cert-managerを使用して、実際の証明書(Let's Encryptの証明書など)を設定してください。</para>
</note>
<para>設定したホスト名をブラウズし、使用した<literal>bootstrapPassword</literal>でRancherにログインします。ガイドに従って簡単なセットアッププロセスを完了します。</para>
</section>
<section xml:id="install-elemental">
<title>Elementalのインストール</title>
<para>Rancherをインストールしたら、続いてElementalのオペレータと必要なCRDをインストールできます。Elemental用のHelmチャートはOCIアーティファクトとして公開されているため、インストールは他のチャートよりも若干シンプルです。Rancherのインストールに使用したものと同じシェルからインストールすることも、ブラウザでRancherのシェル内からインストールすることもできます。</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.6.4

helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.6.4</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>(オプション) Elemental UI拡張機能のインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>Elemental UIを使用するには、Rancherインスタンスにログインし、左上の3点リーダーメニューをクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>このページの［Available (使用可能)］タブから、Elementalカードの［Install (インストール)］をクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能をインストールすることを確認します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>インストール後、ページを再ロードするよう求められます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>再ロードすると、［OS Management (OS管理)］グローバルアプリからElemental拡張機能にアクセスできるようになります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能へのアクセス</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-configure-elemental">
<title>Elementalの設定</title>
<para>シンプルにするために、変数<literal>$ELEM</literal>を、設定ディレクトリを配置する場所のフルパスに設定することをお勧めします。</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>マシンがElementalに登録できるようにするために、<literal>fleet-default</literal>ネームスペースに<literal>MachineRegistration</literal>オブジェクトを作成する必要があります。</para>
<para>このオブジェクトの基本的なバージョンを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>この<literal>cat</literal>コマンドでは、各<literal>$</literal>をバックスラッシュ(<literal>\</literal>)でエスケープしています。このため、バッシュではテンプレート化されていません。手動でコピーする場合は、バックスラッシュを削除してください。</para>
</note>
<para>オブジェクトが作成されたら、割り当てられるエンドポイントを見つけてメモを取ります。</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>または、UIからこの操作を実行することもできます。</para>
<variablelist>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>［OS Management extension (OS管理拡張機能)］から［Create Registration Endpoint
(登録エンドポイントの作成) ］をクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png" width=""/>
</imageobject>
<textobject><phrase>［Create Registration (登録の作成)］をクリックします。</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>この設定に名前を付けます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png" width=""/>
</imageobject>
<textobject><phrase>名前の追加</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>［Cloud Configuration (クラウドの設定)］フィールドは無視して構いません。ここのデータは、Edge Image
Builderを使用した次の手順で上書きされるためです。</para>
</note>
</listitem>
<listitem>
<para>次に、下にスクロールして、マシンの登録時に作成されるリソースに付ける各ラベルに対して［Add Label
(ラベルの追加)］をクリックします。これはマシンを区別するのに役立ちます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png" width=""/>
</imageobject>
<textobject><phrase>ラベルの追加</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>最後に、［Create (作成)］をクリックして、設定を保存します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-create.png" width=""/>
</imageobject>
<textobject><phrase>［Create (作成)］をクリック</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<para>設定を作成した直後の場合は、［Registration URL (登録URL)］が一覧にされます。［Copy
(コピー)］をクリックしてアドレスをコピーできます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width=""/>
</imageobject>
<textobject><phrase>URLのコピー</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>クリックしてその画面から移動してしまった場合は、左側のメニューの［Registration Endpoints
(登録エンドポイント)］をクリックし、先ほど作成したエンドポイント名をクリックできます。</para>
</tip>
</listitem>
</varlistentry>
</variablelist>
<para>このURLは次の手順で使用します。</para>
</section>
<section xml:id="build-installation-media">
<title>イメージの構築</title>
<para>Elementalの現在のバージョンには独自のインストールメディアを構築する方法が用意されていますが、SUSE Edge 3.1では代わりにEdge
Image Builderでインストールメディアを構築します。したがって、生成されるシステムは、<link
xl:href="https://www.suse.com/products/micro/">SLE
Micro</link>をベースオペレーティングシステムとして構築されます。</para>
<tip>
<para>Edge Image Builderの詳細については、導入ガイド(<xref
linkend="quickstart-eib"/>)のほかに、コンポーネントのドキュメント(<xref
linkend="components-eib"/>)も参照してください。</para>
</tip>
<para>PodmanをインストールしたLinuxシステムで、ディレクトリを作成し、ゴールデンイメージを配置します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/downloads/SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.0
image:
    imageType: iso
    arch: x86_64
    baseImage: SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para>エンコードされていないパスワードは<literal>eib</literal>です。</para>
</listitem>
<listitem>
<para>この<literal>cat</literal>コマンドでは、各<literal>$</literal>をバックスラッシュ(<literal>\</literal>)でエスケープしています。このため、バッシュではテンプレート化されていません。手動でコピーする場合は、バックスラッシュを削除してください。</para>
</listitem>
<listitem>
<para>インストールデバイスは、インストール中に消去されます。</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
 build --definition-file eib-config.yaml</screen>
<para>物理デバイスをブートする場合は、イメージをUSBフラッシュ ドライブに書き込む必要があります。これは、次のコマンドで実行できます。</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>ダウンストリームノードのブート</title>
<para>インストールメディアを作成したので、それを使用してダウンストリームノードをブートできます。</para>
<para>Elementalで制御するシステムごとに、インストールメディアを追加してデバイスをブートします。インストールが完了すると、デバイスは再起動して自身を登録します。</para>
<para>UI拡張機能を使用している場合は、［Inventory of Machines (マシンのインベントリ)］にノードが表示されます。</para>
<note>
<para>ログインプロンプトが表示されるまでインストールメディアを取り外さないでください。初回ブート時には、USBスティック上のファイルにアクセスしたままになります。</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>ダウンストリームクラスタの作成</title>
<para>Elementalを使用して新しいクラスタをプロビジョニングする際に作成する必要があるオブジェクトが2つあります。</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>最初のオブジェクトは<literal>MachineInventorySelectorTemplate</literal>です。このオブジェクトにより、クラスタとインベントリ内のマシン間のマッピングを指定できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>インベントリ内のマシンをラベルに一致させるセレクタを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>リソースをクラスタに適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>マシンの名前を取得し、一致するラベルを追加します。</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>シンプルなシングルノードK3sクラスタリソースを作成し、クラスタに適用します。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.30.5+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<para>UI拡張機能では、ショートカットをいくつか使用できます。複数の場所を管理する場合は、手動による作業が多くなりすぎる可能性があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>先ほどと同様に、左側の3点リーダーメニューを開き、［OS Management
(OS管理)］を選択します。これにより、Elementalシステムを管理するためのメイン画面に戻ります。</para>
</listitem>
<listitem>
<para>左側のサイドバーで、［Inventory of Machines (マシンのインベントリ)］をクリックします。登録済みのマシンのインベントリが開きます。</para>
</listitem>
<listitem>
<para>これらのマシンからクラスタを作成するには、必要なシステムを選択し、［Actions (アクション)］ドロップダウンリストから［Create
Elemental Cluster (Elementalクラスタの作成)］をクリックします。［Cluster Creation
(クラスタの作成)］ダイアログが開き、それと同時に、使用するMachineSelectorTemplateがバックグラウンドで作成されます。</para>
</listitem>
<listitem>
<para>この画面では、構築するクラスタを設定します。このクイックスタートでは、K3s
v1.30.5+k3s1が選択され、残りのオプションはそのままにしておきます。</para>
<tip>
<para>他のオプションを表示するには、下にスクロールする必要がある場合があります。</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>これらのオブジェクトを作成したら、先ほどインストールした新しいノードを使用して新しいKubernetesクラスタがスピンアップするはずです。</para>
</section>
<section xml:id="id-node-reset-optional">
<title>ノードリセット(オプション)</title>
<para>SUSE Rancher
Elementalは、「ノードリセット」を実行する機能をサポートしています。ノードリセットは、Rancherからクラスタ全体が削除されたとき、クラスタからシングルノードが削除されたとき、またはマシンインベントリからノードが手動で削除されたときに任意でトリガできます。これは、孤立したリソースをリセットしてクリーンアップし、クリーンアップされたノードを自動的にマシンインベントリに戻して再利用可能にする場合に役立ちます。ノードリセットはデフォルトでは有効になっていないため、削除されたシステムはクリーンアップされず(つまり、データは削除されず、Kubernetesクラスタリソースはダウンストリームクラスタで動作し続けます)、データを消去してマシンをElemental経由でRancherに再登録するには手動操作が必要となります。</para>
<para>この機能をデフォルトで有効にするには、<literal>MachineRegistration</literal>に<literal>config.elemental.reset.enabled:
true</literal>を追加して明示的に有効にする必要があります。例:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>その後、この<literal>MachineRegistration</literal>に登録されているすべてのシステムが自動的に<literal>elemental.cattle.io/resettable:'true'</literal>のアノテーションを受け取って設定に反映します。既存の<literal>MachineInventory</literal>にこのアノテーションがない場合や、すでにノードをデプロイ済みである場合などに、個々のノードで手動でこの操作を実行する場合は、<literal>MachineInventory</literal>を変更し、<literal>resettable</literal>設定を追加します。例:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>SUSE Edge 3.1では、Elemental
Operatorによってオペレーティングシステム上にマーカが配置され、これによってクリーンアッププロセスが自動的にトリガされます。クリーンアッププロセスは、すべてのKubernetesサービスを停止して永続データをすべて削除し、すべてのKubernetesサービスをアンインストールして、残っているKubernetes/Rancherディレクトリをクリーンアップし、元のElemental
<literal>MachineRegistration</literal>設定を使用して強制的にRancherに再登録します。これは自動的に行われるため、手動での操作は必要ありません。呼び出されるスクリプトは<literal>/opt/edge/elemental_node_cleanup.sh</literal>にあり、マーカが配置されるとすぐに<literal>systemd.path</literal>を介してトリガされるため、直ちに実行されます。</para>
<warning>
<para><literal>resettable</literal>機能を使用する場合、Rancherからノード/クラスタを削除する際の望ましい動作は、データを消去して再登録を強制することであると想定されています。この状況ではデータが確実に失われるため、この機能は、自動リセットを実行することがわかっている場合にのみ使用してください。</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>次の手順</title>
<para>このガイドの使用後に調べるべき推奨リソースを次に示します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-fleet"/>のエンドツーエンドの自動化</para>
</listitem>
<listitem>
<para><xref linkend="components-nmc"/>の追加のネットワーク設定オプション</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Edge Image Builderを使用したスタンドアロンクラスタ</title>
<para>Edge Image Builder (EIB)は、完全なエアギャップシナリオでもマシンをブートストラップできるCustomized,
Ready-to-Boot (CRB)ディスクイメージの生成プロセスを効率化するツールです。EIBを使用すると、SUSE
Edgeの3つのデプロイメントフットプリントすべてで使用するデプロイメントイメージを作成できます。これは、EIBが十分に柔軟であり、最小限のカスタマイズ(例:
ユーザの追加やタイムゾーンの設定)から、あらゆる設定を網羅したイメージ(例:
複雑なネットワーク設定を行い、マルチノードKubernetesクラスタをデプロイして、顧客ワークロードをデプロイし、Rancher/ElementalとSUSE
Managerを介して集中管理プラットフォームに登録するイメージ)までを提供できるためです。EIBはコンテナイメージ内で動作するため、プラットフォーム間できわめて容易に移植可能です、さらに、必要な依存関係をすべて備えた自己完結型であるため、EIBツールの操作に使用するシステムにインストール済みのパッケージに及ぼす影響が最小限に抑えられます。</para>
<para>詳細については、Edge Image Builderの紹介(<xref linkend="components-eib"/>)を参照してください。</para>
<warning>
<para>Edge Image Builder v1.1はSUSE Linux Micro 6.0イメージのカスタマイズをサポートしています。  SUSE
Linux Enterprise Micro 5.5などの旧バージョンはサポートされていません。</para>
</warning>
<section xml:id="id-prerequisites-2">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>SLES 15 SP6、openSUSE Leap 15.6、またはopenSUSE
Tumbleweedを実行しているx86_64物理ホスト(または仮想マシン)</para>
</listitem>
<listitem>
<para>利用可能なコンテナランタイム(Podmanなど)</para>
</listitem>
<listitem>
<para>最新のSLE Micro 6.0 SelfInstall ISOイメージのダウンロードコピー(<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>で入手可能)</para>
</listitem>
</itemizedlist>
<note>
<para>互換性のあるコンテナランタイムが利用可能であれば、他のオペレーティングシステムでも機能する可能性はありますが、他のプラットフォームでは広範なテストは行われていません。このドキュメントではPodmanに焦点を当てていますが、Dockerでも同じ機能を実現できるはずです。</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>EIBイメージの取得</title>
<para>EIBのコンテナイメージは一般に公開されており、イメージ構築ホストで次のコマンドを実行することでSUSE Edgeレジストリからダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.1/edge-image-builder:1.1.0</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>イメージ設定ディレクトリの作成</title>
<para>EIBはコンテナ内で動作するため、ホストから設定ディレク
トリをマウントして、必要な設定を指定できるようにする必要があります。ビルドプロセス中に、EIBは必要な入力ファイルやサポートアーティファクトすべてにアクセスできます。このディレクトリは、特定の構造に従う必要があります。このディレクトリがホームディレクトリに存在し、「eib」という名前であると仮定して、ディレクトリを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>前の手順でSLE Micro
6.0の入力イメージをホストする「base-images」ディレクトリを作成しました。ダウンロードしたイメージが設定ディレクトリにコピーされていることを確認しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/downloads/SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIBの実行中に元のゴールデンイメージは変更<emphasis
role="strong">「されません」</emphasis>。EIBの設定ディレクトリのルートに、目的の設定でカスタマイズされた新しいバージョンが作成されます。</para>
</note>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>イメージ定義ファイルの作成</title>
<para>定義ファイルには、Edge Image
Builderがサポートする設定可能なオプションの大部分が記述されています。オプションの完全な例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/pkg/image/testdata/full-valid-example.yaml">こちら</link>を参照してください。以下で説明する例よりも広範な例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md">アップストリームのイメージ構築ガイド</link>を参照することをお勧めします。まずは、OSイメージの非常に基本的な定義ファイルから始めましょう。</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>この定義では、<literal>x86_64</literal>ベースのシステム用の出力イメージを生成するように指定しています。さらに変更を加えるためのベースとして使用するイメージは、<literal>slemicro.iso</literal>という名前の<literal>iso</literal>イメージであり、<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>にあることが想定されています。また、EIBがイメージの変更を完了すると、出力イメージは<literal>eib-image.iso</literal>という名前になり、デフォルトでは<literal>$CONFIG_DIR</literal>に存在することも記述されています。</para>
<para>これで、ディレクトリ構造は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>以降のセクションでは、一般的な操作の例をいくつか紹介していきます。</para>
<section xml:id="id-configuring-os-users">
<title>OSユーザの設定</title>
<para>EIBを使用すると、パスワードやSSHキーなどのログイン情報を事前にユーザに設定できます(固定されたルートパスワードの設定も含む)。この例の一部として、ルートパスワードを修正します。最初の手順は、<literal>OpenSSL</literal>を使用して一方向暗号化パスワードを作成することです。</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>これは次のような出力になります。</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>次に、定義ファイルに<literal>operatingSystem</literal>というセクションを追加し、その中に<literal>users</literal>配列を含めます。作成したファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>ユーザの追加、ホームディレクトリの作成、ユーザIDの設定、ssh-key認証の追加、グループ情報の変更も行うことができます。他の例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md">アップストリームのイメージ構築ガイド</link>を参照してください。</para>
</note>
</section>
<section xml:id="id-configuring-rpm-packages">
<title>RPMパッケージの設定</title>
<para>EIBの主な特徴の1つは、イメージにソフトウェアパッケージを追加するメカニズムを備えていることです。このため、インストールが完了した時点で、システムはインストールされたパッケージをすぐに利用できます。EIBでは、ユーザは以下を行うことができます。</para>
<itemizedlist>
<listitem>
<para>イメージ定義のリスト内の名前でパッケージを指定する</para>
</listitem>
<listitem>
<para>これらのパッケージを検索するネットワークリポジトリを指定する</para>
</listitem>
<listitem>
<para>一覧にされたパッケージをSUSEの公式リポジトリで検索するためのSUSE Customer Center (SCC)資格情報を指定する</para>
</listitem>
<listitem>
<para><literal>$CONFIG_DIR/rpms</literal>ディレクトリ経由で、ネットワークリポジトリに存在しないカスタムRPMをサイドロードする</para>
</listitem>
<listitem>
<para>同じディレクトリ(<literal>$CONFIG_DIR/rpms/gpg-keys</literal>)経由で、サードパーティ製パッケージの検証を有効にするためにGPGキーを指定する</para>
</listitem>
</itemizedlist>
<para>これにより、EIBは、イメージの構築時にパッケージ解決プロセスを実行し、ゴールデンイメージを入力として受け取り、提供されているパッケージ(リストで指定されているか、ローカルで提供されているパッケージ)をすべてプルしてインストールしようと試みます。EIBは、依存関係を含むすべてのパッケージを出力イメージ内に存在するリポジトリにダウンロードし、そのパッケージを初回ブートプロセス中にインストールするようにシステムに指示します。イメージの構築中にこのプロセスを実行することで、初回ブート時にパッケージが目的のプラットフォーム(エッジのノードなど)が正常にインストールされることが保証されます。これは、操作時に追加パッケージをネットワーク経由でプルするのではなく、イメージに事前に書き込んでおきたい環境でも便利です。たとえば、エアギャップ環境や、ネットワークが制限された環境です。</para>
<para>これを示すシンプルな例として、サードパーティベンダがサポートするNVIDIAリポジトリにある<literal>nvidia-container-toolkit</literal>
RPMパッケージをインストールします。</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>生成される定義ファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>上記はシンプルな例ですが、完全を期するために、イメージ生成を実行する前にNVIDIAパッケージ署名キーをダウンロードしてください。</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; $CONFIG_DIR/rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>この方法でRPMを追加することは、サポートされているサードパーティコンポーネント、またはユーザが提供(および保守)するパッケージを追加することを目的としています。このメカニズムは、通常ではSLE
Microでサポートされないパッケージを追加する目的では使用しないでください。このメカニズムを使用して、openSUSEのリポジトリから新しいリリースやサービスパックなどのコンポーネントを追加した場合(この操作はサポートされません)、最終的にサポート対象外の設定になるおそれがあります。特に、依存関係の解決によってオペレーティングシステムのコア部分が置き換えられる場合は、作成されたシステムが期待どおりに機能しているように見えても注意が必要です。確信が持てない場合は、SUSEの担当者に連絡してサポートを依頼し、目的の設定がサポート可能かどうかを判断してください。</para>
</warning>
<note>
<para>追加の例を含む総合的なガイドについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/installing-packages.md">アップストリームのパッケージインストールガイド</link>を参照してください。</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Kubernetesクラスタとユーザワークロードの設定</title>
<para>EIBのもう1つの特徴は、EIBを使用すると、「インプレースでブートストラップ」する、つまり調整のためにどのような形態の集中管理インフラストラクチャも必要としない、シングルノードとマルチノード両方の高可用性Kubernetesクラスタのデプロイメントを自動化できることです。このアプローチは主にエアギャップデプロイメント、すなわちネットワークが制限された環境のためのものですが、ネットワークに制限なく完全にアクセスできる場合であっても、スタンドアロンクラスタを迅速にブートストラップする方法として役立ちます。</para>
<para>この方法を使用すると、カスタマイズされたオペレーティングシステムをデプロイできるだけでなく、Kubernetesの設定を指定したり、Helmチャートを介して追加の階層化コンポーネントを指定したり、指定したKubernetesマニフェストを介してユーザワークロードを指定したりすることもできます。ただしこの方法を使用する場合、その背景にある設計理念として、ユーザがエアギャップ化を望んでいるとデフォルトで想定します。したがって、イメージ定義で指定されているすべての項目を、ユーザが指定したワークロードを含めてイメージにプルします。その際に、EIBは、提供された定義で要求されている検出済みイメージをすべてローカルにコピーし、作成されたデプロイ済みシステムの組み込みのイメージレジストリで提供します。</para>
<para>次の例では、既存のイメージ定義を使用してKubernetesの設定を指定します(この例では、複数のシステムとその役割が一覧にされていないため、デフォルトでシングルノードを想定します)。この設定により、シングルノードのRKE2
KubernetesクラスタをプロビジョニングするようにEIBに指示します。ユーザが指定したワークロード(マニフェストを使用)と階層化コンポーネント(Helmを使用)の両方のデプロイメントの自動化を説明するために、SUSE
EdgeのHelmチャートを使用してKubeVirtをインストールし、Kubernetesマニフェストを使用してNGINXをインストールします。既存のイメージ定義に追加する必要がある設定は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.30.5+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt-chart
        version: 0.4.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/3.1</screen>
<para>作成された完全な定義ファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.30.5+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt-chart
        version: 0.4.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/3.1</screen>
<note>
<para>マルチノードデプロイメント、カスタムネットワーキング、Helmチャートのオプション/値など、オプションの他の例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#kubernetes">アップストリームドキュメント</link>を参照してください。</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>ネットワークの設定</title>
<para>このクイックスタートの最後の例では、EIBで生成したイメージを使ってシステムをプロビジョニングした場合に作成されるネットワークを設定しましょう。ネットワーク設定を指定しない限り、ブート時に検出されたすべてのインタフェースでDHCPが使用されるのがデフォルトのモデルであることを理解することが重要です。ただし、これが常に望ましい設定であるとは限りません。これは特に、DHCPが利用できず静的な設定を指定する必要がある場合や、より複雑なネットワーキング構造(ボンド、LACP、VLANなど)を設定する必要がある場合、特定のパラメータ(ホスト名、DNSサーバ、ルートなど)を上書きする必要がある場合に該当します。</para>
<para>EIBでは、ノードごとの設定を指定することも(対象のシステムをMACアドレスで一意に識別します)、上書きによって各マシンに同一の設定を指定することもできます(システムのMACアドレスが不明な場合に便利です)。またEIDでは、Network
Manager Configurator (<literal>nmc</literal>)というツールも追加で使用されます。Network
Manager ConfiguratorはSUSE Edgeチームによって構築されたツールであり、<link
xl:href="https://nmstate.io/">nmstate.io</link>の宣言型ネットワークスキーマに基づいてカスタムネットワーキング設定を適用できるようにします。また、ブートしているノードをブート時に識別し、必要なネットワーク設定をサービス開始前に適用します。</para>
<para>ここでは、1つのインタフェースを持つシステムに静的ネットワーク設定を適用します。そのために、ネットワークの望ましい状態を、必要な<literal>network</literal>ディレクトリ内にある(目的のホスト名に基づく)ノード固有のファイルに記述します。</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>上記の例は、テストを仮想マシン上で実行すると仮定して、デフォルトの<literal>192.168.122.0/242</literal>サブネット用に設定されています。ご使用の環境に合わせて、MACアドレスも忘れずに変更してください。同じイメージを使用して複数のノードをプロビジョニングできるため、
EIBで(<literal>nmc</literal>を介して)設定されたネットワーキングは、各ノードをMACアドレスで一意に識別できるかどうかに依存しており、その後、ブート中に<literal>nmc</literal>は正しいネットワーキング設定を各マシンに適用します。つまり、インストール先のシステムのMACアドレスを把握する必要があります。また、デフォルトの動作ではDHCPに依存しますが、<literal>configure-network.sh</literal>フックを利用して、すべてのノードに共通の設定を適用することもできます。詳細については、ネットワーキングのガイド
(<xref linkend="components-nmc"/>)を参照してください。</para>
</warning>
<para>作成されるファイル構造は、次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>作成したネットワーク設定が解析され、必要なNetworkManager接続ファイルが自動的に生成されて、EIBが作成する新しいインストールイメージに挿入されます。これらのファイルはホストのプロビジョニング中に適用され、完全なネットワーク設定が生成されます。</para>
<note>
<para>上記の設定のより包括的な説明と、この機能の例については、エッジネットワーキングのコンポーネント(<xref
linkend="components-nmc"/>)を参照してください。</para>
</note>
</section>
</section>
<section xml:id="id-building-the-image">
<title>イメージの構築</title>
<para>EIBで使用するゴールデンイメージとイメージ定義ができたので、イメージを構築してみましょう。これには、<literal>podman</literal>を使用し、定義ファイルを指定して「build」コマンドでEIBコンテナを呼び出すだけです。</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
build --definition-file iso-definition.yaml</screen>
<para>コマンドの出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Downloading file: dl-manifest-1.yaml 100% (498/498 B, 9.5 MB/s)
Pulling selected Helm charts... 100% (1/1, 43 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (3/3, 10 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (657/657 MB, 48 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (368/368 MB, 48 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (35/35 MB, 50 MB/s)
Downloading file: sha256sum-amd64.txt 100% (4.3/4.3 kB, 6.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築されたISOイメージは<literal>$CONFIG_DIR/eib-image.iso</literal>に保存されます。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>ビルドごとに、<literal>$CONFIG_DIR/_build/</literal>内にタイムスタンプ付きのフォルダが作成されます。このフォルダには、ビルドのログ、ビルド中に使用されたアーティファクト、およびCRBイメージに追加されたすべてのスクリプトとアーティファクトを含む<literal>combustion</literal>ディレクトリと<literal>artefacts</literal>ディレクトリが含まれます。</para>
<para>このディレクトリの内容は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:&lt;version&gt;-registry.tar.zst
│   │   │   ├── rancher_kubectl:&lt;version&gt;-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.6_virt-operator:&lt;version&gt;-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   ├── nvidia-container-toolkit-&lt;version&gt;.rpm
│   │   │       │   ├── nvidia-container-toolkit-base-&lt;version&gt;.rpm
│   │   │       │   ├── libnvidia-container1-&lt;version&gt;.rpm
│   │   │       │   └── libnvidia-container-tools-&lt;version&gt;.rpm
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt-chart
│   │       └── kubevirt-0.4.0.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>ビルドが失敗した場合、情報が含まれる最初のログは<literal>eib-build.log</literal>です。そこから、失敗したコンポーネントに移動してデバッグを行います。</para>
<para>この時点で、以下を行う、すぐに使用できるイメージができているはずです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SLE Micro 6.0をデプロイする</para>
</listitem>
<listitem>
<para>ルートパスワードを設定する</para>
</listitem>
<listitem>
<para><literal>nvidia-container-toolkit</literal>パッケージをインストールする</para>
</listitem>
<listitem>
<para>コンテンツをローカルに提供する組み込みのコンテナレジストリを設定する</para>
</listitem>
<listitem>
<para>シングルノードRKE2をインストールする</para>
</listitem>
<listitem>
<para>静的ネットワーキングを設定する</para>
</listitem>
<listitem>
<para>KubeVirtをインストールする</para>
</listitem>
<listitem>
<para>ユーザが指定したマニフェストをデプロイする</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>イメージ構築プロセスのデバッグ</title>
<para>イメージ構築プロセスが失敗する場合は、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/debugging.md">アップストリームのデバッグガイド</link>を参照してください。</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>新しく構築されたイメージのテスト</title>
<para>新しく構築されたCRBイメージをテストする方法については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/testing-guide.md">アップストリームのイメージテストガイド</link>を参照してください。</para>
</section>
</chapter>
</part>
<part xml:id="id-components-used">
<title>使用するコンポーネント</title>
<partintro>
<para>Edgeのコンポーネントのリスト</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para><link
xl:href="https://ranchermanager.docs.rancher.com">https://ranchermanager.docs.rancher.com</link>にあるRancherのアップストリームドキュメントを参照してください。</para>
<blockquote>
<para>Rancherは、オープンソースの強力なKubernetes管理プラットフォームであり、複数の環境にまたがるKubernetesクラスタのデプロイメント、運用、および監視を効率化します。オンプレミス、クラウド、エッジのいずれのクラスタを管理する場合でも、Rancherは、Kubernetesに関するあらゆるニーズに対応する、統合された中央プラットフォームを提供します。</para>
</blockquote>
<section xml:id="id-key-features-of-rancher">
<title>Rancherの主な機能</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">マルチクラスタ管理:</emphasis>
Rancherの直感的なインタフェースを使用して、パブリッククラウド、プライベートデータセンター、エッジロケーションのどこからでもKubernetesクラスタを管理できます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">セキュリティとコンプライアンス:</emphasis>
Rancherでは、Kubernetes環境全体にセキュリティポリシー、ロールベースのアクセス制御(RBAC)、およびコンプライアンス標準が適用されます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">クラスタ操作のシンプル化:</emphasis>
Rancherはクラスタのプロビジョニング、アップグレード、トラブルシューティングを自動化し、あらゆる規模のチームでKubernetesの操作をシンプル化します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">中央型のアプリケーションカタログ:</emphasis>
Rancherアプリケーションカタログは、多種多様なHelmチャートとKubernetes
Operatorを提供し、コンテナ化アプリケーションのデプロイと管理を容易にします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">継続的デリバリ:</emphasis>
RancherはGitOpsと継続的インテグレーション/継続的デリバリパイプラインをサポートしており、自動化および効率化されたアプリケーションデリバリプロセスを実現します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>SUSE EdgeでのRancherの使用</title>
<para>Rancherは、SUSE Edgeスタックに複数のコア機能を提供します。</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Kubernetesの集中管理</title>
<para>大量の分散クラスタが存在する一般的なエッジデプロイメントでは、Rancherは、これらのKubernetesクラスタを管理するための中央コントロールプレーンとして機能します。プロビジョニング、アップグレード、監視、およびトラブルシューティングのための統合インタフェースを提供し、操作をシンプル化し、一貫性を確保します。</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>クラスタデプロイメントのシンプル化</title>
<para>Rancherは、軽量なSLE Micro (SUSE Linux Enterprise
Micro)オペレーティングシステム上でのKubernetesクラスタの作成を効率化し、Kubernetesの堅牢な機能を備えたエッジインフラストラクチャの展開を容易にします。</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>アプリケーションのデプロイメントと管理</title>
<para>統合されたRancherアプリケーションカタログは、SUSE
Edgeクラスタ全体でのコンテナ化アプリケーションのデプロイと管理をシンプル化し、エッジワークロードのシームレスなデプロイメントを可能にします。</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>セキュリティとポリシーの適用</title>
<para>Rancherは、ポリシーベースのガバナンスツール、ロールベースのアクセス制御(RBAC)を備えているほか、外部の認証プロバイダと統合できます。これにより、SUSE
Edgeのデプロイメントは、分散環境において重要なセキュリティとコンプライアンスを維持できます。</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>ベストプラクティス</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>RancherにはビルトインコンポーネントとしてFleetが含まれており、Gitに保存されたコードでクラスタ設定やアプリケーションのデプロイメントを管理できます。</para>
</section>
<section xml:id="id-observability">
<title>可観測性</title>
<para>Rancherには、PrometheusやGrafanaなどのビルトインのモニタリングおよびログツールが含まれており、クラスタのヘルスとパフォーマンスについて包括的な洞察を得ることができます。</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、SLE Micro OSのベースイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBでプロビジョニングしたKubernetesクラスタ上にRancherをエアギャップインストールするには、<xref
linkend="rancher-install"/>に従ってください。</para>
</section>
<section xml:id="id-additional-resources-2">
<title>追加リソース</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancherのドキュメント</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher Community</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Helmチャート</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Kubernetes Operator</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Rancher Dashboard拡張機能</title>
<para>拡張機能により、ユーザ、開発者、パートナー、および顧客はRancher UIを拡張および強化できます。SUSE Edge
3.1では、KubeVirtとAkriのダッシュボード拡張機能を提供しています。</para>
<para>Rancher Dashboard拡張機能の一般的な情報については、<literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.9/integrations-in-rancher/rancher-extensions">Rancherのドキュメント</link></literal>を参照してください。</para>
<section xml:id="id-installation">
<title>インストール</title>
<para>ダッシュボード拡張機能を含むすべてのSUSE Edge 3.1コンポーネントは、OCIアーティファクトとして配布されます。SUSE
Edge拡張機能をインストールするにはRancher Dashboard UI、Helm、またはFleetを使用できます。</para>
<section xml:id="id-installing-with-rancher-dashboard-ui">
<title>Rancher Dashboard UIを使用したインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>ナビゲーションサイドバーの<emphasis role="strong">［ Configuration
(設定)］</emphasis>セクションで、<emphasis role="strong">［Extensions
(拡張機能)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para>［Extensions (拡張機能)］ページで、右上にある3つのドットメニューをクリックし、<emphasis
role="strong">［Manage Repositories (リポジトリの管理)］</emphasis>を選択します。</para>
<para>各拡張機能は、それぞれ独自のOCIアーティファクトを介して配布されます。したがって、インストールする必要のある拡張機能ごとにリポジトリを追加する必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Repositories
(リポジトリ)］</emphasis>ページで、［<literal>Create (作成)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>フォームにリポジトリ名と OCI アーティファクトURLを指定し、［<literal>Create (作成)</literal>］をクリックします。</para>
<para>Akriダッシュボード拡張機能リポジトリURL:
<literal>oci://registry.suse.com/edge/3.1/akri-dashboard-extension-chart</literal></para>
<para>KubeVirtダッシュボード拡張機能リポジトリURL:
<literal>oci://registry.suse.com/edge/3.1/kubevirt-dashboard-extension-chart</literal></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-create-oci-repository.png" width=""/>
</imageobject>
<textobject><phrase>ダッシュボード拡張機能OCIリポジトリの作成</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能リポジトリがリストに追加され、［<literal>Active (アクティブ)</literal>］状態であることがわかります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-repositories-list.png" width=""/>
</imageobject>
<textobject><phrase>ダッシュボード拡張機能リポジトリリスト</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>ナビゲーションサイドバーの <emphasis role="strong">［Configuration (設定)］</emphasis>セクションの
<emphasis role="strong">［Extensions (拡張機能)］</emphasis>に戻ります。</para>
<para><emphasis role="strong">［Available
(利用可能)］</emphasis>タブで、インストール可能な拡張機能を確認できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-available-extensions.png" width=""/>
</imageobject>
<textobject><phrase>ダッシュボード拡張機能利用可能な拡張機能</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能カードで、［<literal>Install (インストール)</literal>］をクリックし、インストールを確認します。</para>
<para>拡張機能がインストールされると、Rancher UIによってページの再ロードが促されます。<literal><link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions#installing-extensions">拡張機能のインストールのRancherドキュメントページ</link></literal>を参照してください。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-helm">
<title>Helmを使用したインストール</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/3.1/kubevirt-dashboard-extension-chart --version 1.1.0 --namespace cattle-ui-plugin-system

# Akri extension
helm install akri-dashboard-extension oci://registry.suse.com/edge/3.1/akri-dashboard-extension-chart --version 1.1.0 --namespace cattle-ui-plugin-system</screen>
<note>
<para>拡張機能は<literal>cattle-ui-plugin-system</literal>ネームスペースにインストールする必要があります。</para>
</note>
<note>
<para>拡張機能がインストールされたら、Rancher Dashboard UIを再ロードする必要があります。</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Fleetを使用したインストール</title>
<para>Fleetを使用してダッシュボード拡張機能をインストールするには、カスタムの<literal>fleet.yaml</literal>バンドル設定ファイルを使用して、Gitリポジトリを指す<literal>gitRepo</literal>リソースを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/3.1/kubevirt-dashboard-extension-chart
  version: "1.1.0"</screen>
<screen language="yaml" linenumbering="unnumbered"># Akri extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: akri-dashboard-extension
  chart: oci://registry.suse.com/edge/3.1/akri-dashboard-extension-chart
  version: "1.1.0"</screen>
<note>
<para>拡張機能を正しくインストールするには、<literal>releaseName</literal>プロパティが必須であり、拡張機能名と一致している必要があります。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>詳細については、Fleet (<xref linkend="components-fleet"/>)のセクションおよび<literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>のリポジトリを参照してください。</para>
<para>拡張機能がインストールされると、その拡張機能が<emphasis role="strong">［Extensions
(拡張機能)］</emphasis>セクションの<emphasis role="strong">［Installed
(インストール済み)］</emphasis>タブに表示されます。拡張機能はApps/Marketplace経由でインストールされたものではないため、「<literal>Third-Party
(サードパーティ)</literal>」というラベルが付きます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width=""/> </imageobject>
<textobject><phrase>インストール済みダッシュボード拡張機能</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>KubeVirtダッシュボード拡張機能</title>
<para>KubeVirt拡張機能は、Rancher Dashboard UIに基本的な仮想マシン管理機能を提供します。その機能については、「KubeVirt
Rancher Dashboard拡張機能の使用」(<xref
linkend="kubevirt-dashboard-extension"/>)を参照してください。</para>
</section>
<section xml:id="id-akri-dashboard-extension">
<title>Akriダッシュボード拡張機能</title>
<para>Akriは、異種リーフデバイス(IPカメラやUSBデバイスなど)をKubernetesクラスタのリソースとして簡単に公開できると同時に、GPUやFPGAなどの組み込みハードウェアリソースの公開もサポートするKubernetesリソースインタフェースです。Akriは、このようなデバイスにアクセスできるノードを継続的に検出し、それらに基づいてワークロードをスケジュールします。</para>
<para>Akriダッシュボード拡張機能を使用すると、Rancher
Dashboardユーザインタフェースを使用して、リーフデバイスを管理および監視し、デバイスが検出されたらワークロードを実行できます。</para>
<para>拡張機能については、Akriに関するセクション(<xref
linkend="akri-dashboard-extension"/>)で詳しく説明されています。</para>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link
xl:href="https://fleet.rancher.io">Fleet</link>は、ユーザがローカルクラスタをより細かく制御できるようにするとともに、GitOpsを通じて常時監視を行えるようにすることを目的に設計されたコンテナ管理およびデプロイメントエンジンです。Fleetはスケール能力に重点を置いているだけでなく、クラスタに何がインストールされているかを正確に監視するための高度な制御と可視性もユーザに提供します。</para>
<para>Fleetは、生のKubernetes
YAML、Helmチャート、Kustomize、またはこれら3つの組み合わせのGitからデプロイメントを管理できます。ソースにかかわらず、すべてのリソースは動的にHelmチャートに変換され、すべてのリソースをクラスタにデプロイするエンジンとしてHelmが使用されます。その結果、ユーザはクラスタの高度な制御、一貫性、監査能力を実現できます。</para>
<para>Fleetの仕組みについては、<link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/architecture">こちらのページ</link>を参照してください。</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Helmを使用したFleetのインストール</title>
<para>FleetはRancherにビルトインされていますが、Helmを使用して、スタンドアロンアプリケーションとして任意のKubernetesクラスタに<link
xl:href="https://fleet.rancher.io/installation">インストール</link>することもできます。</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>RancherでのFleetの使用</title>
<para>Rancherは、Fleetを使用してアプリケーションを管理対象クラスタ全体にデプロイします。Fleetを使用した継続的デリバリにより、大量のクラスタで実行されるアプリケーションを管理するために設計された、大規模なGitOpsが導入されます。</para>
<para>FleetはRancherと統合してその一部として機能します。Rancherで管理されるクラスタには、インストール/インポートプロセスの一部としてFleetエージェントが自動的にデプロイされるため、クラスタはすぐにFleetで管理できるようになります。</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Rancher UIでのFleetへのアクセス</title>
<para>FleetはRancherにプリインストールされており、Rancher UIの<emphasis role="strong">［Continuous
Delivery
(継続的デリバリ)］</emphasis>オプションで管理されます。継続的デリバリに関する追加情報、およびFleetのトラブルシューティングに関する他のヒントについては、<link
xl:href="https://fleet.rancher.io/troubleshooting">こちら</link>を参照してください。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width=""/>
</imageobject>
<textobject><phrase>Fleetダッシュボード</phrase></textobject>
</mediaobject>
</informalfigure>
<para>［Continuous Delivery (継続的デリバリ)］セクションは次の項目で構成されます。</para>
<section xml:id="id-dashboard">
<title>Dashboard (ダッシュボード)</title>
<para>すべてのワークスペースにわたるすべてのGitOpsリポジトリの概要ページ。リポジトリのあるワークスペースのみが表示されます。</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos (Gitリポジトリ)</title>
<para>選択したワークスペース内のGitOpsリポジトリのリスト。ページ上部のドロップダウンリストを使用してアクティブなワークスペースを選択します。</para>
</section>
<section xml:id="id-clusters">
<title>Clusters (クラスタ)</title>
<para>管理対象クラスタのリスト。デフォルトでは、Rancherで管理されているすべてのクラスタが<literal>fleet-default</literal>ワークスペースに追加されます。<literal>fleet-local</literal>ワークスペースにはローカル(管理)クラスタが含まれます。ここから、クラスタを<literal>Pause
(一時停止)</literal>または<literal>Force Update
(強制的に更新)</literal>したり、クラスタを別のワークスペースに移動したりすることができます。クラスタを編集すると、クラスタのグループ化に使用するラベルや注釈を更新できます。</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups (クラスタグループ)</title>
<para>このセクションでは、セレクタを使用してワークスペース内のクラスタのカスタムグループを作成できます。</para>
</section>
<section xml:id="id-advanced">
<title>Advanced (詳細)</title>
<para>［Advanced (詳細)］セクションでは、ワークスペースやその他の関連するFleetリソースを管理できます。</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Rancher Dashboardを使用してRancherおよびFleetとともにKubeVirtをインストールする例</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>fleet.yaml</literal>ファイルを含むGitリポジトリを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/3.1/kubevirt-chart"
  version: "0.4.0"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>Rancher Dashboardで、<emphasis role="strong">☰ &gt; ［Continuous Delivery
(継続的デリバリ)］ &gt; ［Git Repos (Gitリポジトリ)］</emphasis>に移動して、［<literal>Add
Repository (リポジトリの追加)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>リポジトリの作成ウィザードの指示に従ってGitリポジトリを作成します。<emphasis role="strong">［Name
(名前)］</emphasis>、<emphasis role="strong">［Repository URL
(リポジトリのURL)］</emphasis>(前の手順で作成したGitリポジトリを参照)を指定し、適切なブランチまたはリビジョンを選択します。より複雑なリポジトリの場合は、<emphasis
role="strong">［Paths (パス)］</emphasis>を指定して、1つのリポジトリで複数のディレクトリを使用します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリの作成1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［<literal>Next (次へ)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>次の手順では、ワークロードをデプロイする場所を定義できます。クラスタの選択では複数の基本オプションがあります。クラスタをまったく選択しないことも、すべてのクラスタを選択することも、特定の管理対象クラスタやクラスタグループ(定義されている場合)を直接選択することもできます。［Advanced
(詳細)］オプションを使用すると、YAML経由でセレクタを直接編集できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリの作成2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［<literal>Create
(作成)</literal>］をクリックします。リポジトリが作成されます。今後、ワークロードはリポジトリ定義に一致するクラスタにインストールされ、同期が維持されます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>デバッグとトラブルシューティング</title>
<para>ナビゲーションの［Advanced (詳細)］セクションでは、下位レベルのFleetリソースの概要が表示されます。<link
xl:href="https://fleet.rancher.io/ref-bundle-stages">バンドル</link>は、Gitからのリソースのオーケストレーションに使用される内部リソースです。Gitリポジトリがスキャンされると、バンドルが1つ以上生成されます。</para>
<para>特定のリポジトリに関連するバンドルを見つけるには、［Git Repos (Gitリポジトリ)］の［Detail
(詳細)］ページに移動し、［<literal>Bundles (バンドル)</literal>］タブをクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリバンドル</phrase></textobject>
</mediaobject>
</informalfigure>
<para>クラスタごとに、作成されたBundleDeploymentリソースにバンドルが適用されます。BundleDeploymentの詳細を表示するには、［Git
Repos (Gitリポジトリ)］の［Detail (詳細)］ページの右上にある ［<literal>Graph
(グラフ)</literal>］ボタンをクリックします。<emphasis role="strong">［Repo (リポジトリ)
］&gt;［Bundles
(バンドル)］&gt;［BundleDeployments］</emphasis>のグラフがロードされます。グラフ内のBundleDeploymentをクリックすると詳細が表示され、［<literal>Id
(ID)</literal>］をクリックするとBundleDeployment YAMLが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリのグラフ</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Fleetのトラブルシューティングのヒントに関する追加情報については、<link
xl:href="https://fleet.rancher.io/troubleshooting">こちら</link>を参照してください。</para>
</section>
<section xml:id="id-fleet-examples">
<title>Fleetの例</title>
<para>Edgeチームは、Fleetを使用してEdgeプロジェクトをインストールする例を含む<link
xl:href="https://github.com/suse-edge/fleet-examples">リポジトリ</link>を維持しています。</para>
<para>Fleetプロジェクトには、<link
xl:href="https://fleet.rancher.io/gitrepo-content">Gitリポジトリ構造</link>のすべてのユースケースをカバーする<link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>リポジトリが含まれています。</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SLE Micro</title>
<para><link xl:href="https://documentation.suse.com/sle-micro/6.0/">SLE
Micro公式ドキュメント</link>を参照してください。</para>
<blockquote>
<para>SUSE Linux Enterprise Microは、エッジ向けの軽量でセキュアなオペレーティングシステムです。SUSE Linux
Enterprise Microには、SUSE Linux
Enterpriseのエンタープライズ向けに強化されたコンポーネントと、開発者が最新のイミュータブルオペレーティングシステムに求める機能が統合されています。その結果、クラス最高のコンプライアンスを備えた信頼性の高いインフラストラクチャプラットフォームが実現し、使いやすさも向上しています。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-sle-micro">
<title>SUSE EdgeでのSLE Microの用途</title>
<para>SUSEでは、SLE
Microをプラットフォームスタックのベースオペレーティングシステムとして使用します。これにより、構築基盤となる、安全で安定した最小限のベースが提供されます。</para>
<para>SLE
Microでは、独自の方法でファイルシステム(Btrfs)スナップショットを使用しており、アップグレードで問題が発生した場合に簡単にロールバックできます。これにより、問題が発生した場合、物理的にアクセスしなくてもプラットフォーム全体をリモートで安全にアップグレードできます。</para>
</section>
<section xml:id="id-best-practices-2">
<title>ベストプラクティス</title>
<section xml:id="id-installation-media">
<title>インストールメディア</title>
<para>SUSE Edgeは、Edge Image Builder (<xref linkend="components-eib"/>)を使用して、SLE
Microのセルフインストールのインストールイメージを事前設定します。</para>
</section>
<section xml:id="id-local-administration">
<title>ローカル管理</title>
<para>SLE Microには、Webアプリケーションでホストをローカルに管理できるCockpitが付属しています。</para>
<para>このサービスはデフォルトでは無効になっていますが、systemdサービス<literal>cockpit.socket</literal>を有効にすることで開始できます。</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現時点では、SLE Microで利用可能なデスクトップ環境はありませんが、コンテナ化されたソリューションが開発中です。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link
xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>は、Kubernetesにベアメタルインフラストラクチャ管理機能を提供するCNCFプロジェクトです。</para>
<para>Metal<superscript>3</superscript>は、<link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>などのアウトオブバンドプロトコルを介した管理をサポートするベアメタルサーバのライフサイクルを管理するためのKubernetesネイティブリソースを提供します。</para>
<para>また、<link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link>も十分にサポートされており、広く採用されているベンダニュートラルなAPIを使用して、複数のインフラストラクチャプロバイダにわたってインフラストラクチャリソースを管理できます。</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>SUSE EdgeでのMetal3の用途</title>
<para>この方法は、ターゲットハードウェアがアウトオブバンド管理をサポートしていて、完全に自動化されたインフラストラクチャ管理フローが望まれるシナリオで役立ちます。</para>
<para>この方法では宣言型APIが提供されており、このAPIを使用することで、検査、クリーニング、プロビジョニング/プロビジョニング解除の自動化を含む、ベアメタルサーバのインベントリと状態の管理が可能になります。</para>
</section>
<section xml:id="id-known-issues-3">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>アップストリームの<link
xl:href="https://github.com/metal3-io/ip-address-manager">IPアドレス管理コントローラ</link>は、SUSEが選択したネットワーク設定ツールとの互換性がまだないため、現在はサポートされていません。</para>
</listitem>
<listitem>
<para>関連して、IPAMリソースとMetal3DataTemplateのnetworkDataフィールドはサポートされていません。</para>
</listitem>
<listitem>
<para>redfish-virtualmediaを介したデプロイメントのみが現在サポートされています。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para><link
xl:href="https://github.com/suse-edge/edge-image-builder">公式リポジトリ</link>を参照してください。</para>
<para>Edge Image Builder (EIB)は、マシンをブートストラップするためのCustomized, Ready-to-Boot
(CRB)ディスクイメージの生成を効率化するツールです。これらのイメージにより、SUSEソフトウェアスタック全体を単一のイメージでエンドツーエンドにデプロイできます。</para>
<para>EIBはあらゆるプロビジョニングシナリオ向けのCRBイメージを作成できますが、EIBが非常に大きな価値を発揮するのは、ネットワークが制限されているか、完全に分離されているエアギャップデプロイメントにおいてです。</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>SUSE EdgeでのEdge Image Builderでの用途</title>
<para>SUSE Edgeでは、さまざまなシナリオ用にカスタマイズされたSLE
Microイメージをシンプルかつ迅速に設定するためにEIBを使用します。これらのシナリオには、以下を使用する仮想マシンとベアメタル
マシンのブートストラップが含まれます。</para>
<itemizedlist>
<listitem>
<para>K3s/RKE2 Kubernetesの完全なエアギャップデプロイメント(シングルノードとマルチノード)</para>
</listitem>
<listitem>
<para>HelmチャートとKubernetesマニフェストの完全なエアギャップデプロイメント</para>
</listitem>
<listitem>
<para>Elemental APIを介したRancherへの登録</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>カスタマイズされたネットワーキング(静的 IP、ホスト名、VLAN、ボンディングなど)</para>
</listitem>
<listitem>
<para>カスタマイズされたオペレーティングシステム設定(ユーザ、グループ、パスワード、SSHキー、プロキシ、NTP、カスタムSSL証明書など)</para>
</listitem>
<listitem>
<para>ホストレベルおよびサイドロードRPMパッケージのエアギャップインストール(依存関係の解決を含む)</para>
</listitem>
<listitem>
<para>OS管理のためのSUSE Managerへの登録</para>
</listitem>
<listitem>
<para>組み込みコンテナイメージ</para>
</listitem>
<listitem>
<para>カーネルコマンドライン引数</para>
</listitem>
<listitem>
<para>ブート時に有効化/無効化されるsystemdユニット</para>
</listitem>
<listitem>
<para>手動タスク用のカスタムスクリプトとファイル</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>はじめに</title>
<para>Edge Image Builderの使用とテストに関する包括的なドキュメントについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">こちら</link>を参照してください。</para>
<para>また、Edge Image Builderのクイックスタートガイド(<xref
linkend="quickstart-eib"/>)では、基本的なデプロイメントシナリオを説明しています。</para>
</section>
<section xml:id="id-known-issues-4">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>EIBは、Helmチャートをテンプレート化してテンプレート内のすべてのイメージを解析することで、Helmチャートをエアギャップ化します。Helmチャートですべてのイメージをテンプレート内に保持せず、代わりにイメージをサイドロードする場合、EIBではそれらのイメージを自動的にエアギャップ化できません。これを解決するには、検出されないイメージを定義ファイルの<literal>embeddedArtifactRegistry</literal>セクションに手動で追加します。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Edgeネットワーキング</title>
<para>このセクションでは、SUSE Edgeソリューションにおけるネットワーク設定へのアプローチについて説明します。宣言的な方法でSLE
Micro上でNetworkManagerを設定する方法を示し、関連ツールの統合方法について説明します。</para>
<section xml:id="id-overview-of-networkmanager">
<title>NetworkManagerの概要</title>
<para>NetworkManagerは、プライマリネットワーク接続と他の接続インタフェースを管理するツールです。</para>
<para>NetworkManagerは、ネットワーク設定を、望ましい状態が含まれる接続ファイルとして保存します。これらの接続は、<literal>/etc/NetworkManager/system-connections/</literal>ディレクトリにファイルとして保存されます。</para>
<para>NetworkManagerの詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/6.0/html/Micro-network-configuration/index.html">SLE
Microのドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>nmstateの概要</title>
<para>nmstateは広く採用されているライブラリ(CLIツールが付属)であり、定義済みスキーマを使用したネットワーク設定用の宣言型APIを提供します。</para>
<para>nmstateの詳細については、<link
xl:href="https://nmstate.io/">アップストリームドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>NetworkManager Configurator (nmc)の概要</title>
<para>SUSE Edgeで利用可能なネットワークのカスタマイズオプションは、NetworkManager Configurator
(短縮名は<emphasis>nmc</emphasis>)と呼ばれるCLIツールを使用して実行します。このツールはnmstateライブラリによって提供される機能を利用しているため、静的IPアドレス、DNSサーバ、VLAN、ボンディング、ブリッジなどを完全に設定できます。このツールを使用して、事前定義された望ましい状態からネットワーク設定を生成し、その設定を多数のノードに自動的に適用できます。</para>
<para>NetworkManager Configurator (nmc)の詳細については、<link
xl:href="https://github.com/suse-edge/nm-configurator">アップストリームリポジトリ</link>を参照してください。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>SUSE EdgeでのNetworkManager Configuratorの用途</title>
<para>SUSE
Edgeは、<emphasis>nmc</emphasis>を利用して次のようなさまざまなプロビジョニングモデルでネットワークをカスタマイズします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングシナリオにおけるカスタムネットワーク設定(<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>イメージベースのプロビジョニング シナリオにおける宣言的な静的設定(<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Edge Image Builderを使用した設定</title>
<para>Edge Image Builder
(EIB)は、1つのOSイメージで複数のホストを設定できるツールです。このセクションでは、宣言型アプローチを使用して、どのように目的のネットワーク状態を記述するかと、それらがどのように各NetworkManager接続に変換され、プロビジョニングプロセス中に適用されるかを示します。</para>
<section xml:id="id-prerequisites-3">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP6またはopenSUSE Leap 15.6を実行しているx86_64物理ホスト(または仮想マシン)</para>
</listitem>
<listitem>
<para>利用可能なコンテナランタイム(Podmanなど)</para>
</listitem>
<listitem>
<para>SL Micro 6.0 RAWイメージのコピー(<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあります)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Edge Image Builderのコンテナイメージの取得</title>
<para>EIBのコンテナイメージは一般に公開されており、次のコマンドを実行してSUSE Edgeレジストリからダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.1/edge-image-builder:1.1.0</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>イメージ設定ディレクトリの作成</title>
<para>まず設定ディレクトリを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>ダウンロードしたゴールデンイメージのコピーを確実に設定ディレクトリに移動します。</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SL-Micro.x86_64-6.0-Base-GM2.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIBは、ゴールデンイメージの入力を変更することはありません。</para>
</note>
</blockquote>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SL-Micro.x86_64-6.0-Base-GM2.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>イメージ定義ファイルの作成</title>
<para>定義ファイルには、Edge Image Builderがサポートする設定オプションの大部分を記述します。</para>
<para>OSイメージの非常に基本的な定義ファイルから開始しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.0
image:
  arch: x86_64
  imageType: raw
  baseImage: SL-Micro.x86_64-6.0-Base-GM2.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para><literal>image</literal>セクションは必須で、入力イメージ、そのアーキテクチャとタイプ、および出力イメージの名前を指定します。<literal>operatingSystem</literal>セクションはオプションであり、プロビジョニングされたシステムに<literal>root/eib</literal>のユーザ名/パスワードでログインできるようにするための設定を含めます。</para>
<blockquote>
<note>
<para><literal>openssl passwd -6
&lt;password&gt;</literal>を実行して、独自の暗号化パスワードを自由に使用してください。</para>
</note>
</blockquote>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SL-Micro.x86_64-6.0-Base-GM2.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>ネットワーク設定の定義</title>
<para>先ほど作成したイメージ定義ファイルには、望ましいネットワーク設定が含まれていません。そこで、<literal>network/</literal>という特別なディレクトリの下にその設定を入力します。では、作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>前述のように、NetworkManager Configurator
(<emphasis>nmc</emphasis>)ツールでは、事前定義されたスキーマの形式での入力が必要です。さまざまなネットワーキングオプションの設定方法については、<link
xl:href="https://nmstate.io/examples.html">アップストリームのNMStateの例のドキュメント</link>を参照してください。</para>
<para>このガイドでは、次の3つの異なるノードでネットワーキングを設定する方法について説明します。</para>
<itemizedlist>
<listitem>
<para>2つのEthernetインタフェースを使用するノード</para>
</listitem>
<listitem>
<para>ネットワークボンディングを使用するノード</para>
</listitem>
<listitem>
<para>ネットワークブリッジを使用するノード</para>
</listitem>
</itemizedlist>
<warning>
<para>特にKubernetesクラスタを設定する場合、まったく異なるネットワークセットアップを運用ビルドで使用することは推奨されません。ネットワーキング設定は通常、特定のクラスタ内のノード間、または少なくともロール間で同種にすることをお勧めします。このガイドにはさまざまな異なるオプションが含まれていますが、これは参考例として提供することのみを目的としています。</para>
</warning>
<blockquote>
<note>
<para>以下では、IPアドレス範囲<literal>192.168.122.1/24</literal>を使用するデフォルトの<literal>libvirt</literal>ネットワークを想定しています。ご自身の環境でこの範囲が異なる場合は、適宜調整してください。</para>
</note>
</blockquote>
<para><literal>node1.suse.com</literal>という名前の最初のノードに対して、望ましい状態を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>この例では、2つのEthernetインタフェース(eth0とeth3)、要求されたIPアドレス、ルーティング、およびDNS解決の望ましい状態を定義しています。</para>
<warning>
<para>必ず、すべてのEthernetインタフェースのMACアドレスを記述してください。これらのMACアドレスは、プロビジョニングプロセス中にノードの識別子として使用され、どの設定を適用すべきかを判断するのに役立ちます。このようにして、1つのISOまたはRAWイメージを使用して複数のノードを設定できます。</para>
</warning>
<para>次は、<literal>node2.suse.com</literal>という名前の2つ目のノードです。このノードではネットワークボンディングを使用します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>この例では、IPアドレス指定を有効にしていない2つのEthernetインタフェース(eth0とeth1)の望ましい状態と、ラウンドロビンポリシーによるボンディング、およびネットワークトラフィックを転送するために使用する各アドレスを定義します。</para>
<para>最後に、3つ目となる、望ましい状態の最後のファイルを作成します。これはネットワークブリッジを利用し、<literal>node3.suse.com</literal>という名前です。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SL-Micro.x86_64-6.0-Base-GM2.raw</screen>
<blockquote>
<note>
<para><literal>network/</literal>ディレクトリにあるファイル名は意図的なものです。これらの名前は、プロビジョニングプロセス中に設定されるホスト名に対応しています。</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>OSイメージの構築</title>
<para>これで必要な設定はすべて完了したので、次のコマンドを実行するだけでイメージを構築できます。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.1/edge-image-builder:1.1.0 build --definition-file definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>上のスニペットから<literal>Network</literal>コンポーネントが正常に設定されていることがわかるので、エッジノードのプロビジョニングに進むことができます。</para>
<blockquote>
<note>
<para>ログファイル(<literal>network-config.log</literal>)とそれぞれのNetworkManager接続ファイルは、イメージ実行のタイムスタンプ付きディレクトリの下にある、結果の<literal>_build</literal>ディレクトリで検査できます。</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>エッジノードのプロビジョニング</title>
<para>作成されたRAWイメージをコピーしてみましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>構築されたイメージを4回コピーしましたが、3つのノードのネットワーク設定しか指定していません。これは、どの目的の設定にも一致しないノードをプロビジョニングするとどうなるかも紹介したいためです。</para>
<blockquote>
<note>
<para>このガイドでは、ノードプロビジョニングの例に仮想化を使用します。必要な拡張機能がBIOSで有効になっていることを確認してください(詳細については
<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">こちら</link>を参照してください)。</para>
</note>
</blockquote>
<para><literal>virt-install</literal>を使用し、コピーしたRAWディスクを使用して仮想マシンを作成します。各仮想マシンは10GBのRAMと6個のvCPUを使用します。</para>
<section xml:id="id-provisioning-the-first-node">
<title>1つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>上記で説明した望ましい状態のMACアドレスと同じMACアドレスを持つネットワークインタフェースを作成することが重要です。</para>
</note>
</blockquote>
<para>操作が完了すると、次のような内容が表示されます。</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Enterprise Micro 6.0 (x86_64) - Kernel 6.4.0-18-default (tty1).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>これで、<literal>root:eib</literal>の資格情報ペアを使用してログインできます。ここで提示されている<literal>virsh
console</literal>よりもSSHでホストに接続したい場合は、SSHで接続することもできます。</para>
<para>ログインしたら、すべての設定が完了していることを確認しましょう。</para>
<para>ホスト名が適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>2つのEthernetインタフェースが設定されていて、そのうちの1つだけがアクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>2つ目のインタフェースが、目的のネットワーキング状態で指定されている定義済みの<literal>eth3</literal>ではなく、<literal>eth1</literal>になっていることがわかります。これは、NetworkManager
Configurator
(<emphasis>nmc</emphasis>)が、MACアドレス<literal>34:8a:b1:4b:16:e2</literal>を持つNICにOSによって別の名前が付けられていることを検出することができ、それに応じて設定を調整するためです。</para>
<para>プロビジョニングのCombustionのフェーズを検査して、この調整が実際に行われたことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>続いて残りのノードをプロビジョニングしますが、ここでは最終的な設定の違いのみを示します。これからプロビジョニングするすべてのノードに対して、上記のチェックのいずれか、またはすべてを自由に適用してください。</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>2つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがボンディングされたインタフェースを使用しているかどうかを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>ルーティングでボンディングが使用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>静的な接続ファイルが適切に利用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>3つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがネットワークブリッジを使用していることを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>ルーティングでブリッジが使用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>静的な接続ファイルが適切に利用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>4つ目のノードのプロビジョニング</title>
<para>最後に、事前定義されたどの設定ともMACアドレスが一致しないノードをプロビジョニングします。このような場合は、DHCPをデフォルトにしてネットワークインタフェースを設定します。</para>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがそのネットワークインタフェースにランダムなIPアドレスを使用していることを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>nmcがこのノードに静的な設定を適用できなかったことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>EthernetインタフェースがDHCPを介して設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="networking-unified">
<title>統合されたノード設定</title>
<para>既知のMACアドレスに依存できない場合もあります。このような場合は、いわゆる<emphasis>「統合設定」</emphasis>を選択できます。これにより、<literal>_all.yaml</literal>ファイルで設定を指定し、プロビジョニングされたノードすべてに適用することができます。</para>
<para>異なる設定構造を使用して、エッジノードを構築およびプロビジョニングします。<xref
linkend="image-config-dir-creation"/>から<xref
linkend="default-network-definition"/>のすべての手順に従います。</para>
<para>この例では、2つのEthernetインタフェース(eth0とeth1)の望ましい状態を定義します。一方ではDHCPを使用し、他方には静的IPアドレスを割り当てます。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.1/edge-image-builder:1.1.0 build --definition-file definition.yaml</screen>
<para>イメージが正常に構築されたら、それを使用して仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>プロビジョニングプロセスには数分かかる場合があります。終了したら、指定された資格情報でシステムにログインします。</para>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Ethernetインタフェースが設定され、アクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>カスタムネットワーク設定</title>
<para>ここまでは、NetworkManager Configuratorを利用した、Edge Image
Builderのデフォルトのネットワーク設定について説明してきました。一方で、カスタムスクリプトを使用してネットワーク設定を変更するオプションもあります。このオプションは非常に柔軟性が高く、MACアドレスにも依存しませんが、1つのイメージで複数のノードをブートストラップする場合に使用してもあまり便利ではないという制限があります。</para>
<blockquote>
<note>
<para><literal>/network</literal>ディレクトリにある、望ましいネットワーク状態を記述したファイルを介して、デフォルトのネットワーク設定を使用することをお勧めします。カスタムスクリプトを選択するのは、デフォルト設定の動作がユースケースに当てはまらない場合のみにしてください。</para>
</note>
</blockquote>
<para>異なる設定構造を使用して、エッジノードを構築およびプロビジョニングします。<xref
linkend="image-config-dir-creation"/>から<xref
linkend="default-network-definition"/>のすべての手順に従います。</para>
<para>この例では、プロビジョニングされたすべてのノードで<literal>eth0</literal>インタフェースに静的設定を適用し、NetworkManagerによって自動的に作成された有線接続を削除して無効にするカスタムスクリプトを作成します。これは、クラスタ内のすべてのノードに同一のネットワーキング設定を確実に適用したい場合に便利です。その結果、イメージの作成前に各ノードのMACアドレスを気にする必要がなくなります。</para>
<para>まず、<literal>/custom/files</literal>ディレクトリに接続ファイルを保存しましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>静的設定が作成されたので、カスタムネットワークスクリプトも作成します。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>nmcのバイナリはこれまで同様にデフォルトで含まれるため、必要に応じて<literal>configure-network.sh</literal>スクリプトで使用することもできます。</para>
</note>
</blockquote>
<warning>
<para>カスタムスクリプトは常に設定ディレクトリの<literal>/network/configure-network.sh</literal>で提供する必要があります。このファイルが存在する場合、他のファイルはすべて無視されます。YAML形式の静的設定とカスタムスクリプトの両方を同時に使用してネットワークを設定することはできません。</para>
</warning>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SL-Micro.x86_64-6.0-Base-GM2.raw</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.1/edge-image-builder:1.1.0 build --definition-file definition.yaml</screen>
<para>イメージが正常に構築されたら、それを使用して仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>プロビジョニングプロセスには数分かかる場合があります。終了したら、指定された資格情報でシステムにログインします。</para>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>接続ファイルを使用してEthernetインタフェースが静的に設定されていて、アクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elementalは、Kubernetesを使用した完全にクラウドネイティブな集中型のOS管理を可能にするソフトウェアスタックです。Elementalスタックは、Rancher自体またはエッジノード上に存在する多数のコンポーネントで構成されます。中核となるコンポーネントは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> -
Rancher上に存在し、クライアントからの登録リクエストを処理するコアオペレータ。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis> -
エッジノード上で動作し、<literal>elemental-operator</literal>を介して登録できるようにするクライアント。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis> -
エッジノードに存在するエージェント。その設定は<literal>elemental-register</literal>から提供され、<literal>rancher-system-agent</literal>を設定するための<literal>plan</literal>を受け取ります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis> -
エッジノードが完全に登録された後に、<literal>elemental-system-agent</literal>から処理を引き継ぎ、Rancher
Managerからの他の<literal>plans</literal>を待機します(Kubernetesのインストールなど)。</para>
</listitem>
</itemizedlist>
<para>Elemental、およびElementalとRancherとの関係の詳細については、<link
xl:href="https://elemental.docs.rancher.com/">Elementalのアップストリームドキュメント</link>を参照してください。</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>SUSE EdgeでのElementalの用途</title>
<para>SUSEでは、Metal<superscript>3</superscript>を選択できないリモート
デバイス(たとえば、BMCがない、デバイスがNATゲートウェイの背後にあるなど)の管理にElementalの一部を使用しています。このツールにより、オペレータは、デバイスがいつどこに配置されるかがわかる前に、ラボでデバイスをブートストラップできます。すなわち、<literal>elemental-register</literal>と<literal>elemental-system-agent</literal>コンポーネントを利用して、「Phone
Home」ネットワークプロビジョニングのユースケースでSLE MicroホストをRancherにオンボードできます。Edge Image Builder
(EIB)を使用してデプロイメントイメージを作成する場合、EIBの設定ディレクトリで登録設定を指定することで、Rancherを使用してElemental経由で自動登録を行うことができます。</para>
<note>
<para>SUSE Edge 3.1では、Elementalのオペレーティング システム管理の側面を利用して<emphasis
role="strong">「いない」</emphasis>ため、Rancher経由でオペレーティング
システムのパッチを管理することはできません。SUSE Edgeでは、Elementalツールを使用してデプロイメント
イメージを構築する代わりに、登録設定を使用するEdge Image Builderツールを使用します。</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>ベストプラクティス</title>
<section xml:id="id-installation-media-2">
<title>インストールメディア</title>
<para>「Phone
Homeネットワークプロビジョニング」のデプロイメントフットプリントでElementalを利用してRancherに登録可能なデプロイメントイメージを構築する場合、SUSE
Edgeでは、Elementalを使用したリモートホストのオンボーディング(<xref
linkend="quickstart-elemental"/>)のクイックスタートで詳しく説明されている手順に従う方法をお勧めします。</para>
</section>
<section xml:id="id-labels">
<title>ラベル</title>
<para>Elementalは、<literal>MachineInventory</literal>
CRDを使用してインベントリを追跡し、インベントリを選択する方法を提供します。たとえば、Kubernetesクラスタのデプロイ先のマシンをラベルに基づいて選択できます。これにより、ユーザはハードウェアを購入する前に、インフラストラクチャのニーズの(すべてではないにしても)ほとんどを事前に定義しておくことができます。また、ノードはその各インベントリオブジェクトのラベルを追加/削除できるので(<literal>elemental-register</literal>を、追加のフラグ<literal>--label
"FOO=BAR "</literal>を指定して再実行する)、ノードがブートされた場所を検出してRancherに知らせるスクリプトを作成できます。</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現在のところ、Elemental UIは、インストールメディアの構築方法を認識したり、「Elemental
Teal」以外のオペレーティングシステムを更新したりすることはできません。これは将来のリリースで対応予定です。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-akri">
<title>Akri</title>
<para>Akriは、リーフデバイスを検出してKubernetesネイティブリソースとして提供することを目的としたCNCF-Sandboxプロジェクトです。また、検出された各デバイスに対してPodやジョブをスケジュールすることもできます。デバイスはノードローカルでもネットワーク接続されていてもよく、さまざまなプロトコルを使用できます。</para>
<para>Akriのアップストリームドキュメントについては、<link
xl:href="https://docs.akri.sh">https://docs.akri.sh</link>を参照してください。</para>
<section xml:id="id-how-does-suse-edge-use-akri">
<title>SUSE EdgeでのAkriの用途</title>
<warning>
<para>Akriは現在、SUSE Edgeスタックでの技術プレビュー中です。</para>
</warning>
<para>Akriは、リーフデバイスに対するワークロードの検出とスケジューリングが必要な場合はいつでも、Edgeスタックの一部として利用できます。</para>
<section xml:id="id-installing-akri">
<title>Akriのインストール</title>
<para>AkriはEdge
Helmリポジトリ内でHelmチャートとして利用できます。Akriを設定するための推奨方法は、指定したHelmチャートを使用してさまざまなコンポーネント(エージェント、コントローラ、ディスカバリハンドラ)をデプロイし、好みのデプロイメントメカニズムを使用してAkriの設定CRDをデプロイすることです。</para>
</section>
<section xml:id="id-configuring-akri">
<title>Akriの設定</title>
<para>Akriは、<literal>akri.sh/Configuration</literal>オブジェクトを使用して設定します。このオブジェクトは、デバイスの検出方法、および一致するデバイスが検出されたときの処理に関するすべての情報を取得します。</para>
<para>以下に、設定例の内訳を示し、すべてのフィールドについて説明します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: akri.sh/v0
kind: Configuration
metadata:
  name: sample-configuration
spec:</screen>
<para>次の部分では、ディスカバリハンドラの設定を記述しています。ディスカバリハンドラの名前を指定する必要があります(Akriのチャートの一部として利用可能なハンドラは、<literal>udev</literal>、<literal>opcua</literal>、<literal>onvif</literal>です)。<literal>discoveryDetails</literal>はハンドラ固有です。設定方法については、ハンドラのドキュメントを参照してください。</para>
<screen language="yaml" linenumbering="unnumbered">  discoveryHandler:
    name: debugEcho
    discoveryDetails: |+
      descriptions:
        - "foo"
        - "bar"</screen>
<para>次のセクションでは、検出された各デバイスに対してデプロイするワークロードを定義します。この例では、<literal>brokerPodSpec</literal>で<literal>Pod</literal>設定の最小バージョンが示されています。ここでは、Podの仕様の通常のフィールドをすべて使用できます。また、<literal>resources</literal>セクションに、デバイスを要求するためのAkri固有の構文も示されています。</para>
<para>または、Podの代わりにJobを使用することもできます。その場合は、代わりに<literal>brokerJobSpec</literal>キーを使用し、そこにJobの仕様部分を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">  brokerSpec:
    brokerPodSpec:
      containers:
      - name: broker-container
        image: rancher/hello-world
        resources:
          requests:
            "{{PLACEHOLDER}}" : "1"
          limits:
            "{{PLACEHOLDER}}" : "1"</screen>
<para>次の2つのセクションは、ブローカごとにサービスをデプロイするようにAkriを設定するか(<literal>instanceService</literal>)、またはすべてのブローカを指すようにAkriを設定する(<literal>configurationService</literal>)方法を示しています。これらには、通常のサービスに関連する要素がすべて含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">  instanceServiceSpec:
    type: ClusterIp
    ports:
    - name: http
      port: 80
      protocol: tcp
      targetPort: 80
  configurationServiceSpec:
    type: ClusterIp
    ports:
    - name: https
      port: 443
      protocol: tcp
      targetPort: 443</screen>
<para><literal>brokerProperties</literal>フィールドは、検出されたデバイスを要求するPodに追加の環境変数として公開されるキー/値ストアです。</para>
<para>capacityは、検出されたデバイスの許容される同時ユーザ数です。</para>
<screen language="yaml" linenumbering="unnumbered">  brokerProperties:
    key: value
  capacity: 1</screen>
</section>
<section xml:id="id-writing-and-deploying-additional-discovery-handlers">
<title>追加のディスカバリハンドラの記述とデプロイ</title>
<para>デバイスで使用されているプロトコルが既存のディスカバリハンドラでカバーされていない場合は、<link
xl:href="https://docs.akri.sh/development/handler-development">こちらのガイド</link>を使用して、独自に記述できます。</para>
</section>
<section xml:id="akri-dashboard-extension">
<title>Akri Rancher Dashboard拡張機能</title>
<para>Akriダッシュボード拡張機能を使用すると、Rancher
Dashboardユーザインタフェースを使用して、リーフデバイスを管理および監視し、デバイスが検出されたらワークロードを実行できます。</para>
<para>インストールのガイダンスについては、Rancher Dashboard拡張機能(<xref
linkend="components-rancher-dashboard-extensions"/>)を参照してください。</para>
<para>拡張機能をインストールしたら、クラスタエクスプローラを使用してAkri対応の管理対象クラスタに移動できます。［<emphasis
role="strong">Akri</emphasis>］ナビゲーショングループの下に［Configurations
(設定)］セクションと［Instances (インスタンス)］セクションがあります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configurations.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能の設定</phrase></textobject>
</mediaobject>
</informalfigure>
<para>［Configurations
(設定)］リストには、設定ディスカバリハンドラとインスタンス数に関する情報が表示されます。名前をクリックすると、設定の詳細ページが開きます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-detail.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能の設定の詳細</phrase></textobject>
</mediaobject>
</informalfigure>
<para>設定の編集や新規作成も行うことができます。拡張機能を使用すると、ディスカバリハンドラの選択、ブローカPodやブローカJobの設定、設定サービスやインスタンスサービスの設定、および設定容量の設定を行うことができます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-edit.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能の設定の編集</phrase></textobject>
</mediaobject>
</informalfigure>
<para>検出されたデバイスが <emphasis role="strong">［Instances
(インスタンス)］</emphasis>リストに一覧にされます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instances-list.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能インスタンスリスト</phrase></textobject>
</mediaobject>
</informalfigure>
<para>インスタンス名をクリックすると、詳細ページが開き、ワークロードおよびインスタンスサービスを表示できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instance-detail.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能のインスタンス詳細</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link
xl:href="https://k3s.io/">K3s</link>は、リソースに制約のあるリモートの無人の場所やIoTアプライアンス内の運用ワークロード向けに設計された、高可用性のKubernetes認定ディストリビューションです。</para>
<para>単一の小さなバイナリとしてパッケージ化されているため、迅速かつ簡単にインストールおよび更新できます。</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>SUSE EdgeでのK3sの用途</title>
<para>K3sは、SUSE Edgeスタックを支えるKubernetesディストリビューションとして使用できます。K3sはSLE
Microオペレーティングシステムにインストールすることが意図されています。</para>
<para>K3sをSUSE
EdgeスタックのKubernetesディストリビューションとして使用することは、etcdをバックエンドとして使用したのでは制約に合わない場合にのみ推奨します。etcdをバックエンドとして使用できる場合は、RKE2
(<xref linkend="components-rke2"/>)を使用することをお勧めします。</para>
</section>
<section xml:id="id-best-practices-4">
<title>ベストプラクティス</title>
<section xml:id="id-installation-2">
<title>インストール</title>
<para>K3sをSUSE Edgeスタックの一部としてインストールする場合に推奨する方法は、Edge Image Builder
(EIB)を使用することです。K3sをデプロイするようにEIBを設定する方法の詳細については、EIBのドキュメント(<xref
linkend="components-eib"/>)を参照してください。</para>
<para>この方法では、自動的にHAセットアップとElementalセットアップがサポートされます。</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>GitOpsワークフローでのFleet</title>
<para>SUSE Edgeスタックでは、GitOpsの推奨ツールとしてFleetを使用します
。Fleetのインストールと使用の詳細については、このドキュメントの「Fleet」のセクション(<xref
linkend="components-fleet"/>)を参照してください。</para>
</section>
<section xml:id="id-storage-management">
<title>ストレージ管理</title>
<para>K3sではローカルパスストレージが事前設定されており、これはシングルノードクラスタに適しています。複数のノードにまたがるクラスタの場合は、Longhorn
(<xref linkend="components-longhorn"/>)を使用することをお勧めします。</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>負荷分散とHA</title>
<para>EIBを使用してK3sをインストールした場合、ここで説明する部分は、EIBのドキュメントの「HA」のセクションで説明済みです。</para>
<para>EIBを使用しないでK3sをインストールした場合は、MetalLBのドキュメント(<xref
linkend="guides-metallb-k3s"/>)に従ってMetalLBをインストールおよび設定する必要があります。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para><link xl:href="https://docs.rke2.io/">RKE2の公式ドキュメント</link>を参照してください。</para>
<para>RKE2は、以下によってセキュリティとコンプライアンスに重点を置いた、完全準拠のKubernetesディストリビューションです。</para>
<itemizedlist>
<listitem>
<para>クラスタがCIS Kubernetes Benchmark
v1.6またはv1.23に合格できるデフォルト値と設定オプションを、オペレータの介入を最小限に抑えながら提供する</para>
</listitem>
<listitem>
<para>FIPS 140-2準拠を可能にする</para>
</listitem>
<listitem>
<para><link
xl:href="https://trivy.dev">trivy</link>を使用し、コンポーネントを定期的にスキャンしてRKE2ビルドパイプラインにCVEがないかどうかを確認する</para>
</listitem>
</itemizedlist>
<para>RKE2は、コントロールプレーンコンポーネントを、kubeletによって管理される静的Podとして起動します。組み込みコンテナランタイムはcontainerdです。</para>
<para>メモ: RKE2はRKE Governmentとしても知られます。これは、RKE2が現在ターゲットにしている別のユースケースと分野を表すためです。</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2とK3s</title>
<para>K3sはエッジ、loT、ARMに焦点を当てた、完全準拠の軽量なKubernetes
ディストリビューションであり、使いやすさとリソースに制約のある環境向けに最適化されています。</para>
<para>RKE2は、RKEの1.xバージョン(以下「RKE1」)とK3sの両方の長所を兼ね備えています。</para>
<para>RKE2は、K3sから使いやすさ、操作のしやすさ、およびデプロイメントモデルを継承しています。</para>
<para>RKE1から継承しているのは、アップストリームのKubernetesとの緊密な連携です。K3sはエッジデプロイメントに合わせて最適化されているため、アップストリームのKubernetesとは各所で異なりますが、RKE1とRKE2はアップストリームと緊密な連携を保つことができます。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>SUSE EdgeでのRKE2の用途</title>
<para>RKE2はSUSE Edgeスタックの基礎を成す部分です。RKE2はSUSE Linux Micro (<xref
linkend="components-slmicro"/>)上に位置し、Edgeワークロードをデプロイするために必要な標準Kubernetesインタフェースを提供します。</para>
</section>
<section xml:id="id-best-practices-5">
<title>ベストプラクティス</title>
<section xml:id="id-installation-3">
<title>インストール</title>
<para>RKE2をSUSE Edgeスタックの一部としてインストールする場合に推奨される方法は、Edge Image Builder
(EIB)を使用することです。RKE2をデプロイするようにEIBを設定する方法の詳細については、EIBのドキュメント(<xref
linkend="components-eib"/>)を参照してください。</para>
<para>EIBは十分な柔軟性を備えているため、RKE2のバージョン、<link
xl:href="https://docs.rke2.io/reference/server_config">サーバ</link>、または<link
xl:href="https://docs.rke2.io/reference/linux_agent_config">エージェント</link>設定の指定など、RKE2で要求されるあらゆるパラメータをサポートすることができ、Edgeのすべてのユースケースに対応できます。</para>
<para>Metal<superscript>3</superscript>に関連する他のユースケースでも、RKE2が使用およびインストールされます。このような特定のケースでは、<link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">Cluster
API Provider
RKE2</link>によって、Edgeスタックを使用してMetal<superscript>3</superscript>でプロビジョニングされるクラスタにRKE2が自動的にデプロイされます。</para>
<para>このような場合、関係する各種のCRDにRKE2設定を適用する必要があります。<literal>RKE2ControlPlane</literal>
CRDを使用して異なるCNIを提供する方法の例は、次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>Metal<superscript>3</superscript>のユースケースの詳細については、<xref
linkend="components-metal3"/>を参照してください。</para>
</section>
<section xml:id="id-high-availability">
<title>高可用性</title>
<para>HAデプロイメントの場合、EIBはMetalLB (<xref linkend="components-metallb"/>)と<link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link>を自動的にデプロイして設定し、RKE2 APIエンドポイントを外部に公開します。</para>
</section>
<section xml:id="id-networking">
<title>ネットワーキング</title>
<para>EdgeスタックでサポートされているCNIは<link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link>で、オプションでmeta-plugin
<link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>を追加できますが、RKE2では<link
xl:href="https://docs.rke2.io/install/network_options">ほかにもいくつかのプラグイン</link>がサポートされています。</para>
</section>
<section xml:id="id-storage">
<title>ストレージ</title>
<para>RKE2は、どのような種類の永続ストレージクラスやオペレータも提供していません。複数のノードにまたがるクラスタの場合は、Longhorn (<xref
linkend="components-longhorn"/>)を使用することをお勧めします。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-longhorn">
<title>Longhorn</title>
<para>Longhornは、Kubernetes向けに設計された、信頼性が高くユーザフレンドリな軽量の分散ブロックストレージシステムです。オープンソースプロジェクトとして、当初はRancher
Labsによって開発されていましたが、現在はCNCFの下でインキュベートされています。</para>
<section xml:id="id-prerequisites-4">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLE Micro 6.0がインストールされた最低1台のホスト(物理ホストでも仮想ホストでも可)</para>
</listitem>
<listitem>
<para>インストール済みのKubernetesクラスタ1つ(K3sまたはRKE2)</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-longhorn">
<title>Longhornの手動インストール</title>
<section xml:id="id-installing-open-iscsi">
<title>Open-iSCSIのインストール</title>
<para>Longhornをデプロイして使用するための中心的な要件は、<literal>open-iscsi</literal>パッケージをインストールすることと、<literal>iscsid</literal>デーモンをすべてのKubernetesノード上で実行することです。これは、Longhornがホスト上の<literal>iscsiadm</literal>を利用してKubernetesに永続ボリュームを提供するために必要です。</para>
<para>インストールしてみましょう。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>SLE
Microはイミュータブルオペレーティングシステムであるため、操作が完了すると、パッケージは新しいスナップショットにのみインストールされることに注意することが重要です。パッケージをロードし、<literal>iscsid</literal>デーモンの実行を開始するには、作成した新しいスナップショットで再起動する必要があります。準備が整ったら、rebootコマンドを発行します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>open-iscsiのインストールに関する追加のヘルプについては、<link
xl:href="https://longhorn.io/docs/1.7.1/deploy/install/#installing-open-iscsi">Longhornの公式ドキュメント</link>を参照してください。</para>
</tip>
</section>
<section xml:id="id-installing-longhorn">
<title>Longhornのインストール</title>
<para>KubernetesクラスタにLonghornをインストールするには複数の方法があります。
このガイドでは、Helmでのインストールに従いますが、別のアプローチが必要な場合は<link
xl:href="https://longhorn.io/docs/1.7.1/deploy/install/">公式ドキュメント</link>に従ってください。</para>
<orderedlist numeration="arabic">
<listitem>
<para>RancherチャートHelmリポジトリを追加します。</para>
<screen language="shell" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>リポジトリから最新のチャートをフェッチします。</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para><literal>longhorn-system</literal>ネームスペースにLonghornをインストールします。</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn-crd rancher-charts/longhorn-crd --namespace longhorn-system --create-namespace --version 104.2.0+up1.7.1
helm install longhorn rancher-charts/longhorn --namespace longhorn-system --version 104.2.0+up1.7.1</screen>
</listitem>
<listitem>
<para>デプロイメントが成功したことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-longhorn-volumes">
<title>Longhornボリュームの作成</title>
<para>Longhornは、<literal>StorageClass</literal>というKubernetesリソースを利用して、Podの<literal>PersistentVolume</literal>オブジェクトを自動的にプロビジョニングします。<literal>StorageClass</literal>は、管理者が、自身が提供する<emphasis>クラス</emphasis>または<emphasis>プロファイル</emphasis>を記述する方法だと考えてください。</para>
<para>デフォルトのオプションをいくつか使用して<literal>StorageClass</literal>を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para><literal>StorageClass</literal>を作成したので、それを参照する<literal>PersistentVolumeClaim</literal>が必要です。<literal>PersistentVolumeClaim</literal>
(PVC)は、ユーザによるストレージの要求です。PVCは<literal>PersistentVolume</literal>リソースを使用します。クレームでは、特定のサイズとアクセスモードを要求できます(たとえば、1つのノードで読み取り/書き込み可能でマウントすることも、複数のノードで読み取り専用でマウントすることもできます)。</para>
<para><literal>PersistentVolumeClaim</literal>を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>完了です。<literal>PersistentVolumeClaim</literal>を作成したら、それを<literal>Pod</literal>にアタッチする手順に進むことができます。<literal>Pod</literal>がデプロイされると、KubernetesはLonghornボリュームを作成し、ストレージが利用可能な場合は<literal>Pod</literal>にバインドします。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>Kubernetesにおけるストレージの概念は複雑であると同時に重要なトピックです。最も一般的なKubernetesリソースのいくつかを簡単に説明しましたが、Longhornが提供している<link
xl:href="https://longhorn.io/docs/1.7.1/terminology/">用語のドキュメント</link>をよく理解しておくことをお勧めします。</para>
</tip>
<para>この例では、結果は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>UIへのアクセス</title>
<para>kubectlまたはHelmを使用してLonghornをインストールした場合は、クラスタへの外部トラフィックを許可するようにIngressコントローラを設定する必要があります。認証はデフォルトでは有効になっていません。Rancherカタログアプリを使用していた場合、IngressコントローラはRancherによって自動的に作成され、アクセス制御が設定されています(rancher-proxy)。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Longhornの外部サービスのIPアドレスを取得します。</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para><literal>longghorn-frontend</literal>のIPアドレスを取得したら、ブラウザでそのアドレスに移動してUIの使用を開始できます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、<xref linkend="components-eib"/>を使用して、ベースとなるSLE Micro
OSイメージをカスタマイズしています。ここでは、イメージをカスタマイズしてRKE2クラスタとLonghornをSLE
Micro上にプロビジョニングする方法について説明します。</para>
<para>定義ファイルを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.0
image:
  imageType: iso
  baseImage: SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.30.5+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 104.2.0+up1.7.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 104.2.0+up1.7.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>Helmチャートの値のカスタマイズは、<literal>helm.charts[].valuesFile</literal>で提供されている別個のファイルを使用して実行できます。詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#kubernetes">アップストリームドキュメント</link>を参照してください。</para>
</note>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.1/edge-image-builder:1.1.0 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>イメージが構築されたら、それを使用して物理ホストまたは仮想ホストにOSをインストールできます。プロビジョニングが完了すると、<literal>root:eib</literal>の資格情報ペアを使用してシステムにログインできます。</para>
<para>Longhornが正常にデプロイされていることを確認します。</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>このインストールは、完全なエアギャップ環境では動作しません。このような場合は、<xref
linkend="longhorn-install"/>を参照してください。</para>
</note>
</section>
</chapter>
<chapter xml:id="components-neuvector">
<title>NeuVector</title>
<para>NeuVectorはKubernetes向けのセキュリティソリューションであり、L7ネットワークセキュリティ、ランタイムセキュリティ、サプライチェーンセキュリティ、およびコンプライアンスチェックを1つの統合パッケージで提供します。</para>
<para>NeuVectorは、複数のコンテナから成るプラットフォームとしてデプロイされ、各コンテナはさまざまなポートとインタフェースで相互に通信します。デプロイされる各種のコンテナは以下のとおりです。</para>
<itemizedlist>
<listitem>
<para>Manager
。Webベースのコンソールを提供するステートレスコンテナです。通常、マネージャは1つだけ必要で、どこでも実行できます。Managerにエラーが発生しても、ControllerやEnforcerの動作には影響しません。ただし、特定の通知(イベント)と最近の接続データはManagerによってメモリ内にキャッシュされているため、これらの表示には影響があります。</para>
</listitem>
<listitem>
<para>Controller。NeuVectorの「コントロールプレーン」は必ずHA設定でデプロイされるため、ノードのエラーで設定が失われることはありません。Controllerはどこでも実行できますが、その重要性から、顧客はほとんどの場合、「管理」ノード、マスタノード、またはインフラノードに配置することを選択します。</para>
</listitem>
<listitem>
<para>Enforcer。このコンテナはDaemonSetとしてデプロイされるため、保護する各ノードに1つのEnforcerが存在します。通常はすべてのワーカーノードにデプロイされますが、スケジュールを有効にしてマスタノードやインフラノードにデプロイすることもできます。メモ:
Enforcerがクラスタノードに存在しない状況で、そのノード上のPodから接続が行われた場合、その接続はNeuVectorによって「unmanaged」ワークロードとしてラベル付けされます。</para>
</listitem>
<listitem>
<para>Scanner。コントローラの指示に従って、ビルトインCVEデータベースを使用して脆弱性スキャンを実行します。複数のScannerをデプロイしてスキャン能力を拡張できます。Scannerはどこでも実行できますが、コントローラが実行されるノードで実行されることがほとんどです。Scannerノードのサイジングに関する考慮事項については、以下を参照してください。ビルドフェーズのスキャンに使用する場合、Scannerを独立して呼び出すこともできます。たとえば、スキャンをトリガし、結果を取得してScannerを停止するパイプライン内で使用する場合などです。Scannerには最新のCVEデータベースが含まれているため、毎日更新する必要があります。</para>
</listitem>
<listitem>
<para>Updater。Updaterは、CVEデータベースの更新が必要な場合に、Kubernetes
cronジョブを通じてScannerの更新をトリガします。必ず使用環境に合わせて設定してください。</para>
</listitem>
</itemizedlist>
<para>NeuVectorのオンボーディングの詳細とベストプラクティスのドキュメントについては、<link
xl:href="https://open-docs.neuvector.com/deploying/production/NV_Onboarding_5.0.pdf">こちら</link>をご覧ください。</para>
<section xml:id="id-how-does-suse-edge-use-neuvector">
<title>SUSE EdgeでのNeuVectorの用途</title>
<para>SUSE Edgeは、エッジデプロイメントの開始点として簡潔なNeuVector設定を提供します。</para>
<para>NeuVectorの設定の変更については、<link
xl:href="https://github.com/suse-edge/charts/blob/main/packages/neuvector-core/generated-changes/patch/values.yaml.patch">こちら</link>を参照してください。</para>
</section>
<section xml:id="id-important-notes">
<title>重要なメモ</title>
<itemizedlist>
<listitem>
<para><literal>Scanner</literal>コンテナには、スキャンするイメージをメモリに取り込んで解凍するのに十分なメモリが必要です。1GBを超えるイメージをスキャンするには、Scannerのメモリを、予想される最大イメージサイズをわずかに上回るサイズまで増やしてください。</para>
</listitem>
<listitem>
<para>保護モードでは、大量のネットワーク接続が予想されます。保護(インラインファイアウォールでのブロック)モードの場合、<literal>Enforcer</literal>では、接続および想定されるペイロードを保持して検査(DLP)するためにCPUとメモリが必要です。メモリを増やし、1つのCPUコアを<literal>Enforcer</literal>専用にすることで、十分なパケットフィルタリング容量を確保できます。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeでは、<xref linkend="components-eib"/>を使用して、ベースとなるSLE Micro
OSイメージをカスタマイズします。EIBでプロビジョニングしたKubernetesクラスタ上にNeuVectorをエアギャップインストールする場合は、<xref
linkend="neuvector-install"/>に従ってください。</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para><link
xl:href="https://metallb.universe.tf/">MetalLBの公式ドキュメント</link>を参照してください。</para>
<blockquote>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>ベアメタル環境では、ネットワークロードバランサの設定がクラウドセットアップよりも著しく複雑になります。クラウド設定でのわかりやすいAPIコールとは異なり、ベアメタルでは、高可用性(HA)を管理したり、シングルノードのロードバランサに特有の潜在的な単一障害点(SPOF)に対処したりするために、専用のネットワークアプライアンス、またはロードバランサと仮想IP
(VIP)設定の組み合わせが必要になります。このような設定は自動化しにくく、コンポーネントが動的にスケールアップ/ダウンするKubernetesのデプロイメントでは課題となります。</para>
<para>MetalLBでは、こうした課題に対処するために、Kubernetesモデルを利用してLoadBalancerタイプのサービスを作成し、ベアメタルセットアップであってもクラウド環境であるかのように動作させます。</para>
<para>2つの異なるアプローチがあります。<link
xl:href="https://metallb.universe.tf/concepts/layer2/">L2モード</link>(ARP<emphasis>「トリック」</emphasis>を使用する)アプローチか、<link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>を使用するアプローチです。主にL2では特別なネットワーク機器は必要ありませんが、一般的にはBGPのほうが優れています。これはユースケースによって異なります。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>SUSE EdgeでのMetalLBの用途</title>
<para>SUSE Edgeでは、主に次の2つの方法でMetalLBを使用します。</para>
<itemizedlist>
<listitem>
<para>ロードバランサソリューションとして: MetalLBは、ベアメタルマシン用のロードバランサソリューションとして機能します。</para>
</listitem>
<listitem>
<para>HA K3s/RKE2セットアップの場合: MetalLBでは、仮想IPアドレスを使用してKubernetes APIを負荷分散できます。</para>
</listitem>
</itemizedlist>
<note>
<para>APIを公開できるようにするには、<literal>endpoint-copier-operator</literal>を使用して、「kubernetes」サービスから「kubernetes-vip」LoadBalancerサービスへのK8s
APIエンドポイントの同期を保ちます。</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>ベストプラクティス</title>
<para>L2モードでのMetalLBのインストールの詳細については、MetalLBガイド(<xref
linkend="guides-metallb-k3s"/>)を参照してください。</para>
<para>MetalLBをkube-api-serverの前面にインストールしてHAセットアップを実現する方法のガイドについては、「Kubernetes
APIサーバの前面のMetalLB」(<xref
linkend="guides-metallb-kubernetes"/>)のチュートリアルを参照してください。</para>
</section>
<section xml:id="id-known-issues-6">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>K3S LoadBalancerソリューション:
K3Sには、<literal>Klipper</literal>というロードバランサソリューションが付属しています。MetalLBを使用するには、Klipperを無効にする必要があります。このためには、<link
xl:href="https://docs.k3s.io/networking">K3sのドキュメント</link>で説明されているように、<literal>--disable
servicelb</literal>オプションを指定してK3sサーバを起動します。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>このセクションでは、Edge Virtualizationを使用してエッジノードで仮想マシンを実行する方法について説明します。Edge
Virtualizationは、軽量な仮想化ユースケース向けに設計されており、仮想化およびコンテナ化されたアプリケーションのデプロイメントと管理に共通のワークフローが利用されることが想定されています。</para>
<para>SUSE Edge Virtualizationでは、仮想マシンの実行方法として次の2つをサポートしています。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ホストレベルでlibvirt+qemu-kvmを介して仮想マシンを手動でデプロイする(Kubernetesは関与しない)</para>
</listitem>
<listitem>
<para>KubeVirtオペレータをデプロイし、Kubernetesベースで仮想マシンを管理する</para>
</listitem>
</orderedlist>
<para>どちらのオプションも有効ですが、以下では2番目のオプションのみを説明しています。SLE
Microで提供されている、すぐに使用できる標準の仮想化メカニズムを使用する場合は、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/chap-virtualization-introduction.html">こちら</link>で包括的なガイドを参照してください。このガイドは主にSUSE
Linux Enterprise Server用に記載されていますが、概念はほぼ同じです。</para>
<para>このガイドではまず、事前にデプロイ済みのシステムに追加の仮想化コンポーネントをデプロイする方法について説明しますが、その後に続くセクションでは、Edge
Image
Builderを使用してこの設定を最初のデプロイメントに組み込む方法を説明しています。基本手順を実行して環境を手動で設定する必要がない場合は、そちらのセクションに進んでください。</para>
<section xml:id="id-kubevirt-overview">
<title>KubeVirtの概要</title>
<para>KubeVirtでは、仮想マシンと他のコンテナ化ワークロードを併せてKubernetesで管理できます。これを実現するために、Linux仮想化スタックのユーザスペース部分をコンテナ内で実行します。これにより、ホストシステムの要件が最小限に抑えられ、セットアップと管理が容易になります。</para>
<informalexample>
<para>KubeVirtのアーキテクチャの詳細については、<link
xl:href="https://kubevirt.io/user-guide/architecture/">アップストリームドキュメント</link>を参照してください。</para>
</informalexample>
</section>
<section xml:id="id-prerequisites-5">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLE Micro 6.0がインストールされ、BIOSで仮想化拡張機能が有効になっている少なくとも1台の物理ホスト(詳細については、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">こちら</link>を参照してください)。</para>
</listitem>
<listitem>
<para>ノード全体で、K3s/RKE2
Kubernetesクラスタがすでにデプロイされており、クラスタへのスーパーユーザアクセスを可能にする適切な<literal>kubeconfig</literal>が設定されている。</para>
</listitem>
<listitem>
<para>ルートユーザへのアクセス —
以下の説明では、自身がルートユーザであり、<literal>sudo</literal>を使用して特権を昇格して<emphasis>「いない」</emphasis>ことを想定しています。</para>
</listitem>
<listitem>
<para><link
xl:href="https://helm.sh/docs/intro/install/">Helm</link>がローカルで利用可能で、適切なネットワーク接続を備えていて、Kubernetesクラスタに設定をプッシュし、必要なイメージをダウンロードできる。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Edge Virtualizationの手動インストール</title>
<para>このガイドでは、Kubernetesのデプロイメント手順については説明しませんが、SUSE Edgeに適したバージョンの<link
xl:href="https://k3s.io/">K3s</link>または<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>がインストールされていること、およびkubeconfigが適切に設定されていて標準の<literal>kubectl</literal>コマンドをスーパーユーザとして実行できることを想定しています。また、シングルノードクラスタを形成することを想定していますが、マルチノードのデプロイメントでも大きな違いはないと考えられます。</para>
<para>SUSE Edge Virtualizationは、3つの別個のHelmチャートを使用してデプロイします。具体的には次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>:
中心的な仮想化コンポーネント。つまり、Kubernetesが仮想マシンをデプロイおよび管理できるようにするために必要なKubernetes
CRD、オペレータ、およびその他のコンポーネント。</para>
</listitem>
<listitem>
<para><emphasis role="strong">KubeVirtダッシュボード拡張機能</emphasis>:
仮想マシンの起動/停止やコンソールへのアクセスなど、基本的な仮想マシン管理を実行できるオプションのRancher UI拡張機能。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>:
KubeVirtの永続ストレージの統合を可能にする追加コンポーネント。仮想マシンが既存のKubernetesストレージバックエンドをデータ用に使用する機能を提供するだけでなく、ユーザが仮想マシンのデータボリュームのインポートまたはクローンの作成を行うことも可能にします。</para>
</listitem>
</itemizedlist>
<para>これらの各Helmチャートは、現在使用しているSUSE
Edgeのリリースに従ってバージョン管理されています。運用での使用/サポートされる使用のためには、SUSEレジストリにあるアーティファクトを使用してください。</para>
<para>まず、 <literal>kubectl</literal>のアクセスが機能していることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.30.5+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1</screen>
<para>これで、<emphasis role="strong">KubeVirt</emphasis>および<emphasis
role="strong">Containerized Data Importer
(CDI)</emphasis>のHelmチャートのインストールに進むことができます。</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/3.1/kubevirt-chart --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/3.1/cdi-chart --namespace cdi-system --create-namespace</screen>
<para>数分ですべてのKubeVirtおよびCDIコンポーネントがデプロイされるはずです。これを検証するには、<literal>kubevirt-system</literal>および<literal>cdi-system</literal>のネームスペース内にデプロイされたすべてのリソースを確認します。</para>
<para>KubeVirtリソースを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>CDIリソースを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para><literal>VirtualMachine</literal>カスタムリソース定義(CRD)がデプロイされていることを確認するには、次のコマンドで検証できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para><literal>VirtualMachine</literal>オブジェクトの定義が出力され、次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>仮想マシンのデプロイ</title>
<para>KubeVirtとCDIがデプロイされたので、<link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE
Tumbleweed</link>に基づくシンプルな仮想マシンを定義してみましょう。この仮想マシンは最もシンプルな設定であり、標準の「Podネットワーキング」を使用して、他のPodと同じネットワーキング設定を行います。また、非永続ストレージを使用するため、<link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>を持たないコンテナと同様に、ストレージは一時的なものになります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>これにより、<literal>VirtualMachine</literal>が作成されたことを示すメッセージが出力されます。</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>この<literal>VirtualMachine</literal>定義は最小限であり、設定はほとんど指定されていません。この定義は単に、この仮想マシンが、一時的な<literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>に基づくディスクイメージ(つまり、リモートイメージリポジトリからのコンテナイメージに保存されるディスクイメージ)を使用する、2GBのメモリを備えたマシンタイプ「<link
xl:href="https://wiki.qemu.org/Features/Q35">q35</link>」であることを示しています。また、base64でエンコードされたcloudInitディスクを指定しており、このディスクはブート時にユーザを作成してパスワードを適用する目的にのみ使用します(デコードには<literal>base64
-d</literal>を使用します)。</para>
<blockquote>
<note>
<para>この仮想マシンイメージはテスト専用です。このイメージは公式にサポートされておらず、ドキュメントの例としてのみ使用されています。</para>
</note>
</blockquote>
<para>このマシンは、openSUSE
Tumbleweedのディスクイメージをダウンロードする必要があるためブートに数分かかりますが、ブートが完了したら、次のコマンドで仮想マシンの情報をチェックして、仮想マシンの詳細を確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>これにより、仮想マシンが起動されたノードと、仮想マシンのIPアドレスが出力されます。Podネットワーキングを使用しているため、報告されるIPアドレスは他のPodと同様であり、ルーティング可能であることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>これらのコマンドをKubernetesクラスタノード自体で実行する場合は、トラフィックをPodに直接ルーティングするCNI
(Ciliumなど)を使用して、マシン自体に直接<literal>ssh</literal>で接続できるはずです。次のIPアドレスを、仮想マシンに割り当てられているIPアドレスに置き換えます。</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>この仮想マシンに接続すると、さまざまな操作を試すことができますが、リソースの点で制限があり、ディスク容量は1GBしかないことに注意してください。終了したら、<literal>Ctrl-D</literal>または<literal>exit</literal>でSSHセッションを切断します。</para>
<para>仮想マシンプロセスは、依然として標準のKubernetes
Podでラップされています。<literal>VirtualMachine</literal>
CRDは目的の仮想マシンを表していますが、仮想マシンが実際に起動されるプロセスは、他のアプリケーションと同様に、標準のKubernetes
Podである<literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>
Podを介して行われます。起動されたすべての仮想マシンに対して、<literal>virt-launcher</literal>
Podが存在することがわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>次に、定義したTumbleweedマシンの1つの<literal>virt-launcher</literal> Podが表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>この<literal>virt-launcher</literal>
Podを調べてみると、<literal>libvirt</literal>プロセスと<literal>qemu-kvm</literal>プロセスを実行していることがわかります。このPod自体を起動して詳細を確認できます。次のコマンドは、使用しているPodの名前に合わせて調整する必要があることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Podが起動したら、<literal>virsh</literal>コマンドを実行するのと併せて、プロセスを確認してみます。<literal>qemu-system-x86_64</literal>バイナリに加え、仮想マシンを監視するための特定のプロセスも実行されていることがわかります。
また、ディスクイメージの場所と、ネットワーキングが(タップデバイスとして)どのように接続されているかもわかります。</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>最後に、この仮称マシンを削除して、クリーンアップしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>virtctlの使用</title>
<para>KubeVirtには、標準のKubernetes
CLIツールである<literal>kubectl</literal>とともに、仮想化の世界とKubernetesが設計された世界との間のギャップを埋める方法でクラスタとのインタフェースを可能にするCLIユーティリティが付属しています。たとえば、
<literal>virtctl</literal>ツールは、APIやCRDを直接使用することなく、仮想マシンのライフサイクル(起動、停止、再起動など)の管理、仮想コンソールへのアクセスの提供、仮想マシンイメージのアップロード、サービスなどのKubernetesコンストラクトとのインタフェースを行う機能を提供します。</para>
<para><literal>virtctl</literal>ツールの最新の安定バージョンをダウンロードしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.3.1
$ wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64</screen>
<para>別のアーキテクチャまたはLinux以外のマシンを使用している場合は、他のリリースを<link
xl:href="https://github.com/kubevirt/kubevirt/releases">こちら</link>で見つけることができます。続行する前に、この実行可能ファイルを作成する必要があります。また、実行可能ファイルを<literal>$PATH</literal>内の特定の場所に移動すると便利な場合があります。</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-${VERSION}-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>その後、<literal>virtctl</literal>コマンドラインツールを使用して、仮想マシンを作成できます。出力を<literal>kubectl
apply</literal>に直接パイプしていることに注意して、前の仮想マシンを複製してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>これで、仮想マシンが実行されているのがわかります(コンテナイメージがキャッシュされるため、今回はかなり早く起動するはずです)。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>これで、 <literal>virtctl</literal>を使用して仮想マシンに直接接続できるようになりました。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para><literal>virtctl</literal>で使用可能なコマンドはほかにも多数あります。たとえば、 <literal>virtctl
console</literal>を使用すると、ネットワーキングが機能していない場合にシリアルコンソールにアクセスでき、<literal>virtctl
guestosinfo</literal>を使用すると、ゲストに<literal>qemu-guest-agent</literal>がインストールされていて実行されていれば、包括的なOS情報を取得できます。</para>
<para>最後に、仮想マシンを一時停止し、再開してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para><literal>VirtualMachine</literal>オブジェクトが「<emphasis
role="strong">Paused</emphasis>」と表示され、<literal>VirtualMachineInstance</literal>オブジェクトは「<emphasis
role="strong">Running</emphasis>」ですが「<emphasis
role="strong">READY=False</emphasis>」と表示されているのがわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>また、仮想マシンに接続できなくなっていることもわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>仮想マシンを再開して、もう一度試してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>これで、接続を再確立できるはずです。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>最後に、仮想マシンを削除しましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>シンプルなIngressネットワーキング</title>
<para>このセクションでは、仮想マシンを標準のKubernetesサービスとして公開し、<link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINXとRKE2</link>、<link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">TraefikとK3s</link>などのKubernetes
Ingressサービスを介して利用可能にする方法を示します。このドキュメントでは、これらのコンポーネントがすでに適切に設定されていること、およびKubernetesサーバノードまたはIngress仮想IPを指す適切なDNSポインタが設定されていて(ワイルドカードを使用するなど)、Ingressを適切に解決できることを前提としています。</para>
<blockquote>
<note>
<para>SUSE Edge
3.1以降では、マルチサーバーノード設定でK3sを使用している場合、Ingress用にMetalLBベースのVIPを設定する必要がある場合があります。RKE2では、これは不要です。</para>
</note>
</blockquote>
<para>この例の環境では、別のopenSUSE
Tumbleweed仮想マシンをデプロイし、cloud-initを使用して、ブート時にNGINXをシンプルなWebサーバとしてインストールしています。また、呼び出しの実行時に期待どおりに動作することを確認するためにシンプルなメッセージを返すように設定しています。この処理を確認するには、以下の出力のcloud-initのセクションに対して<literal>base64
-d</literal>を実行するだけです。</para>
<para>では、この仮想マシンを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>この仮想マシンが正常に起動したら、<literal>virtctl</literal>コマンドを使用して、外部ポート<literal>8080</literal>とターゲットポート<literal>80</literal>
(NGINXがデフォルトでリスンするポート)で<literal>VirtualMachineInstance</literal>を公開できます。<literal>virtctl</literal>コマンドは、仮想マシンオブジェクトとPodのマッピングを理解しているため、ここではこのコマンドを使用します。これにより、新しいサービスが作成されます。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>これで、適切なサービスが自動的に作成されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>次に、<literal>kubectl create
ingress</literal>を使用すると、このサービスを指すIngressオブジェクトを作成できます。ここでURL (<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link>オブジェクトの「ホスト」として知られている)をDNS設定に合わせて調整し、ポート<literal>8080</literal>を指すようにします。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>DNSが正しく設定されたら、URLに対してすぐにcurlを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>この仮想マシンとそのサービス、およびIngressリソースを削除して、クリーンアップしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Rancher UI拡張機能の使用</title>
<para>SUSE Edge VirtualizationはRancher Manager用のUI拡張機能を提供しており、Rancher Dashboard
UIを使用して基本的な仮想マシン管理を行うことができます。</para>
<section xml:id="id-installation-4">
<title>インストール</title>
<para>インストールのガイダンスについては、Rancher Dashboard拡張機能(<xref
linkend="components-rancher-dashboard-extensions"/>)を参照してください。</para>
</section>
<section xml:id="kubevirt-dashboard-extension">
<title>KubeVirt Rancher Dashboard拡張機能の使用</title>
<para>この拡張機能により、Cluster Explorerに新たに［<emphasis
role="strong">KubeVirt</emphasis>］セクションが導入されます。このセクションは、KubeVirtがインストールされている管理対象クラスタに追加されます。</para>
<para>この拡張機能を使用すると、次の2つのKubeVirtリソースを直接操作できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Virtual Machine Instances</literal> — 実行中の1つの仮想マシンインスタンスを表すリソース。</para>
</listitem>
<listitem>
<para><literal>Virtual Machines</literal> — 仮想マシンのライフサイクルを管理するために使用されるリソース。</para>
</listitem>
</orderedlist>
<section xml:id="id-creating-a-virtual-machine">
<title>仮想マシンの作成</title>
<orderedlist numeration="arabic">
<listitem>
<para>左側のナビゲーションでKubeVirtが有効な管理対象クラスタをクリックして、［<emphasis role="strong">Cluster
Explorer</emphasis>］に移動します。</para>
</listitem>
<listitem>
<para>［<emphasis role="strong">KubeVirt </emphasis>］ &gt; ［Virtual Machines
(仮想マシン)］ページで、画面の右上にある［ <literal>Create from YAML
(YAMLから作成)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>仮想マシンの定義を入力するか貼り付けて、［<literal>Create (作成)</literal>］を押します。「仮想マシンのデプロイ
」セクションの仮想マシンの定義を参考にしてください。</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width=""/>
</imageobject>
<textobject><phrase>［Virtual Machines (仮想マシン)］ページ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-starting-and-stopping-virtual-machines">
<title>仮想マシンの起動と停止</title>
<para>仮想マシンを起動および停止するには、各仮想マシンの右側にある<emphasis
role="strong">⋮</emphasis>ドロップダウンリストからアクセスできるアクションメニューを使用するか、アクションを実行する仮想マシンを選択してリストの上部にあるグループアクションを使用します。</para>
<para><literal>spec.running</literal>プロパティが定義されている仮想マシンに対してのみ、起動および停止アクションを実行できます。<literal>spec.runStrategy</literal>が使用されている場合、そのようなマシンは直接起動および停止できません。詳細については、<link
xl:href="https://kubevirt.io/user-guide/virtual_machines/run_strategies/#run-strategies">KubeVirtのドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>仮想マシンコンソールへのアクセス</title>
<para>［Virtual Machines (仮想マシン)］リストには［<literal>Console</literal>
(コンソール)］ドロップダウンリストがあり、ここから<emphasis
role="strong">VNCまたはシリアルコンソール</emphasis>を使用してマシンに接続できます。このアクションは、実行中のマシンでのみ使用できます。</para>
<para>新しく起動した仮想マシンでは、コンソールにアクセスできるようになるまでにしばらく時間がかかることがあります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width=""/>
</imageobject>
<textobject><phrase>VNCコンソールUI</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、ベースとなるSLE Micro OSイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBによってプロビジョニングされたKubernetesクラスタ上にKubeVirtとCDIの両方をエアギャップインストールするには、<xref
linkend="kubevirt-install"/>に従ってください。</para>
</section>
</chapter>
<chapter xml:id="components-system-upgrade-controller">
<title>System Upgrade Controller</title>
<para><link xl:href="https://github.com/rancher/system-upgrade-controller">System
Upgrade Controllerのドキュメント</link>を参照してください。</para>
<blockquote>
<para>System Upgrade Controller
(SUC)は、汎用のKubernetesネイティブアップグレードコントローラー(ノード用)を提供することを目的としています。あらゆるアップグレードポリシー/要件を定義するための新しいCRDであるPlanが導入されています。Planは、クラスタ内のノードを変更する明確な意図です。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-system-upgrade-controller">
<title>SUSE EdgeでSystem Upgrade Controllerを使用する方法</title>
<para><emphasis
role="strong">SUC</emphasis>は、管理クラスタ/ダウンストリームクラスタをあるEdgeプラットフォームバージョンから別のバージョンにアップグレードするために実行する必要があるさまざまな<literal>Day
2</literal>操作を支援するために使用されます。 <literal>Day 2</literal>操作は、 <emphasis
role="strong">SUC
Plan</emphasis>の形式で定義されます。これらのプランに基づき、SUCは各ノードにワークロードをデプロイし、それぞれの<literal>Day
2</literal>操作を実行します。</para>
</section>
<section xml:id="components-system-upgrade-controller-install">
<title>System Upgrade Controllerのインストール</title>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあるFleet
(<xref linkend="components-fleet"/>)を介して<emphasis
role="strong">SUC</emphasis>をインストールすることをお勧めします。</para>
<note>
<para><literal>suse-edge/fleet-examples</literal>リポジトリで提供されるリソースは常に有効な<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">fleet-examples
release</link>から使用される<emphasis
role="strong">「必要があります」</emphasis>。使用する必要のあるリリースを確認するには、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
</note>
<para><emphasis role="strong">SUC</emphasis>のインストールにFleet (<xref
linkend="components-fleet"/>)を使用できない場合は、RancherのHelmチャートリポジトリを介してインストールするか、RancherのHelmチャートを独自のサードパーティGitOpsワークフローに組み込むことができます。</para>
<para>このセクションでは以下の内容を取り上げます。</para>
<itemizedlist>
<listitem>
<para>Fleetのインストール(<xref linkend="components-system-upgrade-controller-fleet"/>)</para>
</listitem>
<listitem>
<para>Helmのインストール(<xref linkend="components-system-upgrade-controller-helm"/>)</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet">
<title>System Upgrade Controller Fleetのインストール</title>
<para><emphasis role="strong">Fleet</emphasis>を使用する場合は、<emphasis
role="strong">SUC</emphasis>のデプロイに使用可能な2つのリソースがあります。</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソース -
外部/ローカルGitサーバが利用できるユースケース用。インストール手順については、「System Upgrade Controllerのインストール -
GitRepo (<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>)」を参照してください。</para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソース -
ローカルGitサーバオプションをサポートしないエアギャップ環境のユースケース用。インストール手順については、「System Upgrade
Controllerのインストール - バンドル(<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>)」を参照してください。</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet-gitrepo">
<title>System Upgrade Controllerのインストール - GitRepo</title>
<note>
<para>このプロセスは、使用できる場合はRancher UIから実行することもできます。詳細については、「<link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UIでのFleetへのアクセス</link>」を参照してください。</para>
</note>
<para><emphasis role="strong">管理</emphasis>クラスタで、次の操作を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SUC</emphasis>をデプロイするクラスタを決定します。これは、 <emphasis
role="strong">管理</emphasis>クラスタ内の適切なFleetワークスペースに<emphasis role="strong">SUC
GitRepo</emphasis>をデプロイすることで実行されます。デフォルトでは、Fleetには2つのワークスペースがあります。</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - <emphasis
role="strong">管理</emphasis>クラスタにデプロイする必要があるリソース用。</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - <emphasis
role="strong">ダウンストリーム</emphasis>クラスタにデプロイする必要があるリソース用。</para>
<para>Fleetワークスペースの詳細については、<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">アップストリーム</link>ドキュメントを参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをデプロイします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタに<emphasis
role="strong">SUC</emphasis>をデプロイするには、次のようにします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-local -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.1.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF</screen>
</listitem>
<listitem>
<para><emphasis role="strong">ダウンストリーム</emphasis>クラスタに<emphasis
role="strong">SUC</emphasis>をデプロイするには、次のようにします。</para>
<note>
<para>以下のリソースをデプロイする前に、有効な <literal>ターゲット</literal>設定を提供する<emphasis
role="strong">「必要があります」</emphasis>。Fleetがリソースをデプロイするダウンストリームクラスタを認識できるようにするためです。ダウンストリームクラスタへのマッピング方法については、
<link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters (ダウンストリームクラスタへのマッピング</link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.1.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>がデプロイされていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get gitrepo system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.1.0   1/1</screen>
</listitem>
<listitem>
<para><emphasis role="strong">System Upgrade Controller</emphasis>のデプロイメントを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-fleet-bundle">
<title>System Upgrade Controllerのインストール - バンドル</title>
<para>このセクションは、<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>を使用して標準のFleet設定から<emphasis
role="strong">バンドル</emphasis>リソースを構築およびデプロイする方法について説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ネットワークにアクセスできるマシンで、<emphasis role="strong">fleet-cli</emphasis>をダウンロードします。</para>
<note>
<para>ダウンロードする<emphasis
role="strong">fleet-cli</emphasis>のバージョンが、クラスタにデプロイしたFleetのバージョンに一致していることを確認します。</para>
</note>
<itemizedlist>
<listitem>
<para>Macユーザの場合、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</listitem>
<listitem>
<para>LinuxおよびWindowsユーザの場合、Fleet<link
xl:href="https://github.com/rancher/fleet/releases">リリース</link>ごとに<emphasis
role="strong">アセット</emphasis>としてバイナリが存在します。</para>
<itemizedlist>
<listitem>
<para>Linux AMD:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/&lt;FLEET_VERSION&gt;/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/&lt;FLEET_VERSION&gt;/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>fleet-cli</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x fleet-cli</screen>
</listitem>
<listitem>
<para>使用する<emphasis role="strong">suse-edge/fleet-examples</emphasis> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のクローンを作成します。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.1.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para><emphasis role="strong">fleet-examples</emphasis>リポジトリにある、<emphasis
role="strong">SUC</emphasis> fleetに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller</screen>
</listitem>
<listitem>
<para><emphasis role="strong">SUC</emphasis>をデプロイするクラスタを決定します。これは、<emphasis
role="strong">管理</emphasis>クラスタ内の適切なFleetワークスペースに<emphasis
role="strong">SUCバンドル</emphasis>をデプロイすることで実行されます。デフォルトでは、Fleetには2つのワークスペースがあります。</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - <emphasis
role="strong">管理</emphasis>クラスタにデプロイする必要があるリソース用。</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - <emphasis
role="strong">ダウンストリーム</emphasis>クラスタにデプロイする必要があるリソース用。</para>
<para>Fleetワークスペースの詳細については、<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">アップストリーム</link>ドキュメントを参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">ダウンストリームクラスタにのみSUCをデプロイする場合は</emphasis>、特定のクラスタに一致する<emphasis
role="strong">targets.yaml</emphasis>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>ダウンストリームクラスタへのマッピング方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
<listitem>
<para>バンドルの構築に進みます。</para>
<note>
<para><literal>fleet-examples/fleets/day2/system-upgrade-controller</literal>ディレクトリの<emphasis
role="strong">fleet-cli</emphasis>をダウンロード<emphasis
role="strong">「していない」</emphasis>ことを確認してください。これをダウンロードすると、バンドルでパッケージ化され、これは推奨されません。</para>
</note>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタに<emphasis
role="strong">SUC</emphasis>をデプロイするには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">ダウンストリーム</emphasis>クラスタに<emphasis
role="strong">SUC</emphasis>をデプロイするには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
<para>このプロセスの詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートをバンドルに変換する)</link>」を参照してください。</para>
<para><literal>fleet-cli apply</literal>コマンドの詳細については、「<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">system-upgrade-controller-bundle.yaml</emphasis>バンドルを<emphasis
role="strong">管理</emphasis>クラスタマシンに転送します。</para>
<screen language="bash" linenumbering="unnumbered">scp system-upgrade-controller-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタに、<emphasis
role="strong">system-upgrade-controller-bundle.yaml</emphasis>バンドルをデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタで、<emphasis
role="strong">バンドル</emphasis>がデプロイされていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get bundle system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1</screen>
</listitem>
<listitem>
<para><emphasis
role="strong">バンドル</emphasis>をデプロイしたFleetワークスペースに基づいて、クラスタに移動し、<emphasis
role="strong">SUC</emphasis>デプロイメントを検証します。</para>
<note>
<para><emphasis role="strong">SUC</emphasis>は常に、<emphasis
role="strong">cattle-system</emphasis>ネームスペースにデプロイされます。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-helm">
<title>System Upgrade Controller Helmのインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>Rancherチャートリポジトリを追加します。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para><emphasis role="strong">SUC</emphasis>チャートをデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 104.0.0+up0.7.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace</screen>
<para>これにより、Edge 3.1プラットフォームで必要な<emphasis role="strong">SUC</emphasis>
<literal>0.13.4</literal>バージョンがインストールされます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">SUC</emphasis>デプロイメントを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans">
<title>System Upgrade Controller Planのモニタリング</title>
<para><emphasis role="strong">SUC</emphasis> Planは次の方法で表示できます。</para>
<itemizedlist>
<listitem>
<para>Rancher UI (<xref
linkend="components-system-upgrade-controller-monitor-plans-rancher"/>)を介して</para>
</listitem>
<listitem>
<para>クラスタ内の手動モニタリング(<xref
linkend="components-system-upgrade-controller-monitor-plans-manual"/>)を介して</para>
</listitem>
</itemizedlist>
<important>
<para><emphasis role="strong">SUC Plan</emphasis>用にデプロイされたPodは正常実行後、<emphasis
role="strong">15</emphasis>分間維持されます
。その後、作成元の対応するジョブによって削除されます。この時間後もPodのログにアクセスできるようにするには、クラスタのログ記録を有効にする必要があります。Rancherでこれを実行する方法については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.9/integrations-in-rancher/logging">Rancher
Integration with Logging Services (Rancherとログ記録サービスの統合)</link>」を参照してください。</para>
</important>
<section xml:id="components-system-upgrade-controller-monitor-plans-rancher">
<title>System Upgrade Controller Planのモニタリング - Rancher UI</title>
<para>特定の<emphasis role="strong">SUC</emphasis> Planの<emphasis
role="strong">Pod</emphasis>のログを確認するには、次の手順を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → &lt;クラスタ名&gt;</emphasis>を選択します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Workloads (ワークロード)］ → ［Pods］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>［<literal>Only User Namespaces
(ユーザネームスペースのみ)</literal>］ドロップダウンメニューを選択し、［<literal>cattle-system</literal>］ネームスペースを追加します。</para>
</listitem>
<listitem>
<para>［Pods］フィルタバーに<emphasis role="strong">SUC Plan</emphasis>
Podの名前を入力します。名前は<literal>apply-&lt;plan_name&gt;-on-&lt;node_name&gt;</literal>のテンプレート形式に従います。</para>
<note>
<para>特定のSUC Planに対して<emphasis role="strong">［Completed (完了)］</emphasis>
Podと<emphasis role="strong">［Unknown
(不明)］</emphasis>Podの両方が存在する場合があります。これは予期されており、一部のアップグレードの性質により発生します。</para>
</note>
</listitem>
<listitem>
<para>ログを確認するPodを選択し、<emphasis role="strong">⋮ → ［View Logs
(ログの表示)］</emphasis>に移動します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans-manual">
<title>System Upgrade Controller Planのモニタリング - 手動</title>
<note>
<para>以下の手順は、 <literal>kubectl</literal>が、<emphasis role="strong">SUC
Plan</emphasis>がデプロイされたクラスタに接続するように設定されていることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>デプロイした<emphasis role="strong">SUC</emphasis> Planを一覧にします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get plans -n cattle-system</screen>
</listitem>
<listitem>
<para><emphasis role="strong">SUC</emphasis> Plan用Podを入手します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l upgrade.cattle.io/plan=&lt;plan_name&gt; -n cattle-system</screen>
<note>
<para>特定のSUC Planに対して<emphasis role="strong">［Completed (完了)］</emphasis>
Podと<emphasis role="strong">［Unknown
(不明)］</emphasis>Podの両方が存在する場合があります。これは予期されており、一部のアップグレードの性質により発生します。</para>
</note>
</listitem>
<listitem>
<para>Podのログを取得します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n cattle-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="components-upgrade-controller">
<title>Upgrade Controller</title>
<para><link xl:href="https://github.com/suse-edge/upgrade-controller">Upgrade
Controller</link>のドキュメントを参照してください。</para>
<blockquote>
<para>次の要素で構成されるインフラストラクチャプラットフォームアップグレードを実行できるKubernetesコントローラ:</para>
<itemizedlist>
<listitem>
<para>オペレーティングシステム(SL Micro)</para>
</listitem>
<listitem>
<para>Kubernetes (K3s &amp; RKE2)</para>
</listitem>
<listitem>
<para>追加のコンポーネント(Rancher、Elemental、NeuVectorなど)</para>
</listitem>
</itemizedlist>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-upgrade-controller">
<title>SUSE EdgeでUpgrade Controllerを使用する方法</title>
<para><emphasis role="strong">Upgrade Controller</emphasis>は、あるSUSE
Edgeリリースバージョンを次のバージョンに<literal>管理</literal>クラスタをアップグレードするために必要な(かつては手動の)<literal>Day
2</literal>操作を自動化するために必須のものです。</para>
<para>この自動化のために、<literal>Upgrade Controller</literal>はSystem Upgrade Controller
(<xref linkend="components-system-upgrade-controller"/>)や<link
xl:href="https://github.com/k3s-io/helm-controller/">Helm
Controller</link>などのツールを利用します。</para>
<para><literal>Upgrade Controller</literal>の仕組みの詳細については、「Upgrade
Controllerの仕組み」(<xref
linkend="components-upgrade-controller-how"/>)を参照してください。</para>
<para><literal>Upgrade Controller</literal>の既知の制限事項については、「既知の制限事項」(<xref
linkend="components-upgrade-controller-known-issues"/>)セクションを参照してください。</para>
</section>
<section xml:id="components-upgrade-controller-installation">
<title>Upgrade Controllerのインストール</title>
<section xml:id="id-prerequisites-6">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://helm.sh/docs/intro/install/">Helm</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://cert-manager.io/v1.14-docs/installation/helm/#installing-with-helm">cert-manager</link></para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller-install"/>)</para>
</listitem>
<listitem>
<para>Kubernetesクラスタ: K3sまたはRKE2のいずれか</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-steps">
<title>手順</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタに <literal>Upgrade
Controller</literal> Helmチャートをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install upgrade-controller oci://registry.suse.com/edge/3.1/upgrade-controller-chart --version 0.1.0 --create-namespace --namespace upgrade-controller-system</screen>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal>デプロイメントを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal>ポッドを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal>ポッドログを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n upgrade-controller-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-how">
<title>Upgrade Controllerの仕組み</title>
<para>Edgeリリースアップグレードを実行するため、<emphasis role="strong">Upgrade
Controller</emphasis>では<literal>2つ</literal>の新しいKubernetes<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>が導入されました。</para>
<itemizedlist>
<listitem>
<para>UpgradePlan (<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>) -
<literal>ユーザによって作成されます</literal>。Edgeリリースアップグレードに関する設定を保持します。</para>
</listitem>
<listitem>
<para>ReleaseManifest (<xref
linkend="components-upgrade-controller-extensions-release-manifest"/>) -
<literal>Upgrade
Controllerによって作成されます</literal>。特定のEdgeリリースバージョンに固有のコンポーネントバージョンを保持します。<emphasis
role="strong">ユーザが編集することはできません。</emphasis></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Upgrade Controller</emphasis>は、
<literal>UpgradePlan</literal>リソースの<literal>releaseVersion</literal>プロパティでユーザによって指定されたEdgeリリースバージョンのコンポーネントデータを保持する<literal>ReleaseManifest</literal>リソースの作成に進みます。</para>
<para><emphasis role="strong">Upgrade
Controller</emphasis>は、<literal>ReleaseManifest</literal>のコンポーネントデータを使用して、次の順序のEdgeリリースコンポーネントのアップグレードに進みます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>オペレーティングシステム(OS) (<xref linkend="components-upgrade-controller-how-os"/>)</para>
</listitem>
<listitem>
<para>Kubernetes (<xref linkend="components-upgrade-controller-how-k8s"/>)</para>
</listitem>
<listitem>
<para>追加のコンポーネント(<xref linkend="components-upgrade-controller-how-additional"/>)</para>
</listitem>
</orderedlist>
<note>
<para>アップグレードプロセス中に、<emphasis role="strong">Upgrade
Controller</emphasis>は、作成した<literal>UpgradePlan</literal>にアップグレード情報を絶えず出力します。アップグレードプロセスを追跡する方法の詳細については、「アップグレードプロセスの追跡」(<xref
linkend="components-upgrade-controller-how-track"/>)を参照してください。</para>
</note>
<section xml:id="components-upgrade-controller-how-os">
<title>オペレーティングシステムのアップグレード</title>
<para><emphasis role="strong">OS</emphasis>コンポーネントをアップグレードするため、<emphasis
role="strong">Upgrade Controller</emphasis>は、次の命名テンプレートを持つSUC (<xref
linkend="components-system-upgrade-controller"/>) Planを作成します。</para>
<itemizedlist>
<listitem>
<para><literal>コントロールプレーン</literal>ノードのOSアップグレードに関連するSUC Planの場合 -
<literal>control-plane-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードのOSアップグレードに関連するSUC Planの場合 -
<literal>workers-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal></para>
</listitem>
</itemizedlist>
<para>これらのプランに基づいて、<emphasis
role="strong">SUC</emphasis>は、実際のOSアップグレードを実行するクラスタの各ノードに<emphasis
role="strong">ワークロード</emphasis>を作成します。</para>
<para><literal>ReleaseManifest</literal>に応じて、<emphasis
role="strong">OS</emphasis>アップグレードには次のものが含まれる場合があります。</para>
<itemizedlist>
<listitem>
<para><literal>Package only updates (パッケージのみの更新)</literal> -
OSバージョンがEdgeリリース間で変更されないユースケース用。</para>
</listitem>
<listitem>
<para><literal>Full OS migration (完全なOSマイグレーション)</literal> -
OSバージョンがEdgeリリース間で変更されるユースケース用。</para>
</listitem>
</itemizedlist>
<para>アップグレードは、<literal>コントロールプレーン</literal>ノードから順に一度に<emphasis
role="strong">1つ</emphasis>のノードが実行されます。<literal>コントロールプレーン</literal>ノードのアップグレードが終了した場合にのみ、<literal>ワーカー</literal>ノードのアップグレードが開始されます。</para>
<note>
<para>クラスタに特定のタイプの<emphasis role="strong">1つ</emphasis>以上のノードがある場合、 <emphasis
role="strong">Upgrade Controller</emphasis>は、クラスタノードの<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link>を実行するように<literal>OS
SUC Plan</literal>を設定します。</para>
<para><literal>コントロールプレーン</literal>ノードが1台を<emphasis
role="strong">超え</emphasis>、<emphasis
role="strong">1つのみ</emphasis>ワーカーノードがあるクラスタでは、<literal>コントロールプレーン</literal>ノードに対してのみ<literal>drain</literal>が実行されます。その逆も同様です。</para>
<para>ノードのdrainを完全に無効にする方法については、「UpgradePlan」(<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>)のセクションを参照してください。</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-k8s">
<title>Kubernetesのアップグレード</title>
<para>クラスタの<emphasis
role="strong">Kubernetesディストリビューション</emphasis>をアップグレードするため、<emphasis
role="strong">Upgrade Controller</emphasis>は、次の命名テンプレートを持つSUC (<xref
linkend="components-system-upgrade-controller"/>) Planを作成します。</para>
<itemizedlist>
<listitem>
<para><literal>コントロールプレーン</literal>ノードのKubernetesアップグレードに関連するSUC Planの場合 -
<literal>control-plane-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードのKubernetesアップグレードに関連するSUC Planの場合 -
<literal>workers-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
</itemizedlist>
<para>これらのプランに基づいて、<emphasis
role="strong">SUC</emphasis>は、実際のKubernetesアップグレードを実行するクラスタのノードごとに<emphasis
role="strong">ワークロード</emphasis>を作成します。</para>
<para><emphasis
role="strong">Kubernetes</emphasis>アップグレードは、<literal>コントロールプレーン</literal>ノードから順に一度に<emphasis
role="strong">1つ</emphasis>のノードが実行されます。
<literal>コントロールプレーン</literal>ノードのアップグレードが終了した場合にのみ、<literal>ワーカー</literal>ノードのアップグレードが開始されます。</para>
<note>
<para>クラスタに特定のタイプの<emphasis role="strong">1つ</emphasis>以上のノードがある場合、クラスタノードの<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link>を実行するように<emphasis
role="strong">Upgrade Controller</emphasis>は<literal>Kubernetes SUC
Plan</literal>を設定します。</para>
<para><literal>コントロールプレーン</literal>ノードが1台を<emphasis
role="strong">超え</emphasis>、<emphasis
role="strong">1つのみ</emphasis>ワーカーノードがあるクラスタでは、<literal>コントロールプレーン</literal>ノードに対してのみ<literal>drain</literal>が実行されます。その逆も同様です。</para>
<para>ノードのdrainを完全に無効にする方法については、「UpgradePlan」(<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>)のセクションを参照してください。</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-additional">
<title>追加のコンポーネントのアップグレード</title>
<para>現在、追加のコンポーネントはすべてHelmチャートを介してインストールされます。特定のリリースのコンポーネントの全リストについては、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
<para>EIB (<xref linkend="components-eib"/>)を通じてデプロイされたHelmチャートの場合、<emphasis
role="strong">Upgrade Controller</emphasis>は各コンポーネントの既存の<link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart
CR</link>を更新します。</para>
<para>EIB以外でデプロイされたHelmチャートの場合、<emphasis role="strong">Upgrade
Controller</emphasis>は、コンポーネントごとに<literal>HelmChart</literal>リソースを作成します。</para>
<para><literal>HelmChart</literal>リソースの<literal>作成/更新</literal>後、<emphasis
role="strong">Upgrade Controller</emphasis>は、<link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>に依存してこの変更を取得し、実際のコンポーネントアップグレードを続行します。</para>
<para>チャートは<literal>ReleaseManifest</literal>の順序に基づいて順次アップグレードされます。追加の値は<literal>UpgradePlan</literal>を通じて渡すこともできます。これに関する詳細については、「UpgradePlan」(<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>)を参照してください。</para>
</section>
</section>
<section xml:id="components-upgrade-controller-extensions">
<title>Kubernetes API拡張機能</title>
<para><emphasis role="strong">Upgrade Controller</emphasis>によって導入されたKubernetes
APIの拡張機能。</para>
<section xml:id="components-upgrade-controller-extensions-upgrade-plan">
<title>UpgradePlan</title>
<para><literal>Upgrade
Controller</literal>は、<literal>UpgradePlan</literal>と呼ばれる新たなKubernetes <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>を導入しました。</para>
<para><literal>UpgradePlan</literal>は、<literal>Upgrade
Controller</literal>の指示メカニズムとして機能し、次の設定をサポートします。</para>
<itemizedlist>
<listitem>
<para><literal>releaseVersion</literal> -
クラスタをアップグレースする必要があるEdgeリリースバージョン。リリースバージョンは、<link
xl:href="https://semver.org">セマンティック</link>バージョン管理に従う必要があり、リリースノート(<xref
linkend="release-notes"/>)から取得する必要があります。</para>
</listitem>
<listitem>
<para><literal>disableDrain</literal> - <emphasis
role="strong">オプション</emphasis>。<emphasis role="strong">Upgrade
Controller</emphasis>にノードの<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drains</link>を無効にするかどうかを指示します。<link
xl:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Disruption
Budgets</link>を含むワークロードがある場合に役立ちます。</para>
<itemizedlist>
<listitem>
<para><literal>コントローラプレーン</literal>ノードのdrainの無効化の例:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true</screen>
</listitem>
<listitem>
<para><literal>コントローラプレーン</literal>と<literal>ワーカー</literal>ノードのdrainの無効化の例:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true
    worker: true</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>helm</literal> - <emphasis
role="strong">オプション</emphasis>。Helmを介してインストールされたコンポーネントの追加の値を指定します。</para>
<warning>
<para>アップグレードに重要な値についてのみこのフィールドを使用することをお勧めします。標準のチャート値の更新は、各チャートが次のバージョンにアップグレードされた後に実行する必要があります。</para>
</warning>
<itemizedlist>
<listitem>
<para>例:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  helm:
  - chart: foo
    values:
      bar: baz</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-upgrade-controller-extensions-release-manifest">
<title>ReleaseManifest</title>
<para><literal>Upgrade
Controller</literal>は、<literal>ReleaseManifest</literal>と呼ばれる新たなKubernetes
<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>を導入しました。</para>
<para><literal>ReleaseManifest</literal>は、<literal>Upgrade
Controller</literal>によって作成され、<emphasis
role="strong">1つ</emphasis>の特定のEdgeリリースバージョンのコンポーネントデータを保持します。つまり、各Edgeリリースバージョンのアップグレードは、異なる<literal>ReleaseManifest</literal>リソースによって表されます。</para>
<warning>
<para><literal>ReleaseManifest</literal>は常に、 <literal>Upgrade
Controller</literal>によって作成される必要があります。</para>
<para><literal>ReleaseManifest</literal>を手動で作成または編集することはお勧めしていません。
ユーザが手動で作成または編集することを決めた場合は、<emphasis role="strong">自己責任</emphasis>で行う必要があります。</para>
</warning>
<para><literal>ReleaseManifest</literal>が提供するコンポーネントデータは、以下が含まれますがこれに制限されません。</para>
<itemizedlist>
<listitem>
<para><literal>オペレーティングシステムデータ</literal> (バージョン、サポートされているアーキテクチャ、追加のアップグレードデータなど)。</para>
</listitem>
<listitem>
<para><literal>Kubernetesディストリビューションデータ</literal> (<link
xl:href="https://docs.rke2.io">RKE2</link>/<link
xl:href="https://k3s.io">K3s</link>でサポートされているバージョン)。</para>
</listitem>
<listitem>
<para><literal>追加のコンポーネントデータ</literal> - SUSE Helmチャートデータ(場所、バージョン、名前など)。</para>
</listitem>
</itemizedlist>
<para><literal>ReleaseManifest</literal>の例については、<link
xl:href="https://github.com/suse-edge/upgrade-controller/blob/main/config/samples/lifecycle_v1alpha1_releasemanifest.yaml">アップストリーム</link>ドキュメントを参照してください。<emphasis>これは単なる例であって、有効な<literal>ReleaseManifest</literal>リソース</emphasis>として作成することを目的としたものではないことに注意してください。</para>
</section>
</section>
<section xml:id="components-upgrade-controller-how-track">
<title>アップグレードプロセスの追跡</title>
<para>このセクションは、ユーザが<literal>UpgradePlan</literal>を作成すると、<literal>Upgrade
Controller</literal>が開始する<literal>アップグレードプロセス</literal>を追跡およびデバッグする手段として機能します。</para>
<section xml:id="components-upgrade-controller-how-track-general">
<title>一般</title>
<para><literal>アップグレードプロセス</literal>の状態に関する一般情報は、
<literal>UpgradePlan</literal>のステータス状況で確認できます。</para>
<para><literal>UpgradePlan</literal>リソースのステータスは、次の方法で確認できます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get upgradeplan &lt;upgradeplan_name&gt; -n upgrade-controller-system -o yaml</screen>
<formalpara>
<title><literal>UpgradePlan</literal>の実行例:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt-3-1-0
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.1.0
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Control plane nodes are being upgraded
    reason: InProgress
    status: "False"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Kubernetes upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Rancher upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Longhorn upgrade is not yet started
    reason: Pending
    status: Unknown
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: MetalLB upgrade is not yet started
    reason: Pending
    status: Unknown
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: CDI upgrade is not yet started
    reason: Pending
    status: Unknown
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: KubeVirt upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: NeuVector upgrade is not yet started
    reason: Pending
    status: Unknown
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: EndpointCopierOperator upgrade is not yet started
    reason: Pending
    status: Unknown
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Elemental upgrade is not yet started
    reason: Pending
    status: Unknown
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: SRIOV upgrade is not yet started
    reason: Pending
    status: Unknown
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Akri upgrade is not yet started
    reason: Pending
    status: Unknown
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Metal3 upgrade is not yet started
    reason: Pending
    status: Unknown
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: RancherTurtles upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherTurtlesUpgraded
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
<para>ここで、 <literal>Upgrade
Controller</literal>がアップグレードをスケジュールしようとするすべてのコンポーネントを確認できます。各状態は、次のテンプレートに従います。</para>
<itemizedlist>
<listitem>
<para><literal>lastTransitionTime</literal> -
このコンポーネントの状態があるステータスから別のステータスに遷移した最後の時刻。</para>
</listitem>
<listitem>
<para><literal>message (メッセージ)</literal> - 特定のコンポーネントの状態の現在のアップグレード状態を示すメッセージ。</para>
</listitem>
<listitem>
<para><literal>reason (理由)</literal> -
特定のコンポーネントの状態の現在のアップグレード状態。考えられる<literal>理由</literal>には次のものが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>Succeeded (成功)</literal> - 特定のコンポーネントのアップグレードが成功しました。</para>
</listitem>
<listitem>
<para><literal>Failed (失敗)</literal> - 特定のコンポーネントアップグレードが失敗しました。</para>
</listitem>
<listitem>
<para><literal>InProgress (進行中)</literal> - 特定のコンポーネントのアップグレードは現在進行中です。</para>
</listitem>
<listitem>
<para><literal>Pending (保留中)</literal> - 特定のコンポーネントのアップグレードは、まだスケジュールされていません。</para>
</listitem>
<listitem>
<para><literal>Skipped (スキップ)</literal> -
特定のコンポーネントがクラスタで見つからないため、アップグレードはスキップされます。</para>
</listitem>
<listitem>
<para><literal>Error (エラー)</literal> - 特定のコンポーネントで一時的なエラーが発生しました。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>status (ステータス)</literal> - 現在の状態 <literal>type
(タイプ)</literal>のステータス。<literal>True、False、Unknown (不明)</literal>のいずれか。</para>
</listitem>
<listitem>
<para><literal>type (タイプ)</literal> - 現在アップグレードされたコンポーネントのインジケータ。</para>
</listitem>
</itemizedlist>
<para><literal>Upgrade
Controller</literal>は、<emphasis>"OSUpgraded"</emphasis>および<emphasis>"KubernetesUpgraded"</emphasis>タイプのコンポーネント状態の<literal>SUC
Plan</literal>を作成します。これらのコンポーネント用に作成された<emphasis role="strong">SUC
Plan</emphasis>を詳細に追跡するには、「System Upgrade Controller Planのモニタリング」(<xref
linkend="components-system-upgrade-controller-monitor-plans"/>)セクションを参照してください。</para>
<para>他のすべてのコンポーネント状態のタイプは、<link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>によって作成されたリソースを表示することで詳細に追跡できます。詳細については、「Helm
Controller 」(<xref
linkend="components-upgrade-controller-how-track-helm"/>)セクションを参照してください。</para>
<para><literal>Upgrade
Controller</literal>によってスケジュールされた<literal>UpgradePlan</literal>は、次の場合に<literal>successful
(成功)</literal>とマーク付けすることができます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Pending (保留中)</literal>または<literal>InProgress
(進行中)</literal>のコンポーネントの状態がありません。</para>
</listitem>
<listitem>
<para><literal>lastSuccessfulReleaseVersion</literal>プロパティが<literal>UpgradePlan</literal>の設定で指定された<literal>releaseVersion</literal>を指しています。<emphasis>このプロパティは、
<literal>アップグレードプロセス</literal>が成功すると、 <literal>Upgrade
Controller</literal>によって<literal>UpgradePlan</literal>のステータスに追加されます。</emphasis></para>
</listitem>
</orderedlist>
<formalpara>
<title>成功した<literal>UpgradePlan</literal>の例:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt-3-1-0
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.1.0
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:48Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:59Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart rancher upgrade succeeded
    reason: Succeeded
    status: "True"
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart longhorn is not installed
    reason: Skipped
    status: "False"
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Specified version of chart metallb is already installed
    reason: Skipped
    status: "False"
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart cdi is not installed
    reason: Skipped
    status: "False"
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart kubevirt is not installed
    reason: Skipped
    status: "False"
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart neuvector-crd is not installed
    reason: Skipped
    status: "False"
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Specified version of chart endpoint-copier-operator is already installed
    reason: Skipped
    status: "False"
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Chart elemental-operator upgrade succeeded
    reason: Succeeded
    status: "True"
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:27:15Z"
    message: Chart sriov-crd is not installed
    reason: Skipped
    status: "False"
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:27:16Z"
    message: Chart akri is not installed
    reason: Skipped
    status: "False"
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:27:19Z"
    message: Chart metal3 is not installed
    reason: Skipped
    status: "False"
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:27:27Z"
    message: Chart rancher-turtles is not installed
    reason: Skipped
    status: "False"
    type: RancherTurtlesUpgraded
  lastSuccessfulReleaseVersion: 3.1.0
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
</section>
<section xml:id="components-upgrade-controller-how-track-helm">
<title>Helm Controller</title>
<para>このセクションでは、<link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>によって作成されたリソースを追跡する方法について説明します。</para>
<note>
<para>以下の手順は、<literal>kubectl</literal> が <literal>Upgrade
Controller</literal>が導入されているクラスタに接続するように設定されていることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>特定のコンポーネントの<literal>HelmChart</literal>リソースを見つけます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmcharts -n kube-system</screen>
</listitem>
<listitem>
<para><literal>HelmChart</literal>リソースの名前を使用して、
<literal>helm-controller</literal>によって作成されたアップグレードPodを見つけます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n kube-system

# Example for Rancher
kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n kube-system
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-tv9wn   0/1     Completed   0          16m</screen>
</listitem>
<listitem>
<para>コンポーネント固有のPodのログを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n kube-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-known-issues">
<title>既知の制限事項</title>
<itemizedlist>
<listitem>
<para><literal>ダウンストリーム</literal>クラスタのアップグレードは<literal>Upgrade
Controller</literal>によってまだ管理されていません。<literal>ダウンストリーム</literal>クラスタをアップグレードする方法については、「ダウンストリームクラスタ」(<xref
linkend="day2-downstream-clusters"/>)セクションを参照してください。</para>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal>は、 EIB (<xref
linkend="components-eib"/>)を通じてデプロイされる追加のSUSE Edge Helmチャートについて、その、<link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart
CR</link>が<literal>kube-system</literal>ネームスペースにデプロイされていることを想定しています。これを実行するには、EIB定義ファイルで<literal>installationNamespace</literal>プロパティを設定します。詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">アップストリーム</link>ドキュメントを参照してください。</para>
</listitem>
<listitem>
<para>現在、<literal>Upgrade
Controller</literal>には、<literal>管理</literal>クラスタで現在実行中のEdgeリリースバージョンを判断する方法がありません。クラスタで現在実行しているEdgeリリースバージョンより<emphasis
role="strong">大きい</emphasis>Edgeリリースバージョンを指定するようにしてください。</para>
</listitem>
<listitem>
<para>現在、<literal>Upgrade Controller</literal>は、<emphasis
role="strong">非エアギャップ</emphasis>環境のアップグレードのみをサポートしています。<emphasis
role="strong">エアギャップ</emphasis>アップグレードは、<emphasis
role="strong">まだ</emphasis>可能ではありません。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>ハウツーガイド</title>
<partintro>
<para>ハウツーガイドとベストプラクティス</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>K3s上のMetalLB (L2を使用)</title>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>このガイドでは、MetalLBをレイヤ2モードでデプロイする方法について説明します。</para>
<section xml:id="id-why-use-this-method-2">
<title>この方法を使用する理由</title>
<para>MetalLBは、いくつかの理由により、ベアメタルKubernetesクラスタでの負荷分散に最適な選択肢です。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Kubernetesとのネイティブ統合:
MetalLBはKubernetesとシームレスに統合されており、使い慣れたKubernetesツールとプラクティスを使用して簡単にデプロイおよび管理できます。</para>
</listitem>
<listitem>
<para>ベアメタルとの互換性: クラウドベースのロードバランサとは異なり、MetalLB
は、従来のロードバランサが利用できない、または実現できないオンプレミスデプロイメント向けに特別に設計されています。</para>
</listitem>
<listitem>
<para>複数のプロトコルをサポート: MetalLBはレイヤ2モードとBGP (Border Gateway
Protocol)モードの両方をサポートし、さまざまなネットワークアーキテクチャと要件に柔軟に対応します。</para>
</listitem>
<listitem>
<para>高可用性: MetalLBは、負荷分散の責任を複数のノードに分散することで、サービスの高可用性と信頼性を保証します。</para>
</listitem>
<listitem>
<para>スケーラビリティ: MetalLBは大規模なデプロイメントに対応し、Kubernetesクラスタに合わせてスケーリングして需要の増加に対応します。</para>
</listitem>
</orderedlist>
<para>レイヤ2モードでは、1つのノードがローカルネットワークにサービスをアドバタイズする責任を負います。ネットワークの視点からは、マシンのネットワークインタフェースに複数のIPアドレスが割り当てられているように見えます。</para>
<para>レイヤ2モードの主な利点は、その汎用性です。あらゆるEthernetネットワークで動作し、特別なハードウェアも、高価なルータも必要ありません。</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>K3s上のMetalLB (L2を使用)</title>
<para>このクイックスタートではL2モードを使用するので、特別なネットワーク機器は必要なく、ネットワーク範囲内の空きIPをいくつか用意するだけで十分です。DHCPプール外のIPであれば割り当てられることがないため理想的です。</para>
<para>この例では、DHCPプールは、<literal>192.168.122.0/24</literal>のネットワークに対して<literal>192.168.122.100-192.168.122.200</literal>です(IPは3つです。余分なIPの理由については、「TraefikとMetalLB」
(<xref
linkend="traefik-and-metallb"/>)を参照してください)。そのため、この範囲外であれば何でも構いません(ゲートウェイと、すでに実行されている可能性のある他のホストは除きます)。</para>
</section>
<section xml:id="id-prerequisites-7">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>MetalLBがデプロイされるK3sクラスタ。</para>
</listitem>
</itemizedlist>
<warning>
<para>K3sには、Klipperという名前の独自のサービスロードバランサが付属しています。<link
xl:href="https://metallb.universe.tf/configuration/k3s/">MetalLBを実行するにはKlipperを無効にする必要があります</link>。Klipperを無効にするには、<literal>--disable=servicelb</literal>フラグを使用してK3sをインストールする必要があります。</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>ネットワーク範囲内の数個の空きIP (ここでは<literal>192.168.122.10-192.168.122.12</literal>)</para>
</listitem>
</itemizedlist>
<section xml:id="id-deployment">
<title>デプロイメント</title>
<para>MetalLBはHelm (および他の方法)を利用するため、次のようになります。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/3.1/metallb-chart \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>設定</title>
<para>この時点でインストールは完了しています。次に、サンプル値を使用して<link
xl:href="https://metallb.universe.tf/configuration/">設定</link>を行います。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>これで、MetalLBを使用する準備ができました。L2モードでは、次のようにさまざまな設定をカスタマイズできます。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">IPv6とデュアルスタックサービス</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">アドレスの自動割り当てを制御する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">アドレス割り当ての範囲を特定のネームスペースとサービスに縮小する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">サービスをアナウンスできるノードのセットを制限する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">LBのIPをアナウンスできるネットワークインタフェースを指定する</link></para>
</listitem>
</itemizedlist>
<para>また、<link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>についても、さらに多くのカスタマイズが可能です。</para>
</section>
<section xml:id="traefik-and-metallb">
<title>TraefikとMetalLB</title>
<para>Traefikは、デフォルトではK3sとともにデプロイされます(<literal>--disable=traefik</literal>を使用して<link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">K3sを無効にできます</link>)。また、デフォルトで<literal>LoadBalancer</literal>として公開されます(Klipperで使用するため)。ただし、Klipperを無効にする必要があるため、Ingress用Traefikサービスは<literal>LoadBalancer</literal>タイプのままです。そのため、MetalLBをデプロイした時点では、最初のIPは自動的にTraefik
Ingressに割り当てられます。</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>これは、このプロセスの後半(<xref linkend="ingress-with-metallb"/>)で適用されます。</para>
</section>
<section xml:id="id-usage">
<title>使用法</title>
<para>デプロイメントの例を作成してみましょう。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>最終的にサービスは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>実際の動作を見てみましょう。</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
</section>
</section>
<section xml:id="ingress-with-metallb">
<title>IngressとMetalLB</title>
<para>TraefikはすでにIngressコントローラとして機能しているため、次のような<literal>Ingress</literal>オブジェクトを介してHTTP/HTTPSトラフィックを公開できます。</para>
<screen language="yaml" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>これにより、次のような結果が返されます。</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>また、MetalLB が正しく動作していることを確認するため、<literal>arping</literal>を次のように使用できます:</para>
<para><literal>arping hellok3s.${IP}.sslip.io</literal></para>
<para>予期される結果は次のとおりです。</para>
<screen language="console" linenumbering="unnumbered">ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>上記の例では、トラフィックは次のように流れます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal>が実際のIPに解決されます。</para>
</listitem>
<listitem>
<para>続いて、トラフィックが<literal>metallb-speaker</literal> Podによって処理されます。</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal>がトラフィックを<literal>traefik</literal>コントローラにリダイレクトします。</para>
</listitem>
<listitem>
<para>最後に、Traefikが要求を<literal>hello-kubernetes</literal>サービスに転送します。</para>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>Kubernetes APIサーバの前面のMetalLB</title>
<para>このガイドでは、MetalLBサービスを使用して、3つのコントロールプレーンノードを持つHAクラスタ上でRKE2/K3s
APIを外部に公開する方法を示します。これを実現するために、<literal>LoadBalancer</literal>タイプのKubernetes
ServiceとEndpointsを手動で作成します。Endpointsは、クラスタで使用可能なすべてのコントロールプレーンノードのIPを保持します。Endpointsをクラスタで発生するイベント(ノードの追加/削除やノードのオフライン化)と継続的に同期するために<link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link>がデプロイされます。Operatorはデフォルトの<literal>kubernetes</literal>
Endpointで発生するイベントを監視し、管理対象を自動的に更新して同期を維持します。管理対象のServiceは<literal>LoadBalancer</literal>タイプであるため、<literal>MetalLB</literal>は静的な<literal>ExternalIP</literal>を割り当てます。この<literal>ExternalIP</literal>はAPI
Serverとの通信に使用されます。</para>
<section xml:id="id-prerequisites-8">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>RKE2/K3sをデプロイするための3つのホスト。</para>
<itemizedlist>
<listitem>
<para>ホスト名は各ホストで違う名前にしてください。</para>
</listitem>
<listitem>
<para>テストの場合は仮想マシンを使用できます。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ネットワーク内で2つ以上のIPが使用可能(Traefik/Nginx用に1つ、管理対象サービス用に1つ)。</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-rke2k3s">
<title>RKE2/K3sのインストール</title>
<note>
<para>新しいクラスタを使用せず、既存のクラスタを使用する場合は、この手順をスキップして次の手順に進んでください。</para>
</note>
<para>まず、ネットワーク内の空きIPを、後で管理対象のServiceの<literal>ExternalIP</literal>で使用できるように予約する必要があります。</para>
<para>最初のホストにSSHで接続して、クラスタモードで必要なディストリビューションをインストールします。</para>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC="server \
 --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" sh -

systemctl enable rke2-server.service
systemctl start rke2-server.service

# Fetch the cluster token:
RKE2_TOKEN=$(tr -d '\n' &lt; /var/lib/rancher/rke2/server/node-token)</screen>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>必ず、<literal>k3s
server</literal>コマンドで<literal>--disable=servicelb</literal>フラグを指定してください。</para>
</note>
<important>
<para>これ以降、コマンドはローカルマシンで実行する必要があります。</para>
</important>
<para>APIサーバに外部からアクセスするには、RKE2/K3s VMのIPを使用します。</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
export KUBE_DISTRIBUTION=&lt;k3s/rke2&gt;

scp ${NODE_IP}:/etc/rancher/${KUBE_DISTRIBUTION}/${KUBE_DISTRIBUTION}.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-cluster">
<title>既存のクラスタの設定</title>
<note>
<para>この手順は、既存のRKE2/K3sクラスタを使用する 場合にのみ有効です。</para>
</note>
<para>既存のクラスタを使用するには、<literal>tls-san</literal>フラグを変更し、また、K3sに対して<literal>servicelb</literal>
LBを無効にする必要があります。</para>
<para>RKE2またはK3sサーバのフラグを変更するには、ディストリビューションに応じて、クラスタのすべてのVM上で<literal>/etc/systemd/system/rke2.service</literal>または<literal>/etc/systemd/system/k3s.service</literal>ファイルを変更する必要があります。</para>
<para>フラグは<literal>ExecStart</literal>に挿入する必要があります。例:</para>
<para>RKE2の場合:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/rke2 \
    server \
        '--write-kubeconfig-mode=644' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>K3sの場合:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>次に、次のコマンドを実行して、新しい設定をロードする必要があります。</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart ${KUBE_DISTRIBUTION}</screen>
</section>
<section xml:id="id-installing-metallb">
<title>MetalLBのインストール</title>
<para><literal>MetalLB</literal>をデプロイするには、「<link
xl:href="https://suse-edge.github.io/docs/quickstart/metallb">K3s上のMetalLB</link>」のガイドを使用できます。</para>
<para><emphasis role="strong">メモ: </emphasis><literal>ip-pool</literal>
IPAddressPoolのIPアドレスが、以前に<literal>LoadBalancer</literal>サービスに対して選択したIPアドレスと重複していないことを確認してください。</para>
<para>管理対象サービスにのみ使用する<literal>IpAddressPool</literal>を別途作成します。</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Endpoint Copier Operatorのインストール</title>
<screen language="bash" linenumbering="unnumbered">helm install \
endpoint-copier-operator oci://registry.suse.com/edge/3.1/endpoint-copier-operator-chart \
--namespace endpoint-copier-operator \
--create-namespace</screen>
<para>上記のコマンドは2つのレプリカを持つ<literal>endpoint-copier-operator</literal>オペレータのデプロイメントをデプロイします。一方がリーダーとなり、他方は必要に応じてリーダーの役割を引き継ぎます。</para>
<para>これで、<literal>kubernetes-vip</literal>サービスがデプロイされ、オペレータによって調整され、設定されたポートとIPを持つエンドポイントが作成されるはずです。</para>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  type: LoadBalancer
EOF</screen>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer
EOF</screen>
<para><literal>kubernetes-vip</literal>サービスのIPアドレスが正しいことを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para><literal>default</literal>ネームスペースの<literal>kubernetes-vip</literal>および<literal>kubernetes</literal>
のEndpointsリソースが同じIPを指していることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpoints kubernetes kubernetes-vip</screen>
<para>すべて問題なければ、最後に<literal>Kubeconfig</literal>で<literal>VIP_SERVICE_IP</literal>を使用します。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>これ以降、<literal>kubectl</literal>はすべて<literal>kubernetes-vip</literal>サービスを経由するようになります。</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>コントロールプレーンノードの追加</title>
<para>プロセス全体を監視するため、端末タブを2つ以上開くことができます。</para>
<para>最初の端末:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>2つ目の端末:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpoints</screen>
<para>次に、以下のコマンドを2つ目のノードと3つ目のノードで実行します。</para>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
systemctl enable rke2-server.service


mkdir -p /etc/rancher/rke2/
cat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml
server: https://${VIP_SERVICE_IP}:9345
token: ${RKE2_TOKEN}
EOF

systemctl start rke2-server.service</screen>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Edge Image Builderを使用したエアギャップデプロイメント</title>
<section xml:id="id-intro">
<title>概要</title>
<para>このガイドでは、Edge Image Builder (EIB) (<xref
linkend="components-eib"/>)を使用し、完全にエアギャップされた環境で複数のSUSE EdgeコンポーネントをSLE Micro
6.0上にデプロイする方法を示します。これにより、EIBで作成したCustomized, Ready to Boot
(CRB)イメージでブートし、指定したコンポーネントをインターネット接続や手動手順なしにRKE2クラスタまたはK3sクラスタにデプロイできます。この設定は、デプロイメントに必要なアーティファクトをすべてOSイメージにプリベイクし、ブート後すぐに利用できるようにしたいお客様にとって非常に便利です。</para>
<para>ここでは、以下のエアギャップインストールについて説明します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-neuvector"/></para>
</listitem>
<listitem>
<para><xref linkend="components-longhorn"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIBは、指定したHelmチャートとKubernetesマニフェストで参照されているイメージをすべて解析し、事前にダウンロードします。ただし、その一部がコンテナイメージをプルし、そのイメージに基づいて実行時にKubernetesリソースを作成しようとする場合があります。このような場合、完全なエアギャップ環境を設定するには、必要なイメージを定義ファイルに手動で指定する必要があります。</para>
</warning>
</section>
<section xml:id="id-prerequisites-9">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、すでにEIB (<xref
linkend="components-eib"/>)に精通していることを想定しています。まだEIBに精通していない場合は、クイックスタートガイド(<xref
linkend="quickstart-eib"/>)に従って、以下の演習で示されている概念の理解を深めてください。</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Libvirtのネットワーク設定</title>
<note>
<para>エアギャップデプロイメントのデモを示すため、このガイドはシミュレートされたエアギャップ<literal>libvirt</literal>ネットワークを使用して実施し、それに合わせて以下の設定を調整します。ご自身のデプロイメントでは、<literal>host1.local.yaml</literal>の設定の変更が必要になる場合があります。これについては、次の手順で説明します。</para>
</note>
<para>同じ<literal>libvirt</literal>ネットワーク設定を使用する場合は、このまま読み進めてください。そうでない場合は、<xref
linkend="config-dir-creation"/>までスキップしてください。</para>
<para>DHCPのIPアドレス範囲<literal>192.168.100.2/24</literal>で、分離されたネットワーク設定を作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>あとはネットワークを作成して起動するだけです。</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>ベースディレクトリの設定</title>
<para>ベースディレクトリの設定は、各種のコンポーネントすべてで同じであるため、ここで設定します。</para>
<para>まず、必要なサブディレクトリを作成します。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>必ず、使用する予定のゴールデンイメージを<literal>base-images</literal>ディレクトリに追加してください。このガイドでは、<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあるセルフインストールISOに焦点を当てて説明します。</para>
<para>ダウンロードしたイメージをコピーしましょう。</para>
<screen language="shell" linenumbering="unnumbered">cp SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIBは、ゴールデンイメージの入力を変更することはありません。</para>
</note>
<para>目的のネットワーク設定を含むファイルを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>この設定により、プロビジョニングされたシステムに以下が確実に存在するようになります(指定されたMACアドレスを使用)。</para>
<itemizedlist>
<listitem>
<para>静的IPアドレスを持つEthernetインタフェース</para>
</listitem>
<listitem>
<para>ルーティング</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>ホスト名(<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>結果のファイル構造は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>ベース定義ファイル</title>
<para>Edge Image Builderでは、<emphasis>定義ファイル</emphasis>を使用してSLE
Microイメージを変更します。定義ファイルには、設定可能なオプションの大部分が含まれています。これらのオプションの多くは、異なるコンポーネントのセクションで繰り返し使用されるため、ここで一覧にして説明します。</para>
<tip>
<para>定義ファイルのカスタマイズオプションの全リストについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#image-definition-file">アップストリームドキュメント</link>を参照してください。</para>
</tip>
<para>すべての定義ファイルに存在する次のフィールドを見てみましょう。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.30.5+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para><literal>image</literal>セクションは必須であり、入力イメージ、そのアーキテクチャとタイプ、および出力イメージの名前を指定します。</para>
<para><literal>operatingSystem</literal>セクションはオプションであり、プロビジョニングされたシステムに<literal>root/eib</literal>のユーザ名/パスワードでログインできるようにするための設定が含まれます。</para>
<para><literal>kubernetes</literal>セクションはオプションであり、Kubernetesタイプとバージョンを定義しています。デフォルトではKubernetes
1.30.5とRKE2を使用します。  代わりにK3sが必要な場合は、<literal>kubernetes.version:
v1.30.5+k3s1</literal>を使用します。<literal>kubernetes.nodes</literal>フィールドを介して明示的に設定しない限り、このガイドでブートストラップするすべてのクラスタは、シングルノードクラスタになります。</para>
<para><literal>embeddedArtifactRegistry</literal>セクションには、実行時に特定のコンポーネントでのみ参照されてプルされるイメージがすべて含まれます。</para>
</section>
<section xml:id="rancher-install">
<title>Rancherのインストール</title>
<note>
<para>デモで示すRancher (<xref
linkend="components-rancher"/>)のデプロイメントは、デモのために非常にスリム化されています。実際のデプロイメントでは、設定に応じて追加のアーティファクトが必要な場合があります。</para>
</note>
<para><link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.9.3">Rancher
v2.9.3</link>リリースアセットには、エアギャップインストールに必要なすべてのイメージをリストする<literal>rancher-images.txt</literal>ファイルが含まれています。</para>
<para>コンテナ イメージは合計で600個以上あり、結果として得られるCRBイメージは約30GBになります。Rancher
のインストールでは、そのリストを最小の動作設定にまで削減します。そこから、デプロイメントに必要なイメージを追加し直すことができます。</para>
<para>定義ファイルを作成し、必要最小限のイメージリストを含めます。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.30.5+rke2r1
  network:
    apiVIP: 192.168.100.151
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.9.3
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.15.3
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url:  https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v5.0.2
    - name: registry.rancher.com/rancher/calico-cni:v3.28.1-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.0.16
    - name: registry.rancher.com/rancher/flannel-cni:v1.4.1-rancher1
    - name: registry.rancher.com/rancher/fleet-agent:v0.10.4
    - name: registry.rancher.com/rancher/fleet:v0.10.4
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240910
    - name: registry.rancher.com/rancher/hardened-calico:v3.28.1-build20240911
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.11-build20240910
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.5.1-build20240910
    - name: registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240910
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.23.1-build20240910
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.13-k3s1-build20240910
    - name: registry.rancher.com/rancher/hardened-flannel:v0.25.6-build20240910
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240910
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.30.5-rke2r1-build20240912
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.1.0-build20240910
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.6-build20240822
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.8.0-build20240910
    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.30.5-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.2-build20240828
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.9
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.2
    - name: registry.rancher.com/rancher/kubectl:v1.29.7
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.28
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher118
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.7.3
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.10.4-hardened3
    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.3-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.3-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.9.3
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v4.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.5.3
    - name: registry.rancher.com/rancher/rancher:v2.9.3
    - name: registry.rancher.com/rancher/rke-tools:v0.1.103
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.30.4-build20240910
    - name: registry.rancher.com/rancher/rke2-runtime:v1.30.5-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.30.5-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.2.18
    - name: registry.rancher.com/rancher/shell:v0.2.2
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.30.5-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.30.5-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.10-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.4
    - name: registry.rancher.com/rancher/ui-plugin-catalog:2.1.0
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/kubectl:v1.29.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.4.1
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.4.3
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v20230312-helm-chart-4.5.2-28-g66a760794
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v20231011-8b53cabe0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v20231226-1a7112e06</screen>
<para>600個以上のコンテナイメージの全リストと比較すると、このスリム化されたバージョンには約60個しか含まれておらず、新しいCRBイメージは約7GBになります。</para>
<para>RancherのHelm値も作成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para><literal>systemDefaultRegistry</literal>を<literal>registry.rancher.com</literal>に設定することで、Rancherは、ブート時にCRBイメージ内で起動される組み込みのアーティファクトレジストリ内でイメージを自動的に検索できます。このフィールドを省略すると、ノードでコンテナイメージを見つけられない場合があります。</para>
</warning>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Downloading file: dl-manifest-1.yaml 100% |█████████████████████████████████████████████████████████████████████████████████████████████████████████████| (583/583 kB, 12 MB/s)
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (4/4, 1 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |████████████████████████████████████████████████████████████████████████████████████████████████████████████| (57/57, 2020 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (780/780 MB, 115 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 108 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 117 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 34 MB/s)
Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 106 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、Rancherのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cattle-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS      RESTARTS   AGE
pod/helm-operation-5v24z               0/2     Completed   0          2m18s
pod/helm-operation-jqjkg               0/2     Completed   0          101s
pod/helm-operation-p88bw               0/2     Completed   0          112s
pod/helm-operation-sdnql               2/2     Running     0          73s
pod/helm-operation-xkpkj               0/2     Completed   0          119s
pod/rancher-844dc7f5f6-pz7bz           1/1     Running     0          3m14s
pod/rancher-webhook-5c87686d68-hsllv   1/1     Running     0          97s

NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/rancher           ClusterIP   10.43.96.117    &lt;none&gt;        80/TCP,443/TCP   3m14s
service/rancher-webhook   ClusterIP   10.43.112.253   &lt;none&gt;        443/TCP          97s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rancher           1/1     1            1           3m14s
deployment.apps/rancher-webhook   1/1     1            1           97s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/rancher-844dc7f5f6           1         1         1       3m14s
replicaset.apps/rancher-webhook-5c87686d68   1         1         1       97s</screen>
<para>また、<literal>https://192.168.100.50.sslip.io</literal>に移動し、以前に設定した<literal>adminadminadmin</literal>パスワードでログインすると、Rancherダッシュボードが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width=""/>
</imageobject>
<textobject><phrase>エアギャップRancher</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="neuvector-install">
<title>NeuVectorのインストール</title>
<para>Rancherのインストールとは異なり、NeuVectorのインストールではEIBで特別な処理を行う必要はありません。EIBはNeuVectorに必要なすべてのイメージを自動的にエアギャップ化します。</para>
<para>定義ファイルを作成します。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.30.5+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>NeuVector用のHelm値ファイルも作成します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (6/6, 20 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、NeuVectorのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS    RESTARTS   AGE
pod/neuvector-controller-pod-7db4c6c9f4-qq7cf   1/1     Running   0          2m46s
pod/neuvector-enforcer-pod-qfdp2                1/1     Running   0          2m46s

NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.254.230   &lt;none&gt;        443/TCP                         2m46s
service/neuvector-svc-controller          ClusterIP   None            &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   2m46s

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          2m46s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           2m46s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-7db4c6c9f4   1         1         1       2m46s

NAME                                  SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-updater-pod   0 0 * * *   &lt;none&gt;     False     0        &lt;none&gt;          2m46s</screen>
</section>
<section xml:id="longhorn-install">
<title>Longhornのインストール</title>
<para>Longhornの<link
xl:href="https://longhorn.io/docs/1.7.1/deploy/install/airgap/">公式ドキュメント</link>には、エアギャップインストールに必要なすべてのイメージをリストした<literal>longhorn-images.txt</literal>ファイルが含まれています。
定義ファイルには、 Rancherコンテナレジストリからのミラー化された対応するイメージを含めます。作成してみましょう。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
kubernetes:
  version: v1.30.5+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 104.2.0+up1.7.1
      - name: longhorn-crd
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        version: 104.2.0+up1.7.1
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.6.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v4.0.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.11.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v7.0.2
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.12.0
    - name: registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.14.0
    - name: registry.suse.com/rancher/mirrored-longhornio-openshift-origin-oauth-proxy:4.15
    - name: registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.42
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.7.1</screen>
<note>
<para>定義ファイルには<literal>open-iscsi</literal>パッケージがリストされていることに気づくでしょう。これは、LonghornがKubernetesに永続ボリュームを提供するために、さまざまなノードで実行されている
<literal>iscsiadm</literal> デーモンに依存しているために必要です。</para>
</note>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 20956 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、Longhornのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS        AGE
pod/csi-attacher-5dbc6d6479-jz2kf                       1/1     Running   0               116s
pod/csi-attacher-5dbc6d6479-k2t47                       1/1     Running   0               116s
pod/csi-attacher-5dbc6d6479-ms76j                       1/1     Running   0               116s
pod/csi-provisioner-55749f6bd8-cv7k2                    1/1     Running   0               116s
pod/csi-provisioner-55749f6bd8-qxmdd                    1/1     Running   0               116s
pod/csi-provisioner-55749f6bd8-rjqpl                    1/1     Running   0               116s
pod/csi-resizer-68fc4f8555-7sxr4                        1/1     Running   0               116s
pod/csi-resizer-68fc4f8555-blxlt                        1/1     Running   0               116s
pod/csi-resizer-68fc4f8555-ww6tc                        1/1     Running   0               116s
pod/csi-snapshotter-6876488cb5-fw7vg                    1/1     Running   0               116s
pod/csi-snapshotter-6876488cb5-xmz7l                    1/1     Running   0               116s
pod/csi-snapshotter-6876488cb5-zt6ht                    1/1     Running   0               116s
pod/engine-image-ei-f586bff0-m6vzb                      1/1     Running   0               2m34s
pod/instance-manager-d8b2d035a5c84130de8779e3b4c29113   1/1     Running   0               2m4s
pod/longhorn-csi-plugin-8dgxw                           3/3     Running   0               116s
pod/longhorn-driver-deployer-65b7c7c8cc-pz8lr           1/1     Running   0               3m13s
pod/longhorn-manager-pllq7                              2/2     Running   0               3m13s
pod/longhorn-ui-5c76575888-2rkpj                        1/1     Running   3 (2m52s ago)   3m13s
pod/longhorn-ui-5c76575888-6z69x                        1/1     Running   3 (2m55s ago)   3m13s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.213.17    &lt;none&gt;        9502/TCP   3m14s
service/longhorn-backend              ClusterIP   10.43.11.79     &lt;none&gt;        9500/TCP   3m14s
service/longhorn-conversion-webhook   ClusterIP   10.43.152.173   &lt;none&gt;        9501/TCP   3m14s
service/longhorn-frontend             ClusterIP   10.43.150.97    &lt;none&gt;        80/TCP     3m14s
service/longhorn-recovery-backend     ClusterIP   10.43.99.138    &lt;none&gt;        9503/TCP   3m14s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-f586bff0   1         1         1       1            1           &lt;none&gt;          2m34s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          116s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          3m13s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           116s
deployment.apps/csi-provisioner            3/3     3            3           116s
deployment.apps/csi-resizer                3/3     3            3           116s
deployment.apps/csi-snapshotter            3/3     3            3           116s
deployment.apps/longhorn-driver-deployer   1/1     1            1           3m13s
deployment.apps/longhorn-ui                2/2     2            2           3m13s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-5dbc6d6479               3         3         3       116s
replicaset.apps/csi-provisioner-55749f6bd8            3         3         3       116s
replicaset.apps/csi-resizer-68fc4f8555                3         3         3       116s
replicaset.apps/csi-snapshotter-6876488cb5            3         3         3       116s
replicaset.apps/longhorn-driver-deployer-65b7c7c8cc   1         1         1       3m13s
replicaset.apps/longhorn-ui-5c76575888                2         2         2       3m13s</screen>
</section>
<section xml:id="kubevirt-install">
<title>KubeVirtとCDIのインストール</title>
<para>KubeVirtとCDIの両方のHelmチャートでインストールされるのは、それぞれのオペレータのみです。残りのシステムのデプロイはオペレータに任されています。つまり、必要なコンテナイメージすべてを定義ファイルに含める必要があります。作成してみましょう。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.30.5+rke2r1
  helm:
    charts:
      - name: kubevirt-chart
        repositoryName: suse-edge
        version: 0.4.0
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi-chart
        repositoryName: suse-edge
        version: 0.4.0
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/3.1
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.6/cdi-uploadproxy:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-uploadserver:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-apiserver:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-controller:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-importer:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-cloner:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-api:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-controller:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-launcher:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-handler:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-exportproxy:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-exportserver:1.3.1-150600.5.9.1</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 48 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 4 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、KubeVirtとCDIの両方のインストールを確認できます。</para>
<para>KubeVirtを確認します。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                  READY   STATUS    RESTARTS   AGE
pod/virt-api-59cb997648-mmt67         1/1     Running   0          2m34s
pod/virt-controller-69786b785-7cc96   1/1     Running   0          2m8s
pod/virt-controller-69786b785-wq2dz   1/1     Running   0          2m8s
pod/virt-handler-2l4dm                1/1     Running   0          2m8s
pod/virt-operator-7c444cff46-nps4l    1/1     Running   0          3m1s
pod/virt-operator-7c444cff46-r25xq    1/1     Running   0          3m1s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.167.109   &lt;none&gt;        443/TCP   2m36s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m36s
service/virt-api                      ClusterIP   10.43.18.202    &lt;none&gt;        443/TCP   2m36s
service/virt-exportproxy              ClusterIP   10.43.142.188   &lt;none&gt;        443/TCP   2m36s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   2m8s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m34s
deployment.apps/virt-controller   2/2     2            2           2m8s
deployment.apps/virt-operator     2/2     2            2           3m1s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-59cb997648         1         1         1       2m34s
replicaset.apps/virt-controller-69786b785   2         2         2       2m8s
replicaset.apps/virt-operator-7c444cff46    2         2         2       3m1s

NAME                            AGE    PHASE
kubevirt.kubevirt.io/kubevirt   3m1s   Deployed</screen>
<para>CDIを確認します。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-5598c9bf47-pqfxw     1/1     Running   0          3m44s
pod/cdi-deployment-7cbc5db7f8-g46z7    1/1     Running   0          3m44s
pod/cdi-operator-777c865745-2qcnj      1/1     Running   0          3m48s
pod/cdi-uploadproxy-646f4cd7f7-fzkv7   1/1     Running   0          3m44s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.2.224    &lt;none&gt;        443/TCP    3m44s
service/cdi-prometheus-metrics   ClusterIP   10.43.237.13   &lt;none&gt;        8080/TCP   3m44s
service/cdi-uploadproxy          ClusterIP   10.43.114.91   &lt;none&gt;        443/TCP    3m44s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m44s
deployment.apps/cdi-deployment    1/1     1            1           3m44s
deployment.apps/cdi-operator      1/1     1            1           3m48s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m44s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-5598c9bf47     1         1         1       3m44s
replicaset.apps/cdi-deployment-7cbc5db7f8    1         1         1       3m44s
replicaset.apps/cdi-operator-777c865745      1         1         1       3m48s
replicaset.apps/cdi-uploadproxy-646f4cd7f7   1         1         1       3m44s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>トラブルシューティング</title>
<para>イメージの構築中に問題が発生した場合、またはプロセスをさらにテストおよびデバッグしたい場合は、<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">アップストリームドキュメント</link>を参照してください。</para>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>サードパーティの統合</title>
<partintro>
<para>サードパーティツールの統合方法</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link
xl:href="https://nats.io/">NATS</link>は、ますますハイパーコネクテッド化が進む世界のために構築された接続テクノロジです。NATSは、クラウドベンダ、オンプレミス、エッジ、Web、モバイルデバイスがどのように組み合わさっていてもアプリケーションが安全に通信することを可能にする単一のテクノロジです。NATSはオープンソース製品ファミリで構成されており、各製品は緊密に統合されている一方で、簡単に個別にデプロイできます。NATSは世界中で数千社もの企業で使用されており、マイクロサービス、エッジコンピューティング、モバイル、IoTなどのユースケースに幅広く対応しているため、NATSを使用して従来のメッセージングの強化や置き換えを図ることができます。</para>
<section xml:id="id-architecture">
<title>アーキテクチャ</title>
<para>NATSは、メッセージの形式でアプリケーション間のデータ交換を可能にするインフラストラクチャです。</para>
<section xml:id="id-nats-client-applications">
<title>NATSクライアントアプリケーション</title>
<para>NATSクライアントライブラリを使用すると、アプリケーションが異なるインスタンス間でパブリッシュ、サブスクライブ、要求、および応答できるようになります。このようなアプリケーションを一般的に<literal>クライアントアプリケーション</literal>と呼びます。</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>NATSサービスインフラストラクチャ</title>
<para>NATSサービスは、相互接続されてNATSサービスインフラストラクチャを提供するように設定された1つ以上のNATSサーバプロセスによって提供されます。NATSサービスインフラストラクチャは、1つのエンドデバイスで動作する単一のNATSサーバプロセスから、すべての主要クラウドプロバイダと世界のあらゆる地域にまたがる多数のクラスタからなるパブリックなグローバルスーパークラスタまで拡張可能です。</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>シンプルなメッセージングデザイン</title>
<para>NATSを使用すると、アプリケーションはメッセージを送受信して簡単に通信できます。これらのメッセージはサブジェクト文字列によってアドレス指定および識別され、ネットワークの場所には依存しません。データはエンコードされてメッセージとしてフレーム化され、パブリッシャによって送信されます。メッセージは1人以上のサブスクライバによって受信、デコード、処理されます。</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATSにはJetStreamと呼ばれる分散型の永続化システムが組み込まれています。JetStreamは、今日のテクノロジにおけるストリーミングで明らかになった問題、すなわち複雑性、脆弱性、スケーラビリティの欠如を解決するために作成されました。また、JetStreamは、パブリッシャとサブスクライバのカップリングに関する問題(パブリッシュされたメッセージを受信するにはサブスクライバが稼働している必要がある)も解決します。NATS
JetStreamの詳細については、<link
xl:href="https://docs.nats.io/nats-concepts/jetstream">こちら</link>を参照してください。</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>インストール</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>K3s上へのNATSのインストール</title>
<para>NATSは複数のアーキテクチャ向けに構築されているため、K3s (<xref
linkend="components-k3s"/>)上に簡単にインストールできます。</para>
<para>NATSのデフォルト値を上書きするvaluesファイルを作成しましょう。</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>では、Helmを介してNATSをインストールしてみましょう。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>上記の<literal>values.yaml</literal>ファイルでは、次のコンポーネントが<literal>nats</literal>ネームスペースに配置されます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>NATS StatefulsetのHAバージョン。3つのコンテナ(NATSサーバ + ConfigリローダとMetricsサイドカー)が含まれます。</para>
</listitem>
<listitem>
<para>NATS boxコンテナ。セットアップの確認に使用できる一連の<literal>NATS</literal>ユーティリティが付属します。</para>
</listitem>
<listitem>
<para>JetStreamは、Podにバインドされた<literal>PVC</literal>が付属するKey-Valueバックエンドも利用します。</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>セットアップのテスト</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>テストサブジェクトのサブスクリプションを作成します。</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>テストサブジェクトにメッセージを送信します。</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>クリーンアップ</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>K3sのバックエンドとしてのNATS</title>
<para>K3sが利用するコンポーネントの1つが<link
xl:href="https://github.com/k3s-io/kine">KINE</link>です。KINEは、最初からリレーショナルデータベースをターゲットとした代替ストレージバックエンドでetcdを置き換えることを可能にするシムです。JetStreamはKey
Value APIを備えているので、NATSをK3sクラスタのバックエンドとして利用することが可能です。</para>
<para>K3sのビルトインNATSが容易になるマージ済みのPRがありますが、この変更はまだK3sリリースに<link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">含まれていません</link>。</para>
<para>このため、K3sのバイナリを手動で構築する必要があります。</para>
<para>このチュートリアルでは、<link
xl:href="https://suse-edge.github.io/docs/quickstart/slemicro-utm-aarch64">Appleシリコン上のOSX上のSLE
Micro (UTM)</link>のVMを使用します。</para>
<note>
<para>以下のコマンドはOSX PC上で実行してください。</para>
</note>
<section xml:id="id-building-k3s">
<title>K3sの構築</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>次のコマンドは、ビルドタグに<literal>nats</literal>を追加して、K3sでNATSビルトイン機能を有効にします。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>&lt;node-ip&gt;は、K3sを起動するノードの実際のIPに置き換えます。</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>K3sをローカルで構築するには、buildx Docker CLIプラグインが必要です。<literal>$ make
local</literal>が失敗する場合は、<link
xl:href="https://github.com/docker/buildx#manual-download">手動でインストール</link>できます。</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>NATS CLIのインストール</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>K3sのバックエンドとしてのNATSの実行</title>
<para>ノードで<literal>ssh</literal>を実行し、<literal>--datastore-endpoint</literal>フラグで<literal>nats</literal>を指してK3sを実行しましょう。</para>
<note>
<para>次のコマンドでは、K3sをフォアグランドプロセスとして起動するので、ログを簡単に追跡して問題がないかどうかを確認できます。現在の端末をブロックしないようにするには、コマンドの前に<literal>&amp;</literal>フラグを追加して、バックグラウンドプロセスとして起動できます。</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>NATSバックエンドを使用するK3sサーバを<literal>slemicro</literal>
VM上で永続化するには、次のスクリプトを実行して、必要な設定で<literal>systemd</literal>サービスを作成します。</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>トラブルシューティング</title>
<para>次のコマンドをノード上で実行して、ストリームのすべてが適切に動作していることを確認できます。</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-sle-micro">
<title>SLE Micro上のNVIDIA GPU</title>
<section xml:id="id-intro-2">
<title>概要</title>
<para>このガイドでは、事前構築済みの<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">オープンソースドライバ</link>を使用してホストレベルのNVIDIA
GPUサポートをSLE Micro 6.0に実装する方法を説明します。これらのドライバは、NVIDIAの <link
xl:href="https://github.com/NVIDIA/gpu-operator">GPU
Operator</link>によって動的にロードされるのではなく、オペレーティングシステムにベイクされているドライバです。この設定は、デプロイメントに必要なすべてのアーティファクトをあらかじめイメージにベイクしておき、ドライバのバージョンを動的に選択する必要がない(つまり、ユーザがKubernetesを介してドライバのバージョンを選択する必要がない)お客様に非常に適しています。このガイドでは最初に、すでに事前にデプロイされているシステムに追加コンポーネントをデプロイする方法を説明しますが、その後のセクションでは、Edge
Image
Builderを使用してこの設定を初期デプロイメントに組み込む方法について説明します。基本的な操作を読む必要がない場合や、手動でセットアップしたくない場合は、スキップしてそちらのセクションに進んでください。</para>
<para>これらのドライバのサポートは、SUSEとNVIDIAの両社が緊密に連携して提供しており、ドライバはパッケージリポジトリの一部としてSUSEによって構築および出荷されている点を強調しておくことが重要です。ただし、ドライバを使用する組み合わせについて不安や質問がある場合は、SUSEまたはNVIDIAのアカウントマネージャに問い合わせてサポートを受けてください。<link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE)を使用する予定の場合は、<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">NVAIE認定GPU</link>を使用していることを確認してください。NVAIE認定GPUでは、独自のNVIDIAドライバを使用する必要がある<emphasis>「場合があります」</emphasis>。不明な点がある場合は、NVIDIAの担当者に問い合わせてください。</para>
<para>NVIDIA GPU
Operatorの統合の詳細は、このガイドでは説明<emphasis>「しません」</emphasis>。Kubernetes用のNVIDIA GPU
Operatorの統合についてはここでは説明しませんが、このガイドのほとんどの手順に従って、基礎となるオペレーティングシステムをセットアップできます。そして、NVIDIA
GPU
OperatorのHelmチャートの<literal>driver.enabled=false</literal>フラグを使用して<emphasis>「プリインストール」</emphasis>されたドライバをGPU
Operatorが使用できるようにするだけで、ホスト上にインストールされたドライバが取得されます。より包括的な手順については、 NVIDIA
(<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">こちら</link>)で参照できます。さらにSUSEは先日、<link
xl:href="https://documentation.suse.com/trd/kubernetes/single-html/gs_rke2-slebci_nvidia-gpu-operator/">テクニカルリファレンスドキュメント</link>
(TRD)も公開しました。このドキュメントでは、ご自身のユースケースにGPU
OperatorとNVIDIA独自のドライバが必須の場合にこれらを使用する方法を説明しています。</para>
</section>
<section xml:id="id-prerequisites-10">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLE Micro 6.0がインストールされている少なくても1台のホスト。物理でも仮想でも構いません。</para>
</listitem>
<listitem>
<para>パッケージへのアクセスにはサブスクリプションが必要であるため、ホストがサブスクリプションに接続されていること。評価版は <link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>から入手できます。</para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">互換性のあるNVIDIA
GPU</link>がインストールされていること(またはSLE
Microが実行されている仮想マシンに<emphasis>「完全に」</emphasis> パススルーされていること)。</para>
</listitem>
<listitem>
<para>ルートユーザへのアクセス —
以下の説明では、自身がルートユーザであり、<literal>sudo</literal>を使用して特権を昇格して<emphasis>「いない」</emphasis>ことを想定しています。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>手動インストール</title>
<para>このセクションでは、NVIDIAドライバをSLE
Microオペレーティングシステムに直接インストールします。これはopen版NVIDIAドライバがSLE
Microのコアパッケージリポジトリの一部となったためであり、必須のRPMパッケージをインストールするのと同じように簡単にインストールできるようになりました。実行可能パッケージのコンパイルやダウンロードは必要ありません。以下では、最新のGPUをサポートする「G06」世代ドライバのデプロイについて手順を追って説明します(詳細については
<link
xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">こちら</link>を参照してください)。ご使用のシステムに搭載されているNVIDIA
GPUに適切なドライバ世代を選択してください。最新のGPUでは、「G06」ドライバが最も一般的な選択肢です。</para>
<para>始める前に、SUSEがSLE
Microの一部として出荷するopen版NVIDIAドライバのほかに、ご自身のセットアップに追加のNVIDIAコンポーネントも必要な場合があることを認識しておくことが重要です。たとえば、OpenGLライブラリ、CUDAツールキット、
<literal>nvidia-smi</literal>などのコマンドラインユーティリティ、<literal>nvidia-container-toolkit</literal>などのコンテナ統合コンポーネントです。これらのコンポーネントの多くはNVIDIA独自のソフトウェアであるため、SUSEからは出荷されません。また、NVIDIAの代わりにSUSEが出荷しても意味がありません。そのため、説明の一環として、これらのコンポーネントにアクセスできるようにする追加のリポジトリを設定し、これらのツールの使用方法の例をいくつか説明し、完全に機能するシステムを作成します。SUSEのリポジトリとNVIDIAのリポジトリを区別することが重要です。これは、NVIDIAが提供するパッケージのバージョンとSUSEが構築したものが一致しない場合があるためです。これは通常、SUSEがopen版ドライバの新バージョンを利用可能にしたときに発生し、NVIDIAのリポジトリで同等のパッケージが利用可能になるまでに数日かかります。</para>
<para>以下をチェックして、選択するドライババージョンがGPUと互換性があり、CUDAの要件を満たしていることを確認することをお勧めします。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">CUDAリリースノート</link></para>
</listitem>
<listitem>
<para>デプロイを計画しているドライババージョンが、<link
xl:href="https://download.nvidia.com/suse/sle15sp6/x86_64/">NVIDIA
SLE15-SP6リポジトリ</link>に一致するバージョンがあり、サポートコンポーネントの同等のパッケージバージョンが利用可能であることを確認します。</para>
</listitem>
</itemizedlist>
<tip>
<para>NVIDIAオープンドライババージョンを確認するには、ターゲットマシンで<literal>zypper se -s
nvidia-open-driver</literal>を実行するか、<emphasis>または</emphasis> SUSE Customer
Centerで <link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.0&amp;arch=x86_64">SLE
Micro 6.0 for x86_64</link>の「nvidia-open-driver」を検索します。</para>
<para>執筆時点では、1つのバージョンが利用可能です(<emphasis>550.54.14</emphasis>):</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width=""/>
</imageobject>
<textobject><phrase>SUSE Customer Centre</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>NVIDIAリポジトリで同等のバージョンが利用可能であることを確認したら、ホストオペレーティングシステムにパッケージをインストールできます。
そのためには、<literal>transactional-update</literal>セッションを開く必要があります。これにより、基礎となるオペレーティングシステムの読み込み/書き込みスナップショットが新しく作成され、イミュータブルプラットフォームに変更を加えることが可能になります(<literal>transactional-update</literal>の詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/6.0/html/Micro-transactional-updates/transactional-updates.html">こちら</link>を参照してください)。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para><literal>transactional-update</literal>シェルを起動したら、NVIDIAからパッケージリポジトリを追加します。これにより、<literal>nvidia-smi</literal>などの追加ユーティリティをプルできます。</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-sle15sp6-main
zypper --gpg-auto-import-keys refresh</screen>
<para>その後、ドライバと、追加ユーティリティの<literal>nvidia-compute-utils</literal>をインストールできます。ユーティリティが不要の場合は省略できますが、テスト目的の場合は、この段階でインストールする価値があります。</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>インストールが失敗する場合、選択したドライババージョンとNVIDIAがリポジトリで配布しているバージョンとの間で依存関係の不一致があることを示している可能性があります。前のセクションを参照して、バージョンが一致していることを確認してください。また、別のドライババージョンをインストールしてみてください。たとえば、NVIDIAリポジトリに以前のバージョンがある場合、インストールコマンドに<literal>nvidia-open-driver-G06-signed-kmp=550.54.14</literal>を指定して、一致するバージョンを指定してみることができます。</para>
</note>
<para>次に、サポートされているGPUを使用して<emphasis>「いない」</emphasis>場合は(<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">こちら</link>でリストを確認できます)、モジュールレベルでサポートを有効にすることで、ドライバが動作するかどうかを確認できますが、結果はユーザによって異なります。<emphasis>「サポートされている」</emphasis>GPUを使用している場合は、この手順はスキップしてください。</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>これらのパッケージをインストールしたので、<literal>transactional-update</literal>セッションを終了します。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>次に進む前に、<literal>transactional-update</literal>セッションを終了していることを確認してください。</para>
</note>
<para>ドライバをインストールしたら、再起動します。SLE
Microはイミュータブルオペレーティングシステムであるため、前の手順で作成した新しいスナップショットで再起動する必要があります。ドライバはこの新しいスナップショットにのみインストールされるため、この新しいスナップショットで再起動しないとドライバをロードできません(新しいスナップショットでの再起動は自動的に実行されます)。準備ができたらrebootコマンドを発行します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>システムが正常に再起動したら、ログインし直し、
<literal>nvidia-smi</literal>ツールを使用して、ドライバが正常にロードされていて、GPUへのアクセスと列挙をどちらも実行できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>このコマンドの出力は次のような出力になります。以下の例では、GPUが2つあることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">Wed Feb 28 12:31:06 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>これで、SLE MicroシステムへのNVIDIAドライバのインストールと検証プロセスは完了です。</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>手動インストールの追加検証</title>
<para>この段階で確認できるのは、ホストレベルでNVIDIAデバイスにアクセスできること、およびドライバが正常にロードされていることだけです。ただし、それが機能していることを確認したい場合は、簡単なテストを実施して、GPUがユーザスペースアプリケーションから、理想的にはコンテナ経由で命令を受け取れること、および実際のワークロードが通常使用するものであるCUDAライブラリを通じて命令を受け取れることを検証します。このためには、<literal>nvidia-container-toolkit</literal>
(<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container
Toolkit</link>)をインストールしてホストOSにさらに変更を加えることができます。まず、別の<literal>transactional-update</literal>シェルを開きます。前の手順ではこれを1つのトランザクションで実行できたことに注目し、後のセクションでこれを完全に自動的に実行する方法を確認します。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>次に、NVIDIA Container
Toolkitリポジトリから<literal>nvidia-container-toolkit</literal>パッケージをインストールします。</para>
<itemizedlist>
<listitem>
<para>次の<literal>nvidia-container-toolkit.repo</literal>には、安定版(<literal>nvidia-container-toolkit</literal>)と実験版(<literal>nvidia-container-toolkit-experimental</literal>)のリポジトリが含まれています。運用環境での使用には、安定版リポジトリをお勧めします。実験版リポジトリはデフォルトで無効になっています。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>準備ができたら、<literal>transactional-update</literal>シェルを終了できます。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>…​そして新しいスナップショットでマシンを再起動します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>前述同様に、変更を有効にするには、必ず<literal>transactional-shell</literal>を終了し、マシンを再起動する必要があります。</para>
</note>
<para>マシンが再起動したら、システムがNVIDIA Container
Toolkitを使用してデバイスを正常に列挙できることを確認できます。出力は詳細で、INFOとWARNのメッセージがありますが、ERRORのメッセージはありません。</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>これにより、そのマシンで起動するすべてのコンテナはすべて、検出されたNVIDIA
GPUデバイスを使用できることが確認されます。準備ができたら、podmanベースのコンテナを実行できます。これを<literal>podman</literal>を介して行うことで、コンテナ内からNVIDIAデバイスへのアクセスを効果的に検証することができ、後の段階でKubernetesで同じ操作をするための自信が得られます。<literal>podman</literal>に対し、前のコマンドで<link
xl:href="https://registry.suse.com/repositories/bci-bci-base-15sp6">SLE
BCI</link>に基づいて処理したラベル付きのNVIDIAデバイスへのアクセス権を与え、バッシュコマンドを実行します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>続いて、一時的なpodmanコンテナ内からコマンドを実行します。このコンテナは基盤となるシステムにはアクセスできず一時的であるため、ここで行う操作は永続せず、基盤となるホスト上にあるものを壊すことは一切できないはずです。現在はコンテナ内で作業しているため、必要なCUDAライブラリをインストールできます。ここでもう一度、ご使用のドライバにあったCUDAバージョンを<link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">こちら</link>で確認してください。ただし、必要なCUDAバージョンは<literal>nvidia-smi</literal>の前の出力に表示されているはずです。以下の例では、<emphasis>CUDA
12.3</emphasis>をインストールして多数の例、デモ、開発キットをプルし、GPUを完全に検証できるようにしています。</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-sles15
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>これが正常にインストールされた後にコンテナを終了しないでください。<literal>deviceQuery</literal>
CUDAの例を実行し、CUDAを介して、およびコンテナ自体からGPUアクセスを包括的に検証します。</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>成功すると、次のような出力が表示されます。コマンドの最後にある「<literal>Result =
PASS</literal>」というメッセージに注意してください。また、次の出力では2つのGPUが正しく識別されていますが、ご使用の環境では1つしかない場合があることにも注意してください。</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>ここから、続いて他のCUDAワークロードを実行できます。コンパイラやCUDAエコシステムの他の側面を使用して、さらにテストを実行できます。完了したら、コンテナを終了できます。コンテナにインストールしたものはすべて一時的なものであるため(したがって失われるため)、基盤となるオペレーティングシステムには影響がないことに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Kubernetesを使用した実装</title>
<para>open版NVIDIAドライバをSLE
Microにインストールして使用できることが証明されたので、同じマシンにKubernetesを設定してみましょう。このガイドでは、Kubernetesのデプロイについては説明しませんが、<link
xl:href="https://k3s.io/">K3s</link>または<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>をインストール済みで、kubeconfigが適宜設定されており、標準の<literal>kubectl</literal>コマンドをスーパーユーザとして実行できることを前提としています。ここではノードがシングルノードクラスタを形成していることを想定していますが、中心となる手順はマルチノードクラスタでも同様です。まず、<literal>kubectl</literal>のアクセスが機能していることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.30.5+rke2r1</screen>
<para>k3s/rke2のインストールによってホスト上のNVIDIA Container
Toolkitが検出され、NVIDIAランタイム統合が<literal>containerd</literal>
(k3s/rke2が使用するContainer Runtime
Interface)に自動設定されていることを確認する必要があります。確認するには、containerdの<literal>config.toml</literal>ファイルをチェックします。</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>次のような画面が表示される必要があります。K3sの場合に相当する場所は<literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>です。</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>これらのエントリが存在しない場合は、検出が失敗している可能性があります。この原因として考えられるのは、マシンまたはKubernetesサービスを再起動していないことです。必要に応じて、上記のようにこれらを手動で追加してください。</para>
</note>
<para>次に、NVIDIA
<literal>RuntimeClass</literal>を追加のKubernetesランタイムとしてデフォルト値に設定する必要があります。これにより、GPUへのアクセスが必要なPodに対するユーザ要求が、
<literal>containerd</literal>の設定に従って、NVIDIA Container
Toolkitを使用して<literal>nvidia-container-runtime</literal>を介してアクセスできるようにします。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>次の手順は、<link xl:href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA
Device Plugin</link>を設定することです。これにより、NVIDIA Container
Toolkitと連携して、クラスタ内で使用可能なリソースとしてNVIDIA
GPUを利用するようにKubernetesを設定します。このツールはまず、基盤となるホスト上のすべての機能(GPU、ドライバ、その他の機能(GLなど)を含む)を検出し、その後、ユーザがGPUリソースを要求してアプリケーションの一部として使用できるようにします。</para>
<para>まず、NVIDIA Device Plugin用のHelmリポジトリを追加して更新する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>これで、NVIDIA Device Pluginをインストールできます。</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>数分後、新しいPodが実行されているのがわかります。これで、利用可能なノード上での検出は完了し、検出されたGPUの数を示すタグがノードに付けられます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>これで、このGPUを使用するNVIDIA Podを作成する準備ができました。CUDA Benchmarkコンテナで試してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>すべて問題なければ、ログを見て、ベンチマーク情報を確認できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance)
	-numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
	-device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>最後に、アプリケーションでOpenGLが必要な場合は、必要なNVIDIA OpenGLライブラリをホストレベルでインストールし、 NVIDIA
Device PluginとNVIDIA Container
Toolkitを使用してそのライブラリをコンテナで利用できるようにすることができます。これを行うには、次のようにパッケージをインストールします。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>このパッケージをアプリケーションで使用できるようにするには再起動が必要です。NVIDIA Device Pluginは、NVIDIA Container
Toolkitを介してこれを自動的に再検出します。</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Edge Image Builderを使用した統合</title>
<para>さて、SLE Micro上のアプリケーションとGPUの全機能をデモで示したので、 <xref
linkend="components-eib"/>を使用してすべてをまとめ、デプロイ可能/使用可能なISOまたはRAWディスクイメージで提供したいと思います。このガイドでは、Edge
Image
Builderの使用方法は説明せずに、このようなイメージを構築するために必要な設定について説明します。以下に、必要なすべてのコンポーネントを追加設定なしにデプロイするためのイメージ定義の例と、必要なKubernetes設定ファイルを示します。以下に示す例では、Edge
Image Builderディレクトリは次のようなディレクトリ構造になっています。</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>これらのファイルを調べてみましょう。まず、K3sを実行するシングルノードクラスタのサンプルイメージ定義を次に示します。このイメージ定義では、ユーティリティとOpenGLパッケージもデプロイします(<literal>eib-config-iso.yaml</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: &lt;snip&gt;
kubernetes:
  version: v1.30.5+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>これは単なる例です。要件や期待に合うようにカスタマイズする必要がある場合があります。また、SLE
Microを使用する場合は、パッケージの依存関係を解決してNVIDIAドライバをプルするために、独自の
<literal>sccRegistrationCode</literal>を指定する必要があります。</para>
</note>
<para>これに加えて、他のコンポーネントを追加して、ブート時にKubernetesによってロードされるようにする必要があります。EIBディレクトリにはまず<literal>kubernetes</literal>ディレクトリが必要で、その下に設定、Helmチャート値、必要な追加のマニフェスト用のサブディレクトリが必要です。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>CNIを選択し(選択しない場合はデフォルトでCiliumになります)、SELinuxを有効にして、(オプションの)Kubernetes設定を行いましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>続いて、NVIDIA RuntimeClassがKubernetesクラスタ上に作成されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>ビルトインHelmコントローラを使用して、Kubernetes自体を使用してNVIDIA Device
Pluginをデプロイします。チャートの値ファイルでランタイムクラスを指定しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>次に進む前に、NVIDIA Container Toolkit RPMの公開鍵を取得する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>Kubernetesバイナリ、コンテナイメージ、Helmチャート(および参照イメージ)など、必要なアーティファクトがすべて自動的にエアギャップ化されます。つまり、デプロイ時のシステムにはデフォルトでインターネット接続は不要です。ここで必要なのは<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からSLE
Micro
ISOを取得する(そしてそれを<literal>base-images</literal>ディレクトリに配置する)ことだけです。そうすれば、Edge
Image Builderツールを呼び出してISOを生成できます。この例を完了するために、イメージの構築に使用したコマンドを次に示します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
build --definition-file eib-config-iso.yaml</screen>
<para>Edge Image Builderの詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md">ドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-resolving-issues">
<title>問題の解決</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smiでGPUが検出されない</title>
<para><literal>dmesg</literal>を使用してカーネルメッセージを確認します。<literal>NvKMSKapDevice</literal>を割り当てることができないことを示している場合は、サポート対象外のGPUの回避策を適用します。</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>メモ</emphasis>:
上記の手順でカーネルモジュールの設定を変更した場合は、変更を有効にするために、カーネルモジュールを再ロードするか、再起動する必要があります。</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="day-2-operations">
<title>Day 2操作</title>
<partintro>
<para>このセクションでは、管理者がさまざまな「Day 2」操作タスクを管理クラスタとダウンストリームクラスタの両方で処理する方法について説明します。</para>
</partintro>
<chapter xml:id="day2-migration">
<title>Edge 3.1のマイグレーション</title>
<para>このセクションでは、既存の <literal>Edge 3.0</literal> (3.0.1や3.0.2などのマイナーリリースを含む)
<emphasis role="strong">管理</emphasis>クラスタと<emphasis
role="strong">ダウンストリーム</emphasis>クラスタを<literal>Edge
3.1.0</literal>に移行するためのガイドラインを提供します。</para>
<para><literal>Edge 3.1.0</literal>コンポーネントバージョンのリストについては、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
<section xml:id="day2-migration-mgmt">
<title>管理クラスタ</title>
<para>このセクションでは、<literal>管理</literal>クラスタを<literal>Edge
3.0</literal>から<literal>Edge 3.1.0</literal>に移行する方法について説明します。</para>
<para><literal>管理</literal>クラスタコンポーネントは、次の順序で移行する必要があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>オペレーティングシステム(OS) (<xref linkend="day2-migration-mgmt-os"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="day2-migration-mgmt-rke2"/>)</para>
</listitem>
<listitem>
<para>Edge Helmチャート(<xref linkend="day2-migration-mgmt-helm"/>)</para>
</listitem>
</orderedlist>
<section xml:id="day2-migration-mgmt-os">
<title>オペレーティングシステム(OS)</title>
<para>このセクションでは、<literal>管理</literal>クラスタノードのOSを<literal>Edge
3.1.0</literal>でサポートされているバージョンに移行するために必要な手順について説明します。</para>
<important>
<para>以下の手順は<literal>管理</literal>クラスタの各ノードに実行する必要があります。</para>
<para>予期しない問題を避けるため、クラスタの<literal>コントロールプレーン</literal>ノードを最初に移行してから<literal>ワーカー</literal>ノードを移行してください。</para>
</important>
<section xml:id="id-prerequisites-11">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para><literal>SCC登録ノード</literal> - クラスタのノードのOSが、<literal>Edge
3.1</literal>リリース(<xref
linkend="release-notes"/>)で指定されているオペレーティングシステムのバージョンをサポートするサブスクリプションキーで登録されていることを確認してください。</para>
</listitem>
</itemizedlist>
<para><emphasis>エアギャップ:</emphasis></para>
<itemizedlist>
<listitem>
<para><literal>SUSE RPMリポジトリのミラーリング</literal> - <literal>Edge
3.1.0</literal>リリース(<xref
linkend="release-notes"/>)で指定されるオペレーティングシステムに関連するRPMリポジトリはローカルにミラーリングし、
<literal>transactional-update</literal>がそのリポジトリにアクセスできるようにする必要があります。このためには、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>または<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>を使用します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-migration-steps">
<title>マイグレーション手順</title>
<note>
<para>以下の手順は、<literal>root</literal>として実行しており、<literal>kubectl</literal>が<literal>管理</literal>クラスタに接続するように設定されていることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>ノードをunschedulableとマークします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl cordon &lt;node_name&gt;</screen>
<para><literal>cordon</literal>コマンドのオプションの全リストについては、「<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">kubectl
cordon</link>」を参照してください。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">オプションで</emphasis>、ノードのワークロードに<literal>drain</literal>を実行するユースケースがある場合があります。</para>
<screen language="bash" linenumbering="unnumbered">kubectl drain &lt;node&gt;</screen>
<para><literal>drain</literal>コマンドのオプションの全リストについては、「<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">kubectl
drain</link>」を参照してください。</para>
</listitem>
<listitem>
<para>マイグレーション前に、現在のOSのパッケージが更新されていることを確認する必要があります。これには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">transactional-update</screen>
<para>上記のコマンドは、<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">zypper
up</link>を実行して、OSパッケージを更新します。
<literal>transactional-update</literal>の詳細については、<link
xl:href="https://documentation.suse.com/smart/systems-management/html/Micro-transactional-updates/index.html">transactional-updateガイド</link>を参照してください。</para>
</listitem>
<listitem>
<para>OSマイグレーションに進みます。</para>
<screen language="bash" linenumbering="unnumbered">transactional-update --continue migration</screen>
<note>
<para>ここでは、<literal>--continue</literal>オプションは、システムを再起動せずに以前のスナップショットを再使用するために使用されます。</para>
</note>
<itemizedlist>
<listitem>
<para>サブスクリプションキーが<literal>SUSE Linux Micro
6.0</literal>バージョンをサポートしている場合は、次のようなプロンプトが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-migration-os-migration-prompt.png"
width=""/> </imageobject>
<textobject><phrase>day2マイグレーションOSマイグレーションプロンプト</phrase></textobject>
</mediaobject>
</informalfigure>
<para><literal>SUSE Linux Micro 6.0
&lt;arch&gt;</literal>に対応する<literal>番号</literal>を選択します。</para>
<note>
<para><literal>Edge 3.1.0</literal>リリースは、<literal>SUSE Linux Micro
6.0</literal>オペレーティングシステム<emphasis role="strong">のみ</emphasis>をサポートしています。</para>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>transactional-update</literal>が正常に実行されたら、変更をシステムに適用するために、再起動する必要があります。</para>
<screen language="bash" linenumbering="unnumbered">reboot</screen>
</listitem>
<listitem>
<para>ホストが再起動された後で、オペレーティングシステムが<literal>SUSE Linux Micro
6.0</literal>に移動されていることを検証します。</para>
<screen language="bash" linenumbering="unnumbered">cat /etc/os-release</screen>
<para>出力は次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">NAME="SL-Micro"
VERSION="6.0"
VERSION_ID="6.0"
PRETTY_NAME="SUSE Linux Micro 6.0"
ID="sl-micro"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sl-micro:6.0"
HOME_URL="https://www.suse.com/products/micro/"
DOCUMENTATION_URL="https://documentation.suse.com/sl-micro/6.0/"</screen>
<note>
<para>マイグレーションで何らかの障害が発生した場合は、次のコマンドを使用して、最後の動作しているスナップショットにロールバックできます。</para>
<screen language="bash" linenumbering="unnumbered">transactional-update rollback last</screen>
<para><literal>ロールバック</literal>を有効にするには、システムを再起動する必要があります。ロールバック手順に関する詳細については、<link
xl:href="https://documentation.suse.com/smart/systems-management/html/Micro-transactional-updates/index.html#tr-up-rollback">公式の<literal>transactional-update</literal>ドキュメント</link>を参照してください。</para>
</note>
</listitem>
<listitem>
<para>ノードをschedulableとしてマークします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl uncordon &lt;node_name&gt;</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-rke2">
<title>RKE2</title>
<important>
<para>以下の手順は<literal>管理</literal>クラスタの各ノードに実行する必要があります。</para>
<para><link
xl:href="https://docs.rke2.io/upgrade/manual_upgrade">RKE2ドキュメント</link>で説明してるように、アップグレード手順では、一度にクラスタの<literal>コントロールプレーン</literal>ノードをアップグレードしてから、<literal>エージェント</literal>ノードをアップグレードする必要があります。</para>
</important>
<note>
<para><emphasis
role="strong">ディザスタリカバリ</emphasis>を確実行えるように、RKE2クラスタデータのバックアップを取ることをお勧めします。この実行方法については、『<link
xl:href="https://docs.rke2.io/backup_restore">RKE2 backup and restore guide
(RKE2バックアップおよびリストアガイド)
</link>』を参照してください。<literal>rke2</literal>バイナリのデフォルトの場所は、<literal>/opt/rke2/bin</literal>です。</para>
</note>
<para>次のようにRKE2インストールスクリプトを使用して、RKE2バージョンを<literal>Edge
3.1.0</literal>と互換性のあるバージョンにアップグレードできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ノードをunschedulableとマークします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl cordon &lt;node_name&gt;</screen>
<para><literal>cordon</literal>コマンドのオプションの全リストについては、「<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">kubectl
cordon</link>」を参照してください。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">オプションで</emphasis>、ノードのワークロードに<literal>drain</literal>を実行するユースケースがある場合があります。</para>
<screen language="bash" linenumbering="unnumbered">kubectl drain &lt;node&gt;</screen>
<para><literal>drain</literal>コマンドのオプションの全リストについては、「<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">kubectl
drain</link>」を参照してください。</para>
</listitem>
<listitem>
<para>RKE2インストールスクリプトを使用して、正しい<literal>Edge
3.1.0</literal>と互換性のあるRKE2バージョンをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=v1.30.3+rke2r1 sh -</screen>
</listitem>
<listitem>
<para><literal>rke2</literal>プロセスを再起動します。</para>
<screen language="bash" linenumbering="unnumbered"># For control-plane nodes:
systemctl restart rke2-server

# For worker nodes:
systemctl restart rke2-agent</screen>
</listitem>
<listitem>
<para>ノードのRKE2バージョンがアップグレードされていることを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get nodes</screen>
</listitem>
<listitem>
<para>ノードをschedulableとしてマークします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl uncordon &lt;node_name&gt;</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-migration-mgmt-helm">
<title>Edge Helmチャート</title>
<note>
<para>このセクションでは、システムに<literal>helm</literal>をインストールしており、必要なクラスタを指す有効な<literal>kubeconfig</literal>を持っていることを前提としています。<literal>helm</literal>インストール手順については、『<link
xl:href="https://helm.sh/docs/intro/install">Installing Helm
(Helmのインストール)</link>ガイド』を参照してください。</para>
</note>
<para>このセクションでは、特定のEdgeリリースを構成するHelmチャートコンポーネントに関するガイドラインを提供します。次のトピックについて説明します。</para>
<itemizedlist>
<listitem>
<para>アップグレードプロセスに存在する既知の制限事項(<xref
linkend="day2-migration-mgmt-helm-limitations"/>)。</para>
</listitem>
<listitem>
<para><literal>Rancher Turtles</literal> Helmチャートを通じた(<xref
linkend="day2-migration-mgmt-helm-capi"/>) Cluster APIコントローラの移行方法。</para>
</listitem>
<listitem>
<para>EIB (<xref linkend="components-eib"/>)を通じてデプロイされたEdge Helmチャート(<xref
linkend="day2-migration-mgmt-helm-eib"/>)をアップグレードする方法。</para>
</listitem>
<listitem>
<para>EIB以外の手段を通じてデプロイされたEdge Helm チャート(<xref
linkend="day2-migration-mgmt-helm-non-eib"/>)のアップグレード方法。</para>
</listitem>
</itemizedlist>
<section xml:id="day2-migration-mgmt-helm-limitations">
<title>既知の制限事項</title>
<para>このセクションでは、現在のマイグレーションプロセスに対する既知の制限事項について説明します。ユーザは、helmチャートをアップグレードするために移動する前に、まずここで説明されている手順を実行する必要があります。</para>
<section xml:id="id-rancher-upgrade">
<title>Rancherのアップグレード</title>
<para><literal>Edge
3.1.0</literal>が使用している現在のRKE2バージョンでは、<literal>IngressClass</literal>を含まないIngressがIngressコントローラ
によって無視されるという問題が発生します。この問題を軽減するには、ユーザはデフォルトの<literal>IngressClass</literal>の名前をデフォルトの<literal>Rancher</literal>
Ingressに手動で追加する必要があります。</para>
<para>以下の手順で修正される問題の詳細については、<link
xl:href="https://github.com/rancher/rke2/issues/6510">アップストリームの</link>
RKE2の問題、より具体的には、<link
xl:href="https://github.com/rancher/rke2/issues/6510#issuecomment-2311231917">こちら</link>のコメントを参照してください。</para>
<note>
<para>デフォルトの<literal>IngressClass</literal>の名前が<literal>nginx</literal>とは異なる場合があります。</para>
<para>次のコマンドを実行して、名前を検証してください。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get ingressclass</screen>
</note>
<para><literal>Rancher</literal>をアップグレードする前に、次のコマンドを実行してください。</para>
<itemizedlist>
<listitem>
<para><literal>Rancher</literal>がEIB (<xref
linkend="components-eib"/>)を通じてデプロイされた場合:</para>
<screen language="bash" linenumbering="unnumbered">kubectl patch helmchart rancher -n &lt;namespace&gt; --type='merge' -p '{"spec":{"set":{"ingress.ingressClassName":"nginx"}}}'</screen>
</listitem>
<listitem>
<para><literal>Rancher</literal>がHelmを通じてデプロイされた場合は、<literal>--set
ingress.ingressClassName=nginx</literal>フラグを<link
xl:href="https://helm.sh/docs/helm/helm_upgrade/">upgrade</link>コマンドに追加します。このオプションを利用する方法の完全な例については、次の例(<xref
linkend="day2-migration-mgmt-helm-non-eib-example"/>)を参照してください。</para>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-helm-capi">
<title>Cluster APIコントローラのマイグレーション</title>
<para><literal>Edge
3.1.0</literal>以降、Metal<superscript>3</superscript>管理クラスタ上のCluster API
(CAPI)コントローラは、<link xl:href="https://turtles.docs.rancher.com">Rancher
Turtles</link>を介して管理されます。</para>
<para>CAPIコントローラバージョンを<literal>Edge
3.1.0</literal>と互換性のあるバージョンに移行するには、<literal>Rancher
Turtles</literal>チャートをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install rancher-turtles oci://registry.suse.com/edge/3.1/rancher-turtles-chart --version 0.3.2 --namespace rancher-turtles-system --create-namespace</screen>
<para>しばらくしたら、<literal>capi-system</literal>、
<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>、<literal>rke2-control-plane-system</literal>ネームスペースで実行されているコントローラポッドが<literal>Edge
3.1.0</literal>と互換性のあるコントローラバージョンでアップグレードされます。</para>
<para>エアギャップ環境に<literal>Rancher Turtles</literal>をインストールする方法については、 「Rancher
Turtlesのエアギャップインストール」(<xref
linkend="day2-migration-mgmt-helm-capi-air-gapped"/>)を参照してください。</para>
<section xml:id="day2-migration-mgmt-helm-capi-air-gapped">
<title>Rancher Turtlesのエアギャップインストール</title>
<note>
<para>以下の手順は、アップグレードする<literal>管理</literal>クラスタに接続するように<literal>kubectl</literal>を設定していることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>以下で説明している<literal>rancher-turtles-airgap-resources</literal>
Helmチャートをインストールする前に、作成された<literal>clusterctl</literal>ネームスペースに正しい所有権があることを確認します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>capi-system</literal>所有権の変更:</para>
<screen language="bash" linenumbering="unnumbered">kubectl label namespace capi-system app.kubernetes.io/managed-by=Helm --overwrite

kubectl annotate namespace capi-system meta.helm.sh/release-name=rancher-turtles-airgap-resources --overwrite
kubectl annotate namespace capi-system meta.helm.sh/release-namespace=rancher-turtles-system --overwrite</screen>
</listitem>
<listitem>
<para><literal>capm3-system</literal>所有権の変更:</para>
<screen language="bash" linenumbering="unnumbered">kubectl label namespace capm3-system app.kubernetes.io/managed-by=Helm --overwrite

kubectl annotate namespace capm3-system meta.helm.sh/release-name=rancher-turtles-airgap-resources --overwrite
kubectl annotate namespace capm3-system meta.helm.sh/release-namespace=rancher-turtles-system --overwrite</screen>
</listitem>
<listitem>
<para><literal>rke2-bootstrap-system</literal>所有権の変更:</para>
<screen language="bash" linenumbering="unnumbered">kubectl label namespace rke2-bootstrap-system app.kubernetes.io/managed-by=Helm --overwrite

kubectl annotate namespace rke2-bootstrap-system meta.helm.sh/release-name=rancher-turtles-airgap-resources --overwrite
kubectl annotate namespace rke2-bootstrap-system meta.helm.sh/release-namespace=rancher-turtles-system --overwrite</screen>
</listitem>
<listitem>
<para><literal>rke2-control-plane-system</literal>所有権の変更:</para>
<screen language="bash" linenumbering="unnumbered">kubectl label namespace rke2-control-plane-system app.kubernetes.io/managed-by=Helm --overwrite

kubectl annotate namespace rke2-control-plane-system meta.helm.sh/release-name=rancher-turtles-airgap-resources --overwrite
kubectl annotate namespace rke2-control-plane-system meta.helm.sh/release-namespace=rancher-turtles-system --overwrite</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>rancher-turtles-airgap-resources</literal>と<literal>rancher-turtles</literal>チャートアーカイブを取得します。</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://registry.suse.com/edge/3.1/rancher-turtles-airgap-resources-chart --version 0.3.2
helm pull oci://registry.suse.com/edge/3.1/rancher-turtles-chart --version 0.3.2</screen>
</listitem>
<listitem>
<para><literal>Rancher Turtles</literal>
Helmチャートのエアギャップインストールに必要なリソースを提供するため、<literal>rancher-turtles-airgap-resources</literal>
Helmチャートをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install rancher-turtles-airgap-resources ./rancher-turtles-airgap-resources-chart-0.3.2.tgz --namespace rancher-turtles-system --create-namespace</screen>
</listitem>
<listitem>
<para><literal>Rancher Turtles</literal>
Helmチャートの<literal>cluster-api-operator</literal>を設定し、正しい場所からコントローラデータをフェッチします。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"
EOF</screen>
</listitem>
<listitem>
<para><literal>Rancher Turtles</literal>をインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install rancher-turtles ./rancher-turtles-chart-0.3.2.tgz --namespace rancher-turtles-system --create-namespace --values values.yaml</screen>
</listitem>
</orderedlist>
<para>しばらくすると、<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>、および<literal>rke2-control-plane-system</literal>ネームスペースで実行しているコントローラポッドが<literal>Edge
3.1.0</literal>と互換性のあるバージョンでアップグレードされます。</para>
</section>
</section>
<section xml:id="day2-migration-mgmt-helm-eib">
<title>Edge Helmチャートのアップグレード - EIB</title>
<para>このセクションでは、EIB (<xref
linkend="components-eib"/>)を通じてデプロイされたEdgeコンポーネントスタックから<literal>Edge
3.1.0</literal>と互換性のあるバージョンへのHelmチャートのアップグレード方法について説明します。</para>
<section xml:id="id-prerequisites-12">
<title>前提条件</title>
<para><literal>Edge 3.1</literal>では、EIBはチャートのデプロイ方法を変更し、<link
xl:href="https://docs.rke2.io/helm#automatically-deploying-manifests-and-helm-charts">RKE2</link>/<link
xl:href="https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons">K3s</link>マニフェスト自動デプロイメカニズムを<emphasis
role="strong">「使用していません」</emphasis>。</para>
<para>これは、<literal>Edge
3.1.0</literal>と互換性のあるバージョンにアップグレードする前に、EIBを使用して<literal>Edge
3.0</literal>環境にデプロイされたすべてのHelmチャートは、関連するKubernetesディスとリビューションのマニフェストディレクトリからチャートマニフェストを削除する必要があります。</para>
<warning>
<para>これが実行されない場合、プロセスまたはオペレーティングシステムの再起動時にチャートアップグレードがRKE2/K3sプロセスによって元に戻されます。</para>
</warning>
<note>
<para>RKE2/K3sディレクトリからマニフェストを削除しても、クラスタからリソースが削除されることは<emphasis
role="strong">「ありません」</emphasis>。</para>
<para><link
xl:href="https://docs.rke2.io/helm#automatically-deploying-manifests-and-helm-charts">RKE2</link>/<link
xl:href="https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons">K3s</link>のドキュメンによると:</para>
<blockquote>
<para>「このディレクリのファイルを削除しても、クラスタから対応するリソースは削除されません。」</para>
</blockquote>
</note>
<para>EIBでデプロイされたチャートマニフェストの削除には、次の手順が含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ディザスタリカバリを確保するため、各EIBでデプロイされたマニフェストのバックアップを取ります。</para>
<note>
<para>EIBでデプロイされたマニフェストは、<literal>"edge.suse.com/source:
edge-image-builder"</literal>ラベルが付きます。</para>
</note>
<note>
<para>以下のコマンドに提供する<literal>&lt;backup_location&gt;</literal>が存在すること確認します。</para>
</note>
<screen language="bash" linenumbering="unnumbered">grep -lrIZ 'edge.suse.com/source: edge-image-builder' /var/lib/rancher/rke2/server/manifests | xargs -0 -I{} cp {} &lt;backup_location&gt;</screen>
</listitem>
<listitem>
<para>すべてのEIBでデプロイされたマニフェストを削除します。</para>
<screen language="bash" linenumbering="unnumbered">grep -lrIZ 'edge.suse.com/source: edge-image-builder' /var/lib/rancher/rke2/server/manifests | xargs -0 rm -f --</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-migration-mgmt-helm-upgrade-steps">
<title>アップグレード手順</title>
<note>
<para>以下の手順は、アップグレードする<literal>管理</literal>クラスタに接続するように<literal>kubectl</literal>を設定していることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>リリースノート(<xref linkend="release-notes"/>)を確認して、移行する<literal>Edge
3.1</literal>と互換性のあるチャートバージョンを特定します。</para>
</listitem>
<listitem>
<para>必要なHelmチャートバージョンを<link
xl:href="https://helm.sh/docs/helm/helm_pull/">取得</link> します。</para>
<itemizedlist>
<listitem>
<para>HTTPリポジトリでホストされているチャートの場合:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add &lt;chart_repo_name&gt; &lt;chart_repo_urls&gt;

helm pull &lt;chart_repo_name&gt;/&lt;chart_name&gt; --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>OCIレジストリでホストされているチャートの場合:</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://&lt;chart_oci_url&gt; --version=X.Y.Z</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>取得されたチャートアーカイブをエンコードします。</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 &lt;chart_name&gt;-X.Y.Z.tgz  &gt; &lt;chart_name&gt;-X.Y.Z.txt</screen>
</listitem>
<listitem>
<para>チャートに対して実行する必要がある追加の手順がある場合は、既知の制限事項(<xref
linkend="day2-migration-mgmt-helm-limitations"/>)セクションを確認します。</para>
</listitem>
<listitem>
<para>既存の<literal>HelmChart</literal>リソースにパッチを適用します。</para>
<important>
<para>必ず<literal>HelmChart</literal> <emphasis role="strong">名</emphasis>、
<emphasis role="strong">ネームスペース</emphasis>、<emphasis
role="strong">エンコードされたファイル</emphasis>、および<emphasis
role="strong">バージョン</emphasis>を次のコマンドに渡します。</para>
</important>
<screen language="bash" linenumbering="unnumbered">kubectl patch helmchart &lt;helmchart_name&gt; --type=merge -p "{\"spec\":{\"chartContent\":\"$(cat &lt;helmchart_name&gt;-X.Y.Z.txt)\", \"version\":\"&lt;helmchart_version&gt;\"}}" -n &lt;helmchart_namespace&gt;</screen>
</listitem>
<listitem>
<para>これにより、目的のHelmチャートをアップグレードするPodを作成するジョブをスケジュールするように <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>に指示します。作成されたPodのログを確認するには、次の手順を実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>作成したPodを見つけます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n &lt;namespace&gt;</screen>
</listitem>
<listitem>
<para>Podのログを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n &lt;namespace&gt;</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>エラーのないログを含む<literal>完了した</literal>Podは、目的のHelmチャートのアップグレードが成功したことを示します。</para>
<para>EIBを通じてデプロイされたHelmチャートのアップグレード方法の完全な例については、「例」(<xref
linkend="day2-migration-mgmt-helm-example"/>)のセクションを参照してください。</para>
</section>
<section xml:id="day2-migration-mgmt-helm-example">
<title>例</title>
<para>このセクションでは、<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>
Helmチャートを<literal>Edge
3.1.0</literal>リリースと互換性のあるバージョンにアップグレードする例を示します。「アップグレード手順」(<xref
linkend="day2-migration-mgmt-helm-upgrade-steps"/>)セクションで概説した手順に従います。</para>
<para><emphasis>ユースケース:</emphasis></para>
<itemizedlist>
<listitem>
<para>現在の<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>チャートは、<literal>Edge
3.1.0</literal>と互換性のあるバージョンにアップグレードする必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>Rancher</literal>はEIBを通じてデプロイされ、その<literal>HelmChart</literal>は、<literal>デフォルト</literal>ネームスペースにデプロイされます。</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>はEIBを通じてデプロイされ、その<literal>HelmChart</literal>は<literal>kube-system</literal>ネームスペースにデプロイされます。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para><emphasis>手順:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>リリースノート(<xref
linkend="release-notes"/>)から<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>の目的のバージョンを見つけます。<literal>Edge
3.1.0</literal>の場合、これらのバージョンは、<emphasis>Rancher</emphasis>
の場合は<literal>2.9.1</literal>で、<emphasis>Metal<superscript>3</superscript></emphasis>の場合は<literal>0.8.1</literal>です。</para>
</listitem>
<listitem>
<para>目的のチャートバージョンを取得します。</para>
<itemizedlist>
<listitem>
<para><literal>Rancher</literal>の場合:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm pull rancher-prime/rancher --version=2.9.1</screen>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>の場合:</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://registry.suse.com/edge/3.1/metal3-chart --version=0.8.1</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>
Helmチャートをエンコードします。</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 rancher-2.9.1.tgz &gt; rancher-2.9.1.txt
base64 -w 0 metal3-chart-0.8.1.tgz &gt; metal3-chart-0.8.1.txt</screen>
</listitem>
<listitem>
<para>ディレクトリ構造は次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── metal3-chart-0.8.1.tgz
├── metal3-chart-0.8.1.txt
├── rancher-2.9.1.tgz
└── rancher-2.9.1.txt</screen>
</listitem>
<listitem>
<para>チャートに対して実行する必要がある追加の手順がある場合は、既知の制限事項(<xref
linkend="day2-migration-mgmt-helm-limitations"/>)セクションを確認します。</para>
<itemizedlist>
<listitem>
<para><literal>Rancher</literal>の場合:</para>
<itemizedlist>
<listitem>
<para>「 <literal>既知の制限事項</literal>」セクションで説明されているコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered"># In this example the rancher helmchart is in the 'default' namespace
kubectl patch helmchart rancher -n default --type='merge' -p '{"spec":{"set":{"ingress.ingressClassName":"nginx"}}}'</screen>
</listitem>
<listitem>
<para><literal>ingressClassName</literal>プロパティが正常に追加されたことを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get ingress rancher -n cattle-system -o yaml | grep -w ingressClassName

# Example output
  ingressClassName: nginx</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>
HelmChartリソースにパッチを適用します。</para>
<screen language="bash" linenumbering="unnumbered"># Rancher deployed in the default namespace
kubectl patch helmchart rancher --type=merge -p "{\"spec\":{\"chartContent\":\"$(cat rancher-2.9.1.txt)\", \"version\":\"2.9.1\"}}" -n default

# Metal3 deployed in the kube-system namespace
kubectl patch helmchart metal3 --type=merge -p "{\"spec\":{\"chartContent\":\"$(cat metal3-chart-0.8.1.txt)\", \"version\":\"0.8.1\"}}" -n kube-system</screen>
</listitem>
<listitem>
<para><literal>helm-controller</literal>で作成された<emphasis>Rancher</emphasis>および<emphasis>Metal<superscript>3</superscript></emphasis>
Podを見つけます。</para>
<itemizedlist>
<listitem>
<para><emphasis>Rancher:</emphasis></para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n default

# Example output
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-wg7nf   0/1     Completed   0          5m2s</screen>
</listitem>
<listitem>
<para><emphasis>Metal<superscript>3</superscript>:</emphasis></para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=metal3 -n kube-system

# Example output
NAME                        READY   STATUS      RESTARTS   AGE
helm-install-metal3-57lz5   0/1     Completed   0          4m35s</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/">kubectlログ</link>を使用して各ポッドのログを表示します。</para>
<itemizedlist>
<listitem>
<para><emphasis>Rancher:</emphasis></para>
<screen language="bash" linenumbering="unnumbered">kubectl logs helm-install-rancher-wg7nf -n default

# Example successful output
...
Upgrading rancher
+ helm_v3 upgrade --namespace cattle-system --create-namespace --version 2.9.1 --set-string global.clusterCIDR=10.42.0.0/16 --set-string global.clusterCIDRv4=10.42.0.0/16 --set-string global.clusterDNS=10.43.0.10 --set-string global.clusterDomain=cluster.local --set-string global.rke2DataDir=/var/lib/rancher/rke2 --set-string global.serviceCIDR=10.43.0.0/16 --set-string ingress.ingressClassName=nginx rancher /tmp/rancher.tgz --values /config/values-01_HelmChart.yaml
Release "rancher" has been upgraded. Happy Helming!
...</screen>
</listitem>
<listitem>
<para><emphasis>Metal<superscript>3</superscript>:</emphasis></para>
<screen language="bash" linenumbering="unnumbered">kubectl logs helm-install-metal3-57lz5  -n kube-system

# Example successful output
...
Upgrading metal3
+ echo 'Upgrading metal3'
+ shift 1
+ helm_v3 upgrade --namespace metal3-system --create-namespace --version 0.8.1 --set-string global.clusterCIDR=10.42.0.0/16 --set-string global.clusterCIDRv4=10.42.0.0/16 --set-string global.clusterDNS=10.43.0.10 --set-string global.clusterDomain=cluster.local --set-string global.rke2DataDir=/var/lib/rancher/rke2 --set-string global.serviceCIDR=10.43.0.0/16 metal3 /tmp/metal3.tgz --values /config/values-01_HelmChart.yaml
Release "metal3" has been upgraded. Happy Helming!
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>特定のチャートのポッドが実行されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered"># For Rancher
kubectl get pods -n cattle-system

# For Metal3
kubectl get pods -n metal3-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-helm-non-eib">
<title>Edge Helmチャートのアップグレード - EIB以外</title>
<para>このセクションでは、Helmを介してデプロイされたEdgeコンポーネントスタックから、<literal>Edge
3.1.0</literal>と互換性のあるバージョンにHelmチャートをアップグレードする方法について説明します。</para>
<note>
<para>以下の手順は、アップグレードする<literal>管理</literal>クラスタに接続するように<literal>kubectl</literal>を設定していることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>リリースノート(<xref linkend="release-notes"/>)を参照して、移行する<literal>Edge
3.1.0</literal>と互換性のあるチャートバージョンを見つけます。</para>
</listitem>
<listitem>
<para>現在実行中のHelmチャートのカスタム値を取得します。</para>
<screen language="bash" linenumbering="unnumbered">helm get values &lt;chart_name&gt; -n &lt;chart_namespace&gt; -o yaml &gt; &lt;chart_name&gt;-values.yaml</screen>
</listitem>
<listitem>
<para>追加の手順やチャートに対して実行する必要がある変更がある場合は、「既知の制限事項」(<xref
linkend="day2-migration-mgmt-helm-limitations"/>)セクションを確認します。</para>
</listitem>
<listitem>
<para>Helmチャートを目的のバージョンに<link
xl:href="https://helm.sh/docs/helm/helm_upgrade/">アップグレード</link>します。</para>
<itemizedlist>
<listitem>
<para>非エアギャップセットアップの場合:</para>
<screen language="bash" linenumbering="unnumbered"># For charts hosted in HTTP repositories
helm upgrade &lt;chart_name&gt; &lt;chart_repo&gt;/&lt;chart_name&gt; --version &lt;version&gt; --values &lt;chart_name&gt;-values.yaml -n &lt;chart_namespace&gt;

# For charts hosted in OCI registries
helm upgrade &lt;chart_name&gt; oci://&lt;oci_registry_url&gt;/&lt;chart_name&gt; --namespace &lt;chart_namespace&gt; --values &lt;chart_name&gt;-values.yaml --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>エアギャップセットアップの場合:</para>
<itemizedlist>
<listitem>
<para>インターネットにアクセスできるマシンで、目的のチャートバージョンを取得します。</para>
<screen language="bash" linenumbering="unnumbered"># For charts hosted in HTTP repositories
helm pull &lt;chart_repo_name&gt;/&lt;chart_name&gt; --version=X.Y.Z

# For charts hosted in OCI registries
helm pull oci://&lt;chart_oci_url&gt; --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>チャートアーカイブを<literal>管理</literal>クラスタに転送します。</para>
<screen language="bash" linenumbering="unnumbered">scp &lt;chart&gt;.tgz &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para>チャートをアップグレードします。</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade &lt;chart_name&gt; &lt;chart&gt;.tgz --values &lt;chart_name&gt;-values.yaml -n &lt;chart_namespace&gt;</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>チャートポッドが実行されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n &lt;chart_namespace&gt;</screen>
</listitem>
</orderedlist>
<para>チャートに固有のリソースを確認して、アップグレードの追加検証を行うこともできます。これを実行した後で、アップグレードは成功したとみなされます。</para>
<para>完全な例については、「例」(<xref
linkend="day2-migration-mgmt-helm-non-eib-example"/>)セクションを参照してください。</para>
<section xml:id="day2-migration-mgmt-helm-non-eib-example">
<title>例</title>
<para>このセクションでは、<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>
Helmチャートを<literal>Edge 3.1.0</literal>リリースと互換性のあるバージョンにアップグレードする例を示します。「Edge
Helmチャートのアップグレード - EIB以外」(<xref
linkend="day2-migration-mgmt-helm-non-eib"/>)セクションで概説されている手順い従います。</para>
<para><emphasis>ユースケース:</emphasis></para>
<itemizedlist>
<listitem>
<para>現在の<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>チャートは、<literal>Edge
3.1.0</literal>と互換性のあるバージョンにアップグレードする必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>Rancher</literal> helmチャートは
<literal>cattle-system</literal>ネームスペースの<link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher
Prime</link>リポジトリからデプロイされます。<literal>Rancher
Prime</literal>リポジトリは次の方法で追加されました。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime</screen>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>は<literal>metal3-system</literal>ネームスペースの<literal>registry.suse.com</literal>
OCIレジストリからデプロイされます。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para><emphasis>手順:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>リリースノート(<xref
linkend="release-notes"/>)から目的のバージョンの<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>を見つけます。
<literal>Edge 3.1.0</literal>の場合、これらのバージョンは、Rancherの場合は
<literal>2.9.1</literal>、
Metal<superscript>3</superscript>の場合は<literal>0.8.1</literal>になります。</para>
</listitem>
<listitem>
<para>現在実行中の<literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>
helmチャートのカスタム値を取得します。</para>
<screen language="bash" linenumbering="unnumbered"># For Rancher
helm get values rancher -n cattle-system -o yaml &gt; rancher-values.yaml

# For Metal3
helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml</screen>
</listitem>
<listitem>
<para>チャートに対して実行する必要がある追加の手順がある場合は、既知の制限事項(<xref
linkend="day2-migration-mgmt-helm-limitations"/>)セクションを確認します。</para>
<itemizedlist>
<listitem>
<para><literal>Rancher</literal>の場合、<literal>--set
ingress.ingressClassName=nginx</literal>オプションをアップグレードコマンドに追加する必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Rancher</literal>および<literal>Metal<superscript>3</superscript></literal>
helmチャートをアップグレードします。</para>
<screen language="bash" linenumbering="unnumbered"># For Rancher
helm upgrade rancher rancher-prime/rancher --version 2.9.1 --set ingress.ingressClassName=nginx --values rancher-values.yaml -n cattle-system

# For Metal3
helm upgrade metal3 oci://registry.suse.com/edge/3.1/metal3-chart --version 0.8.1 --values metal3-values.yaml -n metal3-system</screen>
</listitem>
<listitem>
<para><literal>Rancher</literal>およびMetal<superscript>3</superscript>ポッドが実行されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered"># For Rancher
kubectl get pods -n cattle-system

# For Metal3
kubectl get pods -n metal3-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
<section xml:id="day2-migration-downstream">
<title>ダウンストリームクラスタ</title>
<para>このセクションでは、<literal>Edge 3.0.X</literal>ダウンストリームクラスタを<literal>Edge
3.1.0</literal>に移行する方法について説明します。</para>
<section xml:id="day2-migration-downstream-prerequisites">
<title>前提条件</title>
<para>このセクションでは、マイグレーションプロセスを開始する前に、ユーザが実行する必要がある前提条件となる手順について説明します。</para>
<section xml:id="id-charts-deployed-through-eib">
<title>EIBを通じてデプロイされたチャート</title>
<para><literal>Edge 3.1</literal>では、EIB (<xref
linkend="components-eib"/>)はチャートのデプロイ方法を変更し、 <link
xl:href="https://docs.rke2.io/helm#automatically-deploying-manifests-and-helm-charts">RKE2</link>/<link
xl:href="https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons">K3s</link>マニフェスト自動デプロイメカニズムは<emphasis
role="strong">「使用されなくなりました」</emphasis>。</para>
<para>これは、<literal>Edge 3.1.0</literal>と互換性のあるバージョンに移行する前に、EIBを使用して<literal>Edge
3.0</literal>環境にデプロイされたすべてのHelmチャートは、関連する
Kubernetesディストリビューションのマニフェストディレクトリからチャートマニフェストを削除する必要があります。</para>
<warning>
<para>これが実行されない場合、プロセスまたはオペレーティングシステムの再起動時にチャートアップグレードがRKE2/K3sプロセスによって元に戻されます。</para>
</warning>
<para>ダウンストリームクラスタでは、EIBで作成されたチャートマニフェストファイルの削除は、<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリにある<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/main/fleets/day2/system-upgrade-controller-plans/eib-charts-migration-prep">eib-charts-migration-prep</link>と呼ばれるFleetによって処理されます。</para>
<warning>
<para><literal>メイン</literal>ブランチからの <literal>eib-charts-migration-prep</literal>
Fleetファイルの使用は、推奨されて<emphasis
role="strong">「いません」</emphasis>。Fleetファイルは、<emphasis
role="strong">常に</emphasis>有効なEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用する必要があります。</para>
</warning>
<important>
<para>このプロセスには、System Upgrade Controller
(SUC)がすでにデプロイされている必要があります。インストールの詳細については、「System Upgrade
Controllerのインストール」(<xref
linkend="components-system-upgrade-controller-install"/>)を参照してください。</para>
</important>
<para>作成されると、<literal>eib-charts-migration-prep</literal> Fleetは以下を実行するスクリプトを含むSUC
(<xref linkend="components-system-upgrade-controller"/>) Planを配布します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>実行中の現在のノードが <literal>initializer</literal>ノードであるか判断します。そうでない場合、何も実行しません。</para>
</listitem>
<listitem>
<para>ノードが<literal>initializer</literal>の場合、次のようになります。</para>
<itemizedlist>
<listitem>
<para>EIBによってデプロイされたすべての <literal>HelmChart</literal>リソースを検出します。</para>
</listitem>
<listitem>
<para>上記の<literal>HelmChart</literal>リソースのそれぞれのマニフェストファイルを見つけます。</para>
<note>
<para><literal>HelmChart</literal>マニフェストファイルは、RKE2の場合は<literal>/var/lib/rancher/rke2/server/manifests</literal>、K3sの場合は<literal>/var/lib/rancher/k3s/server/manifests</literal>下の
<literal>initializer</literal>ノードにのみあります。</para>
</note>
</listitem>
<listitem>
<para>ディザスタリカバリを確保するため、<literal>/tmp</literal>の下にある各マニフェストのバックアップを作成します。</para>
<note>
<para>バックアップの場所は、FleetのSUC PLANの<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.0/fleets/day2/system-upgrade-controller-plans/eib-charts-migration-prep/plan.yaml#L36"><literal>MANIFEST_BACKUP_DIR</literal></link>環境変数を定義して変更できます。</para>
</note>
</listitem>
<listitem>
<para>EIBによってデプロイされた <literal>HelmChart</literal>リソースに関連する各マニフェストファイルを削除します。</para>
<note>
<para>RKE2/K3sディレクトリからマニフェストを削除しても、クラスタからリソースが削除されることは<emphasis
role="strong">「ありません」</emphasis>。</para>
<para><link
xl:href="https://docs.rke2.io/helm#automatically-deploying-manifests-and-helm-charts">RKE2</link>/<link
xl:href="https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons">K3s</link>のドキュメンによると:</para>
<blockquote>
<para>「このディレクリのファイルを削除しても、クラスタから対応するリソースは削除されません。」</para>
</blockquote>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para>ユースケースに応じて、<literal>eib-charts-migration-prep</literal>
Fleetを次の2つの方法でデプロイできます。</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソースを通じて
- 外部/ローカルGitサーバが利用可能なユースケース向け。詳細については、「EIBチャートマニフェストの削除のFleetデプロイメント -
GitRepo」(<xref
linkend="day2-migration-downstream-prerequisites-fleet-gitrepo"/>)を参照してください。</para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースを通じて -
ローカルGitサーバオプションをサポートしないエアギャップユースケース向け。詳細については、「EIBチャートマニフェストの削除のFleetデプロイメント
- バンドル」 (<xref
linkend="day2-migration-downstream-prerequisites-fleet-bundle"/>)を参照してください。</para>
</listitem>
</itemizedlist>
<section xml:id="day2-migration-downstream-prerequisites-fleet-gitrepo">
<title>EIBチャートマニフェストの削除のFleetデプロイメント - GitRepo</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>管理</literal>クラスタに、次の<literal>GitRepo</literal>リソースをデプロイします。</para>
<note>
<para>以下のリソースをデプロイする前に、有効な <literal>ターゲット</literal>設定を提供する<emphasis
role="strong">「必要があります」</emphasis>。Fleetがリソースをデプロイするダウンストリームクラスタを認識できるようにするためです。ダウンストリームクラスタへのマッピング方法については、
<link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters (ダウンストリームクラスタへのマッピング</link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: eib-chart-migration-prep
spec:
  revision: release-3.1.0
  paths:
  - fleets/day2/system-upgrade-controller-plans/eib-charts-migration-prep
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
<para>または、利用可能な場合はRancherのUIからリソースを作成することもできます。詳細については、「<link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI (Rancher UIでのFleetへのアクセス)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>管理</literal>クラスタに上記の
<literal>GitRepo</literal>を作成することにより、Fleetは<literal>GitRepo</literal>で指定された<literal>ターゲット</literal>に一致する各ダウンストリームクラスタに<literal>SUC
Plan</literal>
(<literal>eib-chart-migration-prep</literal>と呼ばれる)をデプロイします。このプランのライフサイクルを監視するには、「System
Upgrade Controller Plansのモニタリング」 (<xref
linkend="components-system-upgrade-controller-monitor-plans"/>)を参照してください。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-migration-downstream-prerequisites-fleet-bundle">
<title>EIBチャートマニフェストの削除のFleetデプロイメント - バンドル</title>
<para>このセクションでは、<literal>eib-chart-migration-prep</literal>
Fleetをローカルgitサーバを利用できないエアギャップ環境で使用可能な<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースに変換する方法について説明します。</para>
<para><emphasis>手順:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>ネットワークにアクセスできるマシンで、<emphasis role="strong">fleet-cli</emphasis>をダウンロードします。</para>
<note>
<para>ダウンロードする<emphasis
role="strong">fleet-cli</emphasis>のバージョンが、クラスタにデプロイしたFleetのバージョンに一致していることを確認します。</para>
</note>
<itemizedlist>
<listitem>
<para>Macユーザの場合、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</listitem>
<listitem>
<para>Linuxユーザの場合、バイナリが各Fleet<link
xl:href="https://github.com/rancher/fleet/releases">リリース</link>への <emphasis
role="strong">アセット</emphasis>として用意されています。</para>
<itemizedlist>
<listitem>
<para>目的のバイナリを取得します。</para>
<itemizedlist>
<listitem>
<para>Linux AMD:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/&lt;FLEET_VERSION&gt;/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/&lt;FLEET_VERSION&gt;/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>バイナリを<literal>/usr/local/bin</literal>に移動します。</para>
<screen language="bash" linenumbering="unnumbered">sudo mkdir -p /usr/local/bin
sudo mv ./fleet-cli /usr/local/bin/fleet-cli
sudo chmod 755 /usr/local/bin/fleet-cli</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>eib-chart-migration-prep</literal>フリートを使用する<emphasis
role="strong">suse-edge/fleet-examples</emphasis> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のクローンを作成します。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.1.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para><emphasis
role="strong">fleet-examples</emphasis>リポジトリにある、<literal>eib-chart-migration-prep</literal>フリートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller-plans/eib-charts-migration-prep</screen>
</listitem>
<listitem>
<para>フリートをデプロイするすべてのダウンストリームクラスタを指す<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>ダウンストリームクラスタのマッピング方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
<listitem>
<para>バンドルの構築に進みます。</para>
<note>
<para><literal>fleet-examples/fleets/day2/system-upgrade-controller-plans/eib-charts-migration-prep</literal>ディレクトリの<emphasis
role="strong">fleet-cli</emphasis>をダウンロード<emphasis
role="strong">「していない」</emphasis>ことを確認してください。これをダウンロードすると、バンドルにパッケージ化され、これは推奨されません。</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-chart-migration-prep . &gt; eib-chart-migration-prep-bundle.yaml</screen>
<para>このプロセスの詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートをバンドルに変換する)</link>」を参照してください。</para>
<para><literal>fleet-cli apply</literal>コマンドの詳細については、「<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>」を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタマシンに <emphasis
role="strong">eib-chart-migration-prep-bundle.yaml</emphasis>バンドルを転送します。</para>
<screen language="bash" linenumbering="unnumbered">scp eib-chart-migration-prep-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタに、<emphasis
role="strong">eib-chart-migration-prep-bundle.yaml</emphasis>バンドルをデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f eib-chart-migration-prep-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">管理</emphasis>クラスタで、<emphasis
role="strong">バンドル</emphasis>がデプロイされていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundle eib-chart-migration-prep -n fleet-default
NAME                       BUNDLEDEPLOYMENTS-READY   STATUS
eib-chart-migration-prep   1/1</screen>
</listitem>
<listitem>
<para><literal>管理</literal>クラスタに上記の<literal>バンドル</literal>を作成することで、Fleetは、<literal>targets.yaml</literal>ファイルで指定された<literal>ターゲット</literal>に一致する<literal>SUC
Plan</literal>
(<literal>eib-chart-migration-prep</literal>と呼ばれる)を各ダウンストリームクラスタ上にデプロイします。このプランのファイルサイクルを監視するには、「System
Upgrade Controller Planのモニタリング」(<xref
linkend="components-system-upgrade-controller-monitor-plans"/>)を参照してください。</para>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="id-migration-steps-2">
<title>マイグレーション手順</title>
<para>前提条件(<xref
linkend="day2-migration-downstream-prerequisites"/>)の手順を実行した後で、<literal>Edge
3.1.0</literal>リリース用のダウンストリームクラスタ(<xref
linkend="day2-downstream-clusters"/>)アップグレードドキュメントに従って作業を続行できます。</para>
</section>
</section>
</chapter>
<chapter xml:id="day2-mgmt-cluster">
<title>管理クラスタ</title>
<para>このセクションでは、あるEdgeプラットフォームバージョンから別のバージョンに
<literal>管理</literal>クラスタをアップグレードする方法に関連する さまざまな<literal>Day
2</literal>操作を実行する方法について説明します。</para>
<para><literal>Day 2</literal>操作は、Upgrade Controller (<xref
linkend="components-upgrade-controller"/>)によって自動化され、以下のものが含まれます。</para>
<itemizedlist>
<listitem>
<para>SL Micro (<xref linkend="components-slmicro"/>) OSアップグレード</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)/K3s (<xref
linkend="components-k3s"/>)アップグレード</para>
</listitem>
<listitem>
<para>SUSEの追加のコンポーネント(Rancher、Neuvectorなど)のアップグレード</para>
</listitem>
</itemizedlist>
<section xml:id="id-prerequisites-13">
<title>前提条件</title>
<para><literal>管理</literal>クラスタをアップグレードする前に、次の前提条件が満たされている必要があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>SCC登録ノード</literal> - クラスタのノードのOSが、アップグレースするEdgeリリース(<xref
linkend="release-notes"/>)で指定されているOSバージョンをサポートするサブスクリプションキーで登録されていることを確認してください。</para>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal> - <literal>Upgrade
Controller</literal>が<literal>管理</literal>クラスタにデプロイされていることを確認します。インストール手順については、「Upgrade
Controllerのインストール」 (<xref
linkend="components-upgrade-controller-installation"/>)を参照してください。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade">
<title>アップグレード</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>管理</literal>クラスタをアップグレードするEdgeリリース(<xref
linkend="release-notes"/>)バージョンを決定します。</para>
</listitem>
<listitem>
<para><literal>管理</literal>クラスタに、目的の<literal>リリースバージョン</literal>を指定する<literal>UpgradePlan</literal>をデプロイします。<literal>UpgradePlan</literal>は<literal>Upgrade
Controller</literal>のネームスペースにデプロイされる必要があります。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n &lt;upgrade_controller_namespace&gt; -f - &lt;&lt;EOF
apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt-3-1-X
spec:
  # Version retrieved from release notes
  releaseVersion: 3.1.X
EOF</screen>
<note>
<para><literal>UpgradePlan</literal>に追加の設定を行うユースケースがある場合があります。すべての考えられる設定については、「UpgradePlan」
(<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>)セクションを参照してください。</para>
</note>
</listitem>
<listitem>
<para><literal>Upgrade
Controller</literal>のネームスペースに<literal>UpgradePlan</literal>をデプロイすると、<literal>アップグレードプロセス</literal>が開始されます。</para>
<note>
<para>実際の<literal>アップグレードプロセス</literal>の詳細については、「Upgrade Controllerの仕組み」(<xref
linkend="components-upgrade-controller-how"/>)を参照してください。</para>
<para><literal>アップグレードプロセス</literal>の追跡方法については、 「アップグレードプロセスの追跡」(<xref
linkend="components-upgrade-controller-how-track"/>)を参照してください。</para>
</note>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="day2-downstream-clusters">
<title>ダウンストリームクラスタ</title>
<para>このセクションでは、<literal>管理クラスタ</literal>を使用して、ダウンストリームクラスタのさまざまな部分に対して、各種の<literal>Day
2</literal>操作を行う方法について説明します。</para>
<section xml:id="id-introduction">
<title>はじめに</title>
<para>このセクションは、<literal>Day 2</literal>操作に関するドキュメントの<emphasis
role="strong">「出発点」</emphasis>となることを目的としています。次の情報を確認できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>複数のダウンストリームクラスタで<literal>Day 2</literal>操作を実行するために使用するデフォルトコンポーネント(<xref
linkend="day2-downstream-components"/>)。</para>
</listitem>
<listitem>
<para>自身の特定のユースケース(<xref linkend="day2-determine-use-case"/>)にどの<literal>Day
2</literal>リソースを使用するかの判断。</para>
</listitem>
<listitem>
<para><literal>Day 2</literal>操作に推奨されるワークフローシーケンス(<xref
linkend="day2-upgrade-workflow"/>)。</para>
</listitem>
</orderedlist>
<section xml:id="day2-downstream-components">
<title>コンポーネント</title>
<para>以下に、<literal>Day 2</literal>操作を正常に実行できるように、 <literal>管理クラスタ</literal>または
<literal>ダウンストリームクラスタ</literal>のいずれかでセットアップする必要があるデフォルトコンポーネントについて説明します。</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<note>
<para>RancherなしでFleet (<xref
linkend="components-fleet"/>)を利用するユースケースの場合、Rancherコンポーネントを完全にスキップできます。</para>
</note>
<para><literal>ダウンストリームクラスタ</literal>の管理を担当します。<literal>管理クラスタ</literal>上にデプロイする必要があります。</para>
<para>詳細については、<xref linkend="components-rancher"/>を参照してください。</para>
</section>
<section xml:id="id-fleet">
<title>Fleet</title>
<para>マルチクラスタリソースのデプロイメントを担当します。</para>
<para>通常は、<literal>Rancher</literal>コンポーネントによって提供されます。
<literal>Rancher</literal>を使用しないユースケースでは、スタンドアロンコンポーネントとしてデプロイできます。</para>
<para>Fleetをスタンドアロンコンポーネントとしてインストールする方法の詳細については、Fleetの<link
xl:href="https://fleet.rancher.io/installation">インストールの詳細</link>を参照してください。</para>
<para>Fleetコンポーネントに関する詳細については、<xref linkend="components-fleet"/>を参照してください。</para>
<important>
<para>このドキュメントは、GitOps方式で<literal>Day
2</literal>操作に関連するリソースのデプロイメントを自動化するために、<literal>Fleet</literal>、具体的には<literal>GitRepo</literal>と<literal>Bundle</literal>リソース(この詳細は<xref
linkend="day2-determine-use-case"/>で詳しく説明します)に大きく依存しています。</para>
<para>サードパーティのGitOpsツールの使用が必要なユースケースの場合は、以下を参照してください。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>OSアップグレード</literal>の場合 - <xref
linkend="os-upgrade-suc-plan-deployment-third-party"/></para>
</listitem>
<listitem>
<para><literal>Kubernetesディストリビューションの更新</literal>の場合 - <xref
linkend="k8s-upgrade-suc-plan-deployment-third-party"/></para>
</listitem>
<listitem>
<para><literal>EIBでデプロイされたHelmチャートのアップグレード</literal>の場合 - <xref
linkend="day2-helm-upgrade-eib-chart-third-party"/></para>
</listitem>
<listitem>
<para><literal>EIB以外でデプロイされたHelmチャートのアップグレード</literal>の場合 - <xref
linkend="release-notes"/>ページから目的のEdgeリリースによってサポートされているチャートバージョンを取得し、サードパーティのGitOpsツールにチャートバージョンとURLを入力します</para>
</listitem>
</orderedlist>
</important>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System Upgrade Controller (SUC)</title>
<para><emphasis role="strong">System Upgrade Controller (SUC)</emphasis>は、
<literal>Plan</literal>と呼ばれるカスタムリソースを通じて提供される設定データに基づいて指定されたノードでタスクを実行する責任を負います。</para>
<note>
<para><emphasis role="strong">SUC</emphasis>で異なる<emphasis role="strong">Day
2</emphasis>操作をサポートできるようにするには、SUCがアップグレードを必要とする各<emphasis
role="strong">ダウンストリーム</emphasis>クラスタにデプロイされることが重要です。</para>
</note>
<para><emphasis
role="strong">SUC</emphasis>コンポーネントとそれがEdgeスタックにどのように適合するかに関する詳細については、System
Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>)コンポーネントのドキュメントを参照してください。</para>
<para>ダウンストリームクラスタに<emphasis
role="strong">SUC</emphasis>をデプロイする方法については、まず、ユースケース(<xref
linkend="day2-determine-use-case"/>)を決定してから、「System Upgrade
Controllerのインストール - GitRepo」(<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>)または「System
Upgrade Controllerのインストール - バンドル」(<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>)を参照してください。</para>
</section>
</section>
<section xml:id="day2-determine-use-case">
<title>ユースケースの決定</title>
<para>前述のように、<literal>Day
2</literal>操作に関連するリソースは、Fleetの<literal>GitRepo</literal>リソースと<literal>Bundle</literal>リソースを使用してダウンストリームクラスタに伝播されます。</para>
<para>以下に、これらのリソースの機能と、 <literal>Day 2</literal>操作に使用する必要があるユースケースに関する詳細を説明します。</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para><literal>GitRepo</literal>は、<literal>Fleet</literal>が<literal>バンドル</literal>の作成元として使用できるGitリポジトリを表すFleet
(<xref
linkend="components-fleet"/>)リソースです。各<literal>バンドル</literal>は、<literal>GitRepo</literal>リソースの内部で定義された設定パスに基づいて作成されます。詳細については、<link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>のドキュメントを参照してください。</para>
<para><literal>Day
2</literal>操作に関して、<literal>GitRepo</literal>リソースは通常、<emphasis>Fleet
GitOps</emphasis> アプローチを利用する<emphasis
role="strong">非エアギャップ</emphasis>環境への<literal>SUC</literal>または<literal>SUC
Plan</literal>のデプロイに使用されます。</para>
<para>または、リポジトリのセットアップを<emphasis
role="strong">ローカルGitサーバ経由でミラーリングすると</emphasis>、<literal>GitRepo</literal>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
<section xml:id="id-bundle">
<title>バンドル</title>
<para><literal>バンドル</literal>は、ターゲットクラスタにデプロイする<emphasis role="strong">
「生」</emphasis>のKubernetesリソースを保持します。バンドルは通常、<literal>GitRepo</literal>リソースから作成されますが、手動でデプロイできるユースケースもあります。詳細については、<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>のドキュメントを参照してください。</para>
<para><literal>Day 2</literal>操作の観点では、<literal>バンドル</literal>リソースは通常、
何らかの形態の<emphasis>ローカルGitOps</emphasis>手法を使用しない<emphasis
role="strong">エアギャップ</emphasis>環境(<emphasis
role="strong">「ローカルGitサーバ」</emphasis>など
)で<literal>SUC</literal>または<literal>SUC Plan</literal>をデプロイするために使用されます。</para>
<para>または、ご自身のユースケースで<emphasis>GitOps</emphasis>ワークフローを使用できない場合は(Gitリポジトリを使用する場合など)、<emphasis
role="strong">バンドル</emphasis>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">非エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
</section>
<section xml:id="day2-upgrade-workflow">
<title>Day 2ワークフロー</title>
<para>以下に、ダウンストリームクラスタを特定のEdgeリリースにアップグレードする際に従う必要がある<literal>Day
2</literal>ワークフローを示します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>OSアップグレード(<xref linkend="day2-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Kubernetesバージョンアップグレード(<xref linkend="day2-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helmチャートのアップグレード(<xref linkend="day2-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-os-upgrade">
<title>OSのアップグレード</title>
<section xml:id="id-components">
<title>コンポーネント</title>
<para>このセクションでは、 <literal>OSアップグレード</literal>プロセスがデフォルトの<literal>Day
2</literal>コンポーネント(<xref
linkend="day2-downstream-components"/>)よりも使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="id-systemd-service">
<title>systemd.service</title>
<para>あるEdgeバージョンから別のEdgeバージョンにOSが必要とするアップグレードに応じて、異なる<link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>が作成されます。</para>
<itemizedlist>
<listitem>
<para>同じ OS バージョン(例:
<literal>6.0</literal>)を必要とするEdgeバージョンの場合、<literal>os-pkg-update.service</literal>が作成されます。これは<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>コマンドを使用して、<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常のパッケージアップグレード</link>を実行します。</para>
</listitem>
<listitem>
<para>OSバージョンのマイグレーションが必要なEdgeバージョンの場合(例<literal>5.5</literal> →
<literal>6.0</literal>)、<literal>os-migration.service</literal>が作成されます。これは<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>を使用して以下を実行します。</para>
<itemizedlist>
<listitem>
<para>まず、<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常のパッケージアップグレード</link>を実行します。マイグレーション前にすべてのパッケージが最新バージョンであることを確認するために行われます。古いパッケージ
バージョンに関連する障害を軽減します。</para>
</listitem>
<listitem>
<para>その後、 <literal>zypper migration</literal>コマンドを利用して、OSマイグレーションプロセスを続行します。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>OSアップグレードが必要が各 <emphasis
role="strong">ダウンストリームクラスタ</emphasis>に配置されている<emphasis role="strong">SUC
Plan</emphasis>通じて配布されます。</para>
</section>
</section>
<section xml:id="id-requirements">
<title>要件</title>
<para><emphasis>全般:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC登録マシン</emphasis> -
すべてのダウンストリームクラスタを<literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>に登録する必要があります。これは、<literal>os-pkg-update.service/os-migration.service</literal>が必要なOS
RPMリポジトリに正常に接続できるようにするために必要です。</para>
<important>
<para>新しいOSバージョン(例: Edge
3.1)を必要とするEdgeリリースの場合、SCCキーが新しいバージョンへのマイグレーションをサポートしていることを確認します(例: Edge
3.1の場合は、SCCキーがSLE Micro <literal>5.5</literal> →
<literal>6.0</literal>へのマイグレーションをサポートしている必要があります)。</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">control-plane</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの
<literal>.spec.tolerations</literal>セクションに追加する必要があります。OSアップグレードに関連する<emphasis
role="strong">SUC
Plan</emphasis>は、<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>の下の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。<emphasis
role="strong">有効なリポジトリ <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリースrelease</link>タグからのPlanを使用してください。</emphasis></para>
<para><emphasis role="strong">control-plane</emphasis> SUC
Planに対してカスタムのTolerationを定義する例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>エアギャップ:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SUSE RPMリポジトリのミラーリング</emphasis> - OS
RPMリポジトリをローカルにミラーリングし、<literal>os-pkg-update.service/os-migration.service</literal>そのリポジトリにアクセスできるようにする必要があります。このためには、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>または<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>のいずれかを使用します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-update-procedure">
<title>更新手順</title>
<note>
<para>このセクションは、Fleet (<xref
linkend="components-fleet"/>)を使用して<literal>OSアップグレード</literal> <emphasis
role="strong">SUC Plan</emphasis> をデプロイすることを前提としています。異なる方法で<emphasis
role="strong">SUC Plan</emphasis>をデプロイする場合は、 <xref
linkend="os-upgrade-suc-plan-deployment-third-party"/>を参照してください。</para>
</note>
<important>
<para>この手順を使用して以前にアップグレードした環境の場合、ユーザは次の手順の<emphasis
role="strong">いずれか</emphasis>を完了していることを確認する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>ダウンストリームクラスタから古いEdgeリリースバージョンに関連する以前にデプロイしたSUC Planを削除する</literal>
-
これは、既存の<literal>GitRepo/Bundle</literal>ターゲット設定から目的の<emphasis>ダウンストリーム</emphasis>クラスタを削除するか、<literal>GitRepo/バンドル</literal>リソースを完全に削除することで、実行できます。</para>
</listitem>
<listitem>
<para><literal>既存のGitRepo/バンドルリソースを再利用する</literal> -
目的の<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>の正しいフリートを保持する新しいタグにリソースのリビジョンをポイントすることで実行できます。</para>
</listitem>
</itemizedlist>
<para>これは古いEdgeリリースバージョンの<literal>SUC Plan</literal>間のクラッシュを回避するために実行されます。</para>
<para>ユーザがアップグレードを試す場合、<emphasis>ダウンストリーム</emphasis>クラスタに既存の<literal>SUC
Plan</literal>がある場合は、次のフリートエラーが表示されます。</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para><literal>OSアップグレード手順</literal>は、 <emphasis role="strong">SUC
Plan</emphasis>をダウンストリームクラスタにデプロイする手順が中心になります。その後、これらのPlanには、<literal>os-pkg-update.service/os-migration.service</literal>をどのような方法でどのノードにデプロイするかに関する情報が保持されます。<emphasis
role="strong">SUC Plan</emphasis>の構造の詳細については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリーム</link>ドキュメントを参照してください。</para>
<para><literal>OS アップグレード</literal>SUC Planは次の方法で配布されます。</para>
<itemizedlist>
<listitem>
<para><literal>GitRepo</literal>リソースを通じて - <xref
linkend="os-upgrade-suc-plan-deployment-git-repo"/></para>
</listitem>
<listitem>
<para><literal>バンドル</literal>リソースを通じて - <xref
linkend="os-upgrade-suc-plan-deployment-bundle"/></para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref linkend="day2-determine-use-case"/>を参照してください。</para>
<para><emphasis>アップグレード手順</emphasis>の完全な概要については、<xref
linkend="os-update-overview"/>セクションを参照してください。</para>
<section xml:id="os-update-overview">
<title>概要</title>
<para>このセクションは、<emphasis
role="strong"><emphasis>OSアップグレードプロセス</emphasis></emphasis>が最初から最後まで通過する完全なワークフローを説明することを目的としています。</para>
<figure>
<title>OSアップグレードのワークフロー</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_os_pkg_update_diagram.png" width=""/>
</imageobject>
<textobject><phrase>day2 osパッケージ更新の図</phrase></textobject>
</mediaobject></figure>
<para>OSアップグレード手順:</para>
<orderedlist numeration="arabic">
<listitem>
<para>ユースケースに基づいて、ユーザは目的のダウンストリームクラスタへの<literal>OSアップグレードSUC
Plan</literal>のデプロイメントに対して、<emphasis
role="strong">GitRepo</emphasis>リソースまたは<emphasis
role="strong">バンドル</emphasis>リソースのどちらを使用するかどうかを決定します。<emphasis
role="strong">GitRepo/バンドル</emphasis>を特定のダウンストリームクラスタのセットにマッピングする方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">SUC Plan</emphasis>のデプロイメントに<emphasis
role="strong">GitRepo</emphasis>リソースまたは<emphasis
role="strong">バンドル</emphasis>リソースのどちらを使用すべきかわからない場合は、<xref
linkend="day2-determine-use-case"/>を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo/バンドル</emphasis>設定オプションについては、<xref
linkend="os-upgrade-suc-plan-deployment-git-repo"/>または<xref
linkend="os-upgrade-suc-plan-deployment-bundle"/>を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ユーザは設定済みの<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースを自身の<literal>管理クラスタ</literal>の<literal>fleet-default</literal>ネームスペースにデプロイします。これは、<emphasis
role="strong">手動</emphasis>または使用可能な場合は<emphasis role="strong">Rancher
UI</emphasis>から実行できます。</para>
</listitem>
<listitem>
<para>Fleet (<xref
linkend="components-fleet"/>)は、<literal>fleet-default</literal>ネームスペースを絶えず監視し、新しくデプロイされた<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースをすぐに検出します。Fleetが監視するネームスペースの詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/namespaces">Namespaces
(ネームスペース)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para>ユーザが<emphasis
role="strong">GitRepo</emphasis>リソースをデプロイした場合、<literal>Fleet</literal>は、<emphasis
role="strong">GitRepo</emphasis>を調整し、その<emphasis
role="strong">パス</emphasis>と<emphasis
role="strong">fleet.yaml</emphasis>の設定に基づいて、<emphasis
role="strong">バンドル</emphasis>リソースを<literal>fleet-default</literal>ネームスペースにデプロイします。詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリの内容)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><literal>Fleet</literal>は、続いて<literal>Kubernetesリソース</literal>をこの<emphasis
role="strong">バンドル</emphasis>から、ターゲットとするすべての
<literal>ダウンストリームクラスタ</literal>にデプロイします。<literal>OSアップグレード</literal>のコンテキストでは、Fleetは、次のリソースを<emphasis
role="strong">バンドル</emphasis>からデプロイします。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">ワーカーSUC Plan</emphasis> - クラスタの<emphasis
role="strong"><emphasis>ワーカー</emphasis></emphasis>ノードでOSアップグレードを行う方法について<emphasis
role="strong">SUC</emphasis>に指示します。
クラスタが<emphasis>コントロールプレーン</emphasis>ノードのみで構成されている場合、これは解釈 <emphasis
role="strong">「されません」</emphasis>。すべてのコントロールプレーン<emphasis
role="strong">SUC</emphasis> Planが正常に完了した後で実行されます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コントロールプレーンSUC Plan</emphasis> - クラスタ<emphasis
role="strong"><emphasis>コントロールプレーン</emphasis></emphasis>ノードでOSアップグレードを実行する方法について<emphasis
role="strong">SUC</emphasis>に指示します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">スクリプトシークレット</emphasis> - 各 <emphasis
role="strong">SUC
Plan</emphasis>で参照されます。実際のOSアップグレードを実行する<literal>os-pkg-update.service/os-migration.service</literal>の作成を担当する<literal>upgrade.sh</literal>スクリプトが配布されます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">スクリプトデータConfigMap</emphasis> - 各<emphasis
role="strong">SUC
Plan</emphasis>で参照されます。<literal>upgrade.sh</literal>スクリプトで使用される設定が配布されます。</para>
<note>
<para>上記のリソースは、各ダウンストリームクラスタの<literal>cattle-system</literal>ネームスペースにデプロイされます。</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ダウンストリームクラスタで、<emphasis role="strong">SUC</emphasis>は、新しくデプロイされた<emphasis
role="strong">SUC Plan</emphasis>を検出し、<emphasis role="strong">SUC
Plan</emphasis>で定義された<emphasis
role="strong">ノードセレクタ</emphasis>と一致する各ノードに<emphasis
role="strong"><emphasis>更新Pod</emphasis></emphasis>をデプロイします。<emphasis
role="strong">SUC Plan Pod</emphasis>を監視する方法については、 <xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">更新Pod</emphasis> (各ノードにデプロイ)は、スクリプトシークレットを<emphasis
role="strong">マウント</emphasis>し、シークレットによって配布される<literal>upgrade.sh</literal>スクリプトを<emphasis
role="strong">実行</emphasis>します。</para>
</listitem>
<listitem>
<para><literal>upgrade.sh</literal>は次の処理を続行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>その設定に基づいて、OSがパッケージ更新を必要としているか、移行する必要があるかどうかを決定します。</para>
</listitem>
<listitem>
<para>上記の結果に基づいて、<literal>os-pkg-update.service</literal> (パッケージ更新用)、または
<literal>os-migration.service</literal> (マイグレーション用)のいずれかを作成します。サービスは
<emphasis role="strong">oneshot</emphasis>タイプであり、次のワークフローを採用します。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para><literal>os-pkg-update.service</literal>の場合:</para>
<orderedlist numeration="upperalpha">
<listitem>
<para><literal>transactional-update cleanup
up</literal>を実行して、ノードOS上のすべてのパッケージバージョンを更新します。</para>
</listitem>
<listitem>
<para><literal>transactional-update</literal>が成功したら、パッケージバージョンの更新が有効になるように、システムの<emphasis
role="strong">再起動</emphasis>をスケジュールします。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>os-migration.service</literal>の場合:</para>
<orderedlist numeration="upperalpha">
<listitem>
<para><literal>transactional-update cleanup
up</literal>を実行して、ノードOS上のすべてのパッケージバージョンを更新します。これは、
古いパッケージバージョンによってOSマイグレーションエラーが発生しないようにするために実行されます。</para>
</listitem>
<listitem>
<para>OSを目的の値に移行します。マイグレーションは<literal>zypper migration</literal>コマンドを使用して実行されます。</para>
</listitem>
<listitem>
<para>マイグレーションが有効になるように、システムの<emphasis role="strong">再起動</emphasis>をスケジュールします。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>os-pkg-update.service/os-migration.service</literal>を開始し、それが完了するまで待ちます。</para>
</listitem>
<listitem>
<para><emphasis
role="strong"><emphasis>systemd.service</emphasis></emphasis>がそのジョブを実行した後で、<literal>os-pkg-update.service/os-migration.service</literal>をクリーンアップします。今後、誤って実行/再起動されることがないように、システムから削除されます。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>OSアップグレード手順は、<emphasis
role="strong"><emphasis>システムの再起動</emphasis></emphasis>で終了します。再起動後、OSパッケージバージョンはアップグレードされ、Edgeリリースが必要な場合は、OSも同様に移行される場合があります。</para>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment">
<title>OSアップグレード - SUC Planのデプロイメント</title>
<para>このセクションでは、Fleetの <emphasis role="strong">GitRepo</emphasis>および <emphasis
role="strong">バンドル</emphasis>リソースを使用した <emphasis role="strong">SUC
Plan</emphasis>関連のOSアップグレードのデプロイメントをオーケストレーションする方法について説明します。</para>
<section xml:id="os-upgrade-suc-plan-deployment-git-repo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>OSアップグレード</literal> <emphasis role="strong">SUC
Plan</emphasis>を配布する、<emphasis
role="strong">GitRepo</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を通じて - <xref
linkend="os-upgrade-suc-plan-deployment-git-repo-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>(<xref
linkend="os-upgrade-suc-plan-deployment-git-repo-manual"/>)リソースを<literal>管理クラスタ</literal>に手動でデプロイする。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>のドキュメントを参照してください。</para>
<section xml:id="os-upgrade-suc-plan-deployment-git-repo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<para>Rancher UIを通じて<literal>GitRepo</literal>リソースを作成するには、公式の<link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">ドキュメント</link>に従ってください。</para>
<para>Edgeチームは、ユーザがGitRepoリソースの<literal>パス</literal>として追加できるすぐに使用できる<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.1.1/fleets/day2/system-upgrade-controller-plans/os-upgrade">フリート</link>を維持しています。</para>
<important>
<para>常に、このフリートは有効なEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>フリートが配布する<literal>SUC
plan</literal>にカスタム許容値を含める必要がないユースケースでは、ユーザは<literal>suse-edge/fleet-examples</literal>リポジトリから
<literal>os-upgrade</literal>フリートを直接参照できます。</para>
<para>カスタム許容値が必要な場合、ユーザは別のリポジトリから<literal>os-upgrade</literal>フリートを参照して、必要に応じてSUC
Planに許容値を追加できます。</para>
<para><literal>GitRepo</literal>を<literal>suse-edge/fleet-examples</literal>リポジトリのフリートを使用するように設定する方法の例については、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.1/gitrepos/day2/os-upgrade-gitrepo.yaml">こちら</link>を参照してください。</para>
</section>
<section xml:id="os-upgrade-suc-plan-deployment-git-repo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>設定を編集し、<literal>spec.targets</literal>で目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはどのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタ変更に一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.1.1  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="os-upgrade-suc-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な <literal>OSアップグレード</literal> <emphasis role="strong">SUC
Plan</emphasis>を配布する、 <emphasis
role="strong">バンドル</emphasis>リソースは次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を通じて - <xref
linkend="os-upgrade-suc-plan-deployment-bundle-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>(<xref
linkend="os-upgrade-suc-plan-deployment-bundle-manual"/>)リソースを<literal>管理クラスタ</literal>に手動でデプロイする。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>のドキュメントを参照してください。</para>
<section xml:id="os-upgrade-suc-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<para>Edgeチームは、以下の手順で使用可能なすぐに使用できる<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.1/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">バンドル</link>を維持しています。</para>
<important>
<para>このバンドルは常に有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>RancherのUIを通じてバンドルを作成するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、［<emphasis role="strong">☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<note>
<para>バンドルが配布する <literal>SUC
plan</literal>にカスタム許容値を含める必要があるユースケースがある場合があります。以下の手順で生成されるバンドルには、必ずこれらの許容値を含めるようにします。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">バンドルコンテンツ</link>を<literal>suse-edge/fleet-examples</literal>から<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページに手動でコピーする。</para>
</listitem>
<listitem>
<para>目的の <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>のクローンを作成し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。ここからバンドルの場所(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)に移動して、バンドルファイルを選択できます。これにより、バンドルコンテンツを持つ<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページが自動入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="os-upgrade-suc-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-default</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="os-upgrade-suc-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザがOSアップグレード<emphasis role="strong">SUC
Plan</emphasis>を独自のサードパーティGitOpsワークフロー(例:
<literal>Flux</literal>)に組み込むユースケースがある場合があります。</para>
<para>必要なOSアップグレードリソースを取得するには、まず使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを決定します。</para>
<para>その後、リソースは<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>で見つかります。ここでは次のようになります。</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> - <emphasis
role="strong">コントロールプレーン</emphasis>ノードの<literal>system-upgrade-controller</literal>
Planリソース。</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> - <emphasis
role="strong">ワーカー</emphasis>ノードの<literal>system-upgrade-controller</literal>
Planリソース。</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> -
<literal>upgrade.sh</literal>スクリプトを配布するシークレット。</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> -
<literal>upgrade.sh</literal>スクリプトによって使用されるアップグレード設定を提供するConfigMap。</para>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、<literal>system-upgrade-controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。<literal>system-upgrade-controller</literal>をデプロイする方法については、<xref
linkend="components-system-upgrade-controller-install"/>を参照してください。</para>
</important>
<para>GitOpsワークフローをOSアップグレードの <emphasis role="strong">SUC
Plan</emphasis>をデプロイするために使用する方法をよりよく理解するために、<literal>Fleet</literal>を使用して更新手順の概要(<xref
linkend="os-update-overview"/>)を確認すると役立つ場合があります。</para>
</section>
</section>
</section>
<section xml:id="day2-k8s-upgrade">
<title>Kubernetesバージョンアップグレード</title>
<important>
<para>このセクションでは、Rancher (<xref
linkend="components-rancher"/>)インスタンスを使用して作成されて<emphasis
role="strong">「いない」</emphasis>ダウンストリームクラスタのアップグレードについて説明します。<literal>Rancher</literal>で作成したクラスタのKubernetesバージョンをアップグレードする方法については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes (Kubernetesのアップグレードとロールバック)</link>」を参照してください。</para>
</important>
<section xml:id="id-components-2">
<title>コンポーネント</title>
<para>このセクションでは、<literal>Kubernetesアップグレード</literal>プロセスがデフォルトの<literal>Day
2</literal>コンポーネント(<xref
linkend="day2-downstream-components"/>)よりも優先して使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="id-rke2-upgrade">
<title>rke2-upgrade</title>
<para>特定ノードのRKE2バージョンのアップグレードを行うイメージです。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、RKE2のアップグレードが必要な各<emphasis
role="strong">ダウンストリームクラスタ</emphasis>に配置する必要があります。</para>
<para><literal>rke2-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
<section xml:id="id-k3s-upgrade">
<title>k3s-upgrade</title>
<para>特定のノードのK3sバージョンをアップグレードするイメージです。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、K3sのアップグレードが必要な各<emphasis
role="strong">ダウンストリームクラスタ</emphasis>に配置する必要があります。</para>
<para><literal>k3s-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/k3s-io/k3s-upgrade">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
</section>
<section xml:id="id-requirements-2">
<title>要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Kubernetesディストリビューションをバックアップします。</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">インポートしたRKE2クラスタ</emphasis>については、<link
xl:href="https://docs.rke2.io/backup_restore">RKE2のバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">インポートしたK3sクラスタ</emphasis>については、<link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3sのバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">control-plane</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。Kubernetesバージョンアップグレードに関する<emphasis
role="strong">SUC Plan</emphasis>は、次の場所の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">必ず、有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para>RKE2 <emphasis role="strong">control-plane</emphasis> SUC
PlanのカスタムTolerationの定義例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade-procedure">
<title>アップグレード手順</title>
<note>
<para>このセクションでは、<emphasis role="strong">SUC Plan</emphasis>をFleet (<xref
linkend="components-fleet"/>)を使用してデプロイすることを想定しています。別の方法で<emphasis
role="strong">SUC Plan</emphasis>をデプロイする場合は、<xref
linkend="k8s-upgrade-suc-plan-deployment-third-party"/>を参照してください。</para>
</note>
<important>
<para>この手順を使用して以前にアップグレードした環境の場合、ユーザは次の手順の<emphasis
role="strong">いずれか</emphasis>を完了していることを確認する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>ダウンストリームクラスタから古いEdgeリリースバージョンに関連する以前にデプロイしたSUC Planを削除する</literal>
-
これは、既存の<literal>GitRepo/Bundle</literal>ターゲット設定から目的の<emphasis>ダウンストリーム</emphasis>クラスタを削除するか、<literal>GitRepo/バンドル</literal>リソースを完全に削除することで、実行できます。</para>
</listitem>
<listitem>
<para><literal>既存のGitRepo/バンドルリソースを再利用する</literal> -
目的の<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>の正しいフリートを保持する新しいタグにリソースのリビジョンをポイントすることで実行できます。</para>
</listitem>
</itemizedlist>
<para>これは古いEdgeリリースバージョンの<literal>SUC Plan</literal>間のクラッシュを回避するために実行されます。</para>
<para>ユーザがアップグレードを試す場合、<emphasis>ダウンストリーム</emphasis>クラスタに既存の<literal>SUC
Plan</literal>がある場合は、次のフリートエラーが表示されます。</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para><literal>Kubernetesバージョンアップグレード手順</literal>は、<emphasis role="strong">SUC
Plan</emphasis>をダウンストリームクラスタにデプロイする手順が中心になります。その後、これらのPlanには、<emphasis
role="strong">SUC</emphasis>に対し、<literal>rke2/k3s-upgrade</literal>イメージを実行するPodをどのノード上に作成するかを指示する情報が保持されます。<emphasis
role="strong">SUC Plan</emphasis>の構造については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリーム</link>ドキュメントを参照してください。</para>
<para><literal>Kubernetesアップグレード</literal>Planは次のように配布されます。</para>
<itemizedlist>
<listitem>
<para><literal>GitRepo</literal>リソースを使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo"/></para>
</listitem>
<listitem>
<para><literal>バンドル</literal>リソースを使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle"/></para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref linkend="day2-determine-use-case"/>を参照してください。</para>
<para><emphasis>更新手順</emphasis>中の処理の概要については、<xref
linkend="k8s-version-upgrade-overview"/>のセクションを参照してください。</para>
<section xml:id="k8s-version-upgrade-overview">
<title>概要</title>
<para>このセクションでは、 <emphasis
role="strong"><emphasis>Kubernetesバージョンのアップグレードプロセス</emphasis></emphasis>が最初から最後まで通過する完全なワークフローを説明することを目的としています。</para>
<figure>
<title>Kubernetesバージョンアップグレードのワークフロー</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_k8s_version_upgrade_diagram.png"
width=""/> </imageobject>
<textobject><phrase>day2 k8sバージョンアップグレードの図</phrase></textobject>
</mediaobject></figure>
<para>Kubernetesバージョンアップグレードの手順:</para>
<orderedlist numeration="arabic">
<listitem>
<para>ユーザは、<literal>KubernetesアップグレードSUC
Plan</literal>を目的のダウンストリームクラスタにデプロイするために、<emphasis
role="strong">GitRepo</emphasis>リソースを使用するか、それとも<emphasis
role="strong">バンドル</emphasis>リソースを使用するかをそのユースケースに基づいて判断します。<emphasis
role="strong">GitRepo/バンドル</emphasis>を特定のダウンストリームクラスタセットにマップする方法の詳細については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">SUC Plan</emphasis>のデプロイメントに<emphasis
role="strong">GitRepo</emphasis>リソースまたは<emphasis
role="strong">バンドル</emphasis>リソースのどちらを使用すべきかわからない場合は、<xref
linkend="day2-determine-use-case"/>を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo/バンドル</emphasis>の設定オプションについては、<xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo"/>または<xref
linkend="k8s-upgrade-suc-plan-deployment-bundle"/>を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ユーザは設定済みの<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースを自身の<literal>管理クラスタ</literal>の<literal>fleet-default</literal>ネームスペースにデプロイします。これは、<emphasis
role="strong">手動</emphasis>または使用可能な場合は<emphasis role="strong">Rancher
UI</emphasis>から実行できます。</para>
</listitem>
<listitem>
<para>Fleet (<xref
linkend="components-fleet"/>)は、<literal>fleet-default</literal>ネームスペースを絶えず監視し、新しくデプロイされた<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースをすぐに検出します。Fleetが監視するネームスペースの詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/namespaces">Namespaces
(ネームスペース)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para>ユーザが<emphasis
role="strong">GitRepo</emphasis>リソースをデプロイした場合、<literal>Fleet</literal>は、<emphasis
role="strong">GitRepo</emphasis>を調整し、その<emphasis
role="strong">パス</emphasis>と<emphasis
role="strong">fleet.yaml</emphasis>の設定に基づいて、<emphasis
role="strong">バンドル</emphasis>リソースを<literal>fleet-default</literal>ネームスペースにデプロイします。詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリの内容)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><literal>Fleet</literal>は続いて、<literal>Kubernetesリソース</literal>をこの<emphasis
role="strong">バンドル</emphasis>から、ターゲットとするすべての<literal>ダウンストリームクラスタ</literal>にデプロイします。<literal>Kubernetesバージョンのアップグレード</literal>のコンテキストでは、Fleetは、次のリソースを<emphasis
role="strong">バンドル</emphasis> からデプロイします(Kubernetesディストリビューションによって異なります)。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>rke2-upgrade-worker</literal>/<literal>k3s-upgrade-worker</literal>
- クラスタ <emphasis
role="strong"><emphasis>ワーカー</emphasis></emphasis>ノードでKubernetesをアップグレードする方法を<emphasis
role="strong">SUC</emphasis>に指示します。クラスタが
<emphasis>コントロールプレーン</emphasis>ノードからのみ構成されている場合は解釈<emphasis
role="strong">「されません」</emphasis>。</para>
</listitem>
<listitem>
<para><literal>rke2-upgrade-control-plane</literal>/<literal>k3s-upgrade-control-plane</literal>
- クラスタの<emphasis
role="strong"><emphasis>コントロールプレーン</emphasis></emphasis>ノードでKubernetesをアップグレードする方法を<emphasis
role="strong">SUC</emphasis>に指示します。</para>
<note>
<para>上記の<emphasis role="strong">SUC
Plan</emphasis>は、各ダウンストリームクラスタの<literal>cattle-system</literal>ネームスペースにデプロイされます。</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ダウンストリームクラスタで、<emphasis role="strong">SUC</emphasis>は、新しくデプロイされた<emphasis
role="strong">SUC Plan</emphasis>を検出し、<emphasis role="strong">SUC
Plan</emphasis>で定義された<emphasis
role="strong">ノードセレクタ</emphasis>と一致する各ノードに<emphasis
role="strong"><emphasis>更新Pod</emphasis></emphasis>をデプロイします。<emphasis
role="strong">SUC Plan Pod</emphasis>を監視する方法については、 <xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
</listitem>
<listitem>
<para>デプロイした<emphasis role="strong">SUC Plan</emphasis>に応じて、<emphasis
role="strong">更新Pod</emphasis>は、<link
xl:href="https://hub.docker.com/r/rancher/rke2-upgrade/tags">rke2-upgrade</link>イメージまたは<link
xl:href="https://hub.docker.com/r/rancher/k3s-upgrade/tags">k3s-upgrade</link>イメージのいずれかを実行して、<emphasis
role="strong">それぞれの</emphasis>クラスタノードで次のワークフローを実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">Cordon</link>クラスタノード
-
ノードのアップグレード時にこのノードにPodが誤ってスケジュールされないようにするために、このノードを<literal>unschedulable</literal>とマークします。</para>
</listitem>
<listitem>
<para>ノードOSにインストールされている<literal>rke2/k3s</literal>バイナリを、Podが現在実行している<literal>rke2-upgrade/k3s-upgrade</literal>イメージによって配布されるバイナリに置き換えます。</para>
</listitem>
<listitem>
<para>ノードOSで実行されている<literal>rke2/k3s</literal>プロセスを強制終了します -
これにより、新しいバージョンを使用して<literal>rke2/k3s</literal>プロセスを自動的に再起動するように<emphasis
role="strong">スーパーバイザ</emphasis>に指示します。</para>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/">Uncordon</link>クラスタノード
-
Kubernetesディストリビューションの正常なアップグレード後、ノードはもう一度<literal>schedulable</literal>とマークされます。</para>
<note>
<para><literal>rke2-upgrade</literal>イメージおよび<literal>k3s-upgrade</literal>イメージの機能の詳細については、<link
xl:href="https://github.com/rancher/rke2-upgrade">rke2-upgrade</link>および<link
xl:href="https://github.com/k3s-io/k3s-upgrade">k3s-upgrade</link>のアップストリームプロジェクトを参照してください。</para>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>上記の手順を実行すると、各クラスタノードのKubernetesバージョンが目的のEdge互換<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>にアップグレードされているはずです。</para>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment">
<title>Kubernetesバージョンアップグレード - SUC Planのデプロイメント</title>
<para>このセクションでは、Fleetの <emphasis role="strong">GitRepo</emphasis>および<emphasis
role="strong">バンドル</emphasis>リソースを使用して <emphasis role="strong">SUC
Plan</emphasis>関連のKubernetesアップグレードの デプロイメントを調整する方法について説明します。</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>Kubernetesアップグレード</literal><emphasis role="strong">SUC
Plan</emphasis>を配布する<emphasis
role="strong">GitRepo</emphasis>リソースは、次のいずれかの方法でデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>
ドキュメントを参照してください。</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<para>Rancher UIを通じて<literal>GitRepo</literal>リソースを作成するには、公式の<link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">ドキュメント</link>に従ってください。</para>
<para>Edgeチームは、ユーザがGitRepoリソースの<literal>パス</literal>として追加できる<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.1.1/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.1.1/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>
Kubernetesディストリビューションのすぐに使用できるフリートを維持しています。</para>
<important>
<para>常に有効なEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからこのフリートを使用します。</para>
</important>
<para>これらのフリートが配布する<literal>SUC
Plan</literal>にカスタム許容値を含める必要がないユースケースの場合、ユーザは<literal>suse-edge/fleet-examples</literal>リポジトリからフリートを直接参照できます。</para>
<para>カスタム許容値が必要な場合、ユーザは別のリポジトリからフリートを参照して、必要に応じてSUC Planに許容値を追加できるようにする必要があります。</para>
<para><literal>suse-edge/fleet-examples</literal>リポジトリからフリートを使用した<literal>GitRepo</literal>リソースの設定例は、次のとおりです。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.1/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.1/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>設定を編集し、<literal>spec.targets</literal>で目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはどのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタ変更に一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   release-3.1.1   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   release-3.1.1   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>Kubernetesアップグレード</literal><emphasis role="strong">SUC
Plan</emphasis>を配布する<emphasis
role="strong">バンドル</emphasis>リソースは、次のいずれかの方法でデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="k8s-upgrade-suc-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>
ドキュメントを参照してください。</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<para>Edgeチームは、以下の手順で使用可能な<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.1/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1.1/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>
Kubernetesディストリビューションのすぐに使用できるバンドルを維持しています。</para>
<important>
<para>このバンドルは常に有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>RancherのUIを通じてバンドルを作成するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、［<emphasis role="strong">☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<note>
<para>バンドルが配布する <literal>SUC
plan</literal>にカスタム許容値を含める必要があるユースケースがある場合があります。以下の手順で生成されるバンドルには、必ずこれらの許容値を含めるようにします。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>suse-edge/fleet-examples</literal>から<emphasis role="strong">［Create
from YAML (YAMLから作成)］</emphasis>ページに<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>または<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>のバンドルコンテンツを手動でコピーする。</para>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのクローンを作成し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。そこから、必要なバンドルに移動します(RKE2の場合は<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>、K3sの場合は<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>)。<emphasis
role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページにバンドルコンテンツが自動的に入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">作成します</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.1.1/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザがKubernetesアップグレードリソースを独自のサードパーティGitOpsワークフロー(<literal>Flux</literal>など)に統合したいユースケースが存在する場合があります。</para>
<para>必要なアップグレードリソースを取得するには、まず、使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを特定します。</para>
<para>その後、次の場所でリソースを確認できます。</para>
<itemizedlist>
<listitem>
<para>RKE2クラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>K3sクラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、<literal>system-upgrade-controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。<literal>system-upgrade-controller</literal>をデプロイする方法については、<xref
linkend="components-system-upgrade-controller-install"/>を参照してください。</para>
</important>
<para>GitOpsワークフローを使用してKubernetesバージョンアップグレード用の<emphasis role="strong">SUC
Plan</emphasis>をデプロイする方法について理解を深めるには、<literal>Fleet</literal>を使用した更新手順の概要(<xref
linkend="k8s-version-upgrade-overview"/>)を確認すると役に立ちます。</para>
</section>
</section>
</section>
<section xml:id="day2-helm-upgrade">
<title>Helmチャートのアップグレード</title>
<note>
<para>以下の各セクションでは、<literal>Fleet</literal>の機能を使用してHelmチャートの更新を実現する方法を中心に説明します。</para>
<para>サードパーティのGitOpsツールの使用が必要なユースケースの場合は、以下を参照してください。</para>
<itemizedlist>
<listitem>
<para><literal>EIBでデプロイされたHelmチャートのアップグレード</literal>の場合 - <xref
linkend="day2-helm-upgrade-eib-chart-third-party"/>。</para>
</listitem>
<listitem>
<para><literal>EIB以外でデプロイされたHelmチャートのアップグレード</literal>の場合 - リリースノート(<xref
linkend="release-notes"/>)ページから目的のEdgeリリースによってサポートされているチャートバージョンを取得し、サードパーティのGitOpsツールにチャートバージョンとURLを入力します。</para>
</listitem>
</itemizedlist>
</note>
<section xml:id="id-components-3">
<title>コンポーネント</title>
<para>この操作には、デフォルトの<literal>Day 2</literal>コンポーネント(<xref
linkend="day2-downstream-components"/>)以外のカスタムコンポーネントは不要です。</para>
</section>
<section xml:id="id-preparation-for-air-gapped-environments">
<title>エアギャップ環境の準備</title>
<section xml:id="id-ensure-that-you-have-access-to-your-helm-chart-upgrade-fleet">
<title>HelmチャートのアップグレードFleetにアクセスできることの確認</title>
<para>環境でサポートする内容によって、次のオプションのいずれかを選択できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>管理クラスタ</literal>からアクセス可能なローカルGitサーバでチャートのFleetリソースをホストします。</para>
</listitem>
<listitem>
<para>FleetのCLIを使用して直接使用可能で、どこかにホストする必要のない<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">バンドルにHelmチャートを変換</link>します。FleetのCLIは、<link
xl:href="https://github.com/rancher/fleet/releases">リリース</link>ページから取得できます。Macユーザの場合は、
<link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Edgeリリースバージョンに必要なアセットの検索</title>
<orderedlist numeration="arabic">
<listitem>
<para>Day 2<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のページに移動し、チャートのアップグレード先のEdge
3.X.Yリリースを見つけ、<emphasis role="strong">［Assets (アセット)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Assets (アセット)］</emphasis>セクションから、次のファイルをダウンロードします。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">リリースファイル</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">説明</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para><literal>edge-release-images.txt</literal>ファイルで指定されたイメージを取得し、それらを「.tar.gz」アーカイブにパッケージ化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するOCIチャートイメージを取得し、それらを「.tar.gz」アーカイブにパッケージ化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>「.tar.gz」アーカイブからイメージをロードし、再タグ付けして、プライベートレジストリにプッシュします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Edge OCI「.tgz」チャートパッケージを含むディレクトリを取得し、それらをプライベートレジストリにロードします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するOCIチャートイメージのリストを含みます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するイメージのリストを含みます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive">
<title>Edgeリリースイメージアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>イメージアーカイブを生成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>これにより、<literal>edge-images.tar.gz</literal>という名前のすぐにロードできるアーカイブが作成されます。</para>
<note>
<para><literal>-i|--images</literal>オプションが指定される場合、アーカイブの名前は異なる場合があります。</para>
</note>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします。</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive">
<title>Edge OCIチャートイメージアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>OCIチャートイメージアーカイブを生成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>これにより、 <literal>oci-artefacts.tar.gz</literal>という名前のアーカイブが作成されます。</para>
<note>
<para><literal>-a|--archive</literal>オプションが指定される場合、アーカイブの名前は異なる場合があります。</para>
</note>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします。</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine">
<title>Edgeリリースイメージをエアギャップマシンにロード</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>以前に<emphasis role="strong">コピーした</emphasis>
<literal>edge-images.tar.gz</literal>アーカイブを渡して、スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>これにより、<literal>edge-images.tar.gz</literal>からすべてのイメージがロードされ、再タグ付けされて、それらを<literal>--registry</literal>オプションで指定されているレジストリにプッシュします。</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine">
<title>Edge OCIチャートイメージのエアギャップマシンへのロード</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>コピーした<literal>oci-artefacts.tar.gz</literal>アーカイブをuntarします。</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>命名テンプレート<literal>edge-release-oci-tgz-&lt;date&gt;</literal>を含むディレクトリが生成されます。</para>
</listitem>
<listitem>
<para>このディレクトリを<literal>edge-load-oci-artefacts.sh</literal>スクリプトに渡し、Edge
OCIチャートイメージをプライベートレジストリにロードします。</para>
<note>
<para>このスクリプトは、<literal>Helm</literal>
CLIが環境にプリインストールされていることを想定しています。Helmのインストール手順については、「<link
xl:href="https://helm.sh/docs/intro/install/">Helmのインストール</link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-registry-mirrors-pointing-to-your-private-registry-for-your-kubernetes-distribution">
<title>Kubernetesディストリビューションのプライベートレジストリを指すレジストリミラーの作成</title>
<para>RKE2の場合は、「<link
xl:href="https://docs.rke2.io/install/containerd_registry_configuration">Containerd
Registry Configuration (Containerdレジストリの設定)</link>」を参照してください。</para>
<para>K3sの場合は、「<link
xl:href="https://docs.k3s.io/installation/registry-mirror">埋め込みレジストリミラー</link>」を参照してください。</para>
</section>
</section>
<section xml:id="id-upgrade-procedure-2">
<title>アップグレード手順</title>
<para>このセクションでは、Helmアップグレード手順の次のユースケースを中心に説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>新しいクラスタがあり、SUSE Helmチャートをデプロイして管理したい(<xref
linkend="day2-helm-upgrade-new-cluster"/>)</para>
</listitem>
<listitem>
<para>Fleetで管理されているHelmチャートをアップグレードしたい(<xref
linkend="day2-helm-upgrade-fleet-managed-chart"/>)</para>
</listitem>
<listitem>
<para>EIBでデプロイされたHelmチャートをアップグレードしたい(<xref
linkend="day2-helm-upgrade-eib-chart"/>)</para>
</listitem>
</orderedlist>
<important>
<para>手動でデプロイしたHelmチャートを確実にアップグレードすることはできません。<xref
linkend="day2-helm-upgrade-new-cluster"/>で説明する方法を使用してHelmチャートを再デプロイすることをお勧めします。</para>
</important>
<section xml:id="day2-helm-upgrade-new-cluster">
<title>新しいクラスタがあり、SUSE Helmチャートをデプロイして管理したい場合</title>
<para>Fleetを使用してHelmチャートのライフサイクルを管理したいユーザが対象です。</para>
<para>このセクションでは、以下の実行方法について説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Fleetリソースの準備(<xref linkend="day2-helm-upgrade-new-cluster-prepare-fleet"/>)。</para>
</listitem>
<listitem>
<para>Fleetリソースのデプロイ(<xref
linkend="day2-helm-upgrade-new-cluster-deploy-fleet"/>)。</para>
</listitem>
<listitem>
<para>デプロイされたHelmチャートの管理(<xref
linkend="day2-helm-upgrade-new-cluster-manage-chart"/>)。</para>
</listitem>
</orderedlist>
<section xml:id="day2-helm-upgrade-new-cluster-prepare-fleet">
<title>Fleetリソースの準備</title>
<orderedlist numeration="arabic">
<listitem>
<para>チャートのFleetリソースを、使用するEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから取得します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>選択したEdgeリリースタグのリビジョンから、HelmチャートのFleet
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)に移動します。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">GitOpsワークフローを使用する場合は</emphasis>、チャートFleetディレクトリをGitOpsを実行するGitリポジトリにコピーします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis> Helmチャートで<emphasis
role="strong">値</emphasis>を設定する必要がある場合は、コピーしたディレクトリの<literal>fleet.yaml</literal>ファイルに含まれる設定<literal>.helm.values</literal>を編集します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis>
環境に合わせて、チャートのFleetにリソースを追加しなければならないユースケースがあります。Fleetディレクトリを拡張する方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリのコンテンツ)</link>」を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para><literal>longhorn</literal> Helmチャートの<emphasis
role="strong">例</emphasis>は次のようになります。</para>
<itemizedlist>
<listitem>
<para>ユーザGitリポジトリ構造:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>ユーザの<literal>longhorn</literal>データが入力された<literal>fleet.yaml</literal> の内容:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "104.2.0+up1.7.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>これらは値の例であり、<literal>longhorn</literal>チャートのカスタム設定を示すために使用しているだけです。<literal>longhorn</literal>チャートのデプロイメントのガイドラインと<emphasis
role="strong">「みなさない」</emphasis>でください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="day2-helm-upgrade-new-cluster-deploy-fleet">
<title>Fleetのデプロイ</title>
<para>環境がGitOpsワークフローの操作をサポートしている場合は、GitRepo (<xref
linkend="day2-helm-upgrade-new-cluster-deploy-fleet-gitrepo"/>)またはバンドル(<xref
linkend="day2-helm-upgrade-new-cluster-deploy-fleet-bundle"/>)のいずれかを使用してチャートFleetをデプロイできます。</para>
<note>
<para>Fleetをデプロイする場合に、<literal>Modified</literal>メッセージを取得した場合、Fleetの<literal>diff</literal>セクションに対応する<literal>comparePatches</literal>エントリを追加してください。詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore
Modified GitRepos (変更されたGitReposを無視する差分を生成する)</link> 」を参照してください。</para>
</note>
<section xml:id="day2-helm-upgrade-new-cluster-deploy-fleet-gitrepo">
<title>GitRepo</title>
<para>Fleetの<link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソースは、チャートのFleetリソースへのアクセス方法、それらのリソースを適用するのに必要なクラスタに関する情報を保持しています。</para>
<para><literal>GitRepo</literal>リソースは、 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UI</link>を通じて、または手動で<literal>管理クラスタ</literal>にリソースを<link
xl:href="https://fleet.rancher.io/tut-deployment">デプロイ</link>することで、デプロイできます。</para>
<para><emphasis><emphasis role="strong">手動</emphasis>デプロイメントの<emphasis
role="strong">Longhorn</emphasis>
<literal>GitRepo</literal>リソースの例</emphasis></para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: &lt;user_repository_tag&gt;
  #
  # If using a branch
  # branch: &lt;user_repository_branch&gt;
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: &lt;user_repository_url&gt;
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="day2-helm-upgrade-new-cluster-deploy-fleet-bundle">
<title>バンドル</title>
<para><link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースは、Fleetによってデプロイされる必要がある生のKubernetesリソースを保持しています。通常、<literal>GitRepo</literal>アプローチを使用することを推奨されますが、
環境がエアギャップされ、ローカルGitサーバをサポートできないユースケース用に、<literal>バンドル</literal>がHelmチャートFleetをターゲットクラスタに伝播するのに役立ちます。</para>
<para><literal>バンドル</literal>は、Rancher UI (<literal>Continuous Delivery (継続的デリバリ)
→ Advanced (詳細) → Bundles (バンドル) → Create from YAML
(YALMから作成)</literal>)を介して、または<literal>バンドル</literal>リソースを正しいネームスペースに手動でデプロイして、デプロイできます。Fleetネームスペースについては、アップストリーム<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ドキュメント</link>を参照してください。</para>
<para><emphasis><emphasis role="strong">手動による</emphasis> アプローチを使用した<emphasis
role="strong">Longhorn</emphasis>
<literal>バンドル</literal>リソースデプロイメントの例:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>fleets/day2/chart-templates/longhorn/longhorn</literal>にある<literal>Longhorn</literal>チャートフリートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>HelmチャートをデプロイするクラスタをFleetに指示する
<literal>targets.yaml</literal>ファイルを作成します。この場合、1台のダウストリームクラスにデプロイされます。複雑なターゲットをマッピングする方法については「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング</link>」を参照してください。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: foo
EOF</screen>
</listitem>
<listitem>
<para><literal>Longhorn</literal> HelmチャートFleetをバンドルリソースに変換します。詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Helmチャートのバンドルへの変換)」を参照してください。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleets/day2/chart-templates/longhorn/longhorn-crd</literal>にある<literal>Longhorn
CRD</literal>チャートフリートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>HelmチャートをデプロイするクラスタをFleetに指示する
<literal>targets.yaml</literal>ファイルを作成します。この場合、1台のダウストリームクラスにデプロイされます。複雑なターゲットをマッピングする方法については「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング</link>」を参照してください。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: foo
EOF</screen>
</listitem>
<listitem>
<para><literal>Longhorn CRD</literal> HelmチャートFleetをバンドルリソースに変換します。詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートのバンドルへの変換)</link>」を参照してください。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>longhorn-bundle.yaml</literal>と<literal>longhorn-crd-bundle.yaml</literal>を<literal>管理クラスタ</literal>にデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>これらの手順に従うと、<literal>Longhorn</literal>が指定されたターゲットクラスタのすべてにデプロイされます。</para>
</section>
</section>
<section xml:id="day2-helm-upgrade-new-cluster-manage-chart">
<title>デプロイされたHelmチャートの管理</title>
<para>Fleetでデプロイした後のHelmチャートのアップグレードについては、<xref
linkend="day2-helm-upgrade-fleet-managed-chart"/>を参照してください。</para>
</section>
</section>
<section xml:id="day2-helm-upgrade-fleet-managed-chart">
<title>Fleetで管理されているHelmチャートをアップグレードしたい場合</title>
<orderedlist numeration="arabic">
<listitem>
<para>目的のEdgeリリースと互換性を持つように、チャートをアップブレードする必要があるバージョンを決定します。EdgeリリースごとのHelmチャートバージョンはリリースノート<xref
linkend="release-notes"/>)から表示できます。</para>
</listitem>
<listitem>
<para>Fleetで監視されているGitリポジトリで、Helmチャートの<literal>fleet.yaml</literal>ファイルを、リリースノート(<xref
linkend="release-notes"/>)から取得した正しいチャート<emphasis
role="strong">バージョン</emphasis>と<emphasis
role="strong">リポジトリ</emphasis>で編集します。</para>
</listitem>
<listitem>
<para>変更をコミットしてリポジトリにプッシュした後で、目的のHelmチャートのアップグレードがトリガされます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-helm-upgrade-eib-chart">
<title>EIBでデプロイされたHelmチャートをアップグレードしたい</title>
<para>EIBは、<literal>HelmChart</literal>リソースを作成し、 <link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link> Helm
統合機能によって導入された<literal>helm-controller</literal>を利用することで、Helmチャートをデプロイします。</para>
<para>EIBで導入されたHelmチャートが正常にアップグレードされるように、ユーザはEIBによってHelmチャート用に作成された<literal>HelmChart</literal>リソースに対してアップグレードを実行する必要があります。</para>
<para>以下に関する情報が提供されています。</para>
<itemizedlist>
<listitem>
<para>EIBでデプロイされたHelmチャートアップグレードプロセスの一般的な概要(<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>)。</para>
</listitem>
<listitem>
<para>EIBでデプロイされたHelmチャートの正常なアップグレードに必要なアップグレード手順(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)。</para>
</listitem>
<listitem>
<para>説明されていた方法を使用して<link
xl:href="https://longhorn.io">Longhorn</link>チャートをアップグレードする方法を示す例(<xref
linkend="day2-helm-upgrade-eib-chart-example"/>)。</para>
</listitem>
<listitem>
<para>異なるGitOpsツールでアップグレードプロセスを使用する方法(<xref
linkend="day2-helm-upgrade-eib-chart-third-party"/>)。</para>
</listitem>
</itemizedlist>
<section xml:id="day2-helm-upgrade-eib-chart-overview">
<title>概要</title>
<para>このセクションは、EIBによってデプロイされた1つまたは複数のHelmチャートをアップグレードするために実行する必要がある手順の概要を説明することを目的としています。Helmチャートのアップグレードに必要な手順の詳細な説明については、<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>を参照してください。</para>
<figure>
<title>Helmチャートのアップグレードワークフロー</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_diagram.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレードの図</phrase></textobject>
</mediaobject></figure>
<orderedlist numeration="arabic">
<listitem>
<para>このワークフローでは最初に、チャートのアップグレード先にする新しいHelmチャートアーカイブをユーザが<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
</listitem>
<listitem>
<para>アーカイブは、
<literal>generate-chart-upgrade-data.sh</literal>スクリプトによって処理されるディレクトリに配置される必要があります。</para>
</listitem>
<listitem>
<para>次にユーザは、<literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。これにより、提供されたアーカイブディレクトリの各Helmチャートアーカイブに対応するKubernetes
<link
xl:href="https://kubernetes.io/docs/concepts/configuration/secret/">Secret</link>
YAMLファイルが生成されます。これらのシークレットは、Helmチャートをアップグレードするために使用されるFleetに自動的に配置されます。これは、アップグレード手順(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)セクションで詳細に説明されています。</para>
</listitem>
<listitem>
<para>スクリプトが正常に終了したら、ユーザは必要なすべてのK8sリソースをターゲットクラスタに配布する<literal>バンドル</literal>または<literal>GitRepo</literal>リソースのいずれかの設定とデプロイメントを続行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>リソースは<literal>fleet-default</literal>ネームスペースの<literal>管理クラスタ</literal>にデプロイされます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Fleet (<xref
linkend="components-fleet"/>)はデプロイされたリソースを検出し、そのデータを解析して、そのリソースを指定されたターゲットクラスタにデプロイします。デプロイされる最も注目すべきリソースは次のとおりです。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>eib-charts-upgrader</literal> -
<literal>チャートアップグレードPod</literal>をデプロイするジョブ。<literal>eib-charts-upgrader-script</literal>と<literal>helm
chart upgrade
data</literal>シークレットが<literal>チャートアップグレードPod</literal>内にマウントされます。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader-script</literal> -
既存の<literal>HelmChart</literal>リソースにパッチを適用するために、<literal>チャートアップグレードPod</literal>によって使用されるスクリプトを配布するシークレット。</para>
</listitem>
<listitem>
<para><literal>Helmチャートアップグレードデータ</literal>シークレット -
ユーザが提供するデータに基づいて<literal>generate-chart-upgrade-data.sh</literal>スクリプトによって作成されたシークレットYAMLファイル。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>チャートアップグレードPod</literal>がデプロイされると、<literal>eib-charts-upgrader-script</literal>シークレットのスクリプトが実行され、
以下が実行されます。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>他のシークレットによって提供されたすべてのHelmチャートアップグレードを処理します。</para>
</listitem>
<listitem>
<para>提供されたアップグレードデータのそれぞれに対して<literal>HelmChart</literal>リソースがあるかどうかを確認します。</para>
</listitem>
<listitem>
<para>対応するHelmチャートのシークレットから提供されるデータで<literal>HelmChart</literal>リソースにパッチを適用します。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>RKE2/K3s helm-controllerは絶えず既存の<literal>HelmChart</literal>リソースに対する編集を監視します。
<literal>HelmChart</literal>
のパッチを検出し、変更を調整して、<literal>HelmChart</literal>リソースの背後のチャートのアップグレードに進みます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-upgrade-steps">
<title>アップグレード手順</title>
<orderedlist numeration="arabic">
<listitem>
<para>使用するEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリースタグ</link>から<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリのクローンを作成します。</para>
</listitem>
<listitem>
<para>取得したHelmチャートアーカイブを保存するディレクトリを作成します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>新しく作成したアーカイブディレクトリ内で、アップグレードするHelmチャートアーカイブを<link
xl:href="https://helm.sh/docs/helm/helm_pull/">取得</link>します。</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリースタグ</link>から、<literal>generate-chart-upgrade-data.sh</literal>スクリプトをダウンロードします。</para>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。</para>
<important>
<para>ユーザは、<literal>generate-chart-upgrade-data.sh</literal>スクリプトが生成する内容に変更を加えてはなりません。</para>
</important>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>スクリプトは次の論理を実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>ユーザは、
<literal>--fleet-path</literal>を指定して、Helmチャートアップグレードを開始できる有効なFleetを指していることを検証します。</para>
</listitem>
<listitem>
<para>ユーザが作成したアーカイブディレクトリ(例:
<literal>/foo/bar/archives/</literal>)からのすべてHelmチャートアーカイブを処理します。</para>
</listitem>
<listitem>
<para>Helmチャートアーカイブごとに、<literal>Kubernetes Secret
YAML</literal>リソースが作成されます。このリソースには、以下が保持されます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>パッチを適用する必要がある<literal>HelmChart</literal>リソースの<literal>名前</literal>。</para>
</listitem>
<listitem>
<para><literal>HelmChart</literal>リソースの新しい <literal>バージョン</literal>。</para>
</listitem>
<listitem>
<para><literal>HelmChart</literal>の現在実行中の設定を置き換えるために使用される<literal>base64</literal>でエンコードされたHelmチャートアーカイブ。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>各<literal>Kubernetes Secret
YAML</literal>リソースは、<literal>--fleet-path</literal>で指定された<literal>eib-charts-upgrader</literal>
Fleetへのパス内の<literal>base/secrets</literal>ディレクトリに転送されます。</para>
</listitem>
<listitem>
<para>さらに、<literal>generate-chart-upgrade-data.sh</literal>スクリプトは、移動したシークレットが確実に取得され、Helmチャートアップグレード論理で使用されるようにします。これは次のようにして行われます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>新しく追加されたリソースを含むように<literal>base/secrets/kustomization.yaml</literal>ファイルを編集する。</para>
</listitem>
<listitem>
<para>新しく追加されたシークレットをマウント設定に含むように<literal>base/patches/job-patch.yaml</literal>ファイルを編集する。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>が正常に実行されたら、<literal>suse-edge/fleet-examples</literal>リポジトリの次のディレクトリ内に変更が反映されているはずです。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>fleets/day2/eib-charts-upgrader/base/patches</literal></para>
</listitem>
<listitem>
<para><literal>fleets/day2/eib-charts-upgrader/base/secrets</literal></para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>以下の手順は、実行している環境によって異なります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>GitOpsをサポートする環境の場合(例: エアギャップされていない、エアギャップされたがローカルGitサーバーサポートが可能):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>fleets/day2/eib-charts-upgrader</literal>
FleetをGitOpsに使用するリポジトリにコピーします。Fleetに<literal>generate-chart-upgrade-data.sh</literal>スクリプトによって加えられた変更が含まれていることを確認します。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal>
Fleetのすべてのリソースを配布するために使用される<literal>GitRepo</literal>リソースを設定します。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Rancher UIを通じて<literal>GitRepo</literal>を設定およびデプロイする場合、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI (Rancher UIでのFleetへのアクセス)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>GitRepo</literal>手動設定およびデプロイメントの場合、「<link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a Deployment
(デプロイメントの作成)</link>」を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>GitOpsをサポートしていない環境の場合 (例: エアギャップされ、ローカルGitサーバの使用が許可されていない):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>rancher/fleet</literal><link
xl:href="https://github.com/rancher/fleet/releases">リリース</link>ページから<literal>fleet-cli</literal>バイナリをダウンロードします。Macユーザの場合、使用可能なHomebrew
Formulaeがあります - <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal> Fleetに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>リソースをデプロイする場所をFleetに指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: {} # Change this with your target data
EOF</screen>
<para>ターゲットクラスタをマップする方法については、アップストリーム<link
xl:href="https://fleet.rancher.io/gitrepo-targets">ドキュメント</link>を参照してください。</para>
</listitem>
<listitem>
<para><literal>fleet-cli</literal>を使用して、Fleetを <literal>バンドル</literal>リソースに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>これにより、<literal>eib-charts-upgrader</literal>
Fleetからのすべてのテンプレート化されたリソースを保持するバンドル(<literal>bundle.yaml</literal>)が作成されます。</para>
<para><literal>fleet apply</literal>コマンドに関する詳細については、「<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>」を参照してください。</para>
<para>Fleetをバンドルに変換する方法に関する詳細については、「 <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートのバンドルへの変換)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>バンドル</literal>をデプロイします。これは2つの方法のいずれかで実行できます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>RancherのUIを通じて - <emphasis role="strong">［Continuous Delivery
(継続的デリバリ)］→［Advanced (詳細)］→［Bundles (バンドル)］ → ［Create from YAML
(YAMLから作成)］</emphasis>に移動して、<literal>bundle.yaml</literal>
コンテンツを解析するか、［<literal>Read from File
(ファイルから読み取り)</literal>］オプションをクリックして、ファイル自体を渡します。</para>
</listitem>
<listitem>
<para>手動 -
<literal>管理クラスタ</literal>内に<literal>bundle.yaml</literal>ファイルを手動でデプロイします。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>これらの手順を実行すると、正常に<literal>GitRepo/バンドル</literal>リソースがデプロイされます。リソースはFleetによって取得され、そのコンテンツはユーザが以前の手順で指定したターゲットクラスタにデプロイされます。プロセスの概要については、「概要」(<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>)セクションを参照してください。</para>
<para>アップグレードプロセスを追跡する方法については、このドキュメントの「例」(<xref
linkend="day2-helm-upgrade-eib-chart-example"/>)セクションを参照してください。</para>
<important>
<para>チャートアップグレードが正常に確認されたら、<literal>バンドル/GitRepo</literal>リソースを削除します。</para>
<para>これにより、ダウンストリームクラスタから不要になったアップグレードリソースが削除され、今後バージョンクラッシュが発生しないようになります。</para>
</important>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-example">
<title>例</title>
<note>
<para>以下の例は、あるバージョンから別のバージョンにEIBでデプロイされたHelmチャートのアップグレード方法を示しています。この例のバージョンは、バージョン推奨事項として
<emphasis
role="strong">「扱わない」</emphasis>でください。特定のEdgeリリースのバージョン推奨事項は、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
</note>
<para><emphasis>ユースケース:</emphasis></para>
<itemizedlist>
<listitem>
<para>クラスタ名<literal>doc-example</literal>がRancherの<link
xl:href="https://longhorn.io">Longhorn</link>
<literal>103.3.0+up1.6.1</literal>バージョンを実行しています。</para>
</listitem>
<listitem>
<para>クラスタは、次のイメージ定義<emphasis>スニペット</emphasis>を使用して、EIBを通じてデプロイされました。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 103.3.0+up1.6.1
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 103.3.0+up1.6.1
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
<figure>
<title>doc-exampleがインストールされたLonghornバージョン</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例 1</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para><literal>Longhorn</literal>は、 Edge
3.1リリースと互換性のあるバージョンにアップグレードされる必要があります。つまり、<literal>104.2.0+up1.7.1</literal>にアップグレードする必要があります。</para>
</listitem>
<listitem>
<para><literal>doc-example</literal>クラスタの管理を担当する<literal>管理クラスタ</literal>がローカルGitサーバのサポートなしで<emphasis
role="strong">エアギャップ</emphasis>されており、Rancherセットアップが動作していることを前提としています。</para>
</listitem>
</itemizedlist>
<para>アップグレード手順(<xref linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>release-3.1.1</literal>タグから
<literal>suse-edge/fleet-example</literal>リポジトリのクローンを作成します。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.1.1 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para><literal>Longhorn</literal>アップグレードアーカイブが保存されるディレクトリを作成します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>目的の<literal>Longhorn</literal>チャートアーカイブバージョンを取得します。</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.7.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 104.2.0+up1.7.1

# Pull the Longhorn 1.7.1 chart archive
helm pull rancher-charts/longhorn --version 104.2.0+up1.7.1</screen>
</listitem>
<listitem>
<para><literal>アーカイブ</literal>ディレクトリ以外で、<literal>release-3.1.1</literal>リリースタグから<literal>generate-chart-upgrade-data.sh</literal>スクリプトをダウンロードします。</para>
</listitem>
<listitem>
<para>ディレクトリセットアップは次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-104.2.0+up1.7.1.tgz
│   └── longhorn-crd-104.2.0+up1.7.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>スクリプト実行後のディレクトリ構造は次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-104.2.0+up1.7.1.tgz
│   └── longhorn-crd-104.2.0+up1.7.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-104-2-0-up1-7-1.yaml &lt;- secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-104-2-0-up1-7-1.yaml &lt;- secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Gitで変更されたファイルは次のようになるはずです。</para>
<figure>
<title>generate-chart-upgrade-data.shによって作成されたfleet-examplesの変更</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例2</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para><literal>管理クラスタ</literal>がGitOpsワークフローをサポートしていないため、<literal>バンドル</literal>は<literal>eib-charts-upgrader</literal>
Fleet用に作成される必要があります。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>まず、Fleet自体に移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>次に、<literal>doc-example</literal>クラスタをターゲットとする<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: doc-example
EOF</screen>
</listitem>
<listitem>
<para>次に、<literal>fleet-cli</literal>バイナリを使用して、Fleetをバンドルに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
<listitem>
<para>ここで、 <literal>管理クラスタ</literal>マシンに<literal>bundle.yaml</literal>を転送します。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>管理クラスタ</literal>が<literal>Rancher</literal>を実行しているため、Rancher
UIを通じてバンドルをデプロイします。</para>
<figure>
<title>Rancher UIを通じたバンドルのデプロイ</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_3.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例3</phrase></textobject>
</mediaobject></figure>
<para>ここから、<emphasis role="strong">［Read from File
(ファイルから読み取り)］</emphasis>を選択し、システムで<literal>bundle.yaml</literal>ファイルを見つけます。</para>
<para>これにより、RancherのUI内に<literal>バンドル</literal>が自動入力されます。</para>
<figure>
<title>自動入力されたバンドルスニペット</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_4.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例4</phrase></textobject>
</mediaobject></figure>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>正常にデプロイされたら、バンドルは次のようになります。</para>
<figure>
<title>正常にデプロイされたバンドル</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_5.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例5</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para><literal>バンドル</literal>のデプロイメントが成功した後で、アップグレードプロセスを監視するには次のようにします。</para>
<orderedlist numeration="arabic">
<listitem>
<para>まず、<literal>アップグレードPod</literal>のログを確認します。</para>
<figure>
<title>アップグレードポッドのログの表示</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_6.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例6</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>ここで、helm-controllerによってアップグレードに作成されたPodのログを確認します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Pod名は次のテンプレートを使用します -
<literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>Podは、<literal>HelmChart</literal>リソースがデプロイされたネームスペースにあります。この場合、これは、<literal>デフォルト</literal>です。</para>
<figure>
<title>正常にアップグレードされたLonghornチャートのログ</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_8.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例8</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>HelmChartバージョンが上がっていることを確認します。</para>
<figure>
<title>上がったLonghornのバージョン</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_9.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例9</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>最後に、Longhorn Podが実行中であることを確認します。</para>
<figure>
<title>instance-managerポッドの検証例</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_10.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例10</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>上記の検証後、Longhorn
Helmチャートが<literal>103.3.0+up1.6.1</literal>から<literal>104.2.0+up1.7.1</literal>にアップグレードされていると仮定しても問題ないでしょう。</para>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-third-party">
<title>サードパーティのGitOpsツールを使用したHelmチャートのアップグレード</title>
<para>ユーザがこのアップグレード手順をFleet以外のGitOpsワークフロー(<literal>Flux</literal>など)で実行したいユースケースが存在する場合があります。</para>
<para>アップグレード手順に必要なリソースを生成するには、<literal>generate-chart-upgrade-data.sh</literal>スクリプトを使用して、ユーザが提供するデータを<literal>eib-charts-upgrader</literal>
Fleetに入力することができます。この実行方法の詳細については、アップグレード手順(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)を参照してください。</para>
<para>完全なセットアップ後、<link
xl:href="https://kustomize.io">kustomize</link>を使用して、クラスタにデプロイ可能な完全な動作ソリューションを生成することができます。</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>GitOpsワークフローにソリューションを含める場合は、<literal>fleet.yaml</literal>ファイルを削除して、残ったものものを有効な<literal>Kustomize</literal>セットアップとして使用できます。まず、
<literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行し、アップグレードするHelmチャートのデータを<literal>Kustomize</literal>セットアップに読み込ませることを忘れないでください。</para>
<para>このワークフローの使用方法を理解するには、「概要」(<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>)セクションと「アップグレード手順」(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)セクションを参照すると役立つでしょう。</para>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-product-documentation">
<title>製品マニュアル</title>
<partintro>
<para>ATIPのマニュアルはここにあります</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Adaptive Telco Infrastructure Platform (ATIP)</title>
<para>SUSE Adaptive Telco Infrastructure Platform
(<literal>ATIP</literal>)は、通信事業者向けに最適化されたエッジコンピューティングプラットフォームであり、通信事業者はそのネットワークを刷新し、その最新化を加速できます。</para>
<para>ATIPは、5G Packet CoreやCloud RANなどのCNFをホストするための、通信事業者向けの充実したクラウドスタックです。</para>
<itemizedlist>
<listitem>
<para>エッジスタックの複雑な設定を通信事業者の規模で自動的にゼロタッチでロールアウトし、ライフサイクルを管理します。</para>
</listitem>
<listitem>
<para>通信事業者に固有の設定とワークロードを使用して、通信事業者グレードのハードウェアの品質を継続的に保証します。</para>
</listitem>
<listitem>
<para>エッジ専用に設計されたコンポーネントで構成されているため、フットプリントが小さく、ワットパフォーマンスが高くなっています。</para>
</listitem>
<listitem>
<para>ベンダに依存しないAPIを備え、100%オープンソースであるため、柔軟なプラットフォーム戦略を維持します。</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>コンセプトとアーキテクチャ</title>
<para>SUSE
ATIPは、クラウドネイティブな最新の通信事業者向けアプリケーションをコアからエッジまで大規模にホストするために設計されたプラットフォームです。</para>
<para>このページでは、ATIPで使用されるアーキテクチャとコンポーネントについて説明します。これを理解しておくと、ATIPをデプロイおよび使用する際に役立ちます。</para>
<section xml:id="id-atip-architecture">
<title>ATIPのアーキテクチャ</title>
<para>次の図は、ATIPのアーキテクチャの概要を示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-4">
<title>コンポーネント</title>
<para>異なる2つのブロックがあります。管理スタックとランタイムスタックです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理スタック</emphasis>:
ATIP内でランタイムスタックのプロビジョニングとライフサイクルを管理するために使用される部分です。次のコンポーネントが含まれます。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="components-rancher"/>)を使用した、パブリッククラウド環境とプライベートクラウド環境のマルチクラスタ管理</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>)、MetalLB (<xref
linkend="components-metallb"/>)、および<literal>CAPI</literal> (Cluster
API)インフラストラクチャプロバイダを使用したベアメタルサポート</para>
</listitem>
<listitem>
<para>包括的なテナント分離と<literal>IDP</literal> (IDプロバイダ)の統合</para>
</listitem>
<listitem>
<para>サードパーティ統合と拡張機能の大規模なマーケットプレイス</para>
</listitem>
<listitem>
<para>ベンダに依存しないAPIと充実したプロバイダエコシステム</para>
</listitem>
<listitem>
<para>SLE Microのトランザクション更新の制御</para>
</listitem>
<listitem>
<para>GitリポジトリとFleet (<xref
linkend="components-fleet"/>)を使用してクラスタのライフサイクルを管理するGitOpsエンジン</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">ランタイムスタック</emphasis>: ATIP内でワークロードを実行するために使用される要素です。</para>
<itemizedlist>
<listitem>
<para>Kubernetesと、K3s (<xref linkend="components-k3s"/>)やRKE2 (<xref
linkend="components-rke2"/>)などの安全で軽量なディストリビューション(<literal>RKE2</literal>は、政府機関での使用や規制対象産業向けに強化、認証、最適化されています)。</para>
</listitem>
<listitem>
<para>NeuVector (<xref
linkend="components-neuvector"/>)。イメージの脆弱性スキャン、ディープパケットインスペクション、クラスタ内の自動トラフィック制御などのセキュリティ機能を実現します。</para>
</listitem>
<listitem>
<para>ブロックストレージとLonghorn (<xref
linkend="components-longhorn"/>)。クラウドネイティブのストレージソリューションをシンプルかつ簡単に使用できます。</para>
</listitem>
<listitem>
<para>SLE Micro(<xref
linkend="components-slmicro"/>)で最適化されたオペレーティングシステム。コンテナ運用のための、安全、軽量でイミュータブルな(トランザクショナルファイルシステムを備えた)
OSを実現します。SLE
Microは<literal>aarch64</literal>アーキテクチャと<literal>x86_64</literal>アーキテクチャで利用でき、通信事業者およびエッジのユースケース向けに<literal>リアルタイムカーネル</literal>もサポートしています。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>デプロイメントフローの例</title>
<para>管理コンポーネントとランタイムコンポーネントの関係を理解できるように、以下にワークフローの概要の例を示します。</para>
<para>ダイレクトネットワークプロビジョニングは、すべてのコンポーネントを事前設定した状態で新しいダウンストリームクラスタをデプロイできるワークフローであり、手動操作なしですぐにワークロードを実行できます。</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>例1: すべてのコンポーネントがインストールされた新しい管理クラスタをデプロイする</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、管理スタックが含まれる新しい<literal>ISO</literal>イメージを作成します。その後、この<literal>ISO</literal>イメージを使用して、新しい管理クラスタをVMまたはベアメタルにインストールできます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>新しい管理クラスタをデプロイする方法については、ATIPの管理クラスタに関するガイド(<xref
linkend="atip-management-cluster"/>)を参照してください。</para>
</note>
<note>
<para>Edge Image Builderの使用方法については、Edge Image Builderのガイド(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>例2: 通信事業者プロファイルを使用してシングルノードのダウンストリームクラスタをデプロイして通信ワークロードを実行可能にする</title>
<para>管理クラスタが稼働したら、その管理クラスタを使用して、ダイレクトネットワークプロビジョニングワークフローにより、すべての通信機能が有効化および設定された状態でシングルノードのダウンストリームクラスタをデプロイすることができます。</para>
<para>次の図に、これをデプロイするワークフローの概要を示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>ダウンストリームクラスタをデプロイする方法については、ATIPの自動プロビジョニングに関するガイド(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
</note>
<note>
<para>通信機能の詳細については、ATIPの通信機能に関するガイド(<xref linkend="atip-features"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>例3: MetalLBをロードバランサとして使用して高可用性ダウンストリームクラスタをデプロイする</title>
<para>管理クラスタが稼働したら、その管理クラスタを使用して、ダイレクトネットワークプロビジョニングワークフローにより、<literal>MetalLB</literal>をロードバランサとして使用する高可用性ダウンストリームクラスタをデプロイすることができます。</para>
<para>次の図に、これをデプロイするワークフローの概要を示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>ダウンストリームクラスタをデプロイする方法については、ATIPの自動プロビジョニングに関するガイド(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
</note>
<note>
<para><literal>MetalLB</literal>の詳細については、<xref
linkend="components-metallb"/>を参照してください。</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>要件と前提</title>
<section xml:id="id-hardware">
<title>ハードウェア</title>
<para>ATIPノードのハードウェア要件は次のコンポーネントに基づきます。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理クラスタ</emphasis>: 管理クラスタには、<literal>SLE
Micro</literal>、<literal>RKE2</literal>、<literal>Rancher
Prime</literal>、<literal>Metal<superscript>3</superscript></literal>などのコンポーネントが含まれ、管理クラスタを使用して複数のダウンストリームクラスタを管理します。管理するダウンストリームクラスタの数によっては、サーバのハードウェア要件は変わる場合があります。</para>
<itemizedlist>
<listitem>
<para>サーバ(<literal>VM</literal>または<literal>ベアメタル</literal>)の最小要件は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>RAM: 8GB以上(16GB以上を推奨)</para>
</listitem>
<listitem>
<para>CPU: 2個以上(4個以上を推奨)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">ダウンストリームクラスタ</emphasis>:
ダウンストリームクラスタは、ATIPノードにデプロイされて通信ワークロードを実行するクラスタです。<literal>SR-IOV</literal>、<literal>CPUパフォーマンス最適化</literal>などの特定の通信機能を有効にするには、固有の要件が必要になります。</para>
<itemizedlist>
<listitem>
<para>SR-IOV: VF
(仮想関数)をCNF/VNFにパススルーモードでアタッチするには、NICがSR-IOVをサポートしていて、BIOSでVT-d/AMD-Viが有効化されている必要があります。</para>
</listitem>
<listitem>
<para>CPUプロセッサ: 特定の通信ワークロードを実行するには、こちらの参照表(<xref
linkend="atip-features"/>)に記載されているほとんどの機能を利用できるようにCPUプロセッサモデルを適応させる必要があります。</para>
</listitem>
<listitem>
<para>仮想メディアでインストールするためのファームウェア要件:</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>サーバハードウェア</para></entry>
<entry align="left" valign="top"><para>BMCモデル</para></entry>
<entry align="left" valign="top"><para>管理</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Dell製ハードウェア</para></entry>
<entry align="left" valign="top"><para>第15世代</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Supermicro製ハードウェア</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>HPE製ハードウェア</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="id-network">
<title>ネットワーク</title>
<para>ネットワークアーキテクチャの参考として、次の図に、通信事業者環境の一般的なネットワークアーキテクチャを示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirement1.png" width=""/>
</imageobject>
<textobject><phrase>製品atip要件1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>このネットワークアーキテクチャは次のコンポーネントに基づきます。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理ネットワーク</emphasis>:
このネットワークは、ATIPノードの管理や帯域外管理に使用されます。通常は独立した管理スイッチに接続しますが、同じサービススイッチに接続し、VLANを使ってトラフィックを分離することもできます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コントロールプレーンネットワーク</emphasis>:
このネットワークは、ATIPノードと、そこで実行されているサービスとの間の通信に使用されます。また、ATIPノードと外部サービス(<literal>DHCP</literal>サーバや<literal>DNS</literal>サーバなど)との間の通信にも使用されます。接続環境では、スイッチやルータでインターネット経由のトラフィックを処理できる場合もあります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">その他のネットワーク</emphasis>:
場合によっては、お客様の特定の目的に合わせてATIPノードを他のネットワークに接続できます。</para>
</listitem>
</itemizedlist>
<note>
<para>ダイレクトネットワークプロビジョニングワークフローを使用するには、管理クラスタがダウンストリームクラスタサーバのBaseboard Management
Controller (BMC)とネットワークで接続されていて、ホストの準備とプロビジョニングを自動化できる必要があります。</para>
</note>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>サービス(DHCP、DNSなど)</title>
<para>デプロイ先の環境の種類によっては、<literal>DHCP</literal>、<literal>DNS</literal>などの外部サービスが必要な場合があります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">接続環境</emphasis>:
この場合、ATIPノードはインターネットに接続され(L3ルーティングプロトコルを使用)、外部サービスはお客様が提供します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">非接続/エアギャップ環境</emphasis>:
この場合、ATIPノードはインターネットにIPで接続されないため、サービスを追加して、ATIPのダイレクトネットワークプロビジョニングワークフローに必要なコンテンツをローカルにミラーリングする必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ファイルサーバ</emphasis>:
ファイルサーバは、ダイレクトネットワークプロビジョニングワークフローの中で、ATIPノードにプロビジョニングするOSイメージを保存するために使用されます。<literal>metal<superscript>3</superscript></literal>
HelmチャートでメディアサーバをデプロイしてOSイメージを保存できます。次のセクション(<xref
linkend="metal3-media-server"/>)を確認してください。ただし、既存のローカルWebサーバを使用することもできます。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-systemd-services">
<title>systemdサービスの無効化</title>
<para>通信ワークロードの場合、ノード上で実行されているワークロードのパフォーマンス(レイテンシ)に影響を及ぼさないように、ノード上で実行されている一部のサービスを無効にしたり、適切に設定することが重要です。</para>
<itemizedlist>
<listitem>
<para><literal>rebootmgr</literal>は、システムに保留中の更新がある場合の再起動方針を設定できるサービスです。通信ワークロードでは、システムによってスケジュールされた更新がある場合、<literal>rebootmgr</literal>サービスを無効にするか正しく設定してノードの再起動を回避し、ノードで実行中のサービスへの影響を避けることが非常に重要です。</para>
</listitem>
</itemizedlist>
<note>
<para><literal>rebootmgr</literal>の詳細については、<link
xl:href="https://github.com/SUSE/rebootmgr">rebootmgrのGitHubリポジトリ</link>を参照してください。</para>
</note>
<para>次のコマンドを実行して、使用する方針を検証します。</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>また、次のコマンドを実行すると無効にすることができます。</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>または、<literal>rebootmgrctl</literal>コマンドを次のように使用できます。</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para><literal>rebootmgr</literal>の方針を設定するこの設定は、ダイレクトネットワークプロビジョニングワークフローを使用して自動化できます。詳細については、ATIPの自動化されたプロビジョニングに関するドキュメント(<xref
linkend="atip-automated-provisioning"/>)を確認してください。</para>
</note>
<itemizedlist>
<listitem>
<para><literal>transactional-update</literal>は、システムによって制御される自動更新を可能にするサービスです。通信ワークロードの場合、ノードで実行中のサービスに影響を及ぼさないように、自動更新を無効にすることが重要です。</para>
</listitem>
</itemizedlist>
<para>自動更新を無効にするには、次のコマンドを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable transactional-update.timer
systemctl --now disable transactional-update-cleanup.timer</screen>
<itemizedlist>
<listitem>
<para><literal>fstrim</literal>は、ファイルシステムを毎週自動的にトリミングできるサービスです。通信ワークロードでは、ノードで実行中のサービスに影響を及ぼさないように、自動トリミングを無効にすることが重要です。</para>
</listitem>
</itemizedlist>
<para>自動トリミングを無効にするには、次のコマンドを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable fstrim.timer</screen>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>管理クラスタの設定</title>
<section xml:id="id-introduction-2">
<title>はじめに</title>
<para>管理クラスタは、ATIP内でランタイムスタックのプロビジョニングとライフサイクルを管理するために使用されます。技術的観点からは、管理クラスタには次のコンポーネントが含まれています。</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Enterprise Micro</literal>
(OS)。ユースケースに応じて、ネットワーキング、ストレージ、ユーザ、カーネル引数などの一部の設定をカスタマイズできます。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>
(Kubernetesクラスタ)。ユースケースに応じて、<literal>Multus</literal>、<literal>Cilium</literal>などの特定のCNIプラグインを使用するように設定できます。</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> (管理プラットフォーム)。クラスタのライフサイクルを管理します。</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>
(コンポーネント)。ベアメタルノードのライフサイクルを管理します。</para>
</listitem>
<listitem>
<para><literal>CAPI</literal> (コンポーネント)。Kubernetesクラスタ
(ダウンストリームクラスタ)のライフサイクルを管理します。ATIPでは、RKE2クラスタ(ダウンストリームクラスタ)のライフサイクルを管理するために<literal>RKE2
CAPI Provider</literal>も使用します。</para>
</listitem>
</itemizedlist>
<para>上記のコンポーネントをすべて使用すると、管理クラスタは、宣言型アプローチを使用してインフラストラクチャやアプリケーションを管理し、ダウンストリームクラスタのライフサイクルを管理できます。</para>
<note>
<para><literal>SUSE Linux Enterprise Micro</literal>の詳細については、「SLE Micro」(<xref
linkend="components-slmicro"/>)を参照してください。</para>
<para><literal>RKE2</literal>の詳細については、「RKE2」(<xref
linkend="components-rke2"/>)を参照してください。</para>
<para><literal>Rancher</literal>の詳細については、「Rancher」(<xref
linkend="components-rancher"/>)を参照してください。</para>
<para><literal>Metal<superscript>3</superscript></literal>の詳細については、「Metal3」(<xref
linkend="components-metal3"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>管理クラスタの設定手順</title>
<para>管理クラスタを設定するには、次の手順が必要です(シングルノードを使用)。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png" width=""/>
</imageobject>
<textobject><phrase>製品atip管理クラスタ1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>宣言型アプローチを使用して管理クラスタを設定するための主な手順は次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">接続環境のイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>:
最初の手順では、接続環境で使用する必要がある設定をすべて含むマニフェストとファイルを準備します。</para>
<itemizedlist>
<listitem>
<para>接続環境のディレクトリ構造(<xref linkend="mgmt-cluster-directory-structure"/>):
この手順では、Edge Image Builderで使用するディレクトリ構造を作成し、設定ファイルとイメージそのものを保存します。</para>
</listitem>
<listitem>
<para>管理クラスタ定義ファイル(<xref linkend="mgmt-cluster-image-definition-file"/>):
<literal>mgmt-cluster.yaml</literal>ファイルが管理クラスタのメイン定義ファイルです。このファイルには、作成するイメージに関する次の情報が含まれています。</para>
<itemizedlist>
<listitem>
<para>イメージ情報: ゴールデンイメージを使用して作成するイメージに関する情報。</para>
</listitem>
<listitem>
<para>オペレーティングシステム: イメージで使用するオペレーティングシステムの設定。</para>
</listitem>
<listitem>
<para>Kubernetes: Helmチャートとリポジトリ、Kubernetesのバージョン、ネットワーク設定、およびクラスタで使用するノード。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Customフォルダ(<xref linkend="mgmt-cluster-custom-folder"/>):
<literal>custom</literal>フォルダには設定ファイルとスクリプトが含まれ、Edge Image
Builderはこれらを使用して完全に機能する管理クラスタをデプロイします。</para>
<itemizedlist>
<listitem>
<para>ファイル: 管理クラスタが使用する設定ファイルが含まれています。</para>
</listitem>
<listitem>
<para>スクリプト: 管理クラスタが使用するスクリプトが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Kubernetesフォルダ(<xref linkend="mgmt-cluster-kubernetes-folder"/>):
<literal>kubernetes</literal>フォルダには、管理クラスタが使用する設定ファイルが含まれています。</para>
<itemizedlist>
<listitem>
<para>Manifests: 管理クラスタが使用するマニフェストが含まれています。</para>
</listitem>
<listitem>
<para>Helm: 管理クラスタによって使用されるHelm値ファイルが含まれます。</para>
</listitem>
<listitem>
<para>Config: 管理クラスタが使用する設定ファイルが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Networkフォルダ(<xref linkend="mgmt-cluster-network-folder"/>):
<literal>network</literal>フォルダには、管理クラスタノードが使用するネットワーク設定ファイルが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">エアギャップ環境でのイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>:
この手順では、エアギャップシナリオで使用するマニフェストとファイルを準備する際の相違点を示します。</para>
<itemizedlist>
<listitem>
<para>定義ファイルの変更(<xref linkend="mgmt-cluster-image-definition-file-airgap"/>):
<literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIBの出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。</para>
</listitem>
<listitem>
<para>customeフォルダの変更(<xref linkend="mgmt-cluster-custom-folder-airgap"/>):
<literal>custom</literal>フォルダを変更し、管理クラスタをエアギャップ環境で実行するために必要なリソースを含める必要があります。</para>
<itemizedlist>
<listitem>
<para>登録スクリプト:
エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトを削除する必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>helm値フォルダ (<xref linkend="mgmt-cluster-helm-values-folder-airgap"/>)での変更:
<literal>helm/values</literal>フォルダはエアギャップ環境で管理クラスタを実行するために必要な設定を含むように変更する必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">イメージの作成(<xref
linkend="mgmt-cluster-image-creation"/>)</emphasis>: この手順では、Edge Image
Builderツールを使用してイメージを作成します(接続環境とエアギャップ環境の両方が対象です)。ご使用のシステムでEdge Image
Builderツールを実行するための前提条件(<xref linkend="components-eib"/>)を確認してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">管理クラスタのプロビジョニング(<xref
linkend="mgmt-cluster-provision"/>)</emphasis>:
この手順では、前の手順で作成したイメージを使用して管理クラスタをプロビジョニングする方法について説明します(接続シナリオとエアギャップシナリオの両方が対象です)。この手順は、ラップトップ、サーバ、VM、またはUSBポートを搭載した他の任意のx86_64システムを使用して実行できます。</para>
</listitem>
</orderedlist>
<note>
<para>Edge Image Builderの詳細については、「Edge Image Builder」(<xref
linkend="components-eib"/>)およびEdge Image Builderのクイックスタート(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>接続環境用のイメージの準備</title>
<para>Edge Image
Builderは、管理クラスタのイメージを作成するために使用されます。このドキュメントでは、管理クラスタのセットアップに必要な最小設定について説明します。</para>
<para>Edge Image Buildeは、コンテナ内で実行されるため、<link
xl:href="https://podman.io">Podman</link>や <link
xl:href="https://rancherdesktop.io">Rancher
Desktop</link>などのコンテナランタイムが必要です。このガイドでは、Podmanが使用できることを前提としています。</para>
<para>また、高可用性管理クラスタをデプロイするための前提条件として、ネットワークで次の3つのIPを予約する必要があります。-
<literal>apiVIP</literal> (API VIPアドレス用(Kubernetes APIサーバへのアクセスに使用))、-
<literal>ingressVIP</literal> (Ingress VIPアドレス(Rancher UIなどで使用))、-
<literal>metal3VIP</literal> (Metal3 VIPアドレス用)。</para>
<section xml:id="mgmt-cluster-directory-structure">
<title>ディレクトリ構造</title>
<para>EIBを実行する場合、ディレクトリはホストからマウントされます。したがって、最初に実行する手順は、EIBが設定ファイルとイメージ自体を保存するために使用するディレクトリ構造を作成することです。このディレクトリの構造は次のとおりです。</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>イメージ<literal>SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso</literal>は、<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードし、<literal>base-images</literal>フォルダに配置する必要があります。</para>
<para>イメージのSHA256チェックサムを確認し、イメージが改ざんされていないことを確認する必要があります。このチェックサムは、イメージをダウンロードした場所と同じ場所にあります。</para>
<para>ディレクトリ構造の例は、 <link xl:href="https://github.com/suse-edge/atip">SUSE Edge
GitHubリポジトリの「telco-examples」フォルダ</link>にあります。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>管理クラスタ定義ファイル</title>
<para><literal>mgmt-cluster.yaml</literal>ファイルは管理クラスタのメイン定義ファイルで、次の情報が含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.15.3
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 104.2.0+up1.7.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 104.2.0+up1.7.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.8.3
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles-chart
        version: 0.3.3
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.9.3
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/3.1
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: ${API_HOST}
    apiVIP: ${API_VIP}
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para><literal>mgmt-cluster.yaml</literal>定義ファイルのフィールドと値について説明するために、ここではこのファイルを次のセクションに分割しています。</para>
<itemizedlist>
<listitem>
<para>イメージセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>ここで、<literal>baseImage</literal>は、SUSE Customer
CenterまたはSUSEダウンロードページからダウンロードした元のイメージです。<literal>outputImageName</literal>は、管理クラスタのプロビジョニングに使用する新しいイメージの名前です。</para>
<itemizedlist>
<listitem>
<para>オペレーティングシステムセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<para>ここで、<literal>installDevice</literal>はオペレーティングシステムのインストールに使用するデバイス、<literal>username</literal>および<literal>encryptedPassword</literal>はシステムへのアクセスに使用する資格情報、<literal>packageList</literal>はインストールするパッケージのリスト(<literal>jq</literal>はインストールプロセス中に内部的に必要)です。<literal>sccRegistrationCode</literal>は構築時にパッケージと依存関係を取得するために使用する登録コードで、SUSE
Customer
Centerから取得できます。暗号化パスワードは次のように<literal>openssl</literal>コマンドを使用して生成できます。</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>この出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Kubernetesセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.15.3
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 104.2.0+up1.7.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 104.2.0+up1.7.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.8.3
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles-chart
        version: 0.3.3
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.9.3
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/3.1
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>ここで、<literal>version</literal>はインストールされるKubernetesのバージョンです。
この場合、RKE2クラスタを使用しているため、バージョンは<literal>Rancher</literal>と互換性があるように1.29未満のマイナーナンバーである必要があります(たとえば、<literal>v1.30.5+rke2r1</literal>)。</para>
<para><literal>helm</literal>セクションには、インストールするHelmチャートのリスト、使用するリポジトリ、およびこれらすべてのバージョン設定が含まれます。</para>
<para><literal>network</literal>セクションには、<literal>RKE2</literal>コンポーネントが使用する<literal>apiHost</literal>や<literal>apiVIP</literal>などのネットワーク設定が含まれます。<literal>apiVIP</literal>は、ネットワーク内で使用されていないIPアドレスにし、DHCPを使用する場合はDHCPプールから除外してください。マルチノードクラスタでは、<literal>apiVIP</literal>がKubernetes
APIサーバへのアクセスに使用されます。<literal>apiHost</literal>は、<literal>RKE2</literal>コンポーネントが使用する<literal>apiVIP</literal>への名前解決として機能します。</para>
<para><literal>nodes</literal>セクションには、クラスタで使用するノードのリストが含まれています。<literal>nodes</literal>セクションには、クラスタで使用するノードのリストが含まれています。この例では、シングルノードクラスタを使用していますが、リストにノードを追加する(行のコメントを解除する)ことによってマルチノードクラスタに拡張できます。</para>
<note>
<itemizedlist>
<listitem>
<para>ノードの名前はクラスタ内で固有である必要があります。</para>
</listitem>
<listitem>
<para>オプションで、<literal>initializer</literal>フィールドを使用してブートストラップ
ホストを指定します。これを指定しない場合、リストの最初のノードになります。</para>
</listitem>
<listitem>
<para>ネットワーク設定が必要な場合、ノードの名前はネットワークフォルダ(<xref
linkend="mgmt-cluster-network-folder"/>)で定義されたホスト名と同じである必要があります。</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Customフォルダ</title>
<para><literal>custom</literal>フォルダには次のサブフォルダが含まれます。</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para><literal>custom/files</literal>フォルダには、管理クラスタが使用する設定ファイルが含まれます。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>フォルダには、管理クラスタが使用するスクリプトが含まれます。</para>
</listitem>
</itemizedlist>
<para><literal>custom/files</literal>フォルダには、次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>:
使用する<literal>Metal<superscript>3</superscript></literal>バージョンに関する設定パラメータ、および<literal>Rancher</literal>と<literal>MetalLB</literal>の基本パラメータが含まれます。このファイルを変更するのは、使用するコンポーネントのバージョンまたはネームスペースを変更する場合のみにしてください。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>:
使用する<literal>Metal<superscript>3</superscript></literal>コンポーネントの設定が含まれます(変更不要)。今後のバージョンでは、このスクリプトは代わりに<literal>Rancher
Turtles</literal>を使用するように置き換えられて、使いやすくなる予定です。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic-bmo -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
	EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
	EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>:
使用する<literal>Rancher</literal>コンポーネントの設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>:
systemdサービスを作成して初回ブート時にスクリプトを実行するための設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single on
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para><literal>custom/scripts</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>99-alias.sh</literal>スクリプト:
管理クラスタが初回ブート時にkubeconfigファイルを読み込むために使用するエイリアスが含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para><literal>99-mgmt-setup.sh</literal>スクリプト:
初回ブート時にスクリプトをコピーするための設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para><literal>99-register.sh</literal>スクリプト:
SCC登録コードを使用してシステムを登録するための設定が含まれます。アカウントにシステムを登録するには、<literal>${SCC_ACCOUNT_EMAIL}</literal>および<literal>${SCC_REGISTRATION_CODE}</literal>が正しく設定されている必要があります。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
	zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Kubernetesフォルダ</title>
<para><literal>kubernetes</literal>フォルダには次のサブフォルダが含まれます。</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para><literal>kubernetes/config</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>:
デフォルトでは、デフォルトでインストールされている<literal>CNI</literal>プラグインは<literal>Cilium</literal>であるため、このフォルダとファイルを作成する必要はありません。<literal>CNI</literal>プラグインをカスタマイズする必要がある場合に備えて、<literal>kubernetes/config</literal>フォルダにある<literal>server.yaml</literal>ファイルを使用できます。このファイルには次の情報が含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</listitem>
</itemizedlist>
<note>
<para>これは、使用するCNIプラグインなどの特定のKubernetesカスタマイズを定義する任意のファイルです。さまざまなオプションについては、<link
xl:href="https://docs.rke2.io/install/configuration">公式ドキュメント</link>で確認できます。</para>
</note>
<para><literal>kubernetes/manifests</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>:
管理クラスタ用の<literal>Ingress</literal>サービスを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>:
<literal>NeuVector</literal>ネームスペースを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>:
<literal>MetalLB</literal>コンポーネントの<literal>L2Advertisement</literal>を作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>:
<literal>rke2-ingress-nginx</literal>コンポーネントの<literal>IPAddressPool</literal>を作成するための設定が含まれます。<literal>${INGRESS_VIP}</literal>を正しく設定し、<literal>rke2-ingress-nginx</literal>コンポーネントで使用するために予約するIPアドレスを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para><literal>kubernetes/helm/values</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>:
<literal>Rancher</literal>コンポーネントを作成する設定が含まれます。<literal>${INGRESS_VIP}</literal>は<literal>Rancher</literal>コンポーネントによって使用されるIPアドレスを適切に定義するように設定する必要があります。<literal>Rancher</literal>コンポーネントにアクセスするURLは、<literal>https://rancher-${INGRESS_VIP}.sslip.io</literal>です。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global.cattle.psp.enabled: "false"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>:
<literal>NeuVector</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>:
<literal>Metal<superscript>3</superscript></literal>コンポーネントを作成するための設定が含まれます。<literal>${METAL3_VIP}</literal>を正しく設定して、<literal>Metal<superscript>3</superscript></literal>コンポーネントで使用するIPアドレスを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>メディアサーバは、Metal<superscript>3</superscript>に含まれるオプションの機能です(デフォルトでは無効になっています)。このMetal3の機能を使用するには、以前のマニフェストで設定する必要があります。Metal<superscript>3</superscript>メディアサーバを使用するには、次の変数を指定します。</para>
<itemizedlist>
<listitem>
<para>メディアサーバ機能を有効にするために、globalセクションに<literal>enable_metal3_media_server</literal>を追加して<literal>true</literal>に設定します。</para>
</listitem>
<listitem>
<para>メディアサーバ設定に次の内容を含めます。${MEDIA_VOLUME_PATH}はメディアボリュームのパスです(例:
<literal>/home/metal3/bmh-image-cache</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>外部のメディアサーバを使用してイメージを保存できます。外部のメディアサーバをTLSで使用する場合は、次の設定を変更する必要があります。</para>
<itemizedlist>
<listitem>
<para>前の<literal>metal3.yaml</literal>ファイルで<literal>additionalTrustedCAs</literal>を<literal>true</literal>に設定し、外部のメディアサーバから、信頼できる追加のCAを有効にします。</para>
</listitem>
<listitem>
<para><literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal>フォルダに次のシークレット設定を含めて、外部のメディアサーバのCA証明書を保存します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para><literal>additional_ca_cert</literal>は、外部のメディアサーバのbase64エンコードCA証明書です。次のコマンドを使用し、証明書をエンコードして手動でシークレットを生成できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>:
<literal>Cert-Manager</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: "true"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>ネットワーキングフォルダ</title>
<para><literal>network</literal>フォルダには、管理クラスタのノードと同じ数のファイルが含まれます。ここでは、ノードは1つのみであるため、<literal>mgmt-cluster-node1.yaml</literal>というファイルが1つあるだけです。ファイルの名前は、上述のnetwork/nodeセクションで<literal>mgmt-cluster.yaml</literal>定義ファイルに定義されているホスト名と一致させる必要があります。</para>
<para>ネットワーキング設定をカスタマイズして特定の静的IPアドレスを使用する必要がある場合(たとえばDHCPを使用しないシナリオの場合)、<literal>network</literal>フォルダにある<literal>mgmt-cluster-node1.yaml</literal>ファイルを使用できます。このファイルには次の情報が含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: ゲートウェイのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: DNSサーバのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: ネットワークインタフェースのMACアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: 管理クラスタのIPアドレス。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>DHCPを使用してIPアドレスを取得する場合、次の設定を使用できます(<literal>${MGMT_MAC}</literal>変数を使用して、<literal>MAC</literal>アドレスを正しく設定する必要があります)。</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>管理クラスタのノード数に応じて、<literal>mgmt-cluster-node2.yaml</literal>、<literal>mgmt-cluster-node3.yaml</literal>などのように追加のファイルを作成して残りのノードを設定できます。</para>
</listitem>
<listitem>
<para><literal>routes</literal>セクションは、管理クラスタのルーティングテーブルを定義するために使用します。</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>エアギャップ環境のイメージの準備</title>
<para>このセクションでは、エアギャップ環境を準備する方法について説明し、前の各セクションとの相違点のみを示します。エアギャップ環境のイメージを準備するには、前のセクション(接続環境のイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-connected"/>))を次のように変更する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIB出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。</para>
</listitem>
<listitem>
<para><literal>mgmt-cluster.yaml</literal>ファイルは、
<literal>rancher-turtles-airgap-resources</literal>
helmチャートを含むように変更される必要があります。</para>
</listitem>
<listitem>
<para>エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトは削除する必要があります。</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>定義ファイルの変更</title>
<para><literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIB出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。<literal>images</literal>フィールドには、出力イメージに含めるすべてのコンテナイメージのリストを含める必要があります。次に、<literal>embeddedArtifactRegistry</literal>セクションが含まれる<literal>mgmt-cluster.yaml</literal>ファイルの例を示します。</para>
<para><literal>rancher-turtles-airgap-resources</literal>
helmチャートは追加される必要もあり、これにより、 <link
xl:href="https://turtles.docs.rancher.com/getting-started/air-gapped-environment">Rancher
Turtlesエアギャップドキュメント</link>で説明されているリソースが作成されます。
また、必要な設定を指定するために、rancher-turtlesチャート用の turtles.yaml値ファイルも必要です。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.15.3
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 104.2.0+up1.7.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 104.2.0+up1.7.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.8.3
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles-chart
        version: 0.3.3
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources-chart
        version: 0.3.3
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 104.0.1+up2.7.9
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.9.3
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/3.1
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v5.0.2
    - name: registry.rancher.com/rancher/calico-cni:v3.28.1-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.0.16
    - name: registry.rancher.com/rancher/flannel-cni:v1.4.1-rancher1
    - name: registry.rancher.com/rancher/fleet-agent:v0.10.4
    - name: registry.rancher.com/rancher/fleet:v0.10.4
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240910
    - name: registry.rancher.com/rancher/hardened-calico:v3.28.1-build20240911
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.11-build20240910
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.5.1-build20240910
    - name: registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240910
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.23.1-build20240910
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.13-k3s1-build20240910
    - name: registry.rancher.com/rancher/hardened-flannel:v0.25.6-build20240910
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240910
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.30.5-rke2r1-build20240912
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.1.0-build20240910
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.6-build20240822
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.8.0-build20240910
    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.30.5-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.2-build20240828
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.9
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.2
    - name: registry.rancher.com/rancher/kubectl:v1.29.7
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.28
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher118
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.7.3
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.10.4-hardened3
    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.3-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.3-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.9.3
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v4.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.5.3
    - name: registry.rancher.com/rancher/rancher:v2.9.3
    - name: registry.rancher.com/rancher/rke-tools:v0.1.103
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.30.4-build20240910
    - name: registry.rancher.com/rancher/rke2-runtime:v1.30.5-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.30.5-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.2.18
    - name: registry.rancher.com/rancher/shell:v0.2.2
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.30.5-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.30.5-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.10-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.4
    - name: registry.rancher.com/rancher/ui-plugin-catalog:2.1.0
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/kubectl:v1.29.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.4.1
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.4.3
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v20230312-helm-chart-4.5.2-28-g66a760794
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v20231011-8b53cabe0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v20231226-1a7112e06
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.6.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v4.0.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.11.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v7.0.2
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.12.0
    - name: registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.14.0
    - name: registry.suse.com/rancher/mirrored-longhornio-openshift-origin-oauth-proxy:4.15
    - name: registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.7.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.42
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.7.1
    - name: registry.suse.com/edge/3.1/cluster-api-provider-rke2-bootstrap:v0.7.1
    - name: registry.suse.com/edge/3.1/cluster-api-provider-rke2-controlplane:v0.7.1
    - name: registry.suse.com/edge/3.1/cluster-api-controller:v1.7.5
    - name: registry.suse.com/edge/3.1/cluster-api-provider-metal3:v1.7.1
    - name: registry.suse.com/edge/3.1/ip-address-manager:v1.7.1</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>カスタムフォルダの変更</title>
<itemizedlist>
<listitem>
<para>エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトを削除する必要があります。ディレクトリ構造からわかるように、<literal>99-register.sh</literal>スクリプトは<literal>custom/scripts</literal>フォルダに含まれていません。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-helm-values-folder-airgap">
<title>Helm値フォルダの変更</title>
<itemizedlist>
<listitem>
<para><literal>turtles.yaml</literal>: Rancher
Turtlesのエアギャップ操作を指定するために必要な設定が含まれます。これはrancher-turtles-airgap-resourcesチャートのインストールによって異なることに注意してください。</para>
<screen language="yaml" linenumbering="unnumbered">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"</screen>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>イメージの作成</title>
<para>前の各セクションに従ってディレクトリ構造を準備したら(接続シナリオとエアギャップシナリオの両方が対象です)、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>ISO出力イメージファイルが作成されます。ここでは、上記のイメージ定義に基づく<literal>eib-mgmt-cluster-image.iso</literal>という名前のファイルです。</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>管理クラスタのプロビジョニング</title>
<para>前のイメージには、上述のコンポーネントがすべて含まれています。このイメージを使って、仮想マシンまたはベアメタルサーバを使用して(仮想メディア機能を使用して)管理クラスタをプロビジョニングできます。</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>通信機能の設定</title>
<para>このセクションでは、ATIPがデプロイされたクラスタの通信事業者固有の機能について記述および説明します。</para>
<para>ダイレクトネットワークプロビジョニングのデプロイメント方法を使用します。この方法については、ATIPの自動プロビジョニング(<xref
linkend="atip-automated-provisioning"/>)に関するセクションで説明しています。</para>
<para>このセクションでは、次のトピックについて説明します。</para>
<itemizedlist>
<listitem>
<para>リアルタイム用のカーネルイメージ(<xref linkend="kernel-image-for-real-time"/>):
リアルタイムカーネルで使用するカーネルイメージ。</para>
</listitem>
<listitem>
<para>低レイテンシとハイパフォーマンスのためのカーネル引数(<xref linkend="kernel-args"/>):
通信ワークロードを最大のパフォーマンスと低レイテンシで実行するために、リアルタイムカーネルによって使用されるカーネル引数。</para>
</listitem>
<listitem>
<para>CPU調整設定(<xref linkend="cpu-tuned-configuration"/>):
リアルタイムカーネルで使用するために調整した設定。</para>
</listitem>
<listitem>
<para>CNI設定(<xref linkend="cni-configuration"/>): Kubernetesクラスタで使用するCNI設定。</para>
</listitem>
<listitem>
<para>SR-IOV設定(<xref linkend="sriov"/>): Kubernetesワークロードで使用するSR-IOV設定。</para>
</listitem>
<listitem>
<para>DPDK設定(<xref linkend="dpdk"/>): システムで使用するDPDK設定。</para>
</listitem>
<listitem>
<para>vRANアクセラレーションカード(<xref linkend="acceleration"/>):
Kubernetesワークロードで使用するアクセラレーションカードの設定。</para>
</listitem>
<listitem>
<para>Huge Page (<xref linkend="huge-pages"/>): Kubernetesワークロードで使用するHuge Pageの設定。</para>
</listitem>
<listitem>
<para>CPUピニング設定(<xref linkend="cpu-pinning-configuration"/>):
Kubernetesワークロードで使用するCPUピニング設定。</para>
</listitem>
<listitem>
<para>NUMA対応のスケジューリング設定(<xref linkend="numa-aware-scheduling"/>):
Kubernetesワークロードで使用するNUMA対応のスケジューリング設定。</para>
</listitem>
<listitem>
<para>Metal LB設定(<xref linkend="metal-lb-configuration"/>):
Kubernetesワークロードで使用するMetal LB設定。</para>
</listitem>
<listitem>
<para>プライベートレジストリ設定(<xref linkend="private-registry"/>):
Kubernetesワークロードで使用するプライベートレジストリ設定。</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>リアルタイム用のカーネルイメージ</title>
<para>リアルタイムカーネルイメージは必ずしも標準カーネルより優れているとは限りません。リアルタイムカーネルは、特定のユースケース用に調整された別のカーネルです。低レイテンシを実現するために調整されていますが、その結果、スループットが犠牲になります。リアルタイムカーネルは一般的な用途には推奨されませんが、ここでは低レイテンシが重要な要因である通信ワークロード用のカーネルとして推奨されています。</para>
<para>主に4つの機能があります。</para>
<itemizedlist>
<listitem>
<para>決定論的実行:</para>
<para>予測可能性の向上 —
高負荷状態でも重要なビジネスプロセスが期限内に確実に完了し、常に高品質なサービスを提供します。高優先度プロセスのために重要なシステムリソースを保護することで、時間に依存するアプリケーションの予測可能性を向上できます。</para>
</listitem>
<listitem>
<para>低ジッタ:</para>
<para>高度に決定論的な技術に基づいてジッタが低く抑えられているため、アプリケーションと実世界との同期を維持できます。これは、継続的に繰り返し計算を行う必要があるサービスで役立ちます。</para>
</listitem>
<listitem>
<para>優先度の継承:</para>
<para>優先度の継承とは、優先度の高いプロセスがある状況において、そのプロセスがタスクを完了するためには優先度の低いプロセスが完了するのを待つ必要がある場合に、優先度の低いプロセスが高優先度を一時的に引き受ける機能です。SUSE
Linux Enterprise Real Timeは、ミッションクリティカルなプロセスにおけるこのような優先度の逆転の問題を解決します。</para>
</listitem>
<listitem>
<para>スレッドの割り込み:</para>
<para>一般的なオペレーティングシステムでは、割り込みモードで実行中のプロセスはプリエンプト可能ではありません。SUSE Linux Enterprise
Real
Timeでは、このような割り込みをカーネルスレッドでカプセル化して割り込み可能にし、ユーザが定義した高優先度プロセスでハード割り込みとソフト割り込みをプリエンプトできます。</para>
<para>ここでは、<literal>SLE Micro
RT</literal>のようなリアルタイムイメージをインストール済みの場合、カーネルリアルタイムはすでにインストールされています。リアルタイムカーネルイメージは<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>からダウンロードできます。</para>
<note>
<para>リアルタイムカーネルの詳細については、<link
xl:href="https://www.suse.com/products/realtime/">SUSE Real
Time</link>を参照してください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="kernel-args">
<title>低レイテンシとハイパフォーマンスのためのカーネル引数</title>
<para>リアルタイムカーネルを適切に動作させ、通信ワークロードを実行する際には、最高のパフォーマンスと低レイテンシ実現できるようにカーネル引数を設定することが重要です。このユースケースのカーネル引数を設定する際には、いくつかの重要な概念を念頭に置く必要があります。</para>
<itemizedlist>
<listitem>
<para>SUSEリアルタイムカーネルを使用する際には、<literal>kthread_cpus</literal>を削除します。このパラメータは、カーネルスレッドが作成されるCPUを制御します。また、PID
1とカーネルモジュール(kmodユーザスペースヘルパ)のロードにどのCPUが許可されるかも制御します。このパラメータは認識されず、影響は何もありません。</para>
</listitem>
<listitem>
<para><literal>domain,nohz,managed_irq</literal>フラグを<literal>isolcpus</literal>カーネル引数に追加します。何もフラグを指定しない場合、<literal>isolcpus</literal>は<literal>domain</literal>フラグのみを指定するのと同等になります。これにより、指定したCPUがカーネルタスクを含むスケジューリングから分離されます。<literal>nohz</literal>フラグは指定されたCPUのスケジューラティックを停止し(CPUで実行できるタスクが1つのみの場合)、<literal>managed_irq</literal>フラグは指定したCPUの管理対象の外部(デバイス)割り込みのルーティングを回避します。</para>
</listitem>
<listitem>
<para><literal>intel_pstate=passive</literal>を削除します。このオプションは、<literal>intel_pstate</literal>を汎用cpufreqガバナと連携するように設定しますが、この連携を行うには、副作用として、ハードウェア管理Ｐ状態(<literal>HWP</literal>)を無効にします。ハードウェアのレイテンシを削減するため、このオプションはリアルタイムワークロードには推奨されません。</para>
</listitem>
<listitem>
<para><literal>intel_idle.max_cstate=0
processor.max_cstate=1</literal>を<literal>idle=poll</literal>に置き換えます。
C-Stateの遷移を回避するには、<literal>idle=poll</literal>オプションを使用してC-Stateの遷移を無効にし、CPUを最高のC-Stateに維持します。<literal>intel_idle.max_cstate=0</literal>オプションは、<literal>intel_idle</literal>を無効にするため、
<literal>acpi_idle</literal>が使用され、<literal>acpi_idle.max_cstate=1</literal>がacpi_idleの最大のC-stateを設定します。
x86_64アーキテクチャでは、最初のACPI
C-Stateは常に<literal>POLL</literal>ですが、<literal>poll_idle()</literal>関数を使用しており、定期的にクロックを読み取ることで、タイムアウト後に
<literal>do_idle()</literal>でメインループの再起動する際に、若干のレイテンシが発生する可能性があります(これには<literal>TIF_POLL</literal>タスクフラグのクリアと設定も含まれます)。
これに対して、<literal>idle=poll</literal>はタイトなループで実行され、タスクが再スケジュールされるのをビジーウェイトします。これにより、アイドル状態から抜け出すまでのレイテンシが最小化されますが、その代償としてCPUがアイドルスレッドでフルスピードで稼働し続けることになります。</para>
</listitem>
<listitem>
<para>BIOSのC1Eを無効にします。このオプションは、BIOSでC1E状態を無効にし、アイドル時にCPUがCIE状態になるのを回避します。C1E状態は、CPUがアイドル状態のときにレイテンシを発生される可能性のある低電力状態です。</para>
</listitem>
<listitem>
<para><literal>nowatchdog</literal>を追加して、タイマーのハード割り込みコンテキストで実行されるタイマーとして実装されるソフトロックアップウォッチドッグを無効にします。有効期限が切れると(すなわち、ソフトロックアップが検出されると)、(ハード割り込みコンテキストで)警告が出力され、あらゆるレイテンシターゲットを実行します。有効期限が切れていない場合でも、タイマーリストに追加され、タイマー割り込みのオーバーヘッドがわずかに増加します。
このオプションは、NMIウォッチドッグも無効にするため、NMIが干渉できなくなります。</para>
</listitem>
<listitem>
<para><literal>nmi_watchdog=0</literal>を追加します。このオプションはNMIウォッチドッグのみを無効にします。</para>
</listitem>
</itemizedlist>
<para>これは、前述の調整を含む、カーネル引数の例です。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</screen>
</section>
<section xml:id="cpu-tuned-configuration">
<title>CPU調整設定</title>
<para>CPU調整設定を使用すると、リアルタイムカーネルが使用するCPUコアを分離できます。OSがリアルタイムカーネルと同じコアを使用しないようにすることが重要です。OSがそのコアを使用すると、リアルタイムカーネルの遅延が増加するためです。</para>
<para>この機能を有効にして設定するには、まず、分離するCPUコアのプロファイルを作成します。ここでは、コア<literal>1-30</literal>および<literal>33-62</literal>を分離しています。</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-18,21-38" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>次に、GRUBオプションを変更して、CPUコアと、CPUの使用法に関するその他の重要なパラメータを分離する必要があります。現在のハードウェア仕様で次のオプションをカスタマイズすることが重要です。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain、nohz、managed_irq、1-18、21-38</para></entry>
<entry align="left" valign="top"><para>コア1-18と21-38を分離します</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルは分離されたCPU全体でタイマー割り込みをずらすことができます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルはシステムがアイドル状態のときに1つのCPU上でタイマーティックを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-18、21-38</para></entry>
<entry align="left" valign="top"><para>カーネルブートパラメータは、完全なdynticksとCPU分離の設定を行うための現在の主要インタフェースです。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-18、21-38</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルはシステムがアイドル状態のときに1つのCPU上でRCUコールバックを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0、19、20、39</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、システムがアイドル状態のときに1つのCPU上で割り込みを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>これにより、アイドル状態から抜け出すまでのレイテンシが最小化されますが、その代償として、CPUがアイドルスレッドでフルスピードで稼働し続けることになります。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>このオプションはNMIウォッチドッグのみを無効にします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>このオプションは、タイマーのハード割り込みコンテキストで実行されるタイマーとして実装されるソフトロックアップウォッチドッグを無効にします。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>上記の値を使用することで、60個のコアを分離し、4個のコアをOSに使用します。</para>
<para>次のコマンドでGRUB設定を変更し、上記の変更を次回ブート時に適用します。</para>
<para><literal>/etc/default/grub</literal>ファイルを編集し、上記のパラメータを追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</screen>
<para>GRUB設定を更新します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを検証するには、次のコマンドを使用してカーネルコマンドラインを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<para>CPU設定を調整するために使用可能な別のスクリプトがあります。これは基本的には次の手順を実行します。</para>
<itemizedlist>
<listitem>
<para>CPUガバナーを<literal>パフォーマンス</literal>に設定します。</para>
</listitem>
<listitem>
<para>分離されたCPUへのタイマーのマイグレーションの設定を解除します。</para>
</listitem>
<listitem>
<para>kdaemonスレッドをハウスキーピング用CPUに移行します。</para>
</listitem>
<listitem>
<para>分離したCPUレイテンシを可能な限り低い値に設定します。</para>
</listitem>
<listitem>
<para>vmstatの更新を300秒に遅延させます。</para>
</listitem>
</itemizedlist>
<para>スクリプトは、<link
xl:href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.1/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh">SUSE
ATIP Githubリポジトリ - performance-settings.sh</link>で入手できます。</para>
</section>
<section xml:id="cni-configuration">
<title>CNI設定</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal>はATIPのデフォルトのCNIプラグインです。RKE2クラスタでCiliumをデフォルトのプラグインとして有効にするには、<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに次の設定が必要です。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>これはコマンドライン引数でも指定できます。具体的には、<literal>/etc/systemd/system/rke2-server</literal>ファイルのサーバの行に<literal>--cni=cilium</literal>を追加します。</para>
<para>次のセクション(<xref linkend="option2-sriov-helm"/>)で説明する<literal>SR-IOV</literal>
Network
Operatorを使用するには、<literal>Multus</literal>とともに、<literal>Cilium</literal>や<literal>Calico</literal>などの別のCNIプラグインをセカンダリプラグインとして使用します。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
<note>
<para>CNIプラグインの詳細については、「<link
xl:href="https://docs.rke2.io/install/network_options">Network Options
(ネットワークオプション)</link>」を参照してください。</para>
</note>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOVを使用すると、ネットワークアダプタなどのデバイスで、そのリソースへのアクセスをさまざまな<literal>PCIe</literal>ハードウェア機能の間で分離することができます。<literal>SR-IOV</literal>をデプロイするにはさまざまな方法がありますが、ここでは2つの方法を示します。</para>
<itemizedlist>
<listitem>
<para>オプション1: <literal>SR-IOV</literal> CNIデバイスプラグインと設定マップを使用して適切に設定する。</para>
</listitem>
<listitem>
<para>オプション2 (推奨): Rancher Primeから<literal>SR-IOV</literal>
Helmチャートを使用してこのデプロイメントを簡単に行えるようにする。</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">オプション1 - SR-IOV
CNIデバイスプラグインと設定マップをインストールして適切に設定する</emphasis></para>
<itemizedlist>
<listitem>
<para>デバイスプラグインの設定マップを準備する</para>
</listitem>
</itemizedlist>
<para>設定マップに入力する情報を<literal>lspci</literal>コマンドから取得します。</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>設定マップは<literal>JSON</literal>ファイルで構成され、このファイルで、フィルタを使用して検出を行うデバイスを記述し、インタフェースのグループを作成します。フィルタとグループを理解することが重要です。フィルタはデバイスを検出するために使用され、グループはインタフェースを作成するために使用されます。</para>
<para>フィルタを設定することもできます。</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (アクセラレータカード)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (ドライバ)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (物理インタフェース名)</para>
</listitem>
</itemizedlist>
<para>フィルタを設定して、より複雑なインタフェース構文に一致させることもできます。次に例を示します。</para>
<itemizedlist>
<listitem>
<para>pfNames:
<literal>["eth1#1,2,3,4,5,6"]</literal>または<literal>[eth1#1-6]</literal>
(物理インタフェース名)</para>
</listitem>
</itemizedlist>
<para>グループに関連して、<literal>FEC</literal>カード用のグループを1つと、<literal>Intel</literal>カード用のグループを1つ作成し、さらに、ユースケースに応じてプレフィックスを作成することもできます。</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>リソースグループを検出して作成し、一部の<literal>VF</literal>をPodに割り当てる組み合わせは多数あります。</para>
<note>
<para>フィルタとグループの詳細については、「<link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sriov-network-device-plugin
(sr-iovネットワークデバイスプラグイン)</link>」を参照してください。</para>
</note>
<para>フィルタとグループを設定して、ハードウェアとユースケースに応じたインタフェースに一致させると、使用する例が次の設定マップに表示されます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para><literal>daemonset</literal>ファイルを準備して、デバイスプラグインをデプロイします。</para>
</listitem>
</itemizedlist>
<para>このデバイスプラグインは、複数のアーキテクチャ(<literal>arm</literal>、<literal>amd</literal>、<literal>ppc64le</literal>)をサポートしています。したがって、同じファイルを異なるアーキテクチャに使用して、各アーキテクチャに複数の<literal>daemonset</literal>をデプロイできます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>設定マップと<literal>daemonset</literal>を適用すると、デバイスプラグインがデプロイされ、インタフェースが検出されてPodで使用できるようになります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Podで使用するノードでインタフェースが検出されて利用可能であることを確認します。</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para><literal>FEC</literal>は<literal>intel.com/intel_fec_5g</literal>で、値は1です。</para>
</listitem>
<listitem>
<para>Helmチャートを使用せずに、デバイスプラグインと設定マップを使用してデプロイした場合、<literal>VF</literal>は、<literal>intel.com/intel_sriov_odu</literal>または<literal>intel.com/intel_sriov_oru</literal>です。</para>
</listitem>
</itemizedlist>
<important>
<para>ここにインタフェースがない場合、そのインタフェースをPodで使用することはできないため、続行しても意味がありません。まず、設定マップとフィルタを確認して問題を解決してください。</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">オプション2 (推奨) - Rancherを使用し、SR-IOV
CNIおよびデバイスプラグイン用のHelmチャートを使用したインストール</emphasis></para>
<itemizedlist>
<listitem>
<para>Helmがない場合は入手します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>SR-IOVをインストールします。</para>
</listitem>
</itemizedlist>
<para>この部分は2つの方法で実行できます。<literal>CLI</literal>を使用する方法と、<literal>Rancher
UI</literal>を使用する方法です。</para>
<variablelist>
<varlistentry>
<term>CLIからのオペレータのインストール</term>
<listitem>
<screen>helm install sriov-crd oci://registry.suse.com/edge/3.1/sriov-crd-chart -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/3.1/sriov-network-operator-chart -n sriov-network-operator</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Rancher UIからのオペレータのインストール</term>
<listitem>
<para>クラスタがインストールされたら、<literal>Rancher UI</literal>にアクセスできるようになり、［Apps
(アプリ)］タブで<literal>Rancher
UI</literal>から<literal>SR-IOVオペレータ</literal>をインストールできます。</para>
</listitem>
</varlistentry>
</variablelist>
<note>
<para>必ず、正しいネームスペースを選択してオペレータをインストールしてください(例:
<literal>sriov-network-operator</literal>)。</para>
</note>
<para>+ image::features_sriov.png[sriov.png]</para>
<itemizedlist>
<listitem>
<para>デプロイしたリソースのcrdとPodを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>ノードのラベルを確認します。</para>
</listitem>
</itemizedlist>
<para>すべてのリソースが実行されていると、ラベルがノードに自動的に表示されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para><literal>daemonset</literal>を確認し、新しい<literal>sriov-network-config-daemon</literal>および<literal>sriov-rancher-nfd-worker</literal>がアクティブで準備できていることを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>数分後(更新に最大で10分かかる可能性があります)、ノードが検出されて、<literal>SR-IOV</literal>の機能が設定されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>検出されたインタフェースを確認します。</para>
</listitem>
</itemizedlist>
<para>検出されたインタフェースはネットワークデバイスのPCIアドレスである必要があります。この情報は、ホストで<literal>lspci</literal>コマンドを使用して確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>ここでインタフェースが検出されていない場合は、インタフェースが次の設定マップに存在することを確認してください。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>ここにデバイスがない場合は、設定マップを編集して、検出すべき適切な値を追加します(<literal>sriov-network-config-daemon</literal>デーモンセットの再起動が必要になります)。</para>
</note>
<itemizedlist>
<listitem>
<para><literal>NetworkNodeポリシー</literal>を作成して<literal>VF</literal>を設定します。</para>
</listitem>
</itemizedlist>
<para><literal>VF</literal>
(<literal>numVfs</literal>)がデバイス(<literal>rootDevices</literal>)から作成され、ドライバ<literal>deviceType</literal>と<literal>MTU</literal>が設定されます。</para>
<note>
<para><literal>resourceName</literal>フィールドには特殊文字を含めないでください。また、このフィールドはクラスタ全体で一意である必要があります。この例では、<literal>dpdk</literal>を<literal>sr-iov</literal>と組み合わせて使用するため、<literal>deviceType:
vfio-pci</literal>を使用しています。<literal>dpdk</literal>を使用しない場合は、deviceTypeを<literal>deviceType:
netdevice</literal> (デフォルト値)にする必要があります。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>設定を検証します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>sr-iovネットワークを作成します(別のネットワークが必要な場合のオプション)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>作成されたネットワークを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal>
(データプレーン開発キット)は、パケットの高速処理用の一連のライブラリとドライバです。DPDKは、広範なCPUアーキテクチャ上で実行されるパケット処理ワークロードを高速化するために使用されます。DPDKには、データプレーンライブラリと、以下のために最適化されたネットワークインタフェースコントローラ(<literal>NIC</literal>)ドライバが含まれています。</para>
<orderedlist numeration="arabic">
<listitem>
<para>キューマネージャはロックなしのキューを実装します。</para>
</listitem>
<listitem>
<para>バッファマネージャは固定サイズのバッファを事前割り当てします。</para>
</listitem>
<listitem>
<para>メモリマネージャは、メモリ内にオブジェクトのプールを割り当て、リングを使用してフリーオブジェクトを格納します。オブジェクトがすべての<literal>DRAM</literal>チャンネルに均等に分散されるようにします。</para>
</listitem>
<listitem>
<para>ポールモードドライバ(<literal>PMD</literal>)は、非同期通知なしで動作するように設計されているため、オーバーヘッドが軽減されます。</para>
</listitem>
<listitem>
<para>パケット処理を開発するためのヘルパである一連のライブラリとしてのパケットフレームワーク。</para>
</listitem>
</orderedlist>
<para>次の手順では、<literal>DPDK</literal>を有効にする方法と、<literal>DPDK</literal>インタフェースが使用する<literal>NIC</literal>から<literal>VF</literal>を作成する方法を示します。</para>
<itemizedlist>
<listitem>
<para><literal>DPDK</literal>パッケージをインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>カーネルパラメータ:</para>
</listitem>
</itemizedlist>
<para>DPDKを使用するには、ドライバをいくつか使用して、カーネルの特定のパラメータを有効にします。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、DPDKインタフェースに<literal>vfio</literal>ドライバを使用できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、<literal>VF</literal>に<literal>vfio</literal>を使用できます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>これらのパラメータを有効にするには、各パラメータを<literal>/etc/default/grub</literal>ファイルに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para><literal>vfio-pci</literal>カーネルモジュールを読み込み、<literal>NIC</literal>で<literal>SR-IOV</literal>を有効にします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para><literal>NIC</literal>から仮想機能(<literal>VF</literal>)をいくつか作成します。</para>
</listitem>
</itemizedlist>
<para>たとえば、2つの異なる<literal>NIC</literal>に対して<literal>VF</literal>を作成するには、次のコマンドが必要です。</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>新しいVFを<literal>vfio-pci</literal>ドライバにバインドします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>設定が正しく適用されたことを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>vRANアクセラレーション(<literal>Intel ACC100/ACC200</literal>)</title>
<para>4Gから5Gネットワークへの移行に伴い、多くの通信サービスプロバイダが仮想化無線アクセスネットワーク(<literal>vRAN</literal>)アーキテクチャを採用して、チャンネル容量を増やし、エッジベースのサービスとアプリケーションのデプロイメントを容易にしようとしています。vRANソリューションは、ネットワーク上のリアルタイムのトラフィックと需要の量に応じて容量を柔軟に増減できるため、低レイテンシのサービスを提供するのに理想的です。</para>
<para>4Gおよび5Gで最も計算負荷が高いワークロードの1つがRANレイヤ1
(<literal>L1</literal>)の<literal>FEC</literal>です。これは、信頼性の低い通信チャンネルやノイズの多い通信チャンネルでのデータ伝送エラーを解消するものです。<literal>FEC</literal>技術は、4Gまたは5Gデータの一定数のエラーを検出して訂正することで、再送信の必要性を解消します。<literal>FEC</literal>アクセラレーショントランザクションにはセルの状態情報が含まれないため、簡単に仮想化でき、プールするメリットとセルの容易な移行が実現します。</para>
<itemizedlist>
<listitem>
<para>カーネルパラメータ</para>
</listitem>
</itemizedlist>
<para><literal>vRAN</literal>アクセラレーションを有効にするには、次のカーネルパラメータを有効にする必要があります(まだ存在しない場合)。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、DPDKインタフェースにvfioを使用できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、VFにvfioを使用できます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>GRUBファイル<literal>/etc/default/grub</literal>を変更して、これらのパラメータをカーネルコマンドラインに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを確認するには、コマンドラインを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>vfio-pciカーネルモジュールを読み込み、<literal>vRAN</literal>アクセラレーションを有効にします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>インタフェース情報Acc100を取得します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>物理インタフェース(<literal>PF</literal>)を<literal>vfio-pci</literal>ドライバにバインドします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>仮想機能(<literal>VF</literal>)を物理インタフェース(<literal>PF</literal>)から作成します。</para>
</listitem>
</itemizedlist>
<para>2つの<literal>VF</literal>を<literal>PF</literal>から作成し、次の手順に従って<literal>vfio-pci</literal>にバインドします。</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>提案された設定ファイルを使用してacc100を設定します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>FEC PFから作成した新しいVFを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>Huge Page</title>
<para>プロセスが<literal>RAM</literal>を使用すると、<literal>CPU</literal>はそのメモリ領域をプロセスが使用中であるとマークします。効率を高めるために、<literal>CPU</literal>は<literal>RAM</literal>をチャンクで割り当てます。多くのプラットフォームでは<literal>4K</literal>バイトがチャンクのデフォルト値です。これらのチャンクをページと呼び、ディスクなどにスワップできます。</para>
<para>プロセスのアドレススペースは仮想であるため、<literal>CPU</literal>とオペレーティングシステムは、どのページがどのプロセスに属していて、各ページがどこに保管されているかを覚えておく必要があります。ページ数が多いほど、メモリマッピングの検索に時間がかかります。プロセスが<literal>1GB</literal>のメモリを使用する場合、検索するエントリは262,144個になります(<literal>1GB</literal>
/ <literal>4K</literal>)。1つのページテーブルエントリが8バイトを消費する場合、<literal>2MB</literal>
(262,144 * 8)を検索することになります。</para>
<para>最新の<literal>CPU</literal>アーキテクチャはデフォルトより大きいページをサポートしているので、<literal>CPU/OS</literal>が検索するエントリが減少します。</para>
<itemizedlist>
<listitem>
<para>カーネルパラメータ</para>
</listitem>
</itemizedlist>
<para>Huge Pageを有効にするには、次のカーネルパラメータを追加する必要があります。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、Huge Pageを1Gに設定できます</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>前に定義したHuge Pageの数です</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageを取得するためのデフォルト値です</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>GRUBファイル<literal>/etc/default/grub</literal>を変更して、これらのパラメータをカーネルコマンドラインに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="skew_tick=1 BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepages=0 hugepages=40 hugepagesz=1G hugepagesz=2M ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,19,20,39 isolcpus=domain,nohz,managed_irq,1-18,21-38 mce=off nohz=on net.ifnames=0 nmi_watchdog=0 nohz_full=1-18,21-38 nosoftlockup nowatchdog quiet rcu_nocb_poll rcu_nocbs=1-18,21-38 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを検証するには、次のコマンドラインを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Huge Pageの使用</para>
</listitem>
</itemizedlist>
<para>Huge Pageを使用するには、Huge Pageをマウントする必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Kubernetesワークロードをデプロイし、リソースとボリュームを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-configuration">
<title>CPUピニング設定</title>
<itemizedlist>
<listitem>
<para>要件</para>
<orderedlist numeration="arabic">
<listitem>
<para>こちらのセクション(<xref
linkend="cpu-tuned-configuration"/>)で説明したパフォーマンスプロファイルに合わせて<literal>CPU</literal>が調整されていること。</para>
</listitem>
<listitem>
<para>次のブロック(例)を<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに追加して、<literal>RKE2</literal>クラスタのkubeletにCPU管理の引数が設定されていること。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager=true"
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=1"
- "system-reserved=cpu=1"</screen>
<itemizedlist>
<listitem>
<para>KubernetesでのCPUピニングの使用</para>
</listitem>
</itemizedlist>
<para>kubeletで定義された<literal>静的ポリシー</literal>を使ってCPUピニング機能を使用する方法は、ワークロードに対して定義した要求と制限に応じて3つあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>BestEffort</literal> QoSクラス:
<literal>CPU</literal>に対して要求または制限を定義していない場合、Podはシステムで使用できる最初の<literal>CPU</literal>でスケジュールされます。</para>
<para><literal>BestEffort</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para><literal>Burstable</literal> QoSクラス:
CPUに対して要求を定義し、その要求が制限と同じではない場合、またはCPUの要求がない場合。</para>
<para><literal>Burstable</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>または</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para><literal>Guaranteed</literal> QoSクラス: CPUに対して要求を定義し、その要求が制限と同じである場合。</para>
<para><literal>Guaranteed</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="numa-aware-scheduling">
<title>NUMA対応のスケジューリング</title>
<para>Non-Uniform Memory AccessまたはNon-Uniform Memory Architecture
(<literal>NUMA</literal>)は、<literal>SMP</literal>
(マルチプロセッサ)アーキテクチャにおいて使用される物理メモリ設計であり、メモリアクセス時間がプロセッサからのメモリの相対的な位置によって異なります。<literal>NUMA</literal>では、プロセッサは専用のローカルメモリに、非ローカルメモリ、つまり別のプロセッサにローカルなメモリや複数のプロセッサで共有されているメモリよりも高速にアクセスできます。</para>
<section xml:id="id-identifying-numa-nodes">
<title>NUMAノードの特定</title>
<para><literal>NUMA</literal>ノードを特定するには、システムで次のコマンドを使用します。</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>この例では、<literal>NUMA</literal>ノードが1つだけあり、64個の<literal>CPU</literal>が表示されています。</para>
<para><literal>NUMA</literal>は<literal>BIOS</literal>で有効にする必要があります。<literal>dmesg</literal>にブート時のNUMA初期化レコードがない場合、カーネルリングバッファ内の<literal>NUMA</literal>関連のメッセージが上書きされた可能性があります。</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>MetalLB</title>
<para><literal>MetalLB</literal>は、ベアメタルKubernetesクラスタ用のロードバランサの実装であり、<literal>L2</literal>や<literal>BGP</literal>などの標準ルーティングプロトコルをアドバタイズプロトコルとして使用します。ベアメタル環境ではKubernetesサービスタイプ<literal>LoadBalancer</literal>を使用する必要があるため、Kubernetesクラスタ内のサービスを外部に公開するために使用できるのは、ネットワークロードバランサです。</para>
<para><literal>RKE2</literal>クラスタで<literal>MetalLB</literal>を有効にするには、次の手順を実行する必要があります。</para>
<itemizedlist>
<listitem>
<para>次のコマンドを使用して<literal>MetalLB</literal>をインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/3.1/metallb-chart
  targetNamespace: metallb-system
  version: 0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/3.1/endpoint-copier-operator-chart
  targetNamespace: endpoint-copier-operator
  version: 0.2.1
  createNamespace: true
EOF</screen>
<itemizedlist>
<listitem>
<para><literal>IpAddressPool</literal>および<literal>L2advertisement</literal>の設定を作成します。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para><literal>VIP</literal>を公開するためのエンドポイントサービスを作成します。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para><literal>VIP</literal>が作成され、<literal>MetalLB</literal>のPodが実行中であることを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>プライベートレジストリ設定</title>
<para><literal>Containerd</literal>をプライベートレジストリに接続するように設定し、そのプライベートレジストリを使用して各ノードにプライベートイメージをプルできます。</para>
<para>起動時に、<literal>RKE2</literal>は、<literal>registries.yaml</literal>ファイルが<literal>/etc/rancher/rke2/</literal>に存在するかどうかを確認し、このファイルで定義されたレジストリを使用するように<literal>containerd</literal>に指示します。プライベートレジストリを使用するには、このファイルを、レジストリを使用する各ノードにルートとして作成します。</para>
<para>プライベートレジストリを追加するには、ファイル<literal>/etc/rancher/rke2/registries.yaml</literal>を作成して次の内容を設定します。</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>または、認証を使用しない場合は次のように設定します。</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>レジストリの変更を有効にするには、ノード上でRKE2を起動する前にこのファイルを設定するか、または設定した各ノードでRKE2を再起動します。</para>
<note>
<para>詳細については、「<link
xl:href="https://docs.rke2.io/install/containerd_registry_configuration#registries-configuration-file">Containerd
Registry Configuration | RKE2 (Containerdのレジストリ設定 | RKE2)</link>」を確認してください。</para>
</note>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>完全に自動化されたダイレクトネットワークプロビジョニング</title>
<section xml:id="id-introduction-3">
<title>はじめに</title>
<para>ダイレクトネットワークプロビジョニングは、ダウンストリームクラスタのプロビジョニングを自動化できる機能です。この機能は、プロビジョニングするダウンストリームクラスタが多数あり、そのプロセスを自動化したい場合に便利です。</para>
<para>管理クラスタ(<xref linkend="atip-management-cluster"/>)は、次のコンポーネントのデプロイメントを自動化します。</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Enterprise Micro RT</literal>
(OS)。ユースケースに応じて、ネットワーキング、ストレージ、ユーザ、カーネル引数などの設定をカスタマイズできます。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>
(Kubernetesクラスタ)。デフォルトの<literal>CNI</literal>プラグインは<literal>Cilium</literal>です。ユースケースに応じて、特定の<literal>CNI</literal>プラグイン(<literal>Cilium+Multus</literal>など)を使用できます。</para>
</listitem>
<listitem>
<para><literal>Longhorn</literal> (ストレージソリューション)。</para>
</listitem>
<listitem>
<para><literal>NeuVector</literal> (セキュリティソリューション)。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>。高可用性マルチノードクラスタのロードバランサとして使用できます。</para>
</listitem>
</itemizedlist>
<note>
<para><literal>SUSE Linux Enterprise Micro</literal>の詳細については<xref
linkend="components-slmicro"/>を、<literal>RKE2</literal>の詳細については<xref
linkend="components-rke2"/>を、<literal>Longhorn</literal>の詳細については<xref
linkend="components-longhorn"/>を、<literal>NeuVector</literal>の詳細については<xref
linkend="components-neuvector"/>をそれぞれ参照してください。</para>
</note>
<para>以降のセクションでは、さまざまなダイレクトネットワークプロビジョニングワークフローと、プロビジョニングプロセスに追加できる機能について説明します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
<informalexample>
<para>次のセクションでは、ATIPを使用してダイレクトネットワークプロビジョニングワークフロー異なるシナリオを準備する方法について説明します。
デプロイメントの異なる設定オプション(エアギャップ環境、DHCPおよびDHCPなしのネットワーク、プライベートコンテナレジストリなどを含む)の例については、<link
xl:href="https://github.com/suse-edge/atip/tree/release-3.1/telco-examples/edge-clusters">SUSE
ATIPリポジトリ</link>を参照してください。</para>
</informalexample>
</section>
<section xml:id="eib-edge-image-connected">
<title>接続シナリオのダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroベースイメージを準備します。</para>
<para>ほとんどの設定はEdge Image
Builderを使用して行うことができますが、このガイドではダウンストリームクラスタをセットアップするために必要な最小限の設定について説明します。</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>接続シナリオの前提条件</title>
<itemizedlist>
<listitem>
<para>Edge Image Builderを実行するには、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイムが必要です。</para>
</listitem>
<listitem>
<para>ゴールデンイメージ<literal>SL-Micro.x86_64-6.0-Base-RT-GM2.raw</literal>は、 <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>接続シナリオのイメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたベースイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>ディレクトリには初回起動時に実行するスクリプトが含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal>スクリプトは、デプロイメント時にOSルートパーティションをサイズ変更するために必要です。</para>
</listitem>
<listitem>
<para><literal>02-performance.sh</literal>スクリプトはオプションであり、パフォーマンス調整用にシステムを設定するために使用できます。</para>
</listitem>
<listitem>
<para><literal>03-sriov.sh</literal>スクリプトはオプションであり、SR-IOV用にシステムを設定するために使用できます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にイメージにコピーされる<literal>performance-settings.sh</literal>および<literal>sriov-auto-filler.sh</literal>ファイルが含まれます。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.0-Base-RT-GM2.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-config.yaml</literal>ファイルは、ダウンストリームクラスタイメージの主要な設定ファイルです。次に、Metal<superscript>3</superscript>を介したデプロイメントの最小例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.0-Base-RT-GM2.raw
  outputImageName: eibimage-slmicro60rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</screen>
<para><literal>${ROOT_PASSWORD}</literal>はルートユーザの暗号化パスワードで、テスト/デバッグに役立ちます。このパスワードは、<literal>openssl
passwd -6 PASSWORD</literal>コマンドで生成できます。</para>
<para>運用環境では、<literal>${USERKEY1}</literal>を実際のSSHキーに置き換えて、usersブロックに追加できるSSHキーを使用することをお勧めします。</para>
<note>
<para><literal>net.ifnames=1</literal>は、<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link>を有効にします。</para>
<para>これはmetal3チャートのデフォルト設定と一致しますが、この設定は、設定されたチャートの<literal>predictableNicNames</literal>の値と一致する必要があります。</para>
<para>また、<literal>ignition.platform.id=openstack</literal>は必須であり、この引数がないと、Metal<superscript>3</superscript>の自動化フローでIgnitionによるSLEMicroの設定が失敗することにも注意してください。</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト<literal>custom/scripts/01-fix-growfs.sh</literal>が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="add-custom-script-performance">
<title>パフォーマンススクリプト</title>
<para>次のオプションのスクリプト(<literal>custom/scripts/02-performance.sh</literal>)は、パフォーマンス調整用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para><literal>custom/files/performance-settings.sh</literal>のコンテンツは、パフォーマンス調整用にシステムを設定するために使用可能なスクリプトであり、次の<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.1/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">リンク</link>からダウンロードできます。</para>
</section>
<section xml:id="add-custom-script-sriov">
<title>SR-IOVスクリプト</title>
<para>次のオプションスクリプト(<literal>custom/scripts/03-sriov.sh</literal>)はSR-IOV用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para><literal>custom/files/sriov-auto-filler.sh</literal>のコンテンツは、
SR-IOV用にシステムを設定するために使用可能なスクリプトであり、次の <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.1/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">リンク</link>からダウンロードできます。</para>
<note>
<para>同じアプローチを使用して、プロビジョニングプロセス中に実行する独自のカスタムスクリプトを追加します。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>通信ワークロードの追加設定</title>
<para><literal>dpdk</literal>、<literal>sr-iov</literal>、<literal>FEC</literal>などの通信機能を有効にするには、次の例に示すように追加のパッケージが必要な場合があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.0-Base-RT-GM2.raw
  outputImageName: eibimage-slmicro60rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${user1Key1}
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    additionalRepos:
      - url: https://download.opensuse.org/repositories/isv:/SUSE:/Edge:/Telco/SL-Micro_6.0_images/
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<para>ここで、<literal>${SCC_REGISTRATION_CODE}</literal>は<link
xl:href="https://scc.suse.com/">SUSE Customer
Center</link>からコピーした登録コードです。また、パッケージリストには通信事業者プロファイル用の最小限のパッケージが含まれています。<literal>pf-bb-config</literal>パッケージを使用するには(<literal>FEC</literal>機能とドライバへのバインドを有効にするには)、<literal>additionalRepos</literal>ブロックを含めて<literal>SUSE
Edge Telco</literal>リポジトリを追加する必要があります。</para>
</section>
<section xml:id="add-network-eib">
<title>高度なネットワーク設定のための追加スクリプト</title>
<para><xref
linkend="advanced-network-configuration"/>で説明されている静的IPや、より高度なネットワーキングシナリオを設定する必要がある場合、次の追加設定が必要です。</para>
<para><literal>network</literal>フォルダに、次の<literal>configure-network.sh</literal>ファイルを作成します。
このファイルは、初回ブート時に設定ドライブデータを使用し、<link
xl:href="https://github.com/suse-edge/nm-configurator">NM
Configuratorツール</link>を使用してホストネットワーキングを設定します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>イメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>これにより、上記の定義に基づいた、<literal>eibimage-slmicro60rt-telco.raw</literal>という名前の出力ISOイメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、管理クラスタのドキュメントを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>と呼びます。</para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>エアギャップシナリオ用のダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroベースイメージを準備します。</para>
<para>設定の多くはEdge Image
Builderを使用して行うことができますが、このガイドではエアギャップシナリオ用のダウンストリームクラスタの設定に必要な最小限の設定について説明します。</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>エアギャップシナリオの前提条件</title>
<itemizedlist>
<listitem>
<para>Edge Image Builderを実行するには、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイムが必要です。</para>
</listitem>
<listitem>
<para>ゴールデンイメージ<literal>SL-Micro.x86_64-6.0-Base-RT-GM2.raw</literal>は、 <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
</listitem>
<listitem>
<para>コンテナイメージが必要なSR-IOVなどのワークロードを使用する場合、ローカルのプライベートレジストリをデプロイして設定済みである必要があります(TLSまたは認証、あるいはその両方を使用/不使用)。このレジストリを使用して、イメージとHelmチャートOCIイメージを保存します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>エアギャップシナリオのイメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたベースイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>ディレクトリには初回起動時に実行するスクリプトが含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal>スクリプトは、デプロイメント時にOSルートパーティションをサイズ変更するために必要です。</para>
</listitem>
<listitem>
<para><literal>02-airgap.sh</literal>スクリプトは、エアギャップ環境でのイメージ作成プロセス中にイメージを適切な場所にコピーするために必要です。</para>
</listitem>
<listitem>
<para><literal>03-performance.sh</literal>スクリプトはオプションであり、パフォーマンス調整用にシステムを設定するために使用できます。</para>
</listitem>
<listitem>
<para><literal>04-sriov.sh</literal>スクリプトはオプションであり、SR-IOV用にシステムを設定するために使用できます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にイメージにコピーされる<literal>rke2</literal>および<literal>cni</literal>イメージが含まれています。また、オプションの<literal>performance-settings.sh</literal>および<literal>sriov-auto-filler.sh</literal>ファイルを含めることもできます。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.0-Base-RT-GM2.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-airgap-config.yaml</literal>ファイルは、ダウンストリームクラスタ用のメイン設定ファイルです。その内容については、前のセクション(<xref
linkend="add-telco-feature-eib"/>)で説明されています。</para>
</section>
<section xml:id="id-growfs-script-2">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト<literal>custom/scripts/01-fix-growfs.sh</literal>が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>エアギャップスクリプト</title>
<para>イメージ作成プロセス中にイメージを正しい場所にコピーするために、次のスクリプト<literal>custom/scripts/02-airgap.sh</literal>が必要です。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="add-custom-script-performance2">
<title>パフォーマンススクリプト</title>
<para>次のオプションのスクリプト(<literal>custom/scripts/03-performance.sh</literal>)は、パフォーマンス調整用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para><literal>custom/files/performance-settings.sh</literal>のコンテンツは、パフォーマンス調整用にシステムを設定するために使用可能なスクリプトであり、次の<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.1/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">リンク</link>からダウンロードできます。</para>
</section>
<section xml:id="add-custom-script-sriov2">
<title>SR-IOVスクリプト</title>
<para>次のオプションのスクリプト(<literal>custom/scripts/04-sriov.sh</literal>)は、SR-IOV用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para><literal>custom/files/sriov-auto-filler.sh</literal>のコンテンツは、
SR-IOV用にシステムを設定するために使用可能なスクリプトであり、次の <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.1/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">リンク</link>からダウンロードできます。</para>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>エアギャップシナリオのカスタムファイル</title>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にそのイメージにコピーする<literal>rke2</literal>イメージと<literal>cni</literal>イメージが含まれています。イメージを簡単に生成するには、次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-rke2-images.sh">スクリプト</link>と、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-release-rke2-images.txt">こちら</link>にあるイメージのリストを使用してローカルでイメージを準備し、<literal>custom/files</literal>に含める必要があるアーティファクトを生成します。また、最新の<literal>rke2-install</literal>スクリプトを<link
xl:href="https://get.rke2.io/">こちら</link>からダウンロードすることもできます。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>イメージをダウンロードした後、ディレクトリ構造は次のようになるはずです。</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>エアギャップシナリオおよびSR-IOV (オプション)に必要なイメージのプライベートレジストリへのプリロード</title>
<para>エアギャップシナリオまたはその他のワークロードイメージでSR-IOVを使用する場合、次の各手順に従って、ローカルのプライベートレジストリにイメージをプリロードする必要があります。</para>
<itemizedlist>
<listitem>
<para>HelmチャートOCIイメージをダウンロードして抽出し、プライベートレジストリにプッシュする</para>
</listitem>
<listitem>
<para>必要な残りのイメージをダウンロードして抽出し、プライベートレジストリにプッシュする</para>
</listitem>
</itemizedlist>
<para>次のスクリプトを使用して、イメージをダウンロードして抽出し、プライベートレジストリにプッシュできます。これからSR-IOVイメージをプリロードする例を示しますが、その他のカスタムイメージも同じ方法でプリロードすることができます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SR-IOVのHelmチャートOCIイメージのプリロード:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>必要なHelmチャートOCIイメージが含まれるリストを作成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:1.3.0
edge/sriov-crd-chart:1.3.0
EOF</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1/scripts/day2/edge-save-oci-artefacts.sh">スクリプト</link>と上記で作成したリストを使用してローカルtarballファイルを生成します。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/3.1/sriov-network-operator-chart:1.3.0
Pulled: registry.suse.com/edge/3.1/sriov-crd-chart:1.3.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-1.3.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-1.3.0.tgz</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1/scripts/day2/edge-load-oci-artefacts.sh">スクリプト</link>を使用してtarballファイルをプライベートレジストリ(例:
<literal>myregistry:5000</literal>)にアップロードし、前の手順でダウンロードしたHelmチャートOCIイメージをレジストリにプリロードします。</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>SR-IOVに必要な残りのイメージをプリロードします。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>ここでは、通信ワークロードのために「sr-iov」コンテナイメージを含める必要があります(例: 参考として、これは <link
xl:href="https://github.com/suse-edge/charts/blob/release-3.1/charts/sriov-network-operator/1.3.0%2Bup0.1.0/values.yaml">helmチャート値</link>から取得できます)。</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1/scripts/day2/edge-save-images.sh">スクリプト</link>と上記で作成したリストを使用して、必要なイメージを含む、tarballファイルをローカルで生成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.1/scripts/day2/edge-load-images.sh">スクリプト</link>を使用してプライベートレジストリ
(<literal>myregistry:5000</literal>など)にtarballファイルをアップロードし、前の手順でダウンロードしたイメージでプライベートレジストリをプリロードします。</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>エアギャップシナリオのイメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.1/edge-image-builder:1.1.0 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>これにより、上記の定義に基づいた、<literal>eibimage-slmicro60rt-telco.raw</literal>という名前の出力ISOイメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、管理クラスタドキュメントを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
</section>
</section>
<section xml:id="single-node">
<title>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード)</title>
<para>このセクションでは、ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化するために用いるワークフローについて説明します。これは、ダウンストリームクラスタのプロビジョニングを自動化する最もシンプルな方法です。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して、ダウンストリームクラスタを設定するための最小限の設定で生成されたイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上にある管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">ワークフロー</emphasis></para>
<para>次の図は、ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化するために用いるワークフローを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png" width=""/>
</imageobject>
<textobject><phrase>atip自動化シングルノード1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化する手順は2種類です。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ベアメタルホストを登録して、プロビジョニングプロセスで使用できるようにする。</para>
</listitem>
<listitem>
<para>ベアメタルホストをプロビジョニングして、オペレーティングシステムとKubernetesクラスタをインストールして設定する。</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">ベアメタルホストの登録</emphasis></para>
<para>最初の手順では、新しいベアメタルホストを管理クラスタに登録してプロビジョニングできるようにします。そのためには、次のファイル(<literal>bmh-example.yaml</literal>)を管理クラスタ内に作成して、使用する<literal>BMC</literal>資格情報と登録する<literal>BaremetalHost</literal>オブジェクトを指定する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>各項目の内容は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal> —
新しいベアメタルホストの<literal>BMC</literal>のユーザ名。</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal> —
新しいベアメタルホストの<literal>BMC</literal>のパスワード。</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal> — 使用する新しいベアメタルホストの<literal>MAC</literal>アドレス。</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal> —
ベアメタルホストの<literal>BMC</literal>の<literal>URL</literal> (例:
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>)。ハードウェアプロバイダに応じて使用できる各種のオプションの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">こちらのリンク</link>を確認してください。</para>
</listitem>
</itemizedlist>
<para>ファイルを作成したら、管理クラスタで次のコマンドを実行し、管理クラスタで新しいベアメタルホストの登録を開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>新しいベアメタルホストオブジェクトが登録され、その状態が「Registering (登録中)」から「Inspecting
(検査中)」に変わり、「Available (使用可能)」になります。この変更は次のコマンドを使用して確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para><literal>BaremetalHost</literal>オブジェクトは、<literal>BMC</literal>の資格情報が検証されるまでは「<literal>Registering
(登録中)</literal>」の状態です。資格情報の検証が完了すると、<literal>BaremetalHost</literal>オブジェクトの状態が「<literal>Inspecting
(検査中)</literal>」に変わります。この手順はハードウェアによっては多少時間がかかる可能性があります(最大で20分)。「Inspecting
(検査中)」のフェーズ中に、ハードウェア情報が取得されてKubernetesオブジェクトが更新されます。<literal>kubectl get bmh
-o yaml</literal>コマンドを使用して情報を確認してください。</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">プロビジョニング手順</emphasis></para>
<para>ベアメタルホストが登録されて使用可能になったら、次に、ベアメタルホストをプロビジョニングしてオペレーティングシステムとKubernetesクラスタをインストールして設定します。そのためには、次の情報を使用して管理クラスタで以下のファイル<literal>capi-provisioning-example.yaml</literal>を作成する必要があります(<literal>capi-provisioning-example.yaml</literal>は、以下のブロックを結合して生成できます)。</para>
<note>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</note>
<para>次のブロックはクラスタ定義です。ここで、<literal>pods</literal>ブロックと<literal>services</literal>ブロックを使用してネットワーキングを設定できます。また、ここにはコントロールプレーンオブジェクトと、使用するインフラストラクチャ(<literal>Metal<superscript>3</superscript></literal>プロバイダを使用)オブジェクトへの参照も含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<para><literal>Metal3Cluster</literal>オブジェクトには、設定するコントロールプレーンのエンドポイント(<literal>${DOWNSTREAM_CONTROL_PLANE_IP}</literal>を置き換える)と、<literal>noCloudProvider</literal>(ベアメタルノードを使用しているため)を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IP}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>オブジェクトには、使用するコントロールプレーン設定を指定し、<literal>Metal3MachineTemplate</literal>オブジェクトには、使用するコントロールプレーンのイメージを指定します。
また、ここには使用するレプリカの数(ここでは1)と、使用する<literal>CNI</literal>プラグイン(ここでは<literal>Cilium</literal>)に関する情報が含まれています。
agentConfigブロックには、使用する
<literal>Ignition</literal>形式と、使用する<literal>additionalUserData</literal>が含まれます。これを使用して<literal>RKE2</literal>ノードに<literal>rke2-preinstall.service</literal>
という名前のsystemdサービスを設定し、プロビジョニングプロセス中にIronicの情報を使用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換えます。
ciliumでmultusを有効にするには、使用する設定を含むファイルを
<literal>rke2</literal>サーバマニフェストディレクトリに<literal>rke2-cilium-config.yaml</literal>という名前で作成します。
最後の情報ブロックには、使用するKubernetesバージョンが含まれています。
<literal>${RKE2_VERSION}</literal>は使用する<literal>RKE2</literal>のバージョンであり、この値は置き換えます(例:
<literal>v1.30.5+rke2r1</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには次の情報を指定します。</para>
<itemizedlist>
<listitem>
<para>テンプレートへの参照として使用する<literal>dataTemplate</literal>。</para>
</listitem>
<listitem>
<para>登録プロセス中に作成されたラベルとの一致に使用する<literal>hostSelector</literal>。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で<literal>EIB</literal>を使用して生成されたイメージへの参照として使用する<literal>image</literal>、およびそのイメージを検証するために使用する<literal>checksum</literal>と<literal>checksumType</literal>。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro60rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro60rt-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>前のブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行して、新しいベアメタルホストのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード)</title>
<para>このセクションでは、ダイレクトネットワークプロビジョニングと<literal>MetalLB</literal>をロードバランサ戦略として使用して、マルチノードのダウンストリームクラスタのプロビジョニングを自動化するために使用するワークフローについて説明します。これはダウンストリームクラスタのプロビジョニングを自動化する最もシンプルな方法です。次の図は、ダイレクトネットワークプロビジョニングと<literal>MetalLB</literal>を使用してマルチノードのダウンストリームクラスタのプロビジョニングを自動化するためのワークフローを示しています。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して、ダウンストリームクラスタを設定するための最小限の設定で生成されたイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上にある管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">ワークフロー</emphasis></para>
<para>次の図は、ダイレクトネットワークプロビジョニングを使用してマルチノードのダウンストリームクラスタのプロビジョニングを自動化するために使用するワークフローを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png" width=""/>
</imageobject>
<textobject><phrase>atip自動化マルチノード1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>3つのベアメタルホストを登録し、プロビジョニングプロセスで使用できるようにする。</para>
</listitem>
<listitem>
<para>3つのベアメタルホストをプロビジョニングし、オペレーティングシステムと、<literal>MetalLB</literal>を使用するKubernetesクラスタをインストールして設定する。</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">ベアメタルホストの登録</emphasis></para>
<para>最初の手順では、管理クラスタに3つのベアメタルホストを登録してプロビジョニングできるようにします。そのためには、管理クラスタにファイル<literal>bmh-example-node1.yaml</literal>、<literal>bmh-example-node2.yaml</literal>、および<literal>bmh-example-node3.yaml</literal>を作成して、使用する<literal>BMC</literal>資格情報と、管理クラスタに登録する<literal>BaremetalHost</literal>オブジェクトを指定する必要があります。</para>
<note>
<itemizedlist>
<listitem>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</listitem>
<listitem>
<para>1つのホストのプロセスについてのみ説明します。他の2つのノードにも同じ手順が適用されます。</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>各項目の内容は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal> — 最初のベアメタルホストのBMCのユーザ名。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal> — 最初のベアメタルホストのBMCのパスワード。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal> — 使用する最初のベアメタルホストのMACアドレス。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal> — 最初のベアメタルホストのBMCのURL (例:
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>)。ハードウェアプロバイダに応じて使用できる各種のオプションの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">こちらのリンク</link>を確認してください。</para>
</listitem>
</itemizedlist>
<para>ファイルを作成したら、管理クラスタで次のコマンドを実行して、管理クラスタへのベアメタルホストの登録を開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>新しいベアメタルホストオブジェクトが登録され、その状態が「Registering (登録中)」から「Inspecting
(検査中)」に変わり、「Available (使用可能)」になります。この変更は次のコマンドを使用して確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para><literal>BaremetalHost</literal>オブジェクトは、<literal>BMC</literal>の資格情報が検証されるまでは「<literal>Registering
(登録中)</literal>」の状態です。資格情報の検証が完了すると、<literal>BaremetalHost</literal>オブジェクトの状態が「<literal>Inspecting
(検査中)</literal>」に変わります。この手順はハードウェアによっては多少時間がかかる可能性があります(最大で20分)。「Inspecting
(検査中)」のフェーズ中に、ハードウェア情報が取得されてKubernetesオブジェクトが更新されます。<literal>kubectl get bmh
-o yaml</literal>コマンドを使用して情報を確認してください。</para>
</note>
<para><emphasis role="strong">プロビジョニング手順</emphasis></para>
<para>3つのベアメタルホストが登録されて使用可能になったら、次の手順は、ベアメタルホストをプロビジョニングしてオペレーティングシステムとKubernetesクラスタをインストールして設定し、ロードバランサを作成して3つのベアメタルホストを管理することです。そのためには、次の情報を使用して管理クラスタにファイル<literal>capi-provisioning-example.yaml</literal>を作成する必要があります(capi-provisioning-example.yamlは、次のブロックを結合して生成できます)。</para>
<note>
<itemizedlist>
<listitem>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</listitem>
<listitem>
<para><literal>VIP</literal>アドレスは、どのノードにも割り当てられていない予約済みのIPアドレスであり、ロードバランサを設定するために使用されます。</para>
</listitem>
</itemizedlist>
</note>
<para>以下はクラスタ定義です。ここで、<literal>pods</literal>ブロックと<literal>services</literal>ブロックを使用してクラスタのネットワークを設定できます。また、ここにはコントロールプレーンと、使用するインフラストラクチャ(<literal>Metal<superscript>3</superscript></literal>プロバイダを使用)のオブジェクトへの参照も含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<para><literal>Metal3Cluster</literal>オブジェクトには、予約済みの<literal>VIP</literal>アドレス(<literal>${DOWNSTREAM_VIP_ADDRESS}</literal>を置き換え)を使用して設定するコントロールプレーンのエンドポイントと、<literal>noCloudProvider</literal>
(3つのベアメタルノードを使用しているため)を指定しています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>オブジェクトには、使用するコントロールプレーンの設定を指定し、<literal>Metal3MachineTemplate</literal>オブジェクトには、使用するコントロールプレーンのイメージを指定します。</para>
<itemizedlist>
<listitem>
<para>使用するレプリカの数(ここでは3)。</para>
</listitem>
<listitem>
<para>ロードバランサで使用するアドバタイズモード(<literal>address</literal>ではL2実装を使用)、および使用するアドレス(<literal>${EDGE_VIP_ADDRESS}</literal>を<literal>VIP</literal>アドレスに置き換え)。</para>
</listitem>
<listitem>
<para>使用する<literal>CNI</literal>プラグイン(ここでは<literal>Cilium</literal>)と、<literal>VIP</literal>アドレスの設定に使用する<literal>tlsSan</literal>が含まれる<literal>serverConfig</literal>。</para>
</listitem>
<listitem>
<para>agentConfigブロックには、使用する<literal>Ignition</literal>の形式と、<literal>RKE2</literal>ノードに次のような情報を設定するために使用する<literal>additionalUserData</literal>が含まれています。</para>
<itemizedlist>
<listitem>
<para>プロビジョニングプロセス中にIronicの情報を使用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換える、<literal>rke2-preinstall.service</literal>という名前のsystemdサービス。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>と<literal>endpoint-copier-operator</literal>のインストールに使用するHelmチャートが含まれている<literal>storage</literal>ブロック。</para>
</listitem>
<listitem>
<para>使用する<literal>IPaddressPool</literal>と<literal>L2Advertisement</literal>が含まれている<literal>metalLB</literal>カスタムリソースファイル(<literal>${EDGE_VIP_ADDRESS}</literal>を<literal>VIP</literal>アドレスに置き換え)。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>が<literal>VIP</literal>アドレスを管理するために使用する<literal>kubernetes-vip</literal>サービスの設定に使用する<literal>endpoint-svc.yaml</literal>ファイル。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>最後の情報ブロックには、使用するKubernetesバージョンが含まれています。
<literal>${RKE2_VERSION}</literal>は使用する
<literal>RKE2</literal>のバージョンで、この値は置き換えます(例:
<literal>v1.30.5+rke2r1</literal>)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  registrationMethod: "address"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS}
      - https://${EDGE_VIP_ADDRESS}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/3.1/endpoint-copier-operator-chart
                    targetNamespace: endpoint-copier-operator
                    version: 0.2.1
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/3.1/metallb-chart
                    targetNamespace: metallb-system
                    version: 0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS}/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "Node-multinode-cluster"</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには次の情報を指定します。</para>
<itemizedlist>
<listitem>
<para>テンプレートへの参照として使用する<literal>dataTemplate</literal>。</para>
</listitem>
<listitem>
<para>登録プロセス中に作成されたラベルとの一致に使用する<literal>hostSelector</literal>。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で<literal>EIB</literal>を使用して生成されたイメージへの参照として使用する<literal>image</literal>、およびそのイメージを検証するために使用する<literal>checksum</literal>と<literal>checksumType</literal>。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro60rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro60rt-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>前のブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行して、新しい3つのベアメタルホストのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>高度なネットワーク設定</title>
<para>ダイレクトネットワークプロビジョニングのワークフローでは、静的IP、ボンディング、VLANなどのダウンストリームクラスタのネットワーク設定を行うことができます。</para>
<para>次の各セクションでは、高度なネットワーク設定を使用してダウンストリームクラスタをプロビジョニングできるようにするために必要な追加手順について説明します。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>このセクション(<xref
linkend="add-network-eib"/>)に従って、<literal>EIB</literal>を使用して生成したイメージにネットワークフォルダとスクリプトを含める必要があります。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">設定</emphasis></para>
<para>次の2つのセクションをベースとして使用し、ホストを登録してプロビジョニングします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード) (<xref
linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード) (<xref
linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>高度なネットワーク設定を有効にするために必要な変更は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>登録手順: 設定に使用する<literal>networkData</literal>に関する情報(例:
ダウンストリームクラスタの静的<literal>IP</literal>と<literal>VLAN</literal>)を格納したシークレットが含まれる新しいサンプルファイル</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>このファイルには、ダウンストリームクラスタに高度なネットワーク設定(例:
<literal>静的IP</literal>や<literal>VLAN</literal>)を行う場合に使用する<literal>nmstate</literal>形式の<literal>networkData</literal>が含まれています。ご覧のとおり、この例は、静的IPを使用してインタフェースを有効にするための設定と、ベースインタフェースを使用してVLANを有効にするための設定を示しています。その他の<literal>nmstate</literal>の例を定義して、ダウンストリームクラスタのネットワークを特定の要件に適合するように設定できます。ここでは、次の変数を実際の値に置き換える必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE1_INTERFACE}</literal> —
エッジクラスタに使用するコントロールプレーンインタフェース(例: <literal>eth0</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_IP}</literal> —
エッジクラスタのエンドポイントとして使用するIPアドレス(kubeapiサーバのエンドポイントと一致する必要があります)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_PREFIX}</literal> — エッジクラスタに使用するCIDR (例:
<literal>/24</literal>または<literal>255.255.255.0</literal>を使用する場合には<literal>24</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_GATEWAY}</literal> — エッジクラスタに使用するゲートウェイ(例:
<literal>192.168.100.1</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_MAC}</literal> — コントロールプレーンインタフェースに使用するMACアドレス(例:
<literal>00:0c:29:3e:3e:3e</literal>)。</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal> — エッジクラスタに使用するDNS (例:
<literal>192.168.100.2</literal>)。</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal> — エッジクラスタに使用するVLAN ID (例:
<literal>100</literal>)。</para>
</listitem>
</itemizedlist>
<para>また、管理クラスタに登録するには、ファイルの末尾にある<literal>BaremetalHost</literal>オブジェクトに、<literal>preprovisioningNetworkDataName</literal>を使用するシークレットへの参照が必要です。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<para>マルチノードクラスタをデプロイする必要がある場合は、同じプロセスをその他のノードに対して実行する必要があります。</para>
</note>
<itemizedlist>
<listitem>
<para>プロビジョニング手順:
ネットワークデータに関連する情報のブロックを削除する必要があります。その理由は、ネットワークデータの設定は、プラットフォームによってシークレット<literal>controlplane-0-networkdata</literal>に組み込まれるためです。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<note>
<para><literal>Metal3DataTemplate</literal>、<literal>networkData</literal>、および<literal>Metal3
IPAM</literal>は現在サポートされていません。静的シークレットを介した設定のみが完全にサポートされています。</para>
</note>
</section>
<section xml:id="add-telco">
<title>通信機能(DPDK、SR-IOV、CPUの分離、Huge Page、NUMAなど)</title>
<para>ダイレクトネットワークプロビジョニングのワークフローでは、ダウンストリームクラスタで使用する通信機能を自動化して、そのサーバ上で通信ワークロードを実行できます。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>こちらのセクション(<xref
linkend="add-telco-feature-eib"/>)に従って、<literal>EIB</literal>を使用して生成したイメージに特定の通信機能を含める必要がある。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して生成したイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上の管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">設定</emphasis></para>
<para>次の2つのセクションをベースとして使用し、ホストを登録してプロビジョニングします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード) (<xref
linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード) (<xref
linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>このセクションで説明する通信機能を次に示します。</para>
<itemizedlist>
<listitem>
<para>DPDKとVFの作成</para>
</listitem>
<listitem>
<para>ワークロードで使用されるSR-IOVとVFの割り当て</para>
</listitem>
<listitem>
<para>CPUの分離とパフォーマンスの調整</para>
</listitem>
<listitem>
<para>Huge Pageの設定</para>
</listitem>
<listitem>
<para>カーネルパラメータの調整</para>
</listitem>
</itemizedlist>
<note>
<para>通信機能の詳細については、<xref linkend="atip-features"/>を参照してください。</para>
</note>
<para>上記の通信機能を有効にするために必要な変更はすべて、プロビジョニングファイル<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロック内にあります。ファイル<literal>capi-provisioning-example.yaml</literal>内の残りの情報は、プロビジョニングに関するセクション(<xref
linkend="single-node-provision"/>)で指定した情報と同じです。</para>
<para>このプロセスを明確にするために、通信機能を有効にするためにそのブロック(<literal>RKE2ControlPlane</literal>)で必要な変更を次に示します。</para>
<itemizedlist>
<listitem>
<para><literal>RKE2</literal>インストールプロセスの前にコマンドを実行するために使用する<literal>preRKE2Commands</literal>。ここでは、<literal>modprobe</literal>コマンドを使用して、<literal>vfio-pci</literal>と<literal>SR-IOV</literal>のカーネルモジュールを有効にします。</para>
</listitem>
<listitem>
<para>作成してワークロードに公開するインタフェース、ドライバ、および<literal>VFs</literal>の数を定義するために使用するIgnitionファイル<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>。</para>
<itemizedlist>
<listitem>
<para>実際の値に置き換える値は、設定マップ<literal>sriov-custom-auto-config</literal>内の値のみです。</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal> —
最初の<literal>PF</literal>インタフェースに使用するリソース名(例:
<literal>sriov-resource-du1</literal>)。このリソース名はプレフィックス<literal>rancher.io</literal>に追加されてラベルとして使用され、ワークロードで使用されます(例:
<literal>rancher.io/sriov-resource-du1</literal>)。</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal> —
使用する最初の<literal>PF</literal>インタフェースの名前(例: <literal>eth0</literal>)。</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal> —
使用する最初の物理機能<literal>PF</literal>の名前。これを使用して、より複雑なフィルタを生成します(例:
<literal>eth0#2-5</literal>)。</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal> —
最初の<literal>VF</literal>インタフェースに使用するドライバ名(例: <literal>vfio-pci</literal>)。</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal> —
最初の<literal>PF</literal>インタフェース用に作成する<literal>VF</literal>の数(例:
<literal>8</literal>)。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>/var/sriov-auto-filler.sh</literal>は、高レベルの設定マップ<literal>sriov-custom-auto-config</literal>と、低レベルのハードウェア情報を含む<literal>sriovnetworknodepolicy</literal>との間で情報を変換するために使用されます。このスクリプトは、ユーザがハードウェア情報を事前に把握する手間をなくすために作成されています。このファイルを変更する必要はありませんが、<literal>sr-iov</literal>を有効にして<literal>VF</literal>を作成する必要がある場合は、このファイルが存在する必要があります。</para>
</listitem>
<listitem>
<para>次の機能を有効にするために使用するカーネル引数:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>パラメータ</para></entry>
<entry align="left" valign="top"><para>値</para></entry>
<entry align="left" valign="top"><para>説明</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain、nohz、managed_irq、1-3、33-62</para></entry>
<entry align="left" valign="top"><para>コア1-30および33-62を分離します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>分離されたCPU間でカーネルがタイマー割り込みをずらすことができるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでタイマーティックを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>カーネルブートパラメータは、完全なdynticksとCPU分離の設定を行うための現在の主要インタフェースです。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでRCUコールバックを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUで割り込みを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>アイドル状態から抜け出すまでのレイテンシを最小化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>vfioをdpdkインタフェースに使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>vfioをVFに使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageのサイズを1Gに設定できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>前に定義したHuge Pageの数。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageを有効にする場合のデフォルト値。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>ウォッチドッグを無効にします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>NMウォッチドッグを無効にします。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>次のsystemdサービスは、以下のサービスを有効にするために使用します。</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> -
プロビジョニングプロセス中にIronicの情報を利用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換えます。</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> -
<literal>CPU</literal>の分離コアを有効にします(例: <literal>1-30,33-62</literal>)。</para>
</listitem>
<listitem>
<para><literal>performance-settings.service</literal> - CPUのパフォーマンス調整を有効にします。</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> - <literal>sriov</literal>
Helmチャートをインストールし、カスタムリソースが作成されるまで待機し、<literal>/var/sriov-auto-filler.sh</literal>を実行して設定マップ<literal>sriov-custom-auto-config</literal>の値を置き換えてワークロードが使用する<literal>sriovnetworknodepolicy</literal>を作成します。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>${RKE2_VERSION}</literal>は、この値の代わりに使用される<literal>RKE2</literal>のバージョンです(例:
<literal>v1.30.5+rke2r1</literal>)。</para>
</listitem>
</itemizedlist>
<para>これらの変更をすべて行うと、<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロックは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/3.1/sriov-crd-chart
                    targetNamespace: sriov-network-operator
                    version: 1.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/3.1/sriov-network-operator-chart
                    targetNamespace: sriov-network-operator
                    version: 1.3.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>前の各ブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行し、通信機能を使用する新しいダウンストリームクラスタのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>プライベートレジストリ</title>
<para>ワークロードで使用するイメージのミラーとしてプライベートレジストリを設定できます。</para>
<para>そのために、ダウンストリームクラスタで使用するプライベートレジストリに関する情報を含むシークレットを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para><literal>tls.crt</literal>、<literal>tls.key</literal>、および<literal>ca.crt</literal>は、プライベートレジストリを認証するために使用する証明書です。<literal>username</literal>および<literal>password</literal>は、プライベートレジストリを認証するために使用する資格情報です。</para>
<note>
<para><literal>tls.crt</literal>、<literal>tls.key</literal>、<literal>ca.crt</literal>、<literal>username</literal>、および<literal>password</literal>は、シークレットで使用する前にbase64形式でエンコードする必要があります。</para>
</note>
<para>これらの変更をすべて行うと、<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロックは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>ここで、<literal>registry.example.com</literal>は、ダウンストリームクラスタで使用するプライベートレジストリの名前の例です。これは実際の値に置き換える必要があります。</para>
</section>
<section xml:id="airgap-deployment">
<title>エアギャップシナリオでのダウンストリームクラスタのプロビジョニング</title>
<para>ダイレクトネットワークプロビジョニングワークフローでは、エアギャップシナリオでのダウンストリームクラスタのプロビジョニングを自動化できます。</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>エアギャップシナリオの要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>EIB</literal>を使用して生成された<literal>生</literal>のイメージには、エアギャップシナリオでダウンストリームクラスタを実行するために必要な特定のコンテナイメージ(HelmチャートOCIイメージとコンテナイメージ)を含める必要があります。詳細については、こちらのセクション(<xref
linkend="eib-edge-image-airgap"/>)を参照してください。</para>
</listitem>
<listitem>
<para>SR-IOVまたはその他のカスタムワークロードを使用する場合、プライベートレジストリへのプリロードに関するセクション(<xref
linkend="preload-private-registry"/>)に従って、ワークロードを実行するために必要なイメージをプライベートレジストリにプリロードする必要があります。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>エアギャップシナリオでのベアメタルホストの登録</title>
<para>管理クラスタにベアメタルホストを登録するプロセスは、前のセクション(<xref
linkend="enroll-bare-metal-host"/>)で説明したプロセスと同じです。</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>エアギャップシナリオでのダウンストリームクラスタのプロビジョニング</title>
<para>エアギャップシナリオでダウンストリームクラスタをプロビジョニングするために必要となる重要な変更がいくつかあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>capi-provisioning-example.yaml</literal>ファイルの<literal>RKE2ControlPlane</literal>ブロックに<literal>spec.agentConfig.airGapped:
true</literal>ディレクティブを含める必要があります。</para>
</listitem>
<listitem>
<para>プライベートレジストリに関するセクション(<xref
linkend="atip-private-registry"/>)に従って、プライベートレジストリの設定を<literal>capi-provisioning-airgap-example.yaml</literal>ファイルの<literal>RKE2ControlPlane</literal>ブロックに含める必要があります。</para>
</listitem>
<listitem>
<para>SR-IOV、またはHelmチャートをインストールする必要があるその他の<literal>AdditionalUserData</literal>設定(combustionスクリプト)を使用している場合、内容を変更して、パブリックレジストリを使用するのではなくプライベートレジストリを参照する必要があります。</para>
</listitem>
</orderedlist>
<para>次の例は、プライベートレジストリを参照するために必要な変更を行った、<literal>capi-provisioning-airgap-example.yaml</literal>ファイルの<literal>AdditionalUserData</literal>ブロックのSR-IOVの設定を示しています。</para>
<itemizedlist>
<listitem>
<para>プライベートレジストリのシークレットの参照</para>
</listitem>
<listitem>
<para>パブリックOCIイメージではなくプライベートレジストリを使用するHelmチャートの定義</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: ${SRIOV_CRD_VERSION}
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: ${SRIOV_OPERATOR_VERSION}
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>ライフサイクルのアクション</title>
<para>このセクションでは、デプロイしたATIPクラスタのライフサイクル管理アクションについて説明します。</para>
<section xml:id="id-management-cluster-upgrades">
<title>管理クラスタのアップグレード</title>
<para>管理クラスタのアップグレードには複数のコンポーネントが関係します。アップグレードする必要がある一般的なコンポーネントのリストについては、<literal>Day
2</literal>管理クラスタ(<xref linkend="day2-mgmt-cluster"/>)のドキュメントを参照してください。</para>
<para>このセットアップに固有のコンポーネントをアップグレードする手順を以下に示します。</para>
<para><emphasis role="strong">Metal<superscript>3</superscript>のアップグレード</emphasis></para>
<para><literal>Metal<superscript>3</superscript></literal>をアップグレードするには、次のコマンドを使用してHelmリポジトリキャッシュを更新し、最新のチャートをフェッチしてHelmチャートリポジトリから<literal>Metal<superscript>3</superscript></literal>をインストールします。</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>その後、現在の設定をファイルにエクスポートしてから、その前のファイルを使用して<literal>Metal<superscript>3</superscript></literal>のバージョンをアップグレードすると、簡単にアップグレードできます。新しいバージョンで何らかの変更が必要な場合、アップグレードの前にそのファイルを編集できます。</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.8.3</screen>
</section>
<section xml:id="id-downstream-cluster-upgrades">
<title>ダウンストリームクラスタのアップグレード</title>
<para>ダウンストリームクラスタをアップグレードするには、複数のコンポーネントを更新する必要があります。次の各セクションでは、各コンポーネントのアップグレードプロセスについて説明します。</para>
<para><emphasis role="strong">オペレーティングシステムのアップグレード</emphasis></para>
<para>このプロセスでは、次の参照資料(<xref
linkend="eib-edge-image-connected"/>)を確認して、新しいオペレーティングシステムバージョンで新しいイメージを構築します。<literal>EIB</literal>で生成されたこの新しいイメージにより、次のプロビジョニングフェーズでは、指定した新しいオペレーティングシステムバージョンが使用されます。次の手順では、この新しいイメージを使用してノードをアップグレードします。</para>
<para><emphasis role="strong">RKE2クラスタのアップグレード</emphasis></para>
<para>自動化されたワークフローを使用して<literal>RKE2</literal>クラスタをアップグレードするために必要な変更は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>次のセクション(<xref
linkend="single-node-provision"/>)に示す<literal>capi-provisioning-example.yaml</literal>のブロック<literal>RKE2ControlPlane</literal>を次のように変更します。</para>
<itemizedlist>
<listitem>
<para>仕様ファイルにロールアウト戦略を追加します。</para>
</listitem>
<listitem>
<para><literal>${RKE2_NEW_VERSION}</literal>を置き換えて、<literal>RKE2</literal>クラスタのバージョンを新しいバージョンに変更します。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_NEW_VERSION}
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>次のセクション(<xref
linkend="single-node-provision"/>)に示す<literal>capi-provisioning-example.yaml</literal>のブロック<literal>Metal3MachineTemplate</literal>を次のように変更します。</para>
<itemizedlist>
<listitem>
<para>イメージ名およびチェックサムを、前の手順で生成した新しいバージョンに変更します。</para>
</listitem>
<listitem>
<para>ディレクティブ<literal>nodeReuse</literal>を追加して<literal>true</literal>に設定し、新しいノードが作成されないようにします。</para>
</listitem>
<listitem>
<para>ディレクティブ<literal>automatedCleaningMode</literal>を追加して<literal>metadata</literal>に設定し、ノードの自動クリーニングを有効にします。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>これらの変更を行った後、次にコマンドを使用して<literal>capi-provisioning-example.yaml</literal>ファイルをクラスタに適用できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-appendix">
<title>付録</title>
<chapter xml:id="id-release-notes">
<title>リリースノート</title>
<section xml:id="release-notes">
<title>要約</title>
<para>SUSE Edge
3.1は、インフラストラクチャとクラウドネイティブなアプリケーションをエッジにデプロイするという他に例を見ない課題に対処することを目的とした、緊密に統合されて包括的に検証されたエンドツーエンドのソリューションです。SUSE
Edgeが重点を置いているのは、独創的でありながら高い柔軟性とスケーラビリティを持つセキュアなプラットフォームを提供し、初期デプロイメントイメージの構築からノードのプロビジョニングとオンボーディング、アプリケーションのデプロイメント、可観測性、ライフサイクル管理にまで対応することです。</para>
<para>このソリューションは、顧客の要件や期待はさまざまであるため「万能」なエッジプラットフォームは存在しないという考え方に基づいて設計されています。エッジデプロイメントにより、実に困難な問題を解決し、継続的に進化させることが要求されます。たとえば、大規模なスケーラビリティ、ネットワークの可用性の制限、物理的なスペースの制約、新たなセキュリティの脅威と攻撃ベクトル、ハードウェアアーキテクチャとシステムリソースのバリエーション、レガシインフラストラクチャやレガシアプリケーションのデプロイとインタフェースの要件、耐用年数を延長している顧客ソリューションといった課題があります。</para>
<para>SUSE
Edgeは、最良のオープンソースソフトウェアに基づいてゼロから構築されており、SUSEが持つ、30年にわたってセキュアで安定した定評あるSUSE
Linuxプラットフォームを提供してきた歴史と、Rancherポートフォリオによって拡張性に優れ機能豊富なKubernetes管理を提供してきた経験の両方に合致するものです。SUSE
Edgeは、これらの機能の上に構築されており、小売、医療、輸送、物流、通信、スマート製造、産業用IoTなど、さまざまな市場セグメントに対応できる機能を提供します。</para>
<note>
<para>SUSE Adaptive Telco Infrastructure Platform (ATIP)はSUSE
Edgeの派生製品(ダウンストリーム製品)にあたり、このプラットフォームを通信事業者の要件に対処可能にするための最適化とコンポーネントが追加されています。明記されていない限り、すべてのリリースノートはSUSE
Edge 3.1とSUSE ATIP 3.1の両方に適用されます。</para>
</note>
</section>
<section xml:id="id-about">
<title>概要</title>
<para>これらのリリースノートは、明示的に指定および説明されていない限り、すべてのアーキテクチャで同一です。また、最新バージョンは、その他すべてのSUSE製品のリリースノートとともに、常に<link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>でオンラインで参照できます。</para>
<para>エントリが記載されるのは1回だけですが、そのエントリが重要で複数のセクションに属している場合は複数の場所で参照できます。リリースノートには通常、連続する2つのリリース間の変更のみが記載されます。特定の重要なエントリは、以前の製品バージョンのリリースノートから繰り返し記載される場合があります。このようなエントリを特定しやすくするために、該当するエントリにはその旨を示すメモが含まれています。</para>
<para>ただし、繰り返し記載されているエントリは厚意としてのみ提供されています。したがって、リリースを1つ以上スキップする場合は、スキップしたリリースのリリースノートも確認してください。現行リリースのリリースノートしか確認しないと、システムの動作に影響する可能性がある重要な変更を見逃す可能性があります。SUSE
Edgeのバージョンはx.y.zで定義され、「x」はメジャーバージョン、「y」はマイナーバージョン、「z」は「z
ストリーム」とも呼ばれるパッチバージョンを表します。SUSE
Edgeの製品ライフサイクルは、「3.1」のようなマイナーリリースを中心に定義されますが、「3.1.1」のように、ライフサイクルを通じて後続のパッチアップデートが適用されます。</para>
<note>
<para>SUSE Edge
zストリームリリースは、バージョン管理されたスタックとして緊密に統合されていて、綿密にテストされています。個々のコンポーネントを上記のバージョンとは異なるバージョンにアップグレードすると、システムのダウンタイムが発生する可能性が高くなります。テストされていない設定でEdgeクラスタを実行することは可能ですが、推奨されません。また、サポートチャンネルを通じて解決策を提供するのに時間がかかる場合があります。</para>
</note>
</section>
<section xml:id="id-release-3-1-1">
<title>リリース3.1.1</title>
<para>公開日: 2024年11月15日</para>
<para>概要: SUSE Edge 3.1.1はSUSE Edge 3.1リリースストリームの最初のリリースzストリームです。</para>
<section xml:id="id-new-features">
<title>新機能</title>
<itemizedlist>
<listitem>
<para>NeuVector バージョンは、<literal>5.4.0</literal>に更新され、いくつかの新機能が追加されました: <link
xl:href="https://open-docs.neuvector.com/releasenotes/5x#release-notes-for-5x">リリースノート</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>バグおよびセキュリティの修正</title>
<itemizedlist>
<listitem>
<para>Rancherバージョンは<literal>2.9.3</literal>に更新されました: <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.9.3">リリースノート</link></para>
</listitem>
<listitem>
<para>RKE2バージョンは<literal>1.30.5</literal>に更新されました: <link
xl:href="https://docs.rke2.io/release-notes/v1.30.X#release-v1305rke2r1">リリースノート</link></para>
</listitem>
<listitem>
<para>K3sバージョンは<literal>1.30.5</literal>に更新されました: <link
xl:href="https://docs.k3s.io/release-notes/v1.30.X#release-v1305k3s1">リリースノート</link></para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>チャートでは、<literal>predictableNicNames</literal>パラメータの処理に関する問題が修正されています:
<link xl:href="https://github.com/suse-edge/charts/pull/160">SUSE
Edgeの問題#160</link></para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>チャートでは、<link
xl:href="https://www.cve.org/CVERecord?id=CVE-2024-43803:">CVE-2024-43803</link>で特定されたセキュリティの問題を解決しています:
<link xl:href="https://github.com/suse-edge/charts/pull/162">SUSE
Edgeの問題#162</link></para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>チャートでは、 <link
xl:href="https://www.cve.org/CVERecord?id=CVE-2024-44082:">CVE-2024-44082</link>で特定されたセキュリティの問題を解決しています:
<link xl:href="https://github.com/suse-edge/charts/pull/160">SUSE
Edgeの問題#160</link></para>
</listitem>
<listitem>
<para>RKE2 CAPIプロバイダが更新され、ETCDが更新時に利用不可になる問題が解決されました: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/449">RKE2プロバイダの問題#449</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-components-versions">
<title>コンポーネントバージョン</title>
<para>次の表に、3.1.1リリースを構成する個々のコンポーネントを示します。ここには、バージョン、Helmチャートバージョン(該当する場合)、およびリリースされたアーティファクトをバイナリ形式でプル可能な場所も記載されています。使用法とデプロイメントの例については、関連するマニュアルに従ってください。太字の項目は、以前のzストリームリリースからの変更点が強調表示されていることに注意してください。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>名前</para></entry>
<entry align="left" valign="top"><para>バージョン</para></entry>
<entry align="left" valign="top"><para>Helmチャートバージョン</para></entry>
<entry align="left" valign="top"><para>アーティファクトの場所(URL/イメージ)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SLE Micro</para></entry>
<entry align="left" valign="top"><para>6.0 (最新)</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SLE
Microダウンロードページ</link><?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso (sha256
bc7c3210c8a9b688d2713ad87f17e2c90cb99fd6dee1db528a5ff7f239cbcf79)<?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-RT-SelfInstall-GM2.install.iso (sha256
8242895e21745aec15ef526a95272887fa95dd832782b2cea4a95f41493f6648)<?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-GM2.raw.xz (sha256
7ae13d080e66c8b35624b6566b5eaff0875c8c141d0def9fbaee5876781ed81b)<?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-RT-GM2.raw.xz (sha256
9a19078c062ab52c62c0254e11f5a5a9fac938fd094abff5aa5eac2ec00b2d4e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Manager</para></entry>
<entry align="left" valign="top"><para>5.0.0</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE
Managerダウンロードページ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">K3s</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.30.5</emphasis></para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.30.5%2Bk3s1">アップストリームK3sリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">RKE2</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.30.5</emphasis></para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.30.5%2Brke2r1">アップストリームRKE2リリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Rancher Prime</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.9.3</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.9.3</emphasis></para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rancher/releases/download/v2.9.3/rancher-images.txt">Rancher
2.9.3イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher Prime
Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Longhorn</para></entry>
<entry align="left" valign="top"><para>1.7.1</para></entry>
<entry align="left" valign="top"><para>104.2.0+up1.7.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/deploy/longhorn-images.txt">Longhorn
1.7.1イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.longhorn.io">Longhorn Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.1</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.1">NMConfiguratorアップストリームリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">NeuVector</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">5.4.0</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">104.0.2+up2.8.0</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/rancher/mirrored-neuvector-controller:5.4.0</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/mirrored-neuvector-enforcer:5.4.0</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/mirrored-neuvector-manager:5.4.0</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.4.0</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/mirrored-neuvector-compliance-config:1.0.0</emphasis><?asciidoc-br?>
<emphasis role="strong">registry.suse.com/rancher
mirrored-neuvector-registry-adapter:0.1.2</emphasis><?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-scanner:latest<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-updater:latest</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Rancher Turtles (CAPI)</emphasis></para></entry>
<entry align="left" valign="top"><para>0.11</para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.3.3</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/3.1/rancher-turtles-chart:0.3.3</emphasis><?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.11.0<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-operator:0.12.0<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-controller:1.7.5<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-provider-metal3:1.7.1<?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/edge/3.1/cluster-api-provider-rke2-bootstrap:0.7.1</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/edge/3.1/cluster-api-provider-rke2-controlplane:0.7.1</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Metal<superscript>3</superscript></emphasis></para></entry>
<entry align="left" valign="top"><para>0.8.3</para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.8.3</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/3.1/metal3-chart:0.8.3</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/edge/3.1/baremetal-operator:0.6.2</emphasis><?asciidoc-br?>
registry.suse.com/edge/3.1/ip-address-manager:1.7.1<?asciidoc-br?> <emphasis
role="strong">registry.suse.com/edge/3.1/ironic:24.1.3.0</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/edge/3.1/ironic-ipa-downloader:2.0.1</emphasis><?asciidoc-br?>
registry.suse.com/edge/3.1/kube-rbac-proxy:v0.18.0<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/metallb-chart:0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.1/metallb-controller:v0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.1/metallb-speaker:v0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.1/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.1/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.6.4</para></entry>
<entry align="left" valign="top"><para>104.2.0+up1.6.4</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.6.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.6.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.6.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elementalダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>2.0.0</para></entry>
<entry align="left" valign="top"><para>2.0.0</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/2.1.0/charts/elemental/2.0.0">Elemental拡張機能チャート</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.1</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/edge-image-builder:1.1.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.3.1</para></entry>
<entry align="left" valign="top"><para>0.4.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/kubevirt-chart:0.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-operator:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-api:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-controller:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportproxy:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportserver:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-handler:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-launcher:1.3.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirtダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>1.1.0</para></entry>
<entry align="left" valign="top"><para>1.1.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/kubevirt-dashboard-extension-chart:1.1.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.60.1</para></entry>
<entry align="left" valign="top"><para>0.4.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/cdi-chart:0.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-operator:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-controller:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-importer:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-cloner:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-apiserver:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadserver:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadproxy:1.60.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>0.2.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/endpoint-copier-operator:v0.2.1<?asciidoc-br?>
registry.suse.com/edge/3.1/endpoint-copier-operator-chart:0.2.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (技術プレビュー)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/akri-chart:0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-dashboard-extension-chart:1.1.0<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SR-IOV Network Operator</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/sriov-network-operator-chart:1.3.0<?asciidoc-br?>
registry.suse.com/edge/3.1/sriov-crd-chart:1.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.13.4</para></entry>
<entry align="left" valign="top"><para>104.0.0+up0.7.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io">System Upgrade
Controllerチャート</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.13.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.1.0</para></entry>
<entry align="left" valign="top"><para>0.1.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/upgrade-controller-chart:0.1.0<?asciidoc-br?>
registry.suse.com/edge/3.1/upgrade-controller:0.1.0<?asciidoc-br?>
registry.suse.com/edge/3.1/kubectl:1.30.3<?asciidoc-br?> <emphasis
role="strong">registry.suse.com/edge/3.1/release-manifest:3.1.1</emphasis></para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-release-3-1-0">
<title>リリース3.1.0</title>
<para>公開日: 2024年10月11日</para>
<para>概要: SUSE Edge 3.1.0は、SUSE Edge 3.1リリースストリームの最初のリリースです。</para>
<section xml:id="id-new-features-2">
<title>新機能</title>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro 6.0、Kubernetes 1.30、およびRancher Prime 2.9に更新</para>
</listitem>
<listitem>
<para>Cluster APIとMetal3/Ironicバージョンを更新</para>
</listitem>
<listitem>
<para>管理クラスタCAPIコンポーネントは現在Rancher Turtlesを介して管理されています</para>
</listitem>
<listitem>
<para>管理クラスタのアップグレードは現在Upgrade Controller (<xref
linkend="components-upgrade-controller"/>)を介して管理されています</para>
</listitem>
<listitem>
<para>Stack Validation結果は現在<link
xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link>で公開されています</para>
</listitem>
<listitem>
<para>nm-configurator は現在nmstate 2.2.36 (2.2.26からアップグレード)を利用しています</para>
</listitem>
<listitem>
<para>Edge Image Builderの拡張機能:</para>
<itemizedlist>
<listitem>
<para>SL Micro 6.0ベースイメージをカスタマイズするためのサポートを追加</para>
</listitem>
<listitem>
<para>aarch64ホストマシンでaarch64イメージを構築する機能を追加<emphasis
role="strong">(技術プレビュー)</emphasis></para>
</listitem>
<listitem>
<para>ファイルを構築されたイメージファイルシステムに自動的にコピーする機能を追加</para>
</listitem>
<listitem>
<para>FIPSモードを有効にする機能を追加</para>
</listitem>
<listitem>
<para>コンテナイメージのキャッシュを追加</para>
</listitem>
<listitem>
<para>Leftover combustionアーティファクトが初回起動時に削除されるようになりました</para>
</listitem>
<listitem>
<para>OSファイルとユーザ提供の証明書は、最終イメージにコピーされる際に、元の許可を維持するようになりました</para>
</listitem>
<listitem>
<para>依存関係のアップグレード</para>
<itemizedlist>
<listitem>
<para>「Phone Home」デプロイメントは現在、Elemental v1.6 (v1.4からアップグレード)を利用しています</para>
</listitem>
<listitem>
<para>組み込みレジストリは現在、Hauler v1.0.7 (v1.0.1からアップグレード)を利用しています</para>
</listitem>
<listitem>
<para>ネットワークカスタマイズは現在、nm-configurator v0.3.1 (v0.3.0からアップグレード)を利用しています</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>イメージ定義の変更</para>
<itemizedlist>
<listitem>
<para>イメージ定義の現在のバージョンは、以下の変更を含むように1.1に更新されました</para>
<itemizedlist>
<listitem>
<para>ノードでFIPSモードを有効にする、専用のFIPSモードオプション(enableFIPS)が導入されました</para>
</listitem>
<listitem>
<para>スキーマの1.0バージョンを使用する既存の定義は引き続きEIBで動作します</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>イメージ設定ディレクトリの変更</para>
<itemizedlist>
<listitem>
<para>実行時にイメージのファイルシステムにファイルをコピーするために、os-filesという名前のオプションのディレクトリを含めることができます</para>
</listitem>
<listitem>
<para>カスタム/ファイルディレクトリに、イメージをコピーする際に維持されるサブディレクトリを含めることができるようになりました</para>
</listitem>
<listitem>
<para>Elementalの設定は現在、公式ソースから必要なRPMをインストールするために登録コードが必要になりました</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes-2">
<title>バグおよびセキュリティの修正</title>
<itemizedlist>
<listitem>
<para>RKE2 CAPIプロバイダは現在、 SLE Microで有効になっているcisProfileで動作します: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/402">RKE2プロバイダの問題#402</link></para>
</listitem>
<listitem>
<para>RKE2 CAPIプロバイダNTP設定は現在、SLE Microで動作します: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/436">RKE2プロバイダの問題#436</link></para>
</listitem>
<listitem>
<para>RKE2 CAPIプロバイダがローリングアップグレードに関連するノードdrainの問題を解決しました : <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/431">RKE2プロバイダの問題#431</link></para>
</listitem>
<listitem>
<para>Edge Image Builderの修正</para>
<itemizedlist>
<listitem>
<para>特定のHelmチャートは、APIバージョンを指定しないとテンプレート化されるときに失敗します: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/481">EIBの問題#481</link></para>
</listitem>
<listitem>
<para>Large Helmマニフェストがインストールに失敗します: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/491">EIBの問題#491</link></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-components-versions-2">
<title>コンポーネントバージョン</title>
<para>次の表に、3.1リリースを構成する個々のコンポーネントを示します。ここには、バージョン、Helmチャートバージョン(該当する場合)、およびリリースされたアーティファクトをバイナリ形式でプル可能な場所も記載されています。使用法とデプロイメントの例については、関連するマニュアルに従ってください。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>名前</para></entry>
<entry align="left" valign="top"><para>バージョン</para></entry>
<entry align="left" valign="top"><para>Helmチャートバージョン</para></entry>
<entry align="left" valign="top"><para>アーティファクトの場所(URL/イメージ)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SLE Micro</para></entry>
<entry align="left" valign="top"><para>6.0 (最新)</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SLE
Microダウンロードページ</link><?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-SelfInstall-GM2.install.iso (sha256
bc7c3210c8a9b688d2713ad87f17e2c90cb99fd6dee1db528a5ff7f239cbcf79)<?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-RT-SelfInstall-GM2.install.iso (sha256
8242895e21745aec15ef526a95272887fa95dd832782b2cea4a95f41493f6648)<?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-GM2.raw.xz (sha256
7ae13d080e66c8b35624b6566b5eaff0875c8c141d0def9fbaee5876781ed81b)<?asciidoc-br?>
SL-Micro.x86_64-6.0-Base-RT-GM2.raw.xz (sha256
9a19078c062ab52c62c0254e11f5a5a9fac938fd094abff5aa5eac2ec00b2d4e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Manager</para></entry>
<entry align="left" valign="top"><para>5.0.0</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE
Managerダウンロードページ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.30.3</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.30.3%2Bk3s1">アップストリームK3sリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.30.3</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.30.3%2Brke2r1">アップストリームRKE2リリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.9.1</para></entry>
<entry align="left" valign="top"><para>2.9.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rancher/releases/download/v2.9.1/rancher-images.txt">Rancher
2.9.1イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher Prime
Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Longhorn</para></entry>
<entry align="left" valign="top"><para>1.7.1</para></entry>
<entry align="left" valign="top"><para>104.2.0+up1.7.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://raw.githubusercontent.com/longhorn/longhorn/v1.7.1/deploy/longhorn-images.txt">Longhorn
1.7.1イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.longhorn.io">Longhorn Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.1</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.1">NMConfiguratorアップストリームリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NeuVector</para></entry>
<entry align="left" valign="top"><para>5.3.4</para></entry>
<entry align="left" valign="top"><para>104.0.1+up2.7.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/mirrored-neuvector-controller:5.3.4<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.4<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-manager:5.3.4<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.4<?asciidoc-br?>
registry.suse.com/rancher
mirrored-neuvector-registry-adapter:0.1.1-s1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-scanner:latest<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-updater:latest</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles (CAPI)</para></entry>
<entry align="left" valign="top"><para>0.11</para></entry>
<entry align="left" valign="top"><para>0.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/rancher-turtles-chart:0.3.2<?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.11.0<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-operator:0.12.0<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-controller:1.7.5<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-provider-metal3:1.7.1<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-provider-rke2-bootstrap:0.7.0<?asciidoc-br?>
registry.suse.com/edge/3.1/cluster-api-provider-rke2-controlplane:0.7.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>0.8.1</para></entry>
<entry align="left" valign="top"><para>0.8.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/metal3-chart:0.8.1<?asciidoc-br?>
registry.suse.com/edge/3.1/baremetal-operator:0.6.1<?asciidoc-br?>
registry.suse.com/edge/3.1/ip-address-manager:1.7.1<?asciidoc-br?>
registry.suse.com/edge/3.1/ironic:24.1.2.0<?asciidoc-br?>
registry.suse.com/edge/3.1/ironic-ipa-downloader:2.0.0<?asciidoc-br?>
registry.suse.com/edge/3.1/kube-rbac-proxy:v0.18.0<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/metallb-chart:0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.1/metallb-controller:v0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.1/metallb-speaker:v0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.1/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.1/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.6.4</para></entry>
<entry align="left" valign="top"><para>104.2.0+up1.6.4</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.6.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.6.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.6.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elementalダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>2.0.0</para></entry>
<entry align="left" valign="top"><para>2.0.0</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/2.1.0/charts/elemental/2.0.0">Elemental拡張機能チャート</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.1</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/edge-image-builder:1.1.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.3.1</para></entry>
<entry align="left" valign="top"><para>0.4.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/kubevirt-chart:0.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-operator:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-api:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-controller:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportproxy:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportserver:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-handler:1.3.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-launcher:1.3.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirtダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>1.1.0</para></entry>
<entry align="left" valign="top"><para>1.1.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/kubevirt-dashboard-extension-chart:1.1.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.60.1</para></entry>
<entry align="left" valign="top"><para>0.4.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/cdi-chart:0.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-operator:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-controller:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-importer:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-cloner:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-apiserver:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadserver:1.60.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadproxy:1.60.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>0.2.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/endpoint-copier-operator:v0.2.1<?asciidoc-br?>
registry.suse.com/edge/3.1/endpoint-copier-operator-chart:0.2.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (技術プレビュー)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/akri-chart:0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-dashboard-extension-chart:1.1.0<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.1/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SR-IOV Network Operator</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/sriov-network-operator-chart:1.3.0<?asciidoc-br?>
registry.suse.com/edge/3.1/sriov-crd-chart:1.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.13.4</para></entry>
<entry align="left" valign="top"><para>104.0.0+up0.7.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io">System Upgrade
Controllerチャート</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.13.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.1.0</para></entry>
<entry align="left" valign="top"><para>0.1.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.1/upgrade-controller-chart:0.1.0<?asciidoc-br?>
registry.suse.com/edge/3.1/upgrade-controller:0.1.0<?asciidoc-br?>
registry.suse.com/edge/3.1/kubectl:1.30.3<?asciidoc-br?>
registry.suse.com/edge/3.1/release-manifest:3.1.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-components-verification">
<title>コンポーネントの検証</title>
<para>上記のコンポーネントはSoftware Bill Of Materials
(SBOM)のデータを使用して検証できます。たとえば、以下に説明するように<literal>cosign</literal>を使用します。</para>
<para><link
xl:href="https://www.suse.com/support/security/keys/">SUSE署名キーのソース</link>からSUSE
Edge Containerの公開鍵をダウンロードします。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>コンテナイメージのハッシュを検証します。たとえば、<literal>crane</literal>を使用します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/3.1/baremetal-operator:0.6.1
sha256:cacd1496f59c47475f3cfc9774e647ef08ca0aa1c1e4a48e067901cf7635af8a</screen>
<para><literal>cosign</literal>を使用して検証します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.1/baremetal-operator@sha256:cacd1496f59c47475f3cfc9774e647ef08ca0aa1c1e4a48e067901cf7635af8a &gt; /dev/null
#
Verification for registry.suse.com/edge/3.1/baremetal-operator@sha256:cacd1496f59c47475f3cfc9774e647ef08ca0aa1c1e4a48e067901cf7635af8a --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The claims were present in the transparency log
  - The signatures were integrated into the transparency log when the certificate was valid
  - The signatures were verified against the specified public key</screen>
<para><link
xl:href="https://www.suse.com/support/security/sbom/">アップストリームドキュメント</link>の説明に従ってSBOMデータを抽出します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.1/baremetal-operator@sha256:cacd1496f59c47475f3cfc9774e647ef08ca0aa1c1e4a48e067901cf7635af8a | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>アップグレード手順</title>
<para>新しいリリースにアップグレードする方法の詳細については、「<xref linkend="day-2-operations"/>」を参照してください。</para>
<para>以下にEdge 3.0からアップグレードする際に注意すべき技術的な考慮事項についていくつか示します。</para>
<section xml:id="id-ssh-root-login-on-suse-linux-micro-6-0">
<title>SUSE Linux Micro 6.0でのSSHルートログイン</title>
<para>SUSE Linux Micro 5.5では、パスワードベースの認証を使用してSSHをルートとして接続できましたが、SUSE Linux Micro
6.0のみのキーベースの認証がデフォルトで許可されています。</para>
<para>5.xから6.0にアップグレードされたシステムは、古い動作を継続します。新しインストールで新しい動作が強制されます。</para>
<para>非ルートユーザを作成するか、キーベースの認証を使用することをお勧めしますが、必要に応じてパッケージ<literal>openssh-server-config-rootlogin</literal>
をインストールすると、古い動作がリストアされ、ルートユーザ用のパスワードベースのログインが許可されます。</para>
</section>
</section>
<section xml:id="id-known-limitations">
<title>既知の制限事項</title>
<para>特に記載のない限り、これらは3.1.0リリースおよびそれ以降のすべてのzストリームバージョンに適用されます。</para>
<itemizedlist>
<listitem>
<para>Akriは、技術プレビュー製品であり、標準的なサポート範囲の対象外です。</para>
</listitem>
<listitem>
<para>aarch64上のEdge Image Builderは、技術プレビュー製品であり、標準的なサポート範囲の対象外です。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-product-support-lifecycle">
<title>製品サポートライフサイクル</title>
<para>SUSE
Edgeは、SUSEが提供する定評あるサポートに支えられています。SUSEは、エンタープライズ品質のサポートサービスの提供において確固たる実績を誇るテクノロジリーダーです。詳細については、<link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>、およびサポートポリシーのページ(<link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>)を参照してください。サポートケースの作成、SUSEが重大度レベルを分類する方法、またはサポートの範囲について質問がある場合は、テクニカルサポートハンドブック(<link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>)を参照してください。</para>
<para>このマニュアルの発行時点では、SUSE
Edgeの各マイナーバージョン(「3.1」など)は12か月間の運用サポートでサポートされ、最初の6か月間は「完全サポート」、その後の6か月間は「保守サポート」が提供されます。「完全サポート」の対象期間中に、SUSEは新機能(既存の機能を損なわないもの)の導入や、バグ修正の投入、セキュリティパッチの提供を行う場合があります。「保守サポート」の期間中には、重大なセキュリティ修正とバグ修正のみが提供され、その他の修正はSUSEの裁量で提供されます。</para>
<para>明記されていない限り、記載されているコンポーネントはすべて一般提供(GA)とみなされ、SUSEの標準のサポート範囲の対象となります。一部のコンポーネントは「技術プレビュー」として記載されている場合があります。この場合、SUSEは評価のためにGA前の機能への早期アクセスをお客様に提供しますが、これらの機能には標準のサポートポリシーが適用されず、運用ユースケースには推奨されません。SUSEでは、技術プレビューのコンポーネントに関するフィードバックや、当該コンポーネントの改良についてのご提案を心からお待ちしております。しかし、機能がお客様のニーズを満たさない場合やSUSEが求める成熟度に達しない場合、一般提供になる前に技術プレビューの機能を廃止する権利を留保します。</para>
<para>SUSEは場合により、機能の廃止やAPIの仕様変更を行わなければならないことがあることに注意してください。機能の廃止やAPIの変更の理由としては、機能が新しい実装によって更新または置き換えられた、新しい機能セットが導入された、アップストリームの技術が利用できなくなった、アップストリームコミュニティによって互換性のない変更が導入された、などが考えられます。これは特定のマイナーリリース(x.z)内で発生することは意図されていないため、すべてのzストリームリリースではAPIの互換性と機能が維持されます。SUSEは、廃止に関する警告をリリースノート内で十分に余裕をもって提供し、併せて回避策、推奨事項、サービスの中断を最小限に抑える軽減策も提供するよう努めます。</para>
<para>SUSE Edgeチームはコミュニティからのフィードバックも歓迎しており、<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>の各コードリポジトリ内で問題を報告できます。</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>ソースコードの取得</title>
<para>このSUSE製品には、GNU General Public License
(GPL)やその他のさまざまなオープンソースライセンスの下でSUSEにライセンスされた素材が含まれます。SUSEはGPLに従ってGPLでライセンスされた素材に対応するソースコードを提供する必要があるほか、その他すべてのオープンソースライセンスの要件にも準拠します。よって、SUSEはすべてのソースコードを利用可能にしており、一般的にSUSE
Edge GitHubリポジトリ(<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>)にあります。また、依存コンポーネントについてはSUSE
Rancher GitHubリポジトリ(<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)にあり、特にSLE
Microについては<link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>の「Medium
2」でソースコードをダウンロードできます。</para>
</section>
<section xml:id="id-legal-notices">
<title>法的通知</title>
<para>SUSEは、この文書の内容や使用に関していかなる表明や保証も行いません。特に、商品性または特定目的への適合性に関する明示的または暗黙的な保証は一切行いません。さらに、SUSEは本書を改訂し、その内容に随時変更を加える権利を留保しますが、いかなる個人または団体に対しても当該の改訂または変更を通知する義務を負いません。</para>
<para>SUSEは、いかなるソフトウェアに関しても、いかなる表明や保証も行いません。特に、商品性または特定目的への適合性に関する明示的または暗黙的な保証は一切行いません。さらに、SUSEはSUSEソフトウェアのあらゆる部分に随時変更を加える権利を留保しますが、いかなる個人または団体に対しても当該の変更を通知する義務を負いません。</para>
<para>本契約の下で提供されるいかなる製品または技術情報も、米国の輸出管理法規および他国の貿易法の対象となる場合があります。お客様はすべての輸出管理規制を遵守し、成果物の輸出、再輸出、または輸入のために必要なライセンスまたは分類を取得することに同意します。お客様は、現行の米国輸出禁止リストに記載されている団体や米国輸出法に規定された禁輸国やテロ支援国への輸出や再輸出を行わないことに同意します。また、成果物を禁止されている核、ミサイル、または化学/生物兵器の最終用途に使用しないことにも同意します。SUSEソフトウェアの輸出に関する詳細情報については、<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>を参照してください。SUSEは、必要な輸出許可の取得を怠ったことに対する責任を一切負いません。</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>このリリースノート文書は、Creative Commons Attribution-NoDerivatives 4.0 International
License
(CC-BY-ND-4.0)の下でライセンスされています。お客様は、この文書と併せてライセンスのコピーを受け取っている必要があります。受け取っていない場合は、<link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>を参照してください。</para>
<para>SUSEは、本書で説明されている製品に組み込まれた技術に関連する知的財産権を有しています。これらの知的財産権には、特に、<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>に記載されている1つまたは複数の米国特許、ならびに米国およびその他の国における1つまたは複数のその他の特許または出願中の特許申請が含まれていることがありますが、これらに限定されません。</para>
<para>SUSEの商標については、SUSEの商標とサービスマークのリスト(<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>)を参照してください。第三者のすべての商標は各所有者の財産です。SUSEのブランド情報と使用要件については、<link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link>で公開されているガイドラインを参照してください。</para>
</section>
</chapter>
</part>
</book>
