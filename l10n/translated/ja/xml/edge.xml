<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xml:lang="ja-jp">
<info>
<title>SUSE Edgeドキュメント</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>2025年9月26日</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="suse-edge-documentation">
<title>SUSE Edge 3.4ドキュメント</title>
<para>『SUSE
Edgeドキュメント』をお読みいただきありがとうございます。このドキュメントには、高レベルアーキテクチャの概要、クイックスタートガイド、検証済みの設計、コンポーネントの使用に関するガイダンス、サードパーティ統合、エッジコンピューティングインフラストラクチャとワークロードを管理するためのベストプラクティスが記載されています。</para>
<section xml:id="id-what-is-suse-edge">
<title>SUSE Edgeとは</title>
<para>SUSE
Edgeは、インフラストラクチャとクラウドネイティブなアプリケーションをエッジにデプロイするという独自の課題に対処することに特化した、緊密に統合されて包括的に検証されたエンドツーエンドのソリューションです。SUSE
Edgeが重点を置いているのは、独創的でありながら高い柔軟性とスケーラビリティを持つセキュアなプラットフォームを提供し、初期デプロイメントイメージの構築からノードのプロビジョニングとオンボーディング、アプリケーションのデプロイメント、可観測性、ライフサイクル全体の運用にまで対応することです。このプラットフォームは、最良のオープンソースソフトウェアに基づいてゼロから構築されており、SUSEが持つ、30年にわたってセキュアで安定した定評あるSUSE
Linuxプラットフォームを提供してきた歴史と、Rancherポートフォリオによって拡張性に優れ機能豊富なKubernetes管理を提供してきた経験の両方に合致するものです。SUSE
Edgeは、これらの機能の上に構築されており、小売、医療、輸送、物流、通信、スマート製造、産業用IoTなど、さまざまな市場セグメントに対応できる機能を提供します。</para>
</section>
<section xml:id="id-design-philosophy">
<title>設計理念</title>
<para>このソリューションは、顧客の要件や期待はさまざまであるため「万能」なエッジプラットフォームは存在しないという考え方に基づいて設計されています。エッジデプロイメントにより、実に困難な問題を解決し、継続的に進化させることが要求されます。たとえば、大規模なスケーラビリティ、ネットワークの可用性の制限、物理的なスペースの制約、新たなセキュリティの脅威と攻撃ベクトル、ハードウェアアーキテクチャとシステムリソースのバリエーション、レガシインフラストラクチャやレガシアプリケーションのデプロイとインタフェースの要件、耐用年数を延長している顧客ソリューションといった課題があります。こうした課題の多くは、従来の考え方(たとえば、データセンター内やパブリッククラウドへのインフラストラクチャやアプリケーションのデプロイメント)とは異なるため、はるかに細かく設計を検討し、一般的な前提の多くを再検討する必要があります。</para>
<para>たとえば、SUSEはミニマリズム、モジュール性、操作のしやすさに価値を見出しています。システムは複雑化するほど故障しやすくなるため、エッジ環境ではミニマリズムが重要です。数百、数十万カ所に及ぶとなると、複雑なシステムは複雑な故障が発生します。また、SUSEのソリューションはモジュール性を備えているため、ユーザの選択肢を増やしながら、デプロイしたプラットフォームが不必要に複雑になることを解消できます。さらに、ミニマリズムおよびモジュール性と、操作のしやすさとのバランスを取ることも必要です。人間はプロセスを何千回も繰り返すとミスを犯す可能性があるため、プラットフォーム側で潜在的なミスを確実に回復し、技術者が現場に出向かなくても済むようにすると同時に、一貫性と標準化を実現するよう努める必要もあります。</para>
</section>
<section xml:id="id-high-level-architecture">
<title>高レベルアーキテクチャ</title>
<para>SUSE
Edgeの高レベルシステムアーキテクチャは、「管理」クラスタと「ダウンストリーム」クラスタの2つのコアカテゴリに分けられます。管理クラスタは1つまたは複数のダウンストリームクラスタのリモート管理を担当しますが、特定の状況下では、ダウンストリームクラスタはリモート管理なしで動作する必要があります。たとえば、エッジサイトに外部接続がない場合や独立して動作する必要がある場合などです。SUSE
Edgeでは、管理クラスタとダウンストリームクラスタの両方の動作に利用される技術コンポーネントは大部分が共通していますが、システム仕様と最上位に位置するアプリケーションが異なる場合があります。すなわち管理クラスタはシステム管理とライフサイクル操作を有効にするアプリケーションを実行しますが、ダウンストリームクラスタはユーザアプリケーションを提供するための要件を満たします。</para>
<section xml:id="id-components-used-in-suse-edge">
<title>SUSE Edgeで使用されるコンポーネント</title>
<para>SUSE
Edgeは、既存のSUSEとRancherのコンポーネントと、エッジコンピューティングに必要な制約や複雑さに対応できるようにEdgeチームが構築した追加機能とコンポーネントで構成されています。管理クラスタとダウンストリームクラスタの両方で使用されるコンポーネントは、高レベルなアーキテクチャ図とともに以下に説明しますが、これは網羅的なリストではないことに注意してください。</para>
<section xml:id="id-management-cluster">
<title>管理クラスタ</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-management-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>SUSE Edge管理クラスタ</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理</emphasis>:
これは、接続されたダウンストリームクラスタのプロビジョニングとライフサイクルの管理に使用されるSUSE
Edgeの中核です。管理クラスタには通常、以下のコンポーネントが含まれます。</para>
<itemizedlist>
<listitem>
<para>Rancher Prime (<xref
linkend="components-rancher"/>)によるマルチクラスタ管理により、ダウンストリームクラスタのオンボーディングとインフラストラクチャおよびアプリケーションの継続的なライフサイクル管理のための共通ダッシュボードが可能になり、包括的なテナント分離と<literal>IDP</literal>
(アイデンティティプロバイダ)統合、サードパーティの統合および拡張のための大規模なマーケットプレイス、ベンダニュートラルなAPIも提供されます。</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux
Managerを使用したLinuxシステム管理により、ダウンストリームクラスタ上で実行される基礎となるLinuxオペレーティングシステム(*SUSE
Linux Micro (<xref
linkend="components-slmicro"/>))の自動的なLinuxパッチおよび設定管理が可能になります。このコンポーネントはコンテナ化されていますが、現時点では他の管理コンポーネントとは別のシステムで実行する必要があるため、上の図では「Linux管理」とラベル付けされています。</para>
</listitem>
<listitem>
<para>特定のSUSE Edgeリリースへの管理クラスタコンポーネントのアップグレードを処理する専用のライフサイクル管理(<xref
linkend="components-upgrade-controller"/>)コントローラ。</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)を使用したRancher
Primeへのリモートシステムのオンボーディングにより、接続されたエッジノードを目的のKubernetesクラスタに遅延バインディングしたり、GitOps経由でアプリケーションをデプロイメントしたりできます。</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>)、MetalLB (<xref
linkend="components-metallb"/>)、および<literal>CAPI</literal> (Cluster
API)インフラストラクチャプロバイダによるオプションの完全なベアメタルライフサイクルおよび管理サポートにより、リモート管理機能を備えたベアメタルシステムの完全なエンドツーエンドのプロビジョニングが可能になります。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタとそれらに存在するアプリケーションのプロビジョニングとライフサイクルの管理のためのFleet (<xref
linkend="components-fleet"/>)と呼ばれるオプションのGitOpsエンジン。</para>
</listitem>
<listitem>
<para>管理クラスタ自体を支えるのは、ベースオペレーティングシステムとしてのSUSE Linux Micro (<xref
linkend="components-slmicro"/>)と、管理クラスタアプリケーションをサポートするKubernetesディストリビューションとしてのRKE2
(<xref linkend="components-rke2"/>)です。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-downstream-clusters">
<title>ダウンストリームクラスタ</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-downstream-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>SUSE Edgeダウンストリームクラスタ</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ダウンストリーム</emphasis>:
これは、エッジでユーザワークロードを実行するために使用されるSUSE
Edgeの分散部分です。つまり、エッジの場所自体で実行されるソフトウェアであり、通常は次のコンポーネントで構成されます。</para>
<itemizedlist>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)やRKE2 (<xref
linkend="components-rke2"/>)などのセキュアで軽量なディストリビューションを含む、Kubernetesディストリビューションの選択肢(<literal>RKE2</literal>は、政府機関や規制産業での使用に耐えるように強化、認定、最適化されています)。</para>
</listitem>
<listitem>
<para>SUSE Security (<xref
linkend="components-suse-security"/>)を使用するとイメージ脆弱性スキャン、ディープパケットインスペクション、リアルタイム脅威および脆弱性保護のようなセキュリティ機能が有効になります。</para>
</listitem>
<listitem>
<para>SUSE Storage (<xref
linkend="components-suse-storage"/>)によるソフトウェアブロックストレージにより、軽量で永続的、弾力性があり、拡張可能なブロックストレージが可能になります。</para>
</listitem>
<listitem>
<para>SUSE Linux Micro (<xref
linkend="components-slmicro"/>)を搭載した、軽量でコンテナに最適化された堅牢なLinuxオペレーティングシステムで、エッジでのコンテナや仮想マシンの実行に不変で耐障害性に優れたOSを提供します。SUSE
Linux Microは、AArch64およびAMD64/Intel
64アーキテクチャの両方で使用でき、レイテンシの影響を受けやすいアプリケーション(通信事業者のユースケースなど)向けの<literal>リアルタイムカーネル</literal>もサポートしています。</para>
</listitem>
<listitem>
<para>接続されたクラスタ(つまり、管理クラスタに接続しているクラスタ)には、Rancher Primeへの接続を管理するためのRancher System
Agentと、SUSE Multi-Linux
Managerからの指示を受けてLinuxソフトウェアの更新を適用するためのvenv-salt-minionの2つのエージェントがデプロイされます。これらのエージェントは、切断されたクラスタの管理には必要ありません。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-connectivity">
<title>接続</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-connected-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>SUSE Edge接続アーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
<para>上記のイメージは、<emphasis
role="strong">接続された</emphasis>ダウンストリームクラスタと、それらの管理クラスタへの接続に関する高レベルアーキテクチャの概要を示しています。管理クラスタは、ダウンストリームクラスタとターゲット管理クラスタとの間のネットワーキングの可用性に応じて、オンプレミスとクラウドの両方の容量で、さまざまな基礎となるインフラストラクチャプラットフォーム上にデプロイできます。これが機能するための唯一の要件は、ダウンストリームクラスタノードを管理インフラストラクチャに接続するネットワークでアクセス可能なAPIとコールバックURLです。</para>
<para>この接続が確立されるメカニズムは、ダウンストリームクラスタのデプロイメントのメカニズムとは異なるものであることを認識することが重要です。この詳細については、次のセクションでさらに詳しく説明しますが、基本的な理解を深めるために、接続されたダウンストリームクラスタが「管理」クラスタとして確立される主なメカニズムは3つあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ダウンストリームクラスタはまず「切断された」容量でデプロイされ(Edge Image Builder (<xref
linkend="components-eib"/>)経由)、接続が許可されると、管理クラスタにインポートされます。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタは、組み込みオンボーディングメカニズム(たとえばElemental (<xref
linkend="components-elemental"/>)経由)を使用するように設定され、初回ブート時に管理クラスタに自動的に登録されるため、クラスタ設定の遅延バインディングが許可されます。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタにはベアメタル管理機能(CAPI +
Metal<superscript>3</superscript>)がプロビジョニングされており、クラスタがデプロイされ、設定されると(Rancher
Turtlesオペレータ経由)管理クラスタに自動的にインポートされます。</para>
</listitem>
</orderedlist>
<note>
<para>大規模なデプロイメントの規模に対応し、地理的に分散した環境における帯域幅とレイテンシの問題を最適化し、停止時や管理クラスタのアップグレード時の混乱を最小限に抑えるために、複数の管理クラスタを実装することが推奨されます。現在の管理クラスタのスケーラビリティの限界とシステム要件については、<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements">こちら</link>をご覧ください。</para>
</note>
</section>
</section>
<section xml:id="id-common-edge-deployment-patterns">
<title>一般的なEdgeデプロイメントパターン</title>
<para>動作環境とライフサイクル要件はさまざまであるため、SUSEでは、SUSE
Edgeを運用する市場セグメントやユースケースに大まかに一致する別個のデプロイメントパターンを多数サポートしています。また、これらの各デプロイメントパターンに対応するクイックスタートガイドを作成し、ユーザのニーズに基づいてSUSE
Edgeプラットフォームに習熟できるようにしています。以下に、現在サポートされている3つのデプロイメントパターンを、各クイックスタートページへのリンクとともに説明します。</para>
<section xml:id="id-directed-network-provisioning">
<title>ダイレクトネットワークプロビジョニング</title>
<para>ダイレクトネットワークプロビジョニングでは、デプロイ先のハードウェアの詳細がわかっている場合に、アウトオブバンド管理インタフェースに直接アクセスして、プロビジョニングプロセス全体をオーケストレーションして自動化します。このシナリオで顧客が期待するソリューションとは、エッジサイトを一元的な場所から完全に自動化してプロビジョニングすることができ、ブートイメージの作成をはるかに上回る機能を備えていて、エッジロケーションでの手動操作を最小限に抑えられるソリューションです。つまり、ラックに搭載して電源をオンにし、必要なネットワークを物理ハードウェアに接続するだけで、自動化プロセスによってアウトオブバンド管理(Redfish
APIなど)を介してマシンの電源が投入され、ユーザの介入なしにインフラストラクチャのプロビジョニング、オンボーディング、デプロイメントが処理されるソリューションです。これが機能するための鍵は、管理者がシステムを把握している、つまりどのハードウェアがどこにあるかを管理者が把握していることと、デプロイメントが中央で処理されることが想定されていることです。</para>
<para>このソリューションは最も堅牢です。管理者がハードウェアの管理インタフェースを直接操作して既知のハードウェアを扱うことに加え、ネットワークの利用可否に対する制約が少ないためです。機能面では、このソリューションは、Cluster
APIとMetal<superscript>3</superscript>を広範に使用して、ベアメタルからオペレーティングシステム、Kubernetes、階層化アプリケーションまでを自動プロビジョニングし、デプロイメント後にSUSE
Edgeの他の一般的なライフサイクル管理機能にリンクする機能を提供します。このソリューションのクイックスタートについては、<xref
linkend="quickstart-metal3"/>を参照してください。</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>「Phone Home」ネットワークプロビジョニング</title>
<para>場合によっては、中央管理クラスタでハードウェアを直接管理できない環境で運用することがあります(たとえば、リモートネットワークがファイアウォールの背後にある場合や、アウトオブバンド管理インタフェースがない場合などがあり、エッジでよく見られる「PC」タイプのハードウェアで一般的です)。このシナリオの場合のために、SUSEでは、ハードウェアのブートストラップ時にその配置先がわかっていなくても、クラスタとそのワークロードをリモートでプロビジョニングできるツールを提供しています。エッジコンピューティングについて考える場合、ほとんどの人はこう考えます。エッジコンピューティングとは、不明な部分がある数千あるいは数万台のシステムがエッジロケーションで起動し、安全にPhone
Home通信を行い、そのシステムの身元を検証し、実行すべき処理についての指示を受信することです。ここで要件として期待されるのは、工場でマシンを事前イメージングしたり、USBなどでブートイメージをアタッチしたりする以外には、ユーザがほとんど介入しなくてもプロビジョニングとライフサイクル管理ができることです。この領域での主な課題は、こうしたデバイスの規模、一貫性、セキュリティ、ライフサイクルに対処することです。</para>
<para>このソリューションでは、非常に柔軟で一貫性のある方法でシステムをプロビジョニングおよびオンボーディングできます。システムの場所、タイプや仕様、初回電源投入日時などは問いません。SUSE
Edgeでは、Edge Image
Builderを使用してシステムを非常に柔軟にカスタマイズできます。また、ノードのオンボーディングとKubernetesのプロビジョニングにはRancherのElementalが提供する登録機能を活用するとともに、オペレーティングシステムへのパッチの適用にはSUSE
Multi-Linux Managerを活用します。このソリューションのクイックスタートについては、<xref
linkend="quickstart-elemental"/>を参照してください。</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>イメージベースのプロビジョニング</title>
<para>スタンドアロン環境、エアギャップ環境、またはネットワークが制限された環境で運用する必要があるお客様向けに、SUSE
Edgeでは、必要なデプロイメントアーティファクトがすべて含まれる、完全にカスタマイズされたインストールメディアを生成できるソリューションを提供しています。これにより、シングルノードとマルチノード両方の高可用性Kubernetesクラスタを、必要なワークロードと追加の階層化コンポーネントを含めてエッジに設定できます。これはすべて、外部とのネットワーク接続や集中管理プラットフォームの介入なしに行うことができます。ユーザエクスペリエンスは、インストールメディアをターゲットシステムに提供するという点では「Phone
Home」ソリューションによく似ていますが、このソリューションは「インプレースでブートストラップ」する点が異なります。このシナリオでは、生成されたクラスタをRancherに接続して継続的に管理する(つまり、大幅な再設定や再デプロイメントなしに、「非接続」動作モードから「接続」動作モードに移行する)ことも、分離した状態のまま動作を続行することもできます。どちらの場合も、一貫した同じメカニズムを適用してライフサイクル操作を自動化できることに注意してください。</para>
<para>さらに、このソリューションを使用すると、「ダイレクトネットワークプロビジョニング」モデルと「Phone
Homeネットワークプロビジョニング」モデルの両方をサポートする集中型インフラストラクチャをホストできる管理クラスタを迅速に作成することもできます。この方法では、あらゆるタイプのエッジインフラストラクチャを最も迅速・簡単にプロビジョニングできます。このソリューションでは、SUSE
Edge Image
Builderの機能を多用して、完全にカスタマイズされた無人インストールメディアを作成します。クイックスタートについては、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</section>
</section>
<section xml:id="id-suse-edge-stack-validation">
<title>SUSE Edge Stack Validation</title>
<para>すべてのSUSE Edgeリリースは、緊密に統合され、徹底的に検証されたコンポーネントで構成されており、1
つのバージョンとして管理されています。コンポーネント間の統合をテストするだけでなく、強制的な障害シナリオ下でシステムが期待どおりに動作することを保証する継続的な統合とスタック検証の一環として、SUSE
Edgeチームはすべてのテスト実行と結果を公開しています。結果とすべての入力パラメータは<link
xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link>でご確認いただけます。</para>
</section>
<section xml:id="id-full-component-list">
<title>コンポーネントの全リスト</title>
<para>コンポーネントの全リストと、各コンポーネントの概要説明へのリンク、およびSUSE Edgeでの使用方法については、以下をご覧ください。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Rancher Dashboard拡張機能(<xref
linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>Rancher Turtles (<xref linkend="components-rancher-turtles"/>)</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux Manager</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>SUSE Storage (<xref linkend="components-suse-storage"/>)</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>)</para>
</listitem>
<listitem>
<para>Upgrade Controller (<xref linkend="components-upgrade-controller"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>クイックスタート</title>
<partintro>
<para>クイックスタートはこちら</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>Metal<superscript>3</superscript>を使用したBMCの自動デプロイメント</title>
<para>Metal<superscript>3</superscript>は、Kubernetesにベアメタルインフラストラクチャ管理機能を提供する<link
xl:href="https://metal3.io/">CNCFプロジェクト</link>です。</para>
<para>Metal<superscript>3</superscript>は、<link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>などのアウトオブバンドプロトコルを介した管理をサポートするベアメタルサーバのライフサイクルを管理するためのKubernetesネイティブリソースを提供します。</para>
<para>また、<link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link>も十分にサポートされており、広く採用されているベンダニュートラルなAPIを使用して、複数のインフラストラクチャプロバイダにわたってインフラストラクチャリソースを管理できます。</para>
<section xml:id="id-why-use-this-method">
<title>この方法を使用する理由</title>
<para>この方法は、ターゲットハードウェアがアウトオブバンド管理をサポートしていて、完全に自動化されたインフラストラクチャ管理フローが望まれるシナリオで役立ちます。</para>
<para>管理クラスタは宣言型APIを提供するように設定されており、このAPIによってダウンストリームクラスタのベアメタルサーバのインベントリと状態を管理できます。これには、自動検査、クリーニング、プロビジョニング/プロビジョニング解除も含まれます。</para>
</section>
<section xml:id="id-high-level-architecture-2">
<title>高レベルアーキテクチャ</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>クイックスタートmetal3アーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>前提条件</title>
<para>ダウンストリームクラスタのサーバハードウェアとネットワーキングに関連する固有の制約がいくつかあります。</para>
<itemizedlist>
<listitem>
<para>管理クラスタ</para>
<itemizedlist>
<listitem>
<para>ターゲットサーバ管理/BMC APIへのネットワーク接続が必要</para>
</listitem>
<listitem>
<para>ターゲットサーバのコントロールプレーンネットワークへのネットワーク接続が必要</para>
</listitem>
<listitem>
<para>マルチノード管理クラスタの場合、追加の予約済みIPアドレスが必要</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>制御対象ホスト</para>
<itemizedlist>
<listitem>
<para>Redfish、iDRAC、またはiLOのインタフェースを介したアウトオブバンド管理のサポートが必要</para>
</listitem>
<listitem>
<para>仮想メディアを使用したデプロイメントのサポートが必要(PXEは現在サポートされていない)</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>プロビジョニングAPIにアクセスするために管理クラスタへのネットワーク接続が必要</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>ツールがいくつか必要です。ツールは管理クラスタにインストールするか、管理クラスタにアクセス可能なホストにインストールできます。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://helm.sh">Helm</link>、および<link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para><link xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイム</para>
</listitem>
</itemizedlist>
<para><literal>SL-Micro.x86_64-6.1-Base-GM.raw</literal> OSイメージファイルは<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
</section>
<section xml:id="id-deployment">
<title>デプロイメント</title>
<section xml:id="id-setup-management-cluster">
<title>管理クラスタのセットアップ</title>
<para>管理クラスタをインストールし、Metal<superscript>3</superscript>を使用する基本的な手順は次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>RKE2管理クラスタをインストールします。</para>
</listitem>
<listitem>
<para>Rancherのインストール</para>
</listitem>
<listitem>
<para>ストレージプロバイダをインストールします(オプション)。</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>の依存関係をインストールします。</para>
</listitem>
<listitem>
<para>Rancher Turtles経由でCAPIの依存関係をインストールします。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタホスト用のSLEMicro OSイメージを構築します。</para>
</listitem>
<listitem>
<para>BareMetalHost CRを登録し、ベアメタルのインベントリを定義します。</para>
</listitem>
<listitem>
<para>CAPIリソースを定義して、ダウンストリームクラスタを作成します。</para>
</listitem>
</orderedlist>
<para>このガイドでは、既存のRKE2クラスタとRancher (cert-managerを含む)が、たとえばEdge Image Builder (<xref
linkend="components-eib"/>)を使用してインストールされていることを前提としています。</para>
<tip>
<para>ここでの手順は、管理クラスタのドキュメント(<xref
linkend="atip-management-cluster"/>)で説明されているように、完全に自動化することもできます。</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Metal<superscript>3</superscript>の依存関係のインストール</title>
<para>cert-managerがまだRancherのインストールの一部としてインストールされていない場合は、cert-managerをインストールして実行する必要があります。</para>
<para>永続ストレージプロバイダをインストールする必要があります。SUSE
Storageを推奨しますが、開発/PoC環境では<literal>local-path-provisioner</literal>を使用することもできます。以下の手順は、StorageClassが<link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">デフォルトとしてマーク</link>されていることを前提としています。マークされていない場合は、Metal<superscript>3</superscript>チャートに追加の設定が必要です。</para>
<para>追加のIPが必要です。このIPは<link
xl:href="https://metallb.universe.tf/">MetalLB</link>によって管理され、Metal<superscript>3</superscript>管理サービスに一貫したエンドポイントを提供します。このIPは、コントロールプレーンサブネットに属していて、静的設定用に予約されている必要があります(どのDHCPプールにも属していてはなりません)。</para>
<tip>
<para>管理クラスタがシングルノードである場合、MetalLBを介して管理されるフローティングIPを追加する必要はありません。<xref
linkend="id-single-node-configuration"/>を参照してください。</para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>まず、MetalLBをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>続いて、次のように、<literal>STATIC_IRONIC_IP</literal>として定義された予約済みIPを使用して
、<literal>IPAddressPool</literal>と <literal>L2Advertisement</literal>を定義します。</para>
<screen language="bash" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>これでMetal<superscript>3</superscript>をインストールできます。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/charts/metal3 \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="$STATIC_IRONIC_IP"</screen>
</listitem>
<listitem>
<para>initコンテナがこのデプロイメントで実行されるまで約2分かかる場合があるため、続行する前にPodがすべて実行されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para><literal>metal3-system</literal>ネームスペースのすべてのPodが実行されるまで、以下の手順に進まないでください。</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Cluster APIの依存関係のインストール</title>
<para>Cluster APIの依存関係は、Rancher Turtles Helmチャートで管理されます。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/charts/rancher-turtles \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</screen>
<para>しばらくすると、コントローラPodが<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>、および<literal>rke2-control-plane-system</literal>の各ネームスペースで実行されているはずです。</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>ダウンストリームクラスタイメージの準備</title>
<para>Kiwi (<xref linkend="guides-kiwi-builder-images"/>)とEdge Image Builder
(<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroゴールデンイメージを準備します。</para>
<para>このガイドではダウンストリームクラスタをデプロイするために必要な最小限の設定について説明します。</para>
<section xml:id="id-image-configuration">
<title>イメージの設定</title>
<note>
<para>クラスタの作成に必要な最初の手順として、まず<xref
linkend="guides-kiwi-builder-images"/>に従って、新しいイメージを構築してください。</para>
</note>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたゴールデンイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="metal3-add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>custom/scriptsディレクトリには、初回ブート時に実行するスクリプトが含まれます。現在、デプロイメントのOSルートパーティションのサイズを変更するには、<literal>01-fix-growfs.sh</literal>スクリプトが必要です。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-config.yaml</literal>ファイルは、ダウンストリームクラスタイメージの主要な設定ファイルです。次に、Metal<superscript>3</superscript>を介したデプロイメントの最小例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
  sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>ここで、<literal>$SCC_REGISTRATION_CODE</literal>は<link
xl:href="https://scc.suse.com/">SUSE Customer
Center</link>からコピーした登録コードで、パッケージリストには必要な<literal>jq</literal>が含まれています。</para>
<para><literal>$ROOT_PASSWORD</literal>はルートユーザの暗号化パスワードで、テスト/デバッグに役立ちます。このパスワードは、<literal>openssl
passwd -6 PASSWORD</literal>コマンドで生成できます。</para>
<para>運用環境では、<literal>$USERKEY1</literal>を実際のSSHキーに置き換えて、usersブロックに追加できるSSHキーを使用することをお勧めします。</para>
<note>
<para><literal>net.ifnames=1</literal>は、<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link>を有効にします。</para>
<para>これはMetal<superscript>3</superscript>チャートのデフォルト設定と一致しますが、この設定は、設定されたチャートの<literal>predictableNicNames</literal>の値と一致する必要があります。</para>
<para>また、<literal>ignition.platform.id=openstack</literal>は必須であり、この引数がないと、Metal<superscript>3</superscript>の自動化フローでIgnitionによるSUSE
Linux Microの設定が失敗することにも注意してください。</para>
<para><literal>time</literal>
セクションはオプションですが、証明書とクロックスキューに関する潜在的な問題を回避するように設定することを強くお勧めします。この例で指定されている値は説明のみを目的としています。ご自身の特定の要件に合わせて調整してください。</para>
</note>
</section>
<section xml:id="growfs-script">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト(<literal>custom/scripts/01-fix-growfs.sh</literal>)が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>同じアプローチを使用して、プロビジョニングプロセス中に実行する独自のカスタムスクリプトを追加します。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>イメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>これにより、上記の定義に基づいて、<literal>SLE-Micro-eib-output.raw</literal>という名前の出力イメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、Metal3チャートを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
<note>
<para>EIBイメージをダウンストリームクラスタにデプロイする際は、<literal>Metal3MachineTemplate</literal>オブジェクトにイメージのSHA256ハッシュ値を含める必要があります。これは次のように生成できます:</para>
<screen language="shell" linenumbering="unnumbered">sha256sum &lt;image_file&gt; &gt; &lt;image_file&gt;.sha256
# On this example:
sha256sum SLE-Micro-eib-output.raw &gt; SLE-Micro-eib-output.raw.sha256</screen>
</note>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>BareMetalHostインベントリの追加</title>
<para>自動デプロイメント用にベアメタルサーバを登録するには、リソースを2つ作成する必要があります。BMCアクセス資格情報を保存するシークレットと、BMC接続とその他の詳細を定義するMetal<superscript>3</superscript>
BareMetalHostリソースです。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>次の点に注意してください。</para>
<itemizedlist>
<listitem>
<para>シークレットのユーザ名/パスワードはbase64でエンコードされている必要があります。また、末尾に改行を含めないでください(たとえば、単なる<literal>echo</literal>ではなく、<literal>echo
‑n</literal>を使用してください)。</para>
</listitem>
<listitem>
<para><literal>cluster-role</literal>ラベルは、この時点で設定することも、後でクラスタの作成時に設定することもできます。以下の例では、<literal>control-plane</literal>または<literal>worker</literal>を想定しています。</para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal>は、ホストのコントロールプレーンNICに一致する有効なMACである必要があります。</para>
</listitem>
<listitem>
<para><literal>bmc</literal>のアドレスはBMC管理APIへの接続です。次のアドレスがサポートされています。</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/&lt;SYSTEM ID&gt;</literal>:
Redfish仮想メディア(たとえば、SuperMicro)</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/System.Embedded.1</literal>: Dell iDRAC</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>BareMetalHost APIの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">アップストリームのAPIドキュメント</link>を参照してください。</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>静的IPの設定</title>
<para>上記のBareMetalHostの例では、DHCPでコントロールプレーンネットワークの設定を提供することを想定していますが、静的IPなどの手動設定が必要なシナリオでは、以下に説明するように追加の設定を指定できます。</para>
<section xml:id="metal3-add-network-eib">
<title>静的ネットワーク設定用の追加スクリプト</title>
<para>Edge Image
Builderでゴールデンイメージを作成する際には、<literal>network</literal>フォルダ内に次の<literal>configure-network.sh</literal>ファイルを作成します。</para>
<para>このファイルにより、初回ブート時に設定ドライブのデータを使用して、<link
xl:href="https://github.com/suse-edge/nm-configurator">NM
Configuratorツール</link>を使ってホストネットワーキングを設定します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>ホストネットワーク設定の追加シークレット</title>
<para>NM Configurator (<xref linkend="components-nmc"/>)でサポートされている<link
xl:href="https://nmstate.io/">nmstate</link>形式のデータを含む追加シークレットをホストごとに定義できます。</para>
<para>その後、このシークレットは、<literal>BareMetalHost</literal>リソースで<literal>preprovisioningNetworkDataName</literal>の指定フィールドを使用して参照されます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>状況によっては、MACアドレスが省略される場合があります。詳細については、<xref
linkend="networking-unified"/>を参照してください。</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>BareMetalHostの準備</title>
<para>上記の説明に従ってBareMetalHostリソースと関連するシークレットを作成すると、次のようにホスト準備ワークフローがトリガされます。</para>
<itemizedlist>
<listitem>
<para>ターゲットホストのBMCに接続された仮想メディアによってramdiskイメージがブートされる</para>
</listitem>
<listitem>
<para>ramdiskがハードウェア詳細を検査し、ホストをプロビジョニング用に準備する(たとえば、ディスクから以前のデータを消去する)</para>
</listitem>
<listitem>
<para>このプロセスが完了すると、BareMetalHostの<literal>status.hardware</literal>フィールドのハードウェア詳細が更新され、検証可能になる</para>
</listitem>
</itemizedlist>
<para>このプロセスには数分かかる場合がありますが、完了すると、BareMetalHostの状態が<literal>available</literal>になります。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>ダウンストリームクラスタの作成</title>
<para>続いて、ダウンストリームクラスタを定義するCluster
APIリソースと、BareMetalHostリソースをプロビジョニングしてからブートストラップを実行してRKE2クラスタを形成するマシンリソースを作成します。</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>コントロールプレーンのデプロイメント</title>
<para>コントロールプレーンをデプロイするために、以下のリソースを含む次のようなyamlマニフェストを定義します。</para>
<itemizedlist>
<listitem>
<para>クラスタリソースでは、クラスタ名、ネットワーク、およびコントロールプレーン/インフラストラクチャプロバイダのタイプ(この場合はRKE2/Metal3)を定義します。</para>
</listitem>
<listitem>
<para>Metal3Clusterでは、コントロールプレーンのエンドポイント(シングルノードの場合はホストIP、マルチノードの場合はLoadBalancerエンドポイント。この例ではシングルノードを想定)を定義します。</para>
</listitem>
<listitem>
<para>RKE2ControlPlaneでは、RKE2のバージョンと、クラスタのブートストラップ時に必要な追加設定を定義します。</para>
</listitem>
<listitem>
<para>Metal3MachineTemplateではBareMetalHostリソースに適用するOSイメージを定義し、hostSelectorでは使用するBareMetalHostを定義します。</para>
</listitem>
<listitem>
<para>Metal3DataTemplateでは、BareMetalHostに渡す追加のメタデータを定義します(networkDataは現在のところEdgeソリューションではサポートされていないことに注意してください)。</para>
</listitem>
</itemizedlist>
<note>
<para>簡潔にするため、この例では、BareMetalHostのIPアドレスが<literal>192.168.125.200</literal>で設定されている単一ノードのコントロールプレーンを想定しています。より高度なマルチノードの例については、<xref
linkend="atip-automated-provisioning"/>を参照してください。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>自身の環境に合わせて調整したら、<literal>kubectl</literal>を使用して例を適用し、その後
<literal>clusterctl</literal>を使用してクラスタのステータスを監視できます。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>ワーカー/コンピュートのデプロイメント</title>
<para>コントロールプレーンのデプロイメントと同様に、次のリソースを含むYAMLマニフェストを定義します。</para>
<itemizedlist>
<listitem>
<para>MachineDeploymentでは、レプリカ(ホスト)の数とブートストラップ/インフラストラクチャプロバイダ(この場合はRKE2/Metal3)を定義します。</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplateでは、エージェントホストのブートストラップ用のRKE2のバージョンと初回ブート設定を記述します。</para>
</listitem>
<listitem>
<para>Metal3MachineTemplateではBareMetalHostリソースに適用するOSイメージを定義し、hostSelectorでは使用するBareMetalHostを定義します。</para>
</listitem>
<listitem>
<para>Metal3DataTemplateでは、BareMetalHostに渡す追加のメタデータを定義します(<literal>networkData</literal>は現在サポートされていないことに注意してください)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.33.3+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.33.3+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>上記の例をコピーし、自身の環境に合わせて調整したら、<literal>kubectl</literal>を使用して適用し、<literal>clusterctl</literal>でクラスタのステータスを監視できます。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait for the worker nodes to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>クラスタのプロビジョニング解除</title>
<para>ダウンストリームクラスタをプロビジョニング解除するには、上記の作成手順で適用したリソースを削除します。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>これにより、BareMetalHostリソースのプロビジョニング解除がトリガされます。これには数分かかることがあり、その後リソースは再び利用可能な状態になります。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現在、アップストリームの<link
xl:href="https://github.com/metal3-io/ip-address-manager">IPアドレス管理コントローラ</link>はサポートされていません。このコントローラには、SLEMicroで選択されているネットワーク設定ツールと初回ブートツールチェーンとの互換性がまだないためです。</para>
</listitem>
<listitem>
<para>関連して、 IPAMリソースと、Metal3DataTemplateのnetworkDataフィールドは現在のところサポートされていません。</para>
</listitem>
<listitem>
<para>redfish-virtualmediaを介したデプロイメントのみが現在サポートされています。</para>
</listitem>
<listitem>
<para>Ironic Python Agent (IPA)とターゲットオペレーティングシステム(SL Micro
6.0/6.1)の間で、特に、デバイスに予測可能な名前を設定しようとする際に、ネットワークデバイス名の不整合が発生する可能性があります。</para>
</listitem>
</itemizedlist>
<para>これが発生するのは、Ironic Python Agent (IPA)のカーネルが現在、ターゲットオペレーティングシステム(SL Micro
6.0/6.1)のカーネルと整合していないためです。これにより、ネットワークドライバに不整合が生じ、SL
Microが想定する命名パターンとは異なる命名パターンでIPAがネットワークデバイスを検出できるようになります。</para>
<para>当面の回避策として、以下の2つの異なるアプローチが利用可能です。*
ネットワーク設定で2つの異なるシークレットを作成します。1つはIPAが検出するデバイス名を使用したIPAで使用されるシークレットで、<literal>BareMetalHost</literal>定義の<literal>preprovisioningNetworkDataName</literal>として使用します。もう1つはSL
Microが検出するデバイス名を使用したシークレットで、<literal>BareMetalHost</literal>定義の<literal>networkData.name</literal>として使用します。*
代わりに、生成されたnmconnectionファイル上の他のインタフェースを参照するためにUUIDを使用します。詳細は、<link
xl:href="..tips/metal3.adoc">ヒントとコツ</link>セクションを参照してください。</para>
</section>
<section xml:id="id-planned-changes">
<title>予定されている変更</title>
<itemizedlist>
<listitem>
<para>networkDataフィールドを使用した、IPAMリソースと設定のサポートの有効化</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>追加のリソース</title>
<para>SUSE Telco Cloudドキュメント(<xref
linkend="atip"/>)には、通信事業者のユースケースにおけるMetal<superscript>3</superscript>のより高度な使用例が記載されています。</para>
<section xml:id="id-single-node-configuration">
<title>シングルノード設定</title>
<para>管理クラスタがシングルノードであるテスト/PoC環境では、MetalLBを介して管理されるフローティングIPを追加する必要はありません。</para>
<para>このモードでは、管理クラスタAPIのエンドポイントが管理クラスタのIPになるため、DHCPを使用している場合はそのIPを予約するか、管理クラスタのIPが変更されないように静的に設定する必要があります(以下では<literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal>と表記しています)。</para>
<para>このシナリオを有効にするために必要なMetal<superscript>3</superscript>チャートの値は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="disabling-tls-for-virtualmedia-iso-attachment">
<title>仮想メディアISOをアタッチするためのTLSの無効化</title>
<para>一部のサーバベンダは、仮想メディアISOイメージをBMCにアタッチする際にSSL接続を検証しますが、Metal<superscript>3</superscript>のデプロイメント用に生成された証明書は自己署名されているため、問題が発生する可能性があります。この問題を回避するには、次のようなMetal<superscript>3</superscript>チャートの値を使用して、仮想メディアディスクをアタッチする場合にのみTLSを無効にすることができます。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>別の解決策は、CA証明書を使用してBMCを設定することです。この場合、<literal>kubectl</literal>を使用してクラスタから証明書を読み込むことができます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>これにより、証明書をサーバのBMCコンソールで設定できますが、そのプロセスはベンダ固有です(すべてのベンダで可能というわけではなく、可能でない場合は<literal>enable_vmedia_tls</literal>フラグが必要なことがあります)。</para>
</section>
<section xml:id="id-storage-configuration">
<title>ストレージ設定</title>
<para>管理クラスタがシングルノードであるテスト/PoC環境では、永続ストレージは不要ですが、運用ユースケースでは管理クラスタにSUSE Storage
(Longhorn)をインストールすることをお勧めします。これによりPodの再起動/再スケジュール時にMetal<superscript>3</superscript>に関連するイメージを保持できます。</para>
<para>この永続ストレージを有効にするために必要なMetal<superscript>3</superscript>チャート値は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">metal3-ironic:
  persistence:
    ironic:
      size: "5Gi"</screen>
<para>SUSE Telco Cloud管理クラスタのドキュメント(<xref
linkend="atip-management-cluster"/>)には、永続ストレージを使用した管理クラスタの設定方法に関する詳細が記載されています。</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Elementalを使用したリモートホストのオンボーディング</title>
<para>このセクションでは、SUSE Edgeの一部としての「Phone
Homeネットワークプロビジョニング」ソリューションについて説明します。このソリューションは、Elementalを使用してノードのオンボーディングを支援します。Elementalは、Kubernetesを使用してリモートホスト登録と一元化された完全なクラウドネイティブOS管理を可能にするソフトウェアスタックです。SUSE
Edgeスタックでは、Elementalの登録機能を使用して、リモートホストをRancherにオンボーディングできます。これにより、ホストを集中管理プラットフォームに統合し、そこからKubernetesクラスタに加えて、階層化コンポーネント、アプリケーション、およびそのライフサイクルをすべて共通の場所からデプロイおよび管理できるようになります。</para>
<para>このアプローチが役立つシナリオとしては、制御するデバイスが管理クラスタと同じネットワーク上にないか、アウトオブバンド管理コントローラが搭載されておらず、より直接的に制御できない場合や、さまざまな「不明」なシステムをエッジで多数ブートしており、それらを安全にオンボーディングして大規模に管理する必要がある場合が考えられます。これは、小売や産業用IoTなど、デバイスが設置されるネットワークをほとんど制御できない分野のユースケースによく見られるシナリオです。</para>
<section xml:id="id-high-level-architecture-3">
<title>高レベルアーキテクチャ</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>クイックスタートElementalアーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>必要なリソース</title>
<para>このクイックスタートを実行するためのシステムと環境の最小要件を次に示します。</para>
<itemizedlist>
<listitem>
<para>集中管理クラスタ(RancherとElementalをホストするクラスタ)用のホスト:</para>
<itemizedlist>
<listitem>
<para>開発またはテスト用の場合、最小8GBのRAMと20GBのディスク容量(運用環境での使用については<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements#hardware-requirements">こちら</link>を参照)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>プロビジョニングするターゲットノード、すなわちエッジデバイス(デモまたはテストの場合は仮想マシンを使用可能)</para>
<itemizedlist>
<listitem>
<para>最小4GBのRAM、2 CPUコア、20GBのディスク</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>管理クラスタの解決可能なホスト名、またはsslip.ioなどのサービスで使用する静的IPアドレス</para>
</listitem>
<listitem>
<para>Edge Image Builderでインストールメディアを構築するためのホスト</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP6、openSUSE Leap 15.6、またはPodmanをサポートする他の互換性のあるオペレーティングシステムを実行している</para>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://podman.io">Podman</link>、および<link
xl:href="https://helm.sh">Helm</link>がインストールされていること</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ブート用のUSBフラッシュドライブ(物理ハードウェアを使用する場合)</para>
</listitem>
<listitem>
<para>最新のSUSE Linux Micro 6.1 SelfInstall ISOイメージのダウンロードコピー(<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>から入手可能です)</para>
</listitem>
</itemizedlist>
<note>
<para>ターゲットマシンにある既存のデータはこのプロセスの一環として上書きされます。ターゲットデプロイメントノードに接続されているUSBストレージデバイスやディスク上のデータは、必ずバックアップしてください。</para>
</note>
<para>このガイドは、アップストリームクラスタをホストするためにDigital Oceanドロップレットを使用し、ダウンストリームデバイスとしてIntel
NUCを使用して作成されています。インストールメディアの構築には、SUSE Linux Enterprise Serverを使用しています。</para>
</section>
<section xml:id="build-bootstrap-cluster">
<title>ブートストラップクラスタの構築</title>
<para>まず、RancherとElementalをホストできるクラスタを作成します。このクラスタは、ダウンストリームノードが接続されているネットワークからルーティングできる必要があります。</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Kubernetesクラスタの作成</title>
<para>ハイパースケーラ(Azure、AWS、Google
Cloudなど)を使用している場合、クラスタを設定する最も簡単な方法は、ハイパースケーラのビルトインツールを使用することです。このガイドでは、簡潔にするために、これらの各オプションのプロセスについては詳述しません。</para>
<para>ベアメタルや別のホスティングサービスにインストールしようとしていて、Kubernetesディストリビューションそのものも用意する必要がある場合は、<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>を使用することをお勧めします。</para>
</section>
<section xml:id="id-set-up-dns">
<title>DNSの設定</title>
<para>続行する前に、クラスタへのアクセスを設定する必要があります。クラスタ自体のセットアップと同様に、DNSの設定方法は、クラスタがホストされている場所によって異なります。</para>
<tip>
<para>DNSレコードの設定を扱わない場合(たとえば、これが一時的なテストサーバである場合)、代わりに<link
xl:href="https://sslip.io">sslip.io</link>などのサービスを使用できます。このサービスを使用すると、<literal>&lt;address&gt;.sslip.io</literal>を使用して任意のIPアドレスを解決できます。</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Rancherのインストール</title>
<para>Rancherをインストールするには、作成したクラスタのKubernetes
APIにアクセスする必要があります。これは、使用しているKubernetesのディストリビューションによって異なります。</para>
<para>RKE2の場合、kubeconfigファイルは<literal>/etc/rancher/rke2/rke2.yaml</literal>に書き込まれます。このファイルをローカルシステムに<literal>~/.kube/config</literal>として保存します。このファイルを編集して、外部にルーティング可能な正しいIPアドレスまたはホスト名を含めなければならない場合があります。</para>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Rancherのドキュメント</link>に記載されているコマンドを使用して、Rancherを簡単にインストールできます。</para>
<para><link xl:href="https://cert-manager.io">cert-manager</link>をインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true</screen>
<para>次に、Rancher自体をインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt; \
  --version 2.12.1</screen>
<note>
<para>これを運用システムにする予定の場合は、cert-managerを使用して、実際の証明書(Let's Encryptの証明書など)を設定してください。</para>
</note>
<para>設定したホスト名をブラウズし、使用した<literal>bootstrapPassword</literal>でRancherにログインします。ガイドに従って簡単なセットアッププロセスを完了します。</para>
</section>
<section xml:id="install-elemental">
<title>Elementalのインストール</title>
<para>Rancherをインストールしたら、続いてElementalのオペレータと必要なCRDをインストールできます。Elemental用のHelmチャートはOCIアーティファクトとして公開されているため、インストールは他のチャートよりも若干シンプルです。Rancherのインストールに使用したものと同じシェルからインストールすることも、ブラウザでRancherのシェル内からインストールすることもできます。</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.7.3

helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.7.3</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>(オプション) Elemental UI拡張機能のインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>Elemental UIを使用するには、Rancherインスタンスにログインし、左上の三本線のメニューをクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width="85%"/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>このページの［Available (使用可能)］タブから、Elementalカードの［Install (インストール)］をクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width="85%"/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能をインストールすることを確認します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width="100%"/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>インストール後、ページを再ロードするよう求められます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width="100%"/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>再ロードすると、［OS Management (OS管理)］グローバルアプリからElemental拡張機能にアクセスできるようになります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width="100%"/> </imageobject>
<textobject><phrase>Elemental拡張機能へのアクセス</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configure-elemental">
<title>Elementalの設定</title>
<para>シンプルにするために、変数<literal>$ELEM</literal>を、設定ディレクトリを配置する場所のフルパスに設定することをお勧めします。</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>マシンがElementalに登録できるようにするために、<literal>fleet-default</literal>ネームスペースに<literal>MachineRegistration</literal>オブジェクトを作成する必要があります。</para>
<para>このオブジェクトの基本的なバージョンを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>この<literal>cat</literal>コマンドでは、各<literal>$</literal>をバックスラッシュ(<literal>\</literal>)でエスケープしています。このため、バッシュではテンプレート化されていません。手動でコピーする場合は、バックスラッシュを削除してください。</para>
</note>
<para>オブジェクトが作成されたら、割り当てられるエンドポイントを見つけてメモを取ります。</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>または、UIからこの操作を実行することもできます。</para>
<variablelist>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>［OS Management extension (OS管理拡張機能)］から［Create Registration Endpoint
(登録エンドポイントの作成) ］をクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png"
width="100%"/> </imageobject>
<textobject><phrase>［Create Registration (登録の作成)］をクリックします。</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>この設定に名前を付けます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png"
width="100%"/> </imageobject>
<textobject><phrase>名前の追加</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>［Cloud Configuration (クラウドの設定)］フィールドは無視して構いません。ここのデータは、Edge Image
Builderを使用した次の手順で上書きされるためです。</para>
</note>
</listitem>
<listitem>
<para>次に、下にスクロールして、マシンの登録時に作成されるリソースに付ける各ラベルに対して［Add Label
(ラベルの追加)］をクリックします。これはマシンを区別するのに役立ちます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png"
width="100%"/> </imageobject>
<textobject><phrase>ラベルの追加</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［Create (作成)］をクリックして、設定を保存します。</para>
</listitem>
<listitem>
<para>登録が作成されると、登録URLが表示され、［Copy (コピー)］ をクリックしてアドレスをコピーできます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width="100%"/>
</imageobject>
<textobject><phrase>URLのコピー</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>クリックしてその画面から移動してしまった場合は、左側のメニューの［Registration Endpoints
(登録エンドポイント)］をクリックし、先ほど作成したエンドポイント名をクリックできます。</para>
</tip>
<para>このURLは次の手順で使用します。</para>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="build-installation-media">
<title>イメージの構築</title>
<para>Elementalの現在のバージョンには独自のインストールメディアを構築する方法が用意されていますが、SUSE Edge
3.4では代わりにKiwiとEdge Image Builderでインストールメディアを構築します。したがって、生成されるシステムは、<link
xl:href="https://www.suse.com/products/micro/">SUSE Linux
Micro</link>をベースオペレーティングシステムとして構築されます。</para>
<tip>
<para>Kiwiの詳細については、まずKiwi Image Builderプロセス(<xref
linkend="guides-kiwi-builder-images"/>)に従って、新しいイメージを構築してください。また、Edge Image
Builderについては、Edge Image Builder導入ガイド(<xref
linkend="quickstart-eib"/>)のほかに、コンポーネントのドキュメント(<xref
linkend="components-eib"/>)も参照してください。</para>
</tip>
<para>PodmanをインストールしたLinuxシステムで、ディレクトリを作成し、Kiwiによって構築されるゴールデンイメージを配置します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/{micro-base-image-iso} $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.3
image:
    imageType: iso
    arch: x86_64
    baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: XXX
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para><literal>time</literal>
セクションはオプションですが、証明書とクロックスキューに関する潜在的な問題を回避するように設定することを強くお勧めします。この例で指定されている値は説明のみを目的としています。ご自身の特定の要件に合わせて調整してください。</para>
</listitem>
<listitem>
<para>エンコードされていないパスワードは<literal>eib</literal>です。</para>
</listitem>
<listitem>
<para><literal>sccRegistrationCode</literal>は、公式なソースから必要なRPMをダウンロードしてインストールするために必要です(または、代わりに<literal>elemental-register</literal>および<literal>elemental-system-agent</literal>のRPMを手動でサイドロードすることもできます)。</para>
</listitem>
<listitem>
<para>この<literal>cat</literal>コマンドでは、各<literal>$</literal>をバックスラッシュ(<literal>\</literal>)でエスケープしています。このため、バッシュではテンプレート化されていません。手動でコピーする場合は、バックスラッシュを削除してください。</para>
</listitem>
<listitem>
<para>インストールデバイスは、インストール中に消去されます。</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file eib-config.yaml</screen>
<para>物理デバイスをブートする場合は、イメージをUSBフラッシュ ドライブに書き込む必要があります。これは、次のコマンドで実行できます。</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>ダウンストリームノードのブート</title>
<para>インストールメディアを作成したので、それを使用してダウンストリームノードをブートできます。</para>
<para>Elementalで制御するシステムごとに、インストールメディアを追加してデバイスをブートします。インストールが完了すると、デバイスは再起動して自身を登録します。</para>
<para>UI拡張機能を使用している場合は、［Inventory of Machines (マシンのインベントリ)］にノードが表示されます。</para>
<note>
<para>ログインプロンプトが表示されるまでインストールメディアを取り外さないでください。初回ブート時には、USBスティック上のファイルにアクセスしたままになります。</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>ダウンストリームクラスタの作成</title>
<para>Elementalを使用して新しいクラスタをプロビジョニングする際に作成する必要があるオブジェクトが2つあります。</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>最初のオブジェクトは<literal>MachineInventorySelectorTemplate</literal>です。このオブジェクトにより、クラスタとインベントリ内のマシン間のマッピングを指定できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>インベントリ内のマシンをラベルに一致させるセレクタを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>リソースをクラスタに適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>マシンの名前を取得し、一致するラベルを追加します。</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>シンプルなシングルノードK3sクラスタリソースを作成し、クラスタに適用します。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.33.3+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<para>UI拡張機能では、ショートカットをいくつか使用できます。複数の場所を管理する場合は、手動による作業が多くなりすぎる可能性があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>以前と同様に、左側の三本線メニューを開いて、［OS Management
(OS管理)］を選択します。これで、Elementalシステムを管理するためのメイン画面に戻ります。</para>
</listitem>
<listitem>
<para>左側のサイドバーで、［Inventory of Machines (マシンのインベントリ)］をクリックします。登録済みのマシンのインベントリが開きます。</para>
</listitem>
<listitem>
<para>これらのマシンからクラスタを作成するには、必要なシステムを選択し、［Actions (アクション)］ドロップダウンリストから［Create
Elemental Cluster (Elementalクラスタの作成)］をクリックします。［Cluster Creation
(クラスタの作成)］ダイアログが開き、それと同時に、使用するMachineSelectorTemplateがバックグラウンドで作成されます。</para>
</listitem>
<listitem>
<para>この画面では、構築するクラスタを設定します。このクイックスタートでは、K3s
v1.30.5+k3s1が選択され、残りのオプションはそのままにしておきます。</para>
<tip>
<para>他のオプションを表示するには、下にスクロールする必要がある場合があります。</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>これらのオブジェクトを作成したら、先ほどインストールした新しいノードを使用して新しいKubernetesクラスタがスピンアップするはずです。</para>
</section>
<section xml:id="id-node-reset-optional">
<title>ノードリセット(オプション)</title>
<para>SUSE Rancher
Elementalは、「ノードリセット」を実行する機能をサポートしています。ノードリセットは、Rancherからクラスタ全体が削除されたとき、クラスタからシングルノードが削除されたとき、またはマシンインベントリからノードが手動で削除されたときに任意でトリガできます。これは、孤立したリソースをリセットしてクリーンアップし、クリーンアップされたノードを自動的にマシンインベントリに戻して再利用可能にする場合に役立ちます。ノードリセットはデフォルトでは有効になっていないため、削除されたシステムはクリーンアップされず(つまり、データは削除されず、Kubernetesクラスタリソースはダウンストリームクラスタで動作し続けます)、データを消去してマシンをElemental経由でRancherに再登録するには手動操作が必要となります。</para>
<para>この機能をデフォルトで有効にするには、&#x200B;<literal>MachineRegistration</literal>に&#x200B;<literal>config.elemental.reset.enabled:
true</literal>を追加して明示的に有効にする必要があります。例:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>その後、この<literal>MachineRegistration</literal>に登録されているすべてのシステムが自動的に<literal>elemental.cattle.io/resettable:'true'</literal>のアノテーションを受け取って設定に反映します。既存の<literal>MachineInventory</literal>にこのアノテーションがない場合や、すでにノードをデプロイ済みである場合などに、個々のノードで手動でこの操作を実行する場合は、<literal>MachineInventory</literal>を変更し、<literal>resettable</literal>設定を追加します。例:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>SUSE Edge 3.1では、Elemental
Operatorによってオペレーティングシステム上にマーカが配置され、これによってクリーンアッププロセスが自動的にトリガされます。クリーンアッププロセスは、すべてのKubernetesサービスを停止して永続データをすべて削除し、すべてのKubernetesサービスをアンインストールして、残っているKubernetes/Rancherディレクトリをクリーンアップし、元のElemental
<literal>MachineRegistration</literal>設定を使用して強制的にRancherに再登録します。これは自動的に行われるため、手動での操作は必要ありません。呼び出されるスクリプトは<literal>/opt/edge/elemental_node_cleanup.sh</literal>にあり、マーカが配置されるとすぐに<literal>systemd.path</literal>を介してトリガされるため、直ちに実行されます。</para>
<warning>
<para><literal>resettable</literal>機能を使用する場合、Rancherからノード/クラスタを削除する際の望ましい動作は、データを消去して再登録を強制することであると想定されています。この状況ではデータが確実に失われるため、この機能は、自動リセットを実行することがわかっている場合にのみ使用してください。</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>次の手順</title>
<para>このガイドの使用後に調べるべき推奨リソースを次に示します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-fleet"/>のエンドツーエンドの自動化</para>
</listitem>
<listitem>
<para><xref linkend="components-nmc"/>の追加のネットワーク設定オプション</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Edge Image Builderを使用したスタンドアロンクラスタ</title>
<para>Edge Image Builder (EIB)は、完全なエアギャップシナリオでもマシンをブートストラップできるCustomized,
Ready-to-Boot (CRB)ディスクイメージの生成プロセスを効率化するツールです。EIBを使用すると、SUSE
Edgeの3つのデプロイメントフットプリントすべてで使用するデプロイメントイメージを作成できます。これは、EIBが十分に柔軟であり、最小限のカスタマイズ(例:
ユーザの追加やタイムゾーンの設定)から、あらゆる設定を網羅したイメージ(例:
複雑なネットワーク設定を行い、マルチノードKubernetesクラスタをデプロイして、顧客ワークロードをデプロイし、Rancher/ElementalとSUSE
Multi-Linux
Managerを介して集中管理プラットフォームに登録するイメージ)までを提供できるためです。EIBはコンテナイメージ内で動作するため、プラットフォーム間できわめて容易に移植可能です。さらに、必要な依存関係をすべて備えた自己完結型であるため、EIBツールの操作に使用するシステムにインストールされているパッケージに及ぼす影響が最小限に抑えられます。</para>
<note>
<para>マルチノードシナリオの場合、同じ構築されたイメージを使用してプロビジョニングされたホストが
Kubernetesクラスタに自動的に参加できるように、EIBははMetalLBとEndpoint Copier
Operatorを自動的にデプロイします。</para>
</note>
<para>詳細については、Edge Image Builderの紹介(<xref linkend="components-eib"/>)を参照してください。</para>
<warning>
<para>Edge Image Builder 1.3.0は、SUSE Linux Micro 6.1イメージのカスタマイズをサポートしています。SUSE
Linux Enterprise Micro 5.5や6.0などの古いバージョンはサポートされていません。</para>
</warning>
<section xml:id="id-prerequisites-2">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>SLES 15 SP6を実行しているAMD64/Intel 64ビルドホストマシン(物理または仮想)</para>
</listitem>
<listitem>
<para>Podmanコンテナエンジン</para>
</listitem>
<listitem>
<para>Kiwi Builder手順を使用して作成されたSUSE Linux Micro 6.1 SelfInstall ISOイメージ(<xref
linkend="guides-kiwi-builder-images"/>)</para>
</listitem>
</itemizedlist>
<note>
<para>非運用目的の場合は、openSUSE Leap 15.6、またはopenSUSE
Tumbleweedをビルドホストマシンとして使用できます。互換性のあるコンテナランタイムが利用できれば、その他のオペレーティングシステムでも機能します。</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>EIBイメージの取得</title>
<para>EIBのコンテナイメージは一般に公開されており、イメージ構築ホストで次のコマンドを実行することでSUSE Edgeレジストリからダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>イメージ設定ディレクトリの作成</title>
<para>EIBはコンテナ内で動作するため、ホストから設定ディレク
トリをマウントして、必要な設定を指定できるようにする必要があります。ビルドプロセス中に、EIBは必要な入力ファイルやサポートアーティファクトすべてにアクセスできます。このディレクトリは、特定の構造に従う必要があります。このディレクトリがホームディレクトリに存在し、「eib」という名前であると仮定して、ディレクトリを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>前の手順でSUSE Linux Micro
6.1入力イメージをホストする「base-images」ディレクトリを作成しました。このイメージが設定ディレクトリにコピーされていることを確認しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIBの実行中に元のゴールデンイメージは変更<emphasis
role="strong">されません</emphasis>。EIBの設定ディレクトリのルートに、目的の設定でカスタマイズされた新しいバージョンが作成されます。</para>
</note>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>イメージ定義ファイルの作成</title>
<para>定義ファイルには、Edge Image
Builderがサポートする設定可能なオプションの大部分が記述されています。オプションの完全な例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/pkg/image/testdata/full-valid-example.yaml">こちら</link>を参照してください。以下で説明する例よりも広範な例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">アップストリームのイメージ構築ガイド</link>を参照することをお勧めします。まずは、OSイメージの非常に基本的な定義ファイルから始めましょう。</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>この定義では、AMD64/Intel
64ベースのシステム用の出力イメージを生成するように指定しています。さらに変更を加えるためのベースとして使用するイメージは、<literal>slemicro.iso</literal>という名前の<literal>iso</literal>イメージであり、<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>にあることが想定されています。また、EIBがイメージの変更を完了すると、出力イメージは<literal>eib-image.iso</literal>という名前になり、デフォルトでは<literal>$CONFIG_DIR</literal>に存在することも記述されています。</para>
<para>これで、ディレクトリ構造は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>以降のセクションでは、一般的な操作の例をいくつか紹介していきます。</para>
<section xml:id="id-configuring-os-users">
<title>OSユーザの設定</title>
<para>EIBを使用すると、パスワードやSSHキーなどのログイン情報を事前にユーザに設定できます(固定されたルートパスワードの設定も含む)。この例の一部として、ルートパスワードを修正します。最初の手順は、<literal>OpenSSL</literal>を使用して一方向暗号化パスワードを作成することです。</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>これは次のような出力になります。</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>次に、定義ファイルに<literal>operatingSystem</literal>というセクションを追加し、その中に<literal>users</literal>配列を含めます。作成したファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>ユーザの追加、ホームディレクトリの作成、ユーザIDの設定、ssh-key認証の追加、グループ情報の変更も行うことができます。他の例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">アップストリームのイメージ構築ガイド</link>を参照してください。</para>
</note>
</section>
<section xml:id="configuring-os-time">
<title>OSの時刻の設定</title>
<para><literal>time</literal>セクションはオプションですが、証明書とクロックスキューに関する潜在的な問題を避けるために、設定することを強くお勧めします。EIBは、chronydと<literal>/etc/localtime</literal>をここのパラメータに応じて設定します。</para>
<screen language="console" linenumbering="unnumbered">operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2</screen>
<itemizedlist>
<listitem>
<para><literal>timezone</literal>では、タイムゾーンを「地域/都市」(例:
「Europe/London」)の形式で指定します。完全なリストを確認するには、Linuxシステム上で<literal>timedatectl
list-timezones</literal>を実行します。</para>
</listitem>
<listitem>
<para>ntp - NTPの設定(chronydを使用)に関連する属性を定義します。</para>
</listitem>
<listitem>
<para>forceWait - 他のサービスを開始する前にchronydが時刻ソースの同期を試行するよう要求します。タイムアウトは180秒です。</para>
</listitem>
<listitem>
<para>pools -
chronydがデータソースとして使用するプールのリストを指定します(<literal>iburst</literal>を使用すると、最初の同期にかかる時間を短縮できます)。</para>
</listitem>
<listitem>
<para>servers -
chronydがデータソースとして使用するサーバのリストを指定します(<literal>iburst</literal>を使用すると、最初の同期にかかる時間を短縮できます)。</para>
</listitem>
</itemizedlist>
<note>
<para>この例で指定されている値は説明のみを目的としています。ご自身の特定の要件に合わせて調整してください。</para>
</note>
</section>
<section xml:id="adding-certificates">
<title>証明書の追加</title>
<para><literal>certificates</literal>ディレクトリに保存された拡張子「.pem」または「.crt」の証明書ファイルは、ノードのシステム全体の証明書ストアにインストールされます。</para>
<screen language="console" linenumbering="unnumbered">.
├── definition.yaml
└── certificates
    ├── my-ca.pem
    └── my-ca.crt</screen>
<para>詳細については、<link
xl:href="https://documentation.suse.com/smart/security/html/tls-certificates/index.html#tls-adding-new-certificates">「Securing
Communication with TLS Certificate
(TLS証明書による通信のセキュリティ保護)」ガイド</link>を参照してください。</para>
</section>
<section xml:id="eib-configuring-rpm-packages">
<title>RPMパッケージの設定</title>
<para>EIBの主な特徴の1つは、イメージにソフトウェアパッケージを追加するメカニズムを備えていることです。このため、インストールが完了した時点で、システムはインストールされたパッケージをすぐに利用できます。EIBでは、ユーザは以下を行うことができます。</para>
<itemizedlist>
<listitem>
<para>イメージ定義のリスト内の名前でパッケージを指定する</para>
</listitem>
<listitem>
<para>これらのパッケージを検索するネットワークリポジトリを指定する</para>
</listitem>
<listitem>
<para>一覧にされたパッケージをSUSEの公式リポジトリで検索するためのSUSE Customer Center (SCC)資格情報を指定する</para>
</listitem>
<listitem>
<para><literal>$CONFIG_DIR/rpms</literal>ディレクトリ経由で、ネットワークリポジトリに存在しないカスタムRPMをサイドロードする</para>
</listitem>
<listitem>
<para>同じディレクトリ(<literal>$CONFIG_DIR/rpms/gpg-keys</literal>)経由で、サードパーティ製パッケージの検証を有効にするためにGPGキーを指定する</para>
</listitem>
</itemizedlist>
<para>これにより、EIBは、イメージの構築時にパッケージ解決プロセスを実行し、ゴールデンイメージを入力として受け取り、提供されているパッケージ(リストで指定されているか、ローカルで提供されているパッケージ)をすべてプルしてインストールしようと試みます。EIBは、依存関係を含むすべてのパッケージを出力イメージ内に存在するリポジトリにダウンロードし、そのパッケージを初回ブートプロセス中にインストールするようにシステムに指示します。イメージの構築中にこのプロセスを実行することで、初回ブート時にパッケージが目的のプラットフォーム(エッジのノードなど)に正常にインストールされることが保証されます。これは、操作時に追加パッケージをネットワーク経由でプルするのではなく、イメージに事前に書き込んでおきたい環境でも便利です。たとえば、エアギャップ環境や、ネットワークが制限された環境です。</para>
<para>これを示すシンプルな例として、サードパーティベンダがサポートするNVIDIAリポジトリにある<literal>nvidia-container-toolkit</literal>
RPMパッケージをインストールします。</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>生成される定義ファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>上記はシンプルな例ですが、完全を期するために、イメージ生成を実行する前にNVIDIAパッケージ署名キーをダウンロードしてください。</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; $CONFIG_DIR/rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>この方法でRPMを追加することは、サポートされているサードパーティコンポーネント、またはユーザが提供(および保守)するパッケージを追加することを目的としています。このメカニズムは、通常ではSUSE
Linux
Microでサポートされないパッケージを追加する目的では使用しないでください。このメカニズムを使用して、openSUSEのリポジトリから新しいリリースやサービスパックなどのコンポーネントを追加した場合(この操作はサポートされません)、最終的にサポート対象外の設定になるおそれがあります。特に、依存関係の解決によってオペレーティングシステムのコア部分が置き換えられる場合は、作成されたシステムが期待どおりに機能しているように見えても注意が必要です。確信が持てない場合は、SUSEの担当者に連絡してサポートを依頼し、目的の設定がサポート可能かどうかを判断してください。</para>
</warning>
<note>
<para>追加の例を含む総合的なガイドについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">アップストリームのパッケージインストールガイド</link>を参照してください。</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Kubernetesクラスタとユーザワークロードの設定</title>
<para>EIBのもう1つの特徴は、EIBを使用すると、「インプレースでブートストラップ」する(つまり調整のためにどのような形態の集中管理インフラストラクチャも必要としない)シングルノードとマルチノード両方の高可用性Kubernetesクラスタのデプロイメントを自動化できることです。このアプローチは主にエアギャップデプロイメント、すなわちネットワークが制限された環境のためのものですが、ネットワークに制限なく完全にアクセスできる場合であっても、スタンドアロンクラスタを迅速にブートストラップする方法として役立ちます。</para>
<para>この方法を使用すると、カスタマイズされたオペレーティングシステムをデプロイできるだけでなく、Kubernetesの設定を指定したり、Helmチャートを介して追加の階層化コンポーネントを指定したり、指定したKubernetesマニフェストを介してユーザワークロードを指定したりすることもできます。ただしこの方法を使用する場合、その背景にある設計理念として、ユーザがエアギャップ化を望んでいるとデフォルトで想定します。したがって、イメージ定義で指定されているすべての項目を、ユーザが指定したワークロードを含めてイメージにプルします。その際に、EIBは、定義で要求されている検出済みイメージをすべてローカルにコピーし、作成されたデプロイ済みシステムの組み込みのイメージレジストリで提供します。</para>
<para>次の例では、既存のイメージ定義を使用してKubernetesの設定を指定します(この例では、複数のシステムとその役割が一覧にされていないため、デフォルトでシングルノードを想定します)。この設定により、シングルノードのRKE2
KubernetesクラスタをプロビジョニングするようにEIBに指示します。ユーザが指定したワークロード(マニフェストを使用)と階層化コンポーネント(Helmを使用)の両方のデプロイメントの自動化を説明するために、SUSE
EdgeのHelmチャートを使用してKubeVirtをインストールし、Kubernetesマニフェストを使用してNGINXをインストールします。既存のイメージ定義に追加する必要がある設定は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<para>作成された完全な定義ファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.33.3+k3s1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<note>
<para>マルチノードデプロイメント、カスタムネットワーキング、Helmチャートのオプション/値など、オプションの他の例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">アップストリームドキュメント</link>を参照してください。</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>ネットワークの設定</title>
<para>このクイックスタートの最後の例では、EIBで生成したイメージを使ってシステムをプロビジョニングした場合に作成されるネットワークを設定しましょう。ネットワーク設定を指定しない限り、ブート時に検出されたすべてのインタフェースでDHCPが使用されるのがデフォルトのモデルであることを理解することが重要です。ただし、これが常に望ましい設定であるとは限りません。これは特に、DHCPが利用できず静的な設定を指定する必要がある場合や、より複雑なネットワーキング構造(ボンド、LACP、VLANなど)を設定する必要がある場合、特定のパラメータ(ホスト名、DNSサーバ、ルートなど)を上書きする必要がある場合に該当します。</para>
<para>EIBでは、ノードごとの設定を指定することも(対象のシステムをMACアドレスで一意に識別します)、上書きによって各マシンに同一の設定を指定することもできます(システムのMACアドレスが不明な場合に便利です)。またEIBでは、Network
Manager Configurator (<literal>nmc</literal>)というツールも追加で使用されます。Network
Manager ConfiguratorはSUSE Edgeチームによって構築されたツールであり、<link
xl:href="https://nmstate.io/">nmstate.io</link>の宣言型ネットワークスキーマに基づいてカスタムネットワーキング設定を適用できるようにします。また、ブートしているノードをブート時に識別し、必要なネットワーク設定をサービス開始前に適用します。</para>
<para>ここでは、1つのインタフェースを持つシステムに静的ネットワーク設定を適用します。そのために、ネットワークの望ましい状態を、必要な<literal>network</literal>ディレクトリ内にある(目的のホスト名に基づく)ノード固有のファイルに記述します。</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>上記の例は、テストを仮想マシン上で実行すると仮定して、デフォルトの<literal>192.168.122.0/24</literal>サブネット用に設定されています。ご使用の環境に合わせて、MACアドレスも忘れずに変更してください。同じイメージを使用して複数のノードをプロビジョニングできるため、
EIBで(<literal>nmc</literal>を介して)設定されたネットワーキングは、各ノードをMACアドレスで一意に識別できるかどうかに依存しており、その後、ブート中に<literal>nmc</literal>は正しいネットワーキング設定を各マシンに適用します。つまり、インストール先のシステムのMACアドレスを把握する必要があります。また、デフォルトの動作ではDHCPに依存しますが、<literal>configure-network.sh</literal>フックを利用して、すべてのノードに共通の設定を適用することもできます。詳細については、ネットワーキングのガイド(<xref
linkend="components-nmc"/>)を参照してください。</para>
</warning>
<para>作成されるファイル構造は、次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>作成したネットワーク設定が解析され、必要なNetworkManager接続ファイルが自動的に生成されて、EIBが作成する新しいインストールイメージに挿入されます。これらのファイルはホストのプロビジョニング中に適用され、完全なネットワーク設定が生成されます。</para>
<note>
<para>上記の設定のより包括的な説明と、この機能の例については、Edgeネットワーキングのコンポーネント(<xref
linkend="components-nmc"/>)を参照してください。</para>
</note>
</section>
</section>
<section xml:id="eib-how-to-build-image">
<title>イメージの構築</title>
<para>EIBで使用するゴールデンイメージとイメージ定義ができたので、イメージを構築してみましょう。これには、<literal>podman</literal>を使用し、定義ファイルを指定して「build」コマンドでEIBコンテナを呼び出すだけです。</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>コマンドの出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Downloading file: dl-manifest-1.yaml 100% (498/498 B, 9.5 MB/s)
Pulling selected Helm charts... 100% (1/1, 43 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (3/3, 10 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (657/657 MB, 48 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (368/368 MB, 48 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (35/35 MB, 50 MB/s)
Downloading file: sha256sum-amd64.txt 100% (4.3/4.3 kB, 6.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築されたISOイメージは<literal>$CONFIG_DIR/eib-image.iso</literal>に保存されます。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>ビルドごとに、<literal>$CONFIG_DIR/_build/</literal>内にタイムスタンプ付きのフォルダが作成されます。このフォルダには、ビルドのログ、ビルド中に使用されたアーティファクト、およびCRBイメージに追加されたすべてのスクリプトとアーティファクトを含む<literal>combustion</literal>ディレクトリと<literal>artefacts</literal>ディレクトリが含まれます。</para>
<para>このディレクトリの内容は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:&lt;version&gt;-registry.tar.zst
│   │   │   ├── rancher_kubectl:&lt;version&gt;-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.6_virt-operator:&lt;version&gt;-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   ├── nvidia-container-toolkit-&lt;version&gt;.rpm
│   │   │       │   ├── nvidia-container-toolkit-base-&lt;version&gt;.rpm
│   │   │       │   ├── libnvidia-container1-&lt;version&gt;.rpm
│   │   │       │   └── libnvidia-container-tools-&lt;version&gt;.rpm
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt
│   │       └── kubevirt-0.4.0.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>ビルドが失敗した場合、情報が含まれる最初のログは<literal>eib-build.log</literal>です。そこから、失敗したコンポーネントに移動してデバッグを行います。</para>
<para>この時点で、以下を行う、すぐに使用できるイメージができているはずです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUSE Linux Micro 6.1をデプロイする</para>
</listitem>
<listitem>
<para>ルートパスワードを設定する</para>
</listitem>
<listitem>
<para><literal>nvidia-container-toolkit</literal>パッケージをインストールする</para>
</listitem>
<listitem>
<para>コンテンツをローカルに提供する組み込みのコンテナレジストリを設定する</para>
</listitem>
<listitem>
<para>シングルノードRKE2をインストールする</para>
</listitem>
<listitem>
<para>静的ネットワーキングを設定する</para>
</listitem>
<listitem>
<para>KubeVirtをインストールする</para>
</listitem>
<listitem>
<para>ユーザが指定したマニフェストをデプロイする</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>イメージ構築プロセスのデバッグ</title>
<para>イメージ構築プロセスが失敗する場合は、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/debugging.md">アップストリームのデバッグガイド</link>を参照してください。</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>新しく構築されたイメージのテスト</title>
<para>新しく構築されたCRBイメージをテストする方法については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/testing-guide.md">アップストリームのイメージテストガイド</link>を参照してください。</para>
</section>
</chapter>
<chapter xml:id="quickstart-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Multi-Linux ManagerはSUSE Edgeに含まれており、エッジデプロイメントのすべてのノードでSUSE Linux
Microを基盤となるオペレーティングシステムとして常に最新の状態に保つための自動化と制御を提供します。また、エッジノード上のKubernetesとKubernetes上にデプロイされたアプリケーションの管理にも使用できます。</para>
<para>このクイックスタートガイドは、エッジノードにオペレーティングシステムの更新を提供することを目的として、SUSE Multi-Linux
Managerをできるだけ早く使いこなすためのものです。このクイックスタートガイドでは、ストレージのサイズ設定、ステージング目的の追加ソフトウェアチャンネルの作成と管理、大規模なデプロイメントのためのユーザ、システムグループ、組織の管理などのトピックについては説明しません。運用環境で使用する場合は、包括的な<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUSE
Multi-Linux Manager のドキュメント</link>を参照することを強くお勧めします。</para>
<para>SUSE Multi-Linux Managerを効果的に使用するためにSUSE Edgeを準備するには、次の手順が必要です。</para>
<itemizedlist>
<listitem>
<para>SUSE Multi-Linux Manager Serverのデプロイと設定</para>
</listitem>
<listitem>
<para>SUSE Linux Microパッケージリポジトリの同期</para>
</listitem>
<listitem>
<para>システムグループの作成</para>
</listitem>
<listitem>
<para>アクティベーションキーの作成</para>
</listitem>
<listitem>
<para>Edge Image Builderを使用したSUSE Multi-Linux Manager登録用インストールメディアの準備</para>
</listitem>
</itemizedlist>
<section xml:id="id-deploy-suse-multi-linux-manager-server">
<title>SUSE Multi-Linux Manager Serverのデプロイ</title>
<para>SUSE Multi-Linux Manager 5.0.5のインスタンスをすでに実行している場合は、この手順をスキップできます。</para>
<para>SUSE Multi-Linux Manager
Serverは、専用の物理サーバ、自身のハードウェア上の仮想マシン、またはクラウドで実行できます。SUSE Multi-Linux
Serverの事前に設定された仮想マシンイメージはサポートされているパブリッククラウド用に提供されています。</para>
<para>このクイックスタートでは、AMD64/Intel
64用の「qcow2」イメージ<literal>SUSE-Manager-Server.x86_64-5.0.4-Qcow-5.0-2025-04.qcow2</literal>を使用しています。このイメージは、<link
xl:href="https://www.suse.com/download/suse-manager/">https://www.suse.com/download/suse-manager/</link>またはSUSE
Customer
Centerで入手できます。このイメージは、KVMなどのハイパーバイザー上の仮想マシンとして機能します。イメージの最新バージョンを常に確認して、新しいインストールに使用してください。</para>
<para>SUSE Multi-Linux Manager
Serverを他のサポートされているハードウェアアーキテクチャにもインストールできます。その場合は、ハードウェアアーキテクチャに合ったイメージを選択してください。</para>
<para>イメージをダウンロードしたら、次の最小ハードウェア仕様を満たす仮想マシンを作成します。</para>
<itemizedlist>
<listitem>
<para>16GB RAM</para>
</listitem>
<listitem>
<para>4つの物理コアまたは仮想コア</para>
</listitem>
<listitem>
<para>最低100GBの追加のブロックデバイス</para>
</listitem>
</itemizedlist>
<para>qcow2イメージでは、オペレーティングシステムをインストールする必要はありません。イメージをルートパーティションとして直接アタッチできます。</para>
<para>後でエッジノードが完全修飾ドメイン名(「FQDN」)を含むホスト名でSUSE Multi-Linux Manager
Serverにアクセスできるように、ネットワークを設定する必要があります。</para>
<para>SUSE Multi-Linux Managerを初めてブートする際には、いくつかの初期設定を実行する必要があります。</para>
<itemizedlist>
<listitem>
<para>キーボードレイアウトの選択</para>
</listitem>
<listitem>
<para>ライセンス契約の同意</para>
</listitem>
<listitem>
<para>タイムゾーンの選択</para>
</listitem>
<listitem>
<para>オペレーティングシステムのルートパスワードの入力</para>
</listitem>
</itemizedlist>
<para>次の各手順は「ルート」ユーザとして実行する必要があります。</para>
<para>次の手順では、SUSE Customer Centerで入手可能なSUSE Multi-Linux Manager
Extensionの登録コードが必要です。同じコードを使用して、SUSE Linux MicroとSUSE Multi-Linux
Managerの両方を登録できます。</para>
<para>SUSE Linux Microを登録します。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -r &lt;REGCODE&gt; -e &lt;your_email&gt;</screen>
<para>SUSE Multi-Linux Managerを登録します。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -p SUSE-Manager-Server/5.0/x86_64 -r &lt;REGCODE&gt;</screen>
<para>製品文字列は、ハードウェアアーキテクチャによって異なります。たとえば、64ビットArmシステムでSUSE Multi-Linux
Managerを使用している場合、文字列は「SUSE-Manager-Server/5.0/aarch64」になります。</para>
<para>再起動します。</para>
<para>システムを更新します。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update</screen>
<para>変更がなければ、再起動して更新を適用します。</para>
<para>SUSE Multi-Linux
Managerは、Podmanによって管理されるコンテナを介して提供されます。<literal>mgradm</literal>コマンドがセットアップと設定を自動的に行います。</para>
<warning>
<para>SUSE Multi-Linux Manager
Serverのホスト名が、管理対象のエッジノードがネットワークで適切に解決できる完全修飾ドメイン名(「FQDN」)で設定されていることが非常に重要です。</para>
</warning>
<para>SUSE Multi-Linux Manager
Serverコンテナをインストールして設定する前に、以前に追加した追加のブロックデバイスを準備しておく必要があります。そのためには、仮想マシンがそのデバイスに付けた名前を知っている必要があります。たとえば、ブロックデバイスが
<literal>/dev/vdb</literal>の場合、次のコマンドを使用して、SUSE Multi-Linux
Managerで使用するように設定できます。</para>
<screen language="shell" linenumbering="unnumbered">mgr-storage-server /dev/vdb</screen>
<para>SUSE Multi-Linux Managerをデプロイします。</para>
<screen language="shell" linenumbering="unnumbered">mgradm install podman &lt;FQDN&gt;</screen>
<para>CA証明書のパスワードを入力します。このパスワードはログインパスワードとは異なる必要があります。通常は後で入力する必要はありませんが、メモしておいてください。</para>
<para>「admin」ユーザのパスワードを入力します。これは、SUSE Multi-Linux
Managerにログインするための初期ユーザです。後で完全な権利または制限された権利を持つ追加のユーザを作成できます。</para>
</section>
<section xml:id="id-configure-suse-multi-linux-manager">
<title>SUSE Multi-Linux Managerの設定</title>
<para>デプロイメントが完了したら、先ほど指定したホスト名を使用して、SUSE Multi-Linux Manager Web
UIにログインできます。初期ユーザは「admin」です。前の手順で指定したパスワードを使用します。</para>
<para>次の手順では、SUSE Customer
Centerの組織の「ユーザ」タブの2番目のサブタブにある組織の資格情報が必要です。これらの資格情報を使用して、SUSE Multi-Linux
Managerはサブスクリプションを持つすべての製品を同期できます。</para>
<para><literal>［管理］ &gt; ［セットアップウィザード］</literal>を選択します。</para>
<para><literal>組織の資格情報</literal>タブで、SUSE Customer
Centerにある<literal>ユーザ名</literal>と<literal>パスワード</literal>を使用して新しい資格情報を作成します。</para>
<para>次のタブ［<literal>SUSE製品</literal>］に移動します。SUSE Customer
Centerとの最初のデータ同期が完了するまで待つ必要があります。</para>
<para>リストが入力されたら、フィルタを使用して「Micro
6.1」のみを表示します。エッジノードが実行されるハードウェアアーキテクチャ(<literal>x86_64</literal>または<literal>aarch64</literal>)に対応するSUSE
Linux Micro 6.1のチェックボックスをオンにします。</para>
<para>［<literal>製品の追加</literal>］をクリックします。これにより、SUSE Linux
Microのメインパッケージリポジトリ(「チャンネル」)が追加され、SUSE
Managerクライアントツールのチャンネルがサブチャンネルとして自動的に追加されます。</para>
<para>インターネット接続によっては、最初の同期にしばらく時間がかかります。次の手順からすぐに開始できます。</para>
<para><literal>［システム］
&gt;［システムグループ］</literal>で、システムがオンボード時に自動的に参加するグループを少なくとも1つ作成します。グループはシステムを分類する重要な方法であり、設定やアクションをシステムセット全体に一度に適用できます。概念的には、Kubernetesのラベルに似ています。</para>
<para>［<literal>+グループの作成</literal>］をクリックします。</para>
<para>短い名前(「エッジノード 」など)と長い説明を入力します。</para>
<para><literal>［システム］
&gt;［アクティベーションキー］</literal>で、少なくとも1つのアクティベーションキーを作成します。アクティベーションキーは、システムが
SUSE Multi-Linux
Managerにオンボードされたときに自動的に適用される設定プロファイルと考えることができます。特定のエッジノードを異なるグループに追加したり、異なる設定を使用したりする場合は、それら用に個別のアクティベーションキーを作成し、後でEdge
Image Builderでカスタマイズしたインストールメディアを作成する際に使用できます。</para>
<para>アクティベーションキーの典型的な高度なユースケースは、テストクラスタを最新の更新を含むソフトウェアチャンネルに割り当て、運用クラスタを、テストクラスタでテストした後にのみ最新の更新を取得するソフトウェアチャンネルに割り当てることです。</para>
<para>［<literal>+キーの作成</literal>］をクリックします。</para>
<para>短い説明を選択します(「エッジノード」など)。キーを識別する一意の名前を指定します(AMD64/Intel
64ハードウェアアーキテクチャを備えたエッジノードの場合は 「edge-x86_64
」など)。番号のプレフィックスが自動的にキーに追加されます。デフォルトの組織の場合、番号は常に「1」です。SUSE Multi-Linux
Managerで追加の組織を作成し、これらの組織のキーを作成する場合、この番号は異なる可能性があります。</para>
<para>クローンソフトウェアチャンネルを作成していない場合は、ベースチャンネルの設定を「SUSE
Managerの既定値」のままにしておくことができます。これにより、エッジノードに正しいSUSE更新リポジトリが自動的に割り当てられます。</para>
<para>「子チャンネル」として、アクティベーションキーが使用されているハードウェアアーキテクチャの［推奨を含める］スライダを選択します。これにより、「SUSE-Manager-Tools-For-SL-Micro-6.1」チャンネルが追加されます。</para>
<para>［グループ］タブで、以前に作成したグループを追加します。このアクティベーションキーを使用してオンボードされたすべてのノードがそのグループに自動的に追加されます。</para>
</section>
<section xml:id="id-create-a-customized-installation-image-with-edge-image-builder">
<title>Edge Image Builderを使用したカスタマイズされたインストールイメージの作成</title>
<para>Edge Image Builderを使用するには、PodmanでLinuxベースのコンテナを起動できる環境のみが必要です。</para>
<para>最小限のラボのセットアップでは、SUSE Multi-Linux Manager
Serverが実行しているのと同じ仮想マシンを実際に使用できます。仮想マシンに十分なディスク容量があることを確認してください。これは運用環境での使用には推奨されないセットアップです。Edge
Image Builderをテストしたホストオペレーティングシステムについては、 <xref
linkend="id-prerequisites-2"/>を参照してください。</para>
<para>SUSE Multi-Linux Managerサーバホストにrootとしてログインします。</para>
<para>Edge Image Builderコンテナをプルします。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
<para>ディレクトリ<literal>/opt/eib</literal>とサブディレクトリ<literal>base-images</literal>を作成します。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p /opt/eib/base-images</screen>
<para>このクイックスタートでは、SUSE Linux
Microイメージの「セルフインストール」フレーバーを使用しています。このイメージは、後で物理USBメモリに書き込んで、物理サーバにインストールするために使用できます。ご使用のサーバにBMC
(ベースボード管理コントローラ)経由でインストールISOをリモートアタッチするオプションがある場合は、その方法も使用できます。また、このイメージはほとんどの仮想化ツールでも使用できます。</para>
<para>イメージを物理ノードに直接プリロードする場合、またはVMから直接起動する場合は、「生」イメージフレーバーを使用することもできます。</para>
<para>これらのイメージはSUSE Customer Centerまたは<link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro/</link>にあります。</para>
<para>イメージ
<literal>SL-Micro.x86_64-6.1-Default-SelfInstall-GM.install.iso</literal>を<literal>base-images</literal>ディレクトリにダウンロードまたはコピーして、「slemicro.iso」という名前を付けます。</para>
<para>Armベースの構築ホスト上でのAArch64イメージの構築は、SUSE Edge
3.4の技術プレビュー対象です。おそらく動作しますが、まだサポートされていません。お試しいただく場合は、64ビットのArmマシンでPodmanを実行し、すべての例とコードスニペットの「x86_64」を「aarch64」に置き換える必要があります。</para>
<para><literal>/opt/eib</literal>に、<literal>iso-definition.yaml</literal>というファイルを作成します。これはEdge
Image Builderのビルド定義です。</para>
<para>ここでは、SL Micro 6.1をインストールし、ルートパスワードとキーマップを設定して、CockpitグラフィカルUIを起動し、ノードをSUSE
Multi-Linux Managerに登録するシンプルな例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
  - username: root
    createHomeDir: true
    encryptedPassword: $6$aaBTHyqDRUMY1HAp$pmBY7.qLtoVlCGj32XR/Ogei4cngc3f4OX7fwBD/gw7HWyuNBOKYbBWnJ4pvrYwH2WUtJLKMbinVtBhMDHQIY0
  keymap: de
  systemd:
    enable:
      - cockpit.socket
  packages:
    noGPGCheck: true
  suma:
    host: ${fully qualified hostname of your SUSE Multi-Linux Manager Server}
    activationKey: 1-edge-x86_64</screen>
<para>Edge Image
Builderは、ネットワークを設定し、ノードにKubernetesを自動的にインストールして、Helmチャート経由でアプリケーションをデプロイすることもできます。より包括的な例については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
<para><literal>baseImage</literal>には、使用する<literal>base-images</literal>ディレクトリ内のISOの実際の名前を指定します。</para>
<para>この例では、ルートパスワードは「root」になります。使用する安全なパスワードのパスワードハッシュを作成する方法については、<xref
linkend="id-configuring-os-users"/>を参照してください。</para>
<para>キーマップを、インストール後にシステムが使用する実際のキーボードレイアウトに設定します。</para>
<note>
<para>SUSEではRPMパッケージのチェックにGPGキーを提供しないため、オプション<literal>noGPGCheck:
true</literal>を使用しています。運用環境での使用に推奨するより安全なセットアップに関する包括的なガイドについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">アップストリームのパッケージインストールガイド</link>を参照してください。</para>
</note>
<para>何度か言及したように、SUSE Multi-Linux
Managerホストには、エッジノードがブートするネットワークで解決できる完全修飾ホスト名が必要です。</para>
<para><literal>activationKey</literal>の値は、SUSE Multi-Linux
Managerで作成したキーと一致する必要があります。</para>
<para>インストール後にエッジノードをSUSE Multi-Linux
Managerに自動的に登録するインストールイメージを構築するには、次の2つのアーティファクトを準備する必要もあります。</para>
<itemizedlist>
<listitem>
<para>SUSE Multi-Linux Managerの管理エージェントをインストールするSalt minionパッケージ</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux ManagerサーバのCA証明書</para>
</listitem>
</itemizedlist>
<section xml:id="id-download-the-venv-salt-minion-package">
<title>venv-salt-minionパッケージのダウンロード</title>
<para><literal>/opt/eib</literal>に、サブディレクトリ<literal>rpms</literal>を作成します。</para>
<para>SUSE Multi-Linux
Managerサーバからそのディレクトリにパッケージ<literal>venv-salt-minion</literal>をダウンロードします。Web
UIから取得する場合は、<literal>［ソフトウェア］
&gt;［チャンネル一覧］</literal>にあるパッケージを見つけてSUSE-Manager-Tools
…​チャンネルからダウンロードするか、SUSE Multi-Linux
Managerの「ブートストラップリポジトリ」からcurlなどのツールを使用してダウンロードします。</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/repositories/slmicro/6/1/bootstrap/x86_64/venv-salt-minion-3006.0-8.1.x86_64.rpm</screen>
<para>実際のパッケージ名は、新しいリリースがすでにリリースされている場合は異なる場合があります。複数のパッケージから選択できる場合は、必ず最新のパッケージを選択してください。</para>
<para>SUSE Multi-Linux Managerの<link
xl:href="https://www.suse.com/releasenotes/x86_64/multi-linux-manager/5.1/index.html#_bootstrapping_sl_micro_6_1_clients">リリースノート</link>に記載されている問題を回避するには、最新バージョンのビルドキーパッケージを<literal>rpms</literal>ディレクトリ(このドキュメントの作成時点では<literal>suse-build-key-12.0-slfo.1.1_3.1.noarch.rpm</literal>)に配置する必要があります。このパッケージは、SL
MicroのPoolチャンネルの<literal>Packages</literal>タブ経由で、SUSE Multi-Linux
Managerの［<literal>ソフトウェア</literal>］セクションにあります。［<literal>詳細</literal>］ビューには［<literal>ダウンロード</literal>］ボタンがあります。</para>
</section>
</section>
<section xml:id="id-download-the-suse-multi-linux-manager-ca-certificate">
<title>SUSE Multi-Linux Manager CA証明書のダウンロード</title>
<para><literal>/opt/eib</literal>に、サブディレクトリ<literal>certificates</literal>を作成します。</para>
<para>SUSE Multi-Linux ManagerからそのディレクトリにCA証明書をダウンロードします。</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/RHN-ORG-TRUSTED-SSL-CERT</screen>
<warning>
<para>証明書の名前を<literal>RHN-ORG-TRUSTED-SSL-CERT.crt</literal>に変更する必要があります。その後、Edge
Image Builderは、インストール時に証明書がエッジノードにインストールされ、アクティブ化されていることを確認します。</para>
</warning>
<para>これでEdge Image Builderを実行できます。</para>
<screen language="bash" linenumbering="unnumbered">cd /opt/eib
podman run --rm -it --privileged -v /opt/eib:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>YAML定義ファイルに別の名前を使用している場合や別のバージョンのEdge Image
Builderを使用する場合は、それに応じてコマンドを調整する必要があります。</para>
<para>ビルドが完了したら、
<literal>/opt/eib</literal>ディレクトリにインストールISOが<literal>eib-image.iso</literal>として保存されます。</para>
<para>このイメージは、SUSE Multi-Linux Managerに登録しようとするノードをデプロイするために使用できます。</para>
<para>ノードのインストールが完了すると、SUSE Multi-Linux
Managerの［<literal>Salt/キー</literal>］セクションにノードのキーが<literal>保留中</literal>として表示されます。キーを受け入れると、ノードはSUSE
Multi-Linux
Managerに自動的にオンボードされ、そのプロセスが完了すると［<literal>システム</literal>］リストに表示されます。ノードには、アクティベーションキーで指定したシステムグループが割り当てられます。</para>
<para>その後、追加の設定を適用する前に再起動をスケジュールする必要があります。</para>
<para>キーの受け入れは、<link
xl:href="https://docs.saltproject.io/en/latest/topics/tutorials/autoaccept_grains.html">こちら</link>で説明されているように、ホワイトリストを使用して完全に自動化できることに注意してください。</para>
</section>
</chapter>
</part>
<part xml:id="id-components">
<title>コンポーネント</title>
<partintro>
<para>Edgeのコンポーネントのリスト</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12">https://ranchermanager.docs.rancher.com/v2.12</link>にあるRancherのドキュメントを参照してください。</para>
<blockquote>
<para>Rancherは、オープンソースの強力なKubernetes管理プラットフォームであり、複数の環境にまたがるKubernetesクラスタのデプロイメント、運用、および監視を効率化します。オンプレミス、クラウド、エッジのいずれのクラスタを管理する場合でも、Rancherは、Kubernetesに関するあらゆるニーズに対応する、統合された中央プラットフォームを提供します。</para>
</blockquote>
<section xml:id="id-key-features-of-rancher">
<title>Rancherの主な機能</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">マルチクラスタ管理:</emphasis>
Rancherの直感的なインタフェースを使用して、パブリッククラウド、プライベートデータセンター、エッジロケーションのどこからでもKubernetesクラスタを管理できます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">セキュリティとコンプライアンス:</emphasis>
Rancherでは、Kubernetes環境全体にセキュリティポリシー、ロールベースのアクセス制御(RBAC)、およびコンプライアンス標準が適用されます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">クラスタ操作のシンプル化:</emphasis>
Rancherはクラスタのプロビジョニング、アップグレード、トラブルシューティングを自動化し、あらゆる規模のチームでKubernetesの操作をシンプル化します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">中央型のアプリケーションカタログ:</emphasis>
Rancherアプリケーションカタログは、多種多様なHelmチャートとKubernetes
Operatorを提供し、コンテナ化アプリケーションのデプロイと管理を容易にします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">継続的デリバリ:</emphasis>
RancherはGitOpsと継続的インテグレーション/継続的デリバリパイプラインをサポートしており、自動化および効率化されたアプリケーションデリバリプロセスを実現します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>SUSE EdgeでのRancherの使用</title>
<para>Rancherは、SUSE Edgeスタックに複数のコア機能を提供します。</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Kubernetesの集中管理</title>
<para>大量の分散クラスタが存在する一般的なエッジデプロイメントでは、Rancherは、これらのKubernetesクラスタを管理するための中央コントロールプレーンとして機能します。プロビジョニング、アップグレード、監視、およびトラブルシューティングのための統合インタフェースを提供し、操作をシンプル化し、一貫性を確保します。</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>クラスタデプロイメントのシンプル化</title>
<para>Rancherは、軽量なSUSE Linux
Microオペレーティングシステム上でのKubernetesクラスタの作成を効率化し、Kubernetesの堅牢な機能を備えたエッジインフラストラクチャの展開を容易にします。</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>アプリケーションのデプロイメントと管理</title>
<para>統合されたRancherアプリケーションカタログは、SUSE
Edgeクラスタ全体でのコンテナ化アプリケーションのデプロイと管理をシンプル化し、エッジワークロードのシームレスなデプロイメントを可能にします。</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>セキュリティとポリシーの適用</title>
<para>Rancherは、ポリシーベースのガバナンスツール、ロールベースのアクセス制御(RBAC)を備えているほか、外部の認証プロバイダと統合できます。これにより、SUSE
Edgeのデプロイメントは、分散環境において重要なセキュリティとコンプライアンスを維持できます。</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>ベストプラクティス</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>RancherにはビルトインコンポーネントとしてFleetが含まれており、Gitに保存されたコードでクラスタ設定やアプリケーションのデプロイメントを管理できます。</para>
</section>
<section xml:id="id-observability">
<title>可観測性</title>
<para>Rancherには、PrometheusやGrafanaなどのビルトインのモニタリングおよびログツールが含まれており、クラスタのヘルスとパフォーマンスについて包括的な洞察を得ることができます。</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、SUSE Linux Micro OSのゴールデンイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBでプロビジョニングしたKubernetesクラスタ上にRancherをエアギャップインストールするには、<xref
linkend="rancher-install"/>に従ってください。</para>
</section>
<section xml:id="id-additional-resources-2">
<title>追加リソース</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancherのドキュメント</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher Community</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Helmチャート</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Kubernetes Operator</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Rancher Dashboard拡張機能</title>
<para>拡張機能により、ユーザ、開発者、パートナー、および顧客はRancher UIを拡張および強化できます。SUSE
Edgeでは、KubeVirtとAkriのダッシュボード拡張機能を提供しています。</para>
<para>Rancher Dashboard拡張機能の一般的な情報については、<literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions">Rancherのドキュメント</link></literal>を参照してください。</para>
<section xml:id="id-installation">
<title>インストール</title>
<para>ダッシュボード拡張機能を含むすべてのSUSE Edge 3.4コンポーネントは、OCIアーティファクトとして配布されます。SUSE
Edge拡張機能をインストールするには、Rancher Dashboard UI、Helm、またはFleetを使用できます。</para>
<section xml:id="id-installing-with-rancher-dashboard-ui">
<title>Rancher Dashboard UIを使用したインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>ナビゲーションサイドバーの<emphasis role="strong">［ Configuration
(設定)］</emphasis>セクションで、<emphasis role="strong">［Extensions
(拡張機能)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para>［Extensions (拡張機能)］ページで、右上にある3つのドットメニューをクリックし、<emphasis
role="strong">［Manage Repositories (リポジトリの管理)］</emphasis>を選択します。</para>
<para>各拡張機能は、独自のOCIアーティファクトで配布されます。これらはSUSE Edge Helmチャートリポジトリから入手できます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Repositories
(リポジトリ)］</emphasis>ページで、［<literal>Create (作成)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>フォームにリポジトリ名とURLを指定して、［<literal>Create (作成)</literal>］をクリックします。</para>
<para>SUSE Edge HelmチャートリポジトリURL:
<literal>oci://registry.suse.com/edge/charts</literal></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-create-oci-repository.png" width="100%"/>
</imageobject>
<textobject><phrase>ダッシュボード拡張機能OCIリポジトリの作成</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能リポジトリがリストに追加され、［<literal>Active (アクティブ)</literal>］状態であることがわかります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-repositories-list.png" width="100%"/>
</imageobject>
<textobject><phrase>ダッシュボード拡張機能リポジトリリスト</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>ナビゲーションサイドバーの <emphasis role="strong">［Configuration (設定)］</emphasis>セクションの
<emphasis role="strong">［Extensions (拡張機能)］</emphasis>に戻ります。</para>
<para><emphasis role="strong">［Available
(利用可能)］</emphasis>タブで、インストール可能な拡張機能を確認できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-available-extensions.png" width="100%"/>
</imageobject>
<textobject><phrase>ダッシュボード拡張機能利用可能な拡張機能</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能カードで、［<literal>Install (インストール)</literal>］をクリックし、インストールを確認します。</para>
<para>拡張機能がインストールされると、Rancher UIによってページの再ロードが促されます。<literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions#installing-extensions">拡張機能のインストールのRancherドキュメントページ</link></literal>を参照してください。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-helm">
<title>Helmを使用したインストール</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension --version 304.0.2+up1.3.2 --namespace cattle-ui-plugin-system</screen>
<note>
<para>拡張機能は<literal>cattle-ui-plugin-system</literal>ネームスペースにインストールする必要があります。</para>
</note>
<note>
<para>拡張機能がインストールされたら、Rancher Dashboard UIを再ロードする必要があります。</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Fleetを使用したインストール</title>
<para>Fleetを使用してダッシュボード拡張機能をインストールするには、カスタムの<literal>fleet.yaml</literal>バンドル設定ファイルを使用して、Gitリポジトリを指す<literal>gitRepo</literal>リソースを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension
  version: "304.0.2+up1.3.2"</screen>
<note>
<para>拡張機能を正しくインストールするには、<literal>releaseName</literal>プロパティが必須であり、拡張機能名と一致している必要があります。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>詳細については、 <xref linkend="components-fleet"/>と<literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>リポジトリを参照してください。</para>
<para>拡張機能がインストールされると、その拡張機能が<emphasis role="strong">［Extensions
(拡張機能)］</emphasis>セクションの<emphasis role="strong">［Installed
(インストール済み)］</emphasis>タブに表示されます。拡張機能はApps/Marketplace経由でインストールされたものではないため、「<literal>Third-Party
(サードパーティ)</literal>」というラベルが付きます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width="100%"/> </imageobject>
<textobject><phrase>インストール済みダッシュボード拡張機能</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>KubeVirtダッシュボード拡張機能</title>
<para>KubeVirt拡張機能は、Rancher Dashboard UIに基本的な仮想マシン管理機能を提供します。その機能については、<xref
linkend="kubevirt-dashboard-extension-usage"/>を参照してください。</para>
</section>
</chapter>
<chapter xml:id="components-rancher-turtles">
<title>Rancher Turtles</title>
<para><link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/">https://documentation.suse.com/cloudnative/cluster-api/</link>にあるRancher
Turtlesのドキュメントを参照してください。</para>
<blockquote>
<para>Rancher Turtlesは、Rancher ManagerとCluster API (CAPI)の統合を提供するKubernetes
Operatorであり、RancherにCAPIの完全サポートをもたらすことを目的としています。</para>
</blockquote>
<section xml:id="id-key-features-of-rancher-turtles">
<title>Rancher Turtlesの主な機能</title>
<itemizedlist>
<listitem>
<para>CAPIでプロビジョニングされたクラスタにRancher Cluster
Agentをインストールして、CAPIクラスタをRancherに自動的にインポートします。</para>
</listitem>
<listitem>
<para><link xl:href="https://cluster-api-operator.sigs.k8s.io/">CAPI
Operator</link>を介してCAPIコントローラの依存関係をインストールして設定します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-rancher-turtles-use-in-suse-edge">
<title>SUSE EdgeでのRancher Turtlesの使用</title>
<para>SUSE Edgeスタックは、Rancher
Turtlesを次のコンポーネントを有効にする特定の設定でインストールするhelmラッパーチャートを提供します。</para>
<itemizedlist>
<listitem>
<para>コアCAPIコントローラコンポーネント</para>
</listitem>
<listitem>
<para>RKE2コントロールプレーンおよびブートストラッププロバイダコンポーネント</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>)インフラストラクチャプロバイダコンポーネント</para>
</listitem>
</itemizedlist>
<para>ラッパーチャートを介してインストールされるデフォルトプロバイダのみがサポートされます。代替コントロールプレーン、ブートストラップ、およびインフラストラクチャプロバイダは現在、SUSE
Edgeスタックの一部としてサポートされていません。</para>
</section>
<section xml:id="id-installing-rancher-turtles">
<title>Rancher Turtlesのインストール</title>
<para>Rancher Turtlesは、Metal3クイックスタート(<xref
linkend="quickstart-metal3"/>)ガイド、または管理クラスタ(<xref
linkend="atip-management-cluster"/>)ドキュメントに従ってインストールできます。</para>
</section>
<section xml:id="id-additional-resources-3">
<title>追加リソース</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancherのドキュメント</link></para>
</listitem>
<listitem>
<para><link xl:href="https://cluster-api.sigs.k8s.io/">Cluster APIのドキュメント</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link
xl:href="https://fleet.rancher.io">Fleet</link>は、ユーザがローカルクラスタをより細かく制御できるようにするとともに、GitOpsを通じて常時監視を行えるようにすることを目的に設計されたコンテナ管理およびデプロイメントエンジンです。Fleetはスケール能力に重点を置いているだけでなく、クラスタに何がインストールされているかを正確に監視するための高度な制御と可視性もユーザに提供します。</para>
<para>Fleetは、生のKubernetes
YAML、Helmチャート、Kustomize、またはこれら3つの組み合わせのGitからデプロイメントを管理できます。ソースにかかわらず、すべてのリソースは動的にHelmチャートに変換され、すべてのリソースをクラスタにデプロイするエンジンとしてHelmが使用されます。その結果、ユーザはクラスタの高度な制御、一貫性、監査能力を実現できます。</para>
<para>Fleetの仕組みについては、<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/architecture">Fleetアーキテクチャ</link>を参照してください。</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Helmを使用したFleetのインストール</title>
<para>FleetはRancherにビルトインされていますが、Helmを使用して、スタンドアロンアプリケーションとして任意のKubernetesクラスタに<link
xl:href="https://fleet.rancher.io/installation">インストール</link>することもできます。</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>RancherでのFleetの使用</title>
<para>Rancherは、Fleetを使用してアプリケーションを管理対象クラスタ全体にデプロイします。Fleetを使用した継続的デリバリにより、大量のクラスタで実行されるアプリケーションを管理するために設計された、大規模なGitOpsが導入されます。</para>
<para>FleetはRancherと統合してその一部として機能します。Rancherで管理されるクラスタには、インストール/インポートプロセスの一部としてFleetエージェントが自動的にデプロイされるため、クラスタはすぐにFleetで管理できるようになります。</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Rancher UIでのFleetへのアクセス</title>
<para>FleetはRancherにプリインストールされており、Rancher UIの<emphasis role="strong">［Continuous
Delivery (継続的デリバリ)］</emphasis>オプションで管理されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleetダッシュボード</phrase></textobject>
</mediaobject>
</informalfigure>
<para>［Continuous Delivery (継続的デリバリ)］セクションは次の項目で構成されます。</para>
<section xml:id="id-dashboard">
<title>Dashboard (ダッシュボード)</title>
<para>すべてのワークスペースにわたるすべてのGitOpsリポジトリの概要ページ。リポジトリのあるワークスペースのみが表示されます。</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos (Gitリポジトリ)</title>
<para>選択したワークスペース内のGitOpsリポジトリのリスト。ページ上部のドロップダウンリストを使用してアクティブなワークスペースを選択します。</para>
</section>
<section xml:id="id-clusters">
<title>Clusters (クラスタ)</title>
<para>管理対象クラスタのリスト。デフォルトでは、Rancherで管理されているすべてのクラスタが<literal>fleet-default</literal>ワークスペースに追加されます。<literal>fleet-local</literal>ワークスペースにはローカル(管理)クラスタが含まれます。ここから、クラスタを<literal>Pause
(一時停止)</literal>または<literal>Force Update
(強制的に更新)</literal>したり、クラスタを別のワークスペースに移動したりすることができます。クラスタを編集すると、クラスタのグループ化に使用するラベルや注釈を更新できます。</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups (クラスタグループ)</title>
<para>このセクションでは、セレクタを使用してワークスペース内のクラスタのカスタムグループを作成できます。</para>
</section>
<section xml:id="id-advanced">
<title>Advanced (詳細)</title>
<para>［Advanced (詳細)］セクションでは、ワークスペースやその他の関連するFleetリソースを管理できます。</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Rancher Dashboardを使用してRancherおよびFleetとともにKubeVirtをインストールする例</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>fleet.yaml</literal>ファイルを含むGitリポジトリを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/charts/kubevirt"
  version: "304.0.1+up0.6.0"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>Rancher Dashboardで、<emphasis role="strong">［☰］&gt; ［Continuous Delivery
(継続的デリバリ)］ &gt; ［Git Repos (Gitリポジトリ)］</emphasis>に移動して、［<literal>Add
Repository (リポジトリの追加)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>リポジトリの作成ウィザードの指示に従ってGitリポジトリを作成します。<emphasis role="strong">［Name
(名前)］</emphasis>、<emphasis role="strong">［Repository URL
(リポジトリのURL)］</emphasis>(前の手順で作成したGitリポジトリを参照)を指定し、適切なブランチまたはリビジョンを選択します。より複雑なリポジトリの場合は、<emphasis
role="strong">［Paths (パス)］</emphasis>を指定して、1つのリポジトリで複数のディレクトリを使用します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleetリポジトリの作成1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［<literal>Next (次へ)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>次の手順では、ワークロードをデプロイする場所を定義できます。クラスタの選択では複数の基本オプションがあります。クラスタをまったく選択しないことも、すべてのクラスタを選択することも、特定の管理対象クラスタやクラスタグループ(定義されている場合)を直接選択することもできます。［Advanced
(詳細)］オプションを使用すると、YAML経由でセレクタを直接編集できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleetリポジトリの作成2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［<literal>Create
(作成)</literal>］をクリックします。リポジトリが作成されます。今後、ワークロードはリポジトリ定義に一致するクラスタにインストールされ、同期が維持されます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>デバッグとトラブルシューティング</title>
<para>ナビゲーションの［Advanced (詳細)］セクションでは、下位レベルのFleetリソースの概要が表示されます。<link
xl:href="https://fleet.rancher.io/ref-bundle-stages">バンドル</link>は、Gitからのリソースのオーケストレーションに使用される内部リソースです。Gitリポジトリがスキャンされると、バンドルが1つ以上生成されます。</para>
<para>特定のリポジトリに関連するバンドルを見つけるには、［Git Repos (Gitリポジトリ)］の［Detail
(詳細)］ページに移動し、［<literal>Bundles (バンドル)</literal>］タブをクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleetリポジトリバンドル</phrase></textobject>
</mediaobject>
</informalfigure>
<para>クラスタごとに、作成されたBundleDeploymentリソースにバンドルが適用されます。BundleDeploymentの詳細を表示するには、［Git
Repos (Gitリポジトリ)］の［Detail (詳細)］ページの右上にある ［<literal>Graph
(グラフ)</literal>］ボタンをクリックします。<emphasis role="strong">［Repo (リポジトリ)
］&gt;［Bundles
(バンドル)］&gt;［BundleDeployments］</emphasis>のグラフがロードされます。グラフ内のBundleDeploymentをクリックすると詳細が表示され、［<literal>Id
(ID)</literal>］をクリックするとBundleDeployment YAMLが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleetリポジトリのグラフ</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Fleetのトラブルシューティングのヒントに関する追加情報については、<link
xl:href="https://fleet.rancher.io/troubleshooting">こちら</link>を参照してください。</para>
</section>
<section xml:id="id-fleet-examples">
<title>Fleetの例</title>
<para>Edgeチームは、Fleetを使用してEdgeプロジェクトをインストールする例を含む<link
xl:href="https://github.com/suse-edge/fleet-examples">リポジトリ</link>を維持しています。</para>
<para>Fleetプロジェクトには、<link
xl:href="https://fleet.rancher.io/gitrepo-content">Gitリポジトリ構造</link>のすべてのユースケースをカバーする<link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>リポジトリが含まれています。</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SUSE Linux Micro</title>
<para><link xl:href="https://documentation.suse.com/sle-micro/6.1/">SUSE Linux
Micro公式ドキュメント</link>を参照してください。</para>
<blockquote>
<para>SUSE Linux Microは、エッジ向けの軽量でセキュアなオペレーティングシステムです。SUSE Linux Microには、SUSE Linux
Enterpriseのエンタープライズ向けに強化されたコンポーネントと、開発者が最新のイミュータブルオペレーティングシステムに求める機能が統合されています。その結果、クラス最高のコンプライアンスを備えた信頼性の高いインフラストラクチャプラットフォームが実現し、使いやすさも向上しています。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-suse-linux-micro">
<title>SUSE EdgeでのSUSE Linux Microの用途</title>
<para>SUSEでは、SUSE Linux
Microをプラットフォームスタックのベースオペレーティングシステムとして使用します。これにより、構築基盤となる、安全で安定した最小限のベースが提供されます。</para>
<para>SUSE Linux
Microでは、独自の方法でファイルシステム(Btrfs)スナップショットを使用しており、アップグレードで問題が発生した場合に簡単にロールバックできます。これにより、問題が発生した場合、物理的にアクセスしなくてもプラットフォーム全体をリモートで安全にアップグレードできます。</para>
</section>
<section xml:id="id-best-practices-2">
<title>ベストプラクティス</title>
<section xml:id="id-installation-media">
<title>インストールメディア</title>
<para>SUSE Edgeは、Edge Image Builder (<xref linkend="components-eib"/>)を使用して、SUSE
Linux Microのセルフインストールのインストールイメージを事前設定します。</para>
</section>
<section xml:id="id-local-administration">
<title>ローカル管理</title>
<para>SUSE Linux Microには、Webアプリケーションでホストをローカルに管理できるCockpitが付属しています。</para>
<para>このサービスはデフォルトでは無効になっていますが、systemdサービス<literal>cockpit.socket</literal>を有効にすることで開始できます。</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現時点では、SUSE Linux Microで利用可能なデスクトップ環境はありませんが、コンテナ化されたソリューションが開発中です。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link
xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>は、Kubernetesにベアメタルインフラストラクチャ管理機能を提供するCNCFプロジェクトです。</para>
<para>Metal<superscript>3</superscript>は、<link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>などのアウトオブバンドプロトコルを介した管理をサポートするベアメタルサーバのライフサイクルを管理するためのKubernetesネイティブリソースを提供します。</para>
<para>また、<link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link>も十分にサポートされており、広く採用されているベンダニュートラルなAPIを使用して、複数のインフラストラクチャプロバイダにわたってインフラストラクチャリソースを管理できます。</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>SUSE EdgeでのMetal<superscript>3</superscript>の用途</title>
<para>この方法は、ターゲットハードウェアがアウトオブバンド管理をサポートしていて、完全に自動化されたインフラストラクチャ管理フローが望まれるシナリオで役立ちます。</para>
<para>この方法では宣言型APIが提供されており、このAPIを使用することで、検査、クリーニング、プロビジョニング/プロビジョニング解除の自動化を含む、ベアメタルサーバのインベントリと状態の管理が可能になります。</para>
</section>
<section xml:id="id-known-issues-3">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>アップストリームの<link
xl:href="https://github.com/metal3-io/ip-address-manager">IPアドレス管理コントローラ</link>は、SUSEが選択したネットワーク設定ツールとの互換性がまだないため、現在はサポートされていません。</para>
</listitem>
<listitem>
<para>関連して、IPAMリソースとMetal3DataTemplateのnetworkDataフィールドはサポートされていません。</para>
</listitem>
<listitem>
<para>redfish-virtualmediaを介したデプロイメントのみが現在サポートされています。</para>
</listitem>
<listitem>
<para>Ironic Python Agent (IPA)とターゲットオペレーティングシステム(SL Micro
6.0/6.1)の間で、特に、デバイスに予測可能な名前を設定しようとする際に、ネットワークデバイス名の不整合が発生する可能性があります。</para>
</listitem>
</itemizedlist>
<para>これが発生するのは、Ironic Python Agent (IPA)のカーネルが現在、ターゲットオペレーティングシステム(SL Micro
6.0/6.1)のカーネルと整合していないためです。これにより、ネットワークドライバに不整合が生じ、SL
Microが想定する命名パターンとは異なる命名パターンでIPAがネットワークデバイスを検出できるようになります。</para>
<para>当面の回避策として、以下の2つの異なるアプローチが利用可能です。*
ネットワーク設定で2つの異なるシークレットを作成します。1つはIPAが検出するデバイス名を使用したIPAで使用されるシークレットで、<literal>BareMetalHost</literal>定義の<literal>preprovisioningNetworkDataName</literal>として使用します。もう1つはSL
Microが検出するデバイス名を使用したシークレットで、<literal>BareMetalHost</literal>定義の<literal>networkData.name</literal>として使用します。*
代わりに、生成されたnmconnectionファイル上の他のインタフェースを参照するためにUUIDを使用します。詳細は、<link
xl:href="..tips/metal3.adoc">ヒントとコツ</link>セクションを参照してください。</para>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para><link
xl:href="https://github.com/suse-edge/edge-image-builder">公式リポジトリ</link>を参照してください。</para>
<para>Edge Image Builder (EIB)は、マシンをブートストラップするためのCustomized, Ready-to-Boot
(CRB)ディスクイメージの生成を効率化するツールです。これらのイメージにより、SUSEソフトウェアスタック全体を単一のイメージでエンドツーエンドにデプロイできます。</para>
<para>EIBはあらゆるプロビジョニングシナリオ向けのCRBイメージを作成できますが、EIBが非常に大きな価値を発揮するのは、ネットワークが制限されているか、完全に分離されているエアギャップデプロイメントにおいてです。</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>SUSE EdgeでのEdge Image Builderの用途</title>
<para>SUSE Edgeでは、さまざまなシナリオ用にカスタマイズされたSUSE Linux
Microイメージをシンプルかつ迅速に設定するためにEIBを使用します。これらのシナリオには、以下を使用する仮想マシンとベアメタルマシンのブートストラップが含まれます。</para>
<itemizedlist>
<listitem>
<para>K3s/RKE2 Kubernetesの完全なエアギャップデプロイメント(シングルノードとマルチノード)</para>
</listitem>
<listitem>
<para>HelmチャートとKubernetesマニフェストの完全なエアギャップデプロイメント</para>
</listitem>
<listitem>
<para>Elemental APIを介したRancherへの登録</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>カスタマイズされたネットワーキング(静的IP、ホスト名、VLAN、ボンディングなど)</para>
</listitem>
<listitem>
<para>カスタマイズされたオペレーティングシステム設定(ユーザ、グループ、パスワード、SSHキー、プロキシ、NTP、カスタムSSL証明書など)</para>
</listitem>
<listitem>
<para>ホストレベルおよびサイドロードRPMパッケージのエアギャップインストール(依存関係の解決を含む)</para>
</listitem>
<listitem>
<para>OS管理のためのSUSE Multi-Linux Managerへの登録</para>
</listitem>
<listitem>
<para>組み込みコンテナイメージ</para>
</listitem>
<listitem>
<para>カーネルコマンドライン引数</para>
</listitem>
<listitem>
<para>ブート時に有効化/無効化されるsystemdユニット</para>
</listitem>
<listitem>
<para>手動タスク用のカスタムスクリプトとファイル</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>はじめに</title>
<para>Edge Image Builderの使用とテストに関する包括的なドキュメントについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.3/docs">こちら</link>を参照してください。</para>
<para>また、基本的なデプロイメントシナリオを説明している<xref linkend="quickstart-eib"/>を参照してください。</para>
<para>このツールに慣れたら、EIBの「ヒントとコツ」セクション(<xref
linkend="tips-and-tricks"/>)ページでさらに役立つ情報を見つけてください。</para>
</section>
<section xml:id="id-known-issues-4">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>EIBは、Helmチャートをテンプレート化してテンプレート内のすべてのイメージを解析することで、Helmチャートをエアギャップ化します。Helmチャートですべてのイメージをテンプレート内に保持せず、代わりにイメージをサイドロードする場合、EIBではそれらのイメージを自動的にエアギャップ化できません。これを解決するには、検出されないイメージを定義ファイルの<literal>embeddedArtifactRegistry</literal>セクションに手動で追加します。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Edgeネットワーキング</title>
<para>このセクションでは、SUSE Edgeソリューションにおけるネットワーク設定へのアプローチについて説明します。宣言的な方法でSUSE Linux
Micro上でNetworkManagerを設定する方法を示し、関連ツールの統合方法について説明します。</para>
<section xml:id="id-overview-of-networkmanager">
<title>NetworkManagerの概要</title>
<para>NetworkManagerは、プライマリネットワーク接続と他の接続インタフェースを管理するツールです。</para>
<para>NetworkManagerは、ネットワーク設定を、望ましい状態が含まれる接続ファイルとして保存します。これらの接続は、<literal>/etc/NetworkManager/system-connections/</literal>ディレクトリにファイルとして保存されます。</para>
<para>NetworkManagerの詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-network-configuration/index.html">SUSE
Linux Microのドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>nmstateの概要</title>
<para>nmstateは広く採用されているライブラリ(CLIツールが付属)であり、定義済みスキーマを使用したネットワーク設定用の宣言型APIを提供します。</para>
<para>nmstateの詳細については、<link
xl:href="https://nmstate.io/">アップストリームドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>NetworkManager Configurator (nmc)の概要</title>
<para>SUSE Edgeで利用可能なネットワークのカスタマイズオプションは、NetworkManager Configurator
(短縮名は<emphasis>nmc</emphasis>)と呼ばれるCLIツールを使用して実行します。このツールはnmstateライブラリによって提供される機能を利用しているため、静的IPアドレス、DNSサーバ、VLAN、ボンディング、ブリッジなどを完全に設定できます。このツールを使用して、事前定義された望ましい状態からネットワーク設定を生成し、その設定を多数のノードに自動的に適用できます。</para>
<para>NetworkManager Configurator (nmc)の詳細については、<link
xl:href="https://github.com/suse-edge/nm-configurator">アップストリームリポジトリ</link>を参照してください。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>SUSE EdgeでのNetworkManager Configuratorの用途</title>
<para>SUSE
Edgeは、<emphasis>nmc</emphasis>を利用して次のようなさまざまなプロビジョニングモデルでネットワークをカスタマイズします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングシナリオにおけるカスタムネットワーク設定(<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>イメージベースのプロビジョニングシナリオにおける宣言的な静的設定(<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Edge Image Builderを使用した設定</title>
<para>Edge Image Builder
(EIB)は、1つのOSイメージで複数のホストを設定できるツールです。このセクションでは、宣言型アプローチを使用して、どのように目的のネットワーク状態を記述するかと、それらがどのように各NetworkManager接続に変換され、プロビジョニングプロセス中に適用されるかを示します。</para>
<section xml:id="id-prerequisites-3">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP6またはopenSUSE Leap 15.6を実行しているAMD64/Intel 64物理ホスト(または仮想マシン)</para>
</listitem>
<listitem>
<para>利用可能なコンテナランタイム(Podmanなど)</para>
</listitem>
<listitem>
<para>SUSE Linux Micro 6.1 RAWイメージのコピー(<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあります)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Edge Image Builderのコンテナイメージの取得</title>
<para>EIBのコンテナイメージは一般に公開されており、次のコマンドを実行してSUSE Edgeレジストリからダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>イメージ設定ディレクトリの作成</title>
<para>まず設定ディレクトリを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>ダウンロードしたゴールデンイメージのコピーを確実に設定ディレクトリに移動します。</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SL-Micro.x86_64-6.1-Base-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIBは、ゴールデンイメージの入力を変更することはありません。変更を加えた新しいイメージを作成します。</para>
</note>
</blockquote>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>イメージ定義ファイルの作成</title>
<para>定義ファイルには、Edge Image Builderがサポートする設定オプションの大部分を記述します。</para>
<para>OSイメージの非常に基本的な定義ファイルから開始しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.3
image:
  arch: x86_64
  imageType: raw
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para><literal>image</literal>セクションは必須で、入力イメージ、そのアーキテクチャとタイプ、および出力イメージの名前を指定します。<literal>operatingSystem</literal>セクションはオプションであり、プロビジョニングされたシステムに<literal>root/eib</literal>のユーザ名/パスワードでログインできるようにするための設定を含めます。</para>
<blockquote>
<note>
<para><literal>openssl passwd -6
&lt;password&gt;</literal>を実行して、独自の暗号化パスワードを自由に使用してください。</para>
</note>
</blockquote>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>ネットワーク設定の定義</title>
<para>先ほど作成したイメージ定義ファイルには、望ましいネットワーク設定が含まれていません。そこで、<literal>network/</literal>という特別なディレクトリの下にその設定を入力します。では、作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>前述のように、NetworkManager Configurator
(<emphasis>nmc</emphasis>)ツールでは、事前定義されたスキーマの形式での入力が必要です。さまざまなネットワーキングオプションの設定方法については、<link
xl:href="https://nmstate.io/examples.html">アップストリームのNMStateの例のドキュメント</link>を参照してください。</para>
<para>このガイドでは、次の3つの異なるノードでネットワーキングを設定する方法について説明します。</para>
<itemizedlist>
<listitem>
<para>2つのEthernetインタフェースを使用するノード</para>
</listitem>
<listitem>
<para>ネットワークボンディングを使用するノード</para>
</listitem>
<listitem>
<para>ネットワークブリッジを使用するノード</para>
</listitem>
</itemizedlist>
<warning>
<para>特にKubernetesクラスタを設定する場合、まったく異なるネットワークセットアップを運用ビルドで使用することは推奨されません。ネットワーキング設定は通常、特定のクラスタ内のノード間、または少なくともロール間で同種にすることをお勧めします。このガイドにはさまざまな異なるオプションが含まれていますが、これは参考例として提供することのみを目的としています。</para>
</warning>
<blockquote>
<note>
<para>以下では、IPアドレス範囲<literal>192.168.122.1/24</literal>を使用するデフォルトの<literal>libvirt</literal>ネットワークを想定しています。ご自身の環境でこの範囲が異なる場合は、適宜調整してください。</para>
</note>
</blockquote>
<para><literal>node1.suse.com</literal>という名前の最初のノードに対して、望ましい状態を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>この例では、2つのEthernetインタフェース(eth0とeth3)、要求されたIPアドレス、ルーティング、およびDNS解決の望ましい状態を定義しています。</para>
<warning>
<para>必ず、すべてのEthernetインタフェースのMACアドレスを記述してください。これらのMACアドレスは、プロビジョニングプロセス中にノードの識別子として使用され、どの設定を適用すべきかを判断するのに役立ちます。このようにして、1つのISOまたはRAWイメージを使用して複数のノードを設定できます。</para>
</warning>
<para>次は、<literal>node2.suse.com</literal>という名前の2つ目のノードです。このノードではネットワークボンディングを使用します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>この例では、IPアドレス指定を有効にしていない2つのEthernetインタフェース(eth0とeth1)の望ましい状態と、ラウンドロビンポリシーによるボンディング、およびネットワークトラフィックを転送するために使用する各アドレスを定義します。</para>
<para>最後に、3つ目となる、望ましい状態の最後のファイルを作成します。これはネットワークブリッジを利用し、<literal>node3.suse.com</literal>という名前です。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<blockquote>
<note>
<para><literal>network/</literal>ディレクトリにあるファイル名は意図的なものです。これらの名前は、プロビジョニングプロセス中に設定されるホスト名に対応しています。</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>OSイメージの構築</title>
<para>これで必要な設定はすべて完了したので、次のコマンドを実行するだけでイメージを構築できます。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>上のスニペットから<literal>Network</literal>コンポーネントが正常に設定されていることがわかるので、エッジノードのプロビジョニングに進むことができます。</para>
<blockquote>
<note>
<para>ログファイル(<literal>network-config.log</literal>)とそれぞれのNetworkManager接続ファイルは、イメージ実行のタイムスタンプ付きディレクトリの下にある、結果の<literal>_build</literal>ディレクトリで検査できます。</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>エッジノードのプロビジョニング</title>
<para>作成されたRAWイメージをコピーしてみましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>構築されたイメージを4回コピーしましたが、3つのノードのネットワーク設定しか指定していません。これは、どの目的の設定にも一致しないノードをプロビジョニングするとどうなるかも紹介したいためです。</para>
<blockquote>
<note>
<para>このガイドでは、ノードプロビジョニングの例に仮想化を使用します。必要な拡張機能がBIOSで有効になっていることを確認してください(詳細については
<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">こちら</link>を参照してください)。</para>
</note>
</blockquote>
<para><literal>virt-install</literal>を使用し、コピーしたRAWディスクを使用して仮想マシンを作成します。各仮想マシンは10GBのRAMと6個のvCPUを使用します。</para>
<section xml:id="id-provisioning-the-first-node">
<title>1つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>上記で説明した望ましい状態のMACアドレスと同じMACアドレスを持つネットワークインタフェースを作成することが重要です。</para>
</note>
</blockquote>
<para>操作が完了すると、次のような内容が表示されます。</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Micro 6.0 (x86_64) - Kernel 6.4.0-18-default (tty1).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>これで、<literal>root:eib</literal>の資格情報ペアを使用してログインできます。ここで提示されている<literal>virsh
console</literal>よりもSSHでホストに接続したい場合は、SSHで接続することもできます。</para>
<para>ログインしたら、すべての設定が完了していることを確認しましょう。</para>
<para>ホスト名が適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>2つのEthernetインタフェースが設定されていて、そのうちの1つだけがアクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>2つ目のインタフェースが、目的のネットワーキング状態で指定されている定義済みの<literal>eth3</literal>ではなく、<literal>eth1</literal>になっていることがわかります。これは、NetworkManager
Configurator
(<emphasis>nmc</emphasis>)が、MACアドレス<literal>34:8a:b1:4b:16:e2</literal>を持つNICにOSによって別の名前が付けられていることを検出し、それに応じて設定を調整しているためです。</para>
<para>プロビジョニングのCombustionのフェーズを検査して、この調整が実際に行われたことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>続いて残りのノードをプロビジョニングしますが、ここでは最終的な設定の違いのみを示します。これからプロビジョニングするすべてのノードに対して、上記のチェックのいずれか、またはすべてを自由に適用してください。</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>2つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがボンディングされたインタフェースを使用しているかどうかを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>ルーティングでボンディングが使用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>静的な接続ファイルが適切に利用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>3つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがネットワークブリッジを使用していることを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>ルーティングでブリッジが使用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>静的な接続ファイルが適切に利用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>4つ目のノードのプロビジョニング</title>
<para>最後に、事前定義されたどの設定ともMACアドレスが一致しないノードをプロビジョニングします。このような場合は、DHCPをデフォルトにしてネットワークインタフェースを設定します。</para>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがそのネットワークインタフェースにランダムなIPアドレスを使用していることを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>nmcがこのノードに静的な設定を適用できなかったことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>EthernetインタフェースがDHCPを介して設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="networking-unified">
<title>統合されたノード設定</title>
<para>既知のMACアドレスに依存できない場合もあります。このような場合は、いわゆる<emphasis>統合設定</emphasis>を選択できます。これにより、<literal>_all.yaml</literal>ファイルで設定を指定し、プロビジョニングされたノードすべてに適用することができます。</para>
<para>異なる設定構造を使用して、エッジノードを構築およびプロビジョニングします。<xref
linkend="image-config-dir-creation"/>から<xref
linkend="default-network-definition"/>のすべての手順に従います。</para>
<para>この例では、2つのEthernetインタフェース(eth0とeth1)の望ましい状態を定義します。一方ではDHCPを使用し、他方には静的IPアドレスを割り当てます。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>イメージが正常に構築されたら、それを使用して仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>プロビジョニングプロセスには数分かかる場合があります。終了したら、指定された資格情報でシステムにログインします。</para>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Ethernetインタフェースが設定され、アクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>カスタムネットワーク設定</title>
<para>ここまでは、NetworkManager Configuratorを利用した、Edge Image
Builderのデフォルトのネットワーク設定について説明してきました。一方で、カスタムスクリプトを使用してネットワーク設定を変更するオプションもあります。このオプションは非常に柔軟性が高く、MACアドレスにも依存しませんが、1つのイメージで複数のノードをブートストラップする場合に使用してもあまり便利ではないという制限があります。</para>
<blockquote>
<note>
<para><literal>/network</literal>ディレクトリにある、望ましいネットワーク状態を記述したファイルを介して、デフォルトのネットワーク設定を使用することをお勧めします。カスタムスクリプトを選択するのは、デフォルト設定の動作がユースケースに当てはまらない場合のみにしてください。</para>
</note>
</blockquote>
<para>異なる設定構造を使用して、エッジノードを構築およびプロビジョニングします。<xref
linkend="image-config-dir-creation"/>から<xref
linkend="default-network-definition"/>のすべての手順に従います。</para>
<para>この例では、プロビジョニングされたすべてのノードで<literal>eth0</literal>インタフェースに静的設定を適用し、NetworkManagerによって自動的に作成された有線接続を削除して無効にするカスタムスクリプトを作成します。これは、クラスタ内のすべてのノードに同一のネットワーキング設定を確実に適用したい場合に便利です。その結果、イメージの作成前に各ノードのMACアドレスを気にする必要がなくなります。</para>
<para>まず、<literal>/custom/files</literal>ディレクトリに接続ファイルを保存しましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>静的設定が作成されたので、カスタムネットワークスクリプトも作成します。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>nmcのバイナリはこれまで同様にデフォルトで含まれるため、必要に応じて<literal>configure-network.sh</literal>スクリプトで使用することもできます。</para>
</note>
</blockquote>
<warning>
<para>カスタムスクリプトは常に設定ディレクトリの<literal>/network/configure-network.sh</literal>で提供する必要があります。このファイルが存在する場合、他のファイルはすべて無視されます。YAML形式の静的設定とカスタムスクリプトの両方を同時に使用してネットワークを設定することはできません。</para>
</warning>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>イメージが正常に構築されたら、それを使用して仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>プロビジョニングプロセスには数分かかる場合があります。終了したら、指定された資格情報でシステムにログインします。</para>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>接続ファイルを使用してEthernetインタフェースが静的に設定されていて、アクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elementalは、Kubernetesを使用した完全にクラウドネイティブな集中型のOS管理を可能にするソフトウェアスタックです。Elementalスタックは、Rancher自体またはエッジノード上に存在する多数のコンポーネントで構成されます。中核となるコンポーネントは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> -
Rancher上に存在し、クライアントからの登録リクエストを処理するコアオペレータ。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis> -
エッジノード上で動作し、<literal>elemental-operator</literal>を介して登録できるようにするクライアント。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis> -
エッジノードに存在するエージェント。その設定は<literal>elemental-register</literal>から提供され、<literal>rancher-system-agent</literal>を設定するための<literal>plan</literal>を受け取ります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis> -
エッジノードが完全に登録された後に、<literal>elemental-system-agent</literal>から処理を引き継ぎ、Rancher
Managerからの他の<literal>plans</literal>を待機します(Kubernetesのインストールなど)。</para>
</listitem>
</itemizedlist>
<para>Elemental、およびElementalとRancherとの関係の詳細については、<link
xl:href="https://elemental.docs.rancher.com/">Elementalのアップストリームドキュメント</link>を参照してください。</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>SUSE EdgeでのElementalの用途</title>
<para>SUSEでは、Metal<superscript>3</superscript>を選択できないリモートデバイス(たとえば、BMCがない、デバイスがNATゲートウェイの背後にあるなど)の管理にElementalの一部を使用しています。このツールにより、オペレータは、デバイスがいつどこに配置されるかがわかる前に、ラボでデバイスをブートストラップできます。すなわち、<literal>elemental-register</literal>と<literal>elemental-system-agent</literal>コンポーネントを利用して、「Phone
Home」ネットワークプロビジョニングのユースケースでSUSE Linux MicroホストをRancherにオンボードできます。Edge Image
Builder
(EIB)を使用してデプロイメントイメージを作成する場合、EIBの設定ディレクトリで登録設定を指定することで、Rancherを使用してElemental経由で自動登録を行うことができます。</para>
<note>
<para>SUSE Edge 3.4では、Elementalのオペレーティングシステム管理の側面を利用して<emphasis
role="strong">いない</emphasis>ため、Rancher経由でオペレーティングシステムのパッチを管理することはできません。SUSE
Edgeでは、Elementalツールを使用してデプロイメントイメージを構築する代わりに、登録設定を使用するEdge Image
Builderツールを使用します。</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>ベストプラクティス</title>
<section xml:id="id-installation-media-2">
<title>インストールメディア</title>
<para>「Phone
Homeネットワークプロビジョニング」のデプロイメントフットプリントでElementalを利用してRancherに登録可能なデプロイメントイメージを構築する場合、SUSE
Edgeでは、Elementalを使用したリモートホストのオンボーディング(<xref
linkend="quickstart-elemental"/>)のクイックスタートで詳しく説明されている手順に従う方法をお勧めします。</para>
</section>
<section xml:id="id-labels">
<title>ラベル</title>
<para>Elementalは、<literal>MachineInventory</literal>
CRDを使用してインベントリを追跡し、インベントリを選択する方法を提供します。たとえば、Kubernetesクラスタのデプロイ先のマシンをラベルに基づいて選択できます。これにより、ユーザはハードウェアを購入する前に、インフラストラクチャのニーズの(すべてではないにしても)ほとんどを事前に定義しておくことができます。また、ノードはその各インベントリオブジェクトのラベルを追加/削除できるので(<literal>elemental-register</literal>を、追加のフラグ<literal>--label
"FOO=BAR "</literal>を指定して再実行する)、ノードがブートされた場所を検出してRancherに知らせるスクリプトを作成できます。</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現在のところ、Elemental UIは、インストールメディアの構築方法を認識したり、「Elemental
Teal」以外のオペレーティングシステムを更新したりすることはできません。これは将来のリリースで対応予定です。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link
xl:href="https://k3s.io/">K3s</link>は、リソースに制約のあるリモートの無人の場所やIoTアプライアンス内の運用ワークロード向けに設計された、高可用性のKubernetes認定ディストリビューションです。</para>
<para>単一の小さなバイナリとしてパッケージ化されているため、迅速かつ簡単にインストールおよび更新できます。</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>SUSE EdgeでのK3sの用途</title>
<para>K3sは、SUSE Edgeスタックを支えるKubernetesディストリビューションとして使用できます。K3sはSUSE Linux
Microオペレーティングシステムにインストールすることが意図されています。</para>
<para>K3sをSUSE
EdgeスタックのKubernetesディストリビューションとして使用することは、etcdをバックエンドとして使用したのでは制約に合わない場合にのみ推奨します。etcdをバックエンドとして使用できる場合は、RKE2
(<xref linkend="components-rke2"/>)を使用することをお勧めします。</para>
</section>
<section xml:id="id-best-practices-4">
<title>ベストプラクティス</title>
<section xml:id="id-installation-2">
<title>インストール</title>
<para>K3sをSUSE Edgeスタックの一部としてインストールする場合に推奨する方法は、Edge Image Builder
(EIB)を使用することです。K3sをデプロイするようにEIBを設定する方法の詳細については、EIBのドキュメント(<xref
linkend="components-eib"/>)を参照してください。</para>
<para>この方法では、自動的にHAセットアップとElementalセットアップがサポートされます。</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>GitOpsワークフローでのFleet</title>
<para>SUSE Edgeスタックでは、GitOpsの推奨ツールとしてFleetを使用します
。Fleetのインストールと使用の詳細については、このドキュメントの「Fleet」のセクション(<xref
linkend="components-fleet"/>)を参照してください。</para>
</section>
<section xml:id="id-storage-management">
<title>ストレージ管理</title>
<para>K3sではローカルパスストレージが事前設定されており、これはシングルノードクラスタに適しています。複数のノードにまたがるクラスタの場合は、SUSE
Storage (<xref linkend="components-suse-storage"/>)を使用することをお勧めします。</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>負荷分散とHA</title>
<para>EIBを使用してK3sをインストールした場合、ここで説明する部分は、EIBのドキュメントの「HA」のセクションで説明済みです。</para>
<para>EIBを使用しないでK3sをインストールした場合は、MetalLBのドキュメント(<xref
linkend="guides-metallb-k3s"/>)に従ってMetalLBをインストールおよび設定する必要があります。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para><link xl:href="https://docs.rke2.io/">RKE2の公式ドキュメント</link>を参照してください。</para>
<para>RKE2は、以下によってセキュリティとコンプライアンスに重点を置いた、完全準拠のKubernetesディストリビューションです。</para>
<itemizedlist>
<listitem>
<para>クラスタがCIS Kubernetes Benchmark
v1.6またはv1.23に合格できるデフォルト値と設定オプションを、オペレータの介入を最小限に抑えながら提供する</para>
</listitem>
<listitem>
<para>FIPS 140-2準拠を可能にする</para>
</listitem>
<listitem>
<para><link
xl:href="https://trivy.dev">trivy</link>を使用し、コンポーネントを定期的にスキャンしてRKE2ビルドパイプラインにCVEがないかどうかを確認する</para>
</listitem>
</itemizedlist>
<para>RKE2は、コントロールプレーンコンポーネントを、kubeletによって管理される静的Podとして起動します。組み込みコンテナランタイムはcontainerdです。</para>
<para>メモ: RKE2はRKE Governmentとしても知られます。これは、RKE2が現在ターゲットにしている別のユースケースと分野を表すためです。</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2とK3s</title>
<para>K3sはエッジ、loT、ARMに焦点を当てた、完全準拠の軽量なKubernetes
ディストリビューションであり、使いやすさとリソースに制約のある環境向けに最適化されています。</para>
<para>RKE2は、RKEの1.xバージョン(以下「RKE1」)とK3sの両方の長所を兼ね備えています。</para>
<para>RKE2は、K3sから使いやすさ、操作のしやすさ、およびデプロイメントモデルを継承しています。</para>
<para>RKE1から継承しているのは、アップストリームのKubernetesとの緊密な連携です。K3sはエッジデプロイメントに合わせて最適化されているため、アップストリームのKubernetesとは各所で異なりますが、RKE1とRKE2はアップストリームと緊密な連携を保つことができます。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>SUSE EdgeでのRKE2の用途</title>
<para>RKE2はSUSE Edgeスタックの基礎を成す部分です。RKE2はSUSE Linux Micro (<xref
linkend="components-slmicro"/>)上に位置し、Edgeワークロードをデプロイするために必要な標準Kubernetesインタフェースを提供します。</para>
</section>
<section xml:id="id-best-practices-5">
<title>ベストプラクティス</title>
<section xml:id="id-installation-3">
<title>インストール</title>
<para>RKE2をSUSE Edgeスタックの一部としてインストールする場合に推奨される方法は、Edge Image Builder
(EIB)を使用することです。RKE2をデプロイするようにEIBを設定する方法の詳細については、EIBのドキュメント(<xref
linkend="components-eib"/>)を参照してください。</para>
<para>EIBは十分な柔軟性を備えているため、RKE2のバージョン、<link
xl:href="https://docs.rke2.io/reference/server_config">サーバ</link>、または<link
xl:href="https://docs.rke2.io/reference/linux_agent_config">エージェント</link>設定の指定など、RKE2で要求されるあらゆるパラメータをサポートすることができ、Edgeのすべてのユースケースに対応できます。</para>
<para>Metal<superscript>3</superscript>に関連する他のユースケースでも、RKE2が使用およびインストールされます。このような特定のケースでは、<link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">Cluster
API Provider
RKE2</link>によって、Edgeスタックを使用してMetal<superscript>3</superscript>でプロビジョニングされるクラスタにRKE2が自動的にデプロイされます。</para>
<para>このような場合、関係する各種のCRDにRKE2設定を適用する必要があります。<literal>RKE2ControlPlane</literal>
CRDを使用して異なるCNIを提供する方法の例は、次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>Metal<superscript>3</superscript>のユースケースの詳細については、<xref
linkend="components-metal3"/>を参照してください。</para>
</section>
<section xml:id="id-high-availability">
<title>高可用性</title>
<para>HAデプロイメントの場合、EIBはMetalLB (<xref linkend="components-metallb"/>)とEndpoint
Copier Operator (<xref linkend="components-eco"/>)を自動的にデプロイして設定し、RKE2
APIエンドポイントを外部に公開します。</para>
</section>
<section xml:id="id-networking">
<title>ネットワーキング</title>
<para>SUSE Edgeスタックは、 <link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link>、 <link
xl:href="https://docs.tigera.io/calico/latest/about/">Calico</link>をサポートしており、CiliumをデフォルトのCNIとしています。Podが複数のネットワークインタフェースを必要とする場合には<link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>
meta-pluginも使用できます。RKE2スタンドアロンは、<link
xl:href="https://docs.rke2.io/install/network_options">より幅広いCNIオプション</link>をサポートしています。</para>
</section>
<section xml:id="id-storage">
<title>ストレージ</title>
<para>RKE2は、どのような種類の永続ストレージクラスやオペレータも提供していません。複数のノードにまたがるクラスタの場合は、SUSE Storage
(<xref linkend="components-suse-storage"/>)を使用することをお勧めします。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-suse-storage">
<title><link xl:href="https://www.suse.com/products/rancher/storage/">SUSE
Storage</link></title>
<para>SUSE
Storageは、Kubernetes向けに設計された、信頼性が高くユーザフレンドリな軽量の分散ブロックストレージシステムです。当初はRancher
Labsによって開発され、現在はCNCFの下でインキュベートされているオープンソースプロジェクトであるLonghornをベースにした製品です。</para>
<section xml:id="id-prerequisites-4">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro 6.1がインストールされた最低1台のホスト(物理ホストでも仮想ホストでも可)</para>
</listitem>
<listitem>
<para>インストール済みのKubernetesクラスタ1つ(K3sまたはRKE2)</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-suse-storage">
<title>SUSE Storageの手動インストール</title>
<section xml:id="id-installing-open-iscsi">
<title>Open-iSCSIのインストール</title>
<para>SUSE
Storageをデプロイして使用するための中心的な要件は、<literal>open-iscsi</literal>パッケージをインストールすることと、<literal>iscsid</literal>デーモンをすべてのKubernetesノード上で実行することです。これは、Longhornがホスト上の<literal>iscsiadm</literal>を利用してKubernetesに永続ボリュームを提供するために必要です。</para>
<para>インストールしてみましょう。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>SUSE Linux
Microはイミュータブルオペレーティングシステムであるため、操作が完了すると、パッケージは新しいスナップショットにのみインストールされることに注意することが重要です。パッケージをロードし、<literal>iscsid</literal>デーモンの実行を開始するには、作成した新しいスナップショットで再起動する必要があります。準備が整ったら、rebootコマンドを発行します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>open-iscsiのインストールに関する追加のヘルプについては、<link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/#installing-open-iscsi">Longhornの公式ドキュメント</link>を参照してください。</para>
</tip>
</section>
<section xml:id="id-installing-suse-storage">
<title>SUSE Storageのインストール</title>
<para>KubernetesクラスタにSUSE
Storageをインストールするには複数の方法があります。このガイドでは、Helmでのインストールに従いますが、別のアプローチが必要な場合は<link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/">公式ドキュメント</link>に従ってください。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Rancher Charts Helmリポジトリを追加します。</para>
<screen language="shell" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>リポジトリから最新のチャートをフェッチします。</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para><literal>longhorn-system</literal>ネームスペースにSUSE Storageをインストールします。</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn-crd rancher-charts/longhorn-crd --namespace longhorn-system --create-namespace --version 107.0.0+up1.9.1
helm install longhorn rancher-charts/longhorn --namespace longhorn-system --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>デプロイメントが成功したことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-suse-storage-volumes">
<title>SUSE Storageボリュームの作成</title>
<para>SUSE
Storageは、<literal>StorageClass</literal>というKubernetesリソースを利用して、Podの<literal>PersistentVolume</literal>オブジェクトを自動的にプロビジョニングします。<literal>StorageClass</literal>は、管理者が、自身が提供する<emphasis>クラス</emphasis>または<emphasis>プロファイル</emphasis>を記述する方法だと考えてください。</para>
<para>デフォルトのオプションをいくつか使用して<literal>StorageClass</literal>を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para><literal>StorageClass</literal>を作成したので、それを参照する<literal>PersistentVolumeClaim</literal>が必要です。<literal>PersistentVolumeClaim</literal>
(PVC)は、ユーザによるストレージの要求です。PVCは<literal>PersistentVolume</literal>リソースを使用します。クレームでは、特定のサイズとアクセスモードを要求できます(たとえば、1つのノードで読み取り/書き込み可能でマウントすることも、複数のノードで読み取り専用でマウントすることもできます)。</para>
<para><literal>PersistentVolumeClaim</literal>を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>完了です。<literal>PersistentVolumeClaim</literal>を作成したら、それを<literal>Pod</literal>にアタッチする手順に進むことができます。<literal>Pod</literal>がデプロイされると、KubernetesはLonghornボリュームを作成し、ストレージが利用可能な場合は<literal>Pod</literal>にバインドします。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>Kubernetesにおけるストレージの概念は複雑であると同時に重要なトピックです。最も一般的なKubernetesリソースのいくつかを簡単に説明しましたが、Longhornが提供している<link
xl:href="https://longhorn.io/docs/1.9.1/terminology/">用語のドキュメント</link>をよく理解しておくことをお勧めします。</para>
</tip>
<para>この例では、結果は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>UIへのアクセス</title>
<para>kubectlまたはHelmを使用してLonghornをインストールした場合は、クラスタへの外部トラフィックを許可するようにIngressコントローラを設定する必要があります。認証はデフォルトでは有効になっていません。Rancherカタログアプリを使用していた場合、IngressコントローラはRancherによって自動的に作成され、アクセス制御が設定されています(rancher-proxy)。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Longhornの外部サービスのIPアドレスを取得します。</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para><literal>longghorn-frontend</literal>のIPアドレスを取得したら、ブラウザでそのアドレスに移動してUIの使用を開始できます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、<xref linkend="components-eib"/>を使用して、ベースとなるSUSE Linux Micro
OSイメージをカスタマイズしています。ここでは、イメージをカスタマイズしてRKE2クラスタとLonghornをSUSE Linux
Micro上にプロビジョニングする方法について説明します。</para>
<para>定義ファイルを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>Helmチャートの値のカスタマイズは、<literal>helm.charts[].valuesFile</literal>で提供されている別個のファイルを使用して実行できます。詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md#kubernetes">アップストリームドキュメント</link>を参照してください。</para>
</note>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>イメージが構築されたら、それを使用して物理ホストまたは仮想ホストにOSをインストールできます。プロビジョニングが完了すると、<literal>root:eib</literal>の資格情報ペアを使用してシステムにログインできます。</para>
<para>Longhornが正常にデプロイされていることを確認します。</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>このインストールは、完全なエアギャップ環境では動作しません。このような場合は、<xref
linkend="suse-storage-install"/>を参照してください。</para>
</note>
</section>
</chapter>
<chapter xml:id="components-suse-security">
<title><link xl:href="https://www.suse.com/products/rancher/security/">SUSE
Security</link></title>
<para>SUSE
SecurityはKubernetes向けのセキュリティソリューションであり、L7ネットワークセキュリティ、ランタイムセキュリティ、サプライチェーンセキュリティ、およびコンプライアンスチェックを1つの統合パッケージで提供します。</para>
<para>SUSE
Securityは、複数のコンテナのプラットフォームとしてデプロイされ、各コンテナがさまざまなポートやインタフェースを介して通信する製品です。内部的には、その基礎となるコンテナセキュリティコンポーネントとしてNeuVectorを使用しています。SUSE
Securityプラットフォームを構成するコンテナは次のとおりです。</para>
<itemizedlist>
<listitem>
<para>Manager
。Webベースのコンソールを提供するステートレスコンテナです。通常、Managerは1つだけ必要で、どこでも実行できます。Managerにエラーが発生しても、ControllerやEnforcerの動作には影響しません。ただし、特定の通知(イベント)と最近の接続データはManagerによってメモリ内にキャッシュされているため、これらの表示には影響があります。</para>
</listitem>
<listitem>
<para>Controller。SUSE
Securityの「コントロールプレーン」は必ずHA設定でデプロイされるため、ノードのエラーで設定が失われることはありません。Controllerはどこでも実行できますが、その重要性から、顧客はほとんどの場合、「管理」ノード、マスタノード、またはインフラノードに配置することを選択します。</para>
</listitem>
<listitem>
<para>Enforcer。このコンテナはDaemonSetとしてデプロイされるため、保護する各ノードに1つのEnforcerが存在します。通常はすべてのワーカーノードにデプロイされますが、スケジュールを有効にしてマスタノードやインフラノードにデプロイすることもできます。メモ:
Enforcerがクラスタノードに存在しない状況で、そのノード上のPodから接続が行われた場合、その接続はSUSE
Securityによって「unmanaged」ワークロードとしてラベル付けされます。</para>
</listitem>
<listitem>
<para>Scanner。コントローラの指示に従って、ビルトインCVEデータベースを使用して脆弱性スキャンを実行します。複数のScannerをデプロイしてスキャン能力を拡張できます。Scannerはどこでも実行できますが、コントローラが実行されるノードで実行されることがほとんどです。Scannerノードのサイジングに関する考慮事項については、以下を参照してください。ビルドフェーズのスキャンに使用する場合、Scannerを独立して呼び出すこともできます。たとえば、スキャンをトリガし、結果を取得してScannerを停止するパイプライン内で使用する場合などです。Scannerには最新のCVEデータベースが含まれているため、毎日更新する必要があります。</para>
</listitem>
<listitem>
<para>Updater。Updaterは、CVEデータベースの更新が必要な場合に、Kubernetes
cronジョブを通じてScannerの更新をトリガします。必ず使用環境に合わせて設定してください。</para>
</listitem>
</itemizedlist>
<para>SUSE Securityのオンボーディングの詳細とベストプラクティスのドキュメントについては、<link
xl:href="https://open-docs.neuvector.com/">こちら</link>をご覧ください。</para>
<section xml:id="id-how-does-suse-edge-use-suse-security">
<title>SUSE EdgeでのSUSE Securityの用途</title>
<para>SUSE Edgeは、エッジデプロイメントの開始点として簡潔なSUSE Security設定を提供します。</para>
</section>
<section xml:id="id-important-notes">
<title>重要なメモ</title>
<itemizedlist>
<listitem>
<para><literal>Scanner</literal>コンテナには、スキャンするイメージをメモリに取り込んで解凍するのに十分なメモリが必要です。1GBを超えるイメージをスキャンするには、Scannerのメモリを、予想される最大イメージサイズをわずかに上回るサイズまで増やしてください。</para>
</listitem>
<listitem>
<para>保護モードでは、大量のネットワーク接続が予想されます。保護(インラインファイアウォールでのブロック)モードの場合、<literal>Enforcer</literal>では、接続および想定されるペイロードを保持して検査(DLP)するためにCPUとメモリが必要です。メモリを増やし、1つのCPUコアを<literal>Enforcer</literal>専用にすることで、十分なパケットフィルタリング容量を確保できます。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、SUSE Linux Micro OSのゴールデンイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBでプロビジョニングしたKubernetesクラスタ上にSUSE
Securityをエアギャップインストールするには、<xref linkend="suse-security-install"/>に従ってください。</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para><link
xl:href="https://metallb.universe.tf/">MetalLBの公式ドキュメント</link>を参照してください。</para>
<blockquote>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>ベアメタル環境では、ネットワークロードバランサの設定がクラウドセットアップよりも著しく複雑になります。クラウド設定でのわかりやすいAPIコールとは異なり、ベアメタルでは、高可用性(HA)を管理したり、シングルノードのロードバランサに特有の潜在的な単一障害点(SPOF)に対処したりするために、専用のネットワークアプライアンス、またはロードバランサと仮想IP
(VIP)設定の組み合わせが必要になります。このような設定は自動化しにくく、コンポーネントが動的にスケールアップ/ダウンするKubernetesのデプロイメントでは課題となります。</para>
<para>MetalLBでは、こうした課題に対処するために、Kubernetesモデルを利用してLoadBalancerタイプのサービスを作成し、ベアメタルセットアップであってもクラウド環境であるかのように動作させます。</para>
<para>2つの異なるアプローチがあります。<link
xl:href="https://metallb.universe.tf/concepts/layer2/">L2モード</link>(ARP<emphasis>トリック</emphasis>を使用する)アプローチか、<link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>を使用するアプローチです。主にL2では特別なネットワーク機器は必要ありませんが、一般的にはBGPのほうが優れています。これはユースケースによって異なります。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>SUSE EdgeでのMetalLBの用途</title>
<para>SUSE Edgeは、MetalLBを主に次の3つの方法で使用します。</para>
<itemizedlist>
<listitem>
<para>ロードバランサソリューションとして: MetalLBは、ベアメタルマシン用のロードバランサソリューションとして機能します。</para>
</listitem>
<listitem>
<para>HA K3s/RKE2セットアップの場合: MetalLBでは、仮想IPアドレスを使用してKubernetes APIを負荷分散できます。</para>
</listitem>
<listitem>
<para>MetalLBがサービスIPへのルートを近隣ルータにアドバタイズするL3 BGPソリューションとして。</para>
</listitem>
</itemizedlist>
<note>
<para>APIを公開できるようにするため、Endpoint Copier Operator (<xref
linkend="components-eco"/>)を使用して、
<literal>kubernetes</literal>サービスから<literal>kubernetes-vip</literal>
LoadBalancerサービスへのK8s APIエンドポイントの同期を維持します。</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>ベストプラクティス</title>
<para>L2モードでのMetalLBのインストールについては、<xref
linkend="guides-metallb-k3s"/>で説明されており、L3モードの場合は<xref
linkend="guides-metallb-k3s-l3"/>で説明されています。</para>
<para><literal>kube-api-server</literal>の前面にインストールして高可用性トポロジを実現する方法のガイドについては、<xref
linkend="guides-metallb-kubernetes"/>を参照してください。</para>
</section>
<section xml:id="id-known-issues-6">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>K3sには、<literal>Klipper</literal>というロードバランサソリューションが付属しています。MetalLBを使用するには、<literal>Klipper</literal>を無効にする必要があります。このためには、<link
xl:href="https://docs.k3s.io/networking">K3sのドキュメント</link>で説明されているように、<literal>--disable
servicelb</literal>オプションを指定してK3sサーバを起動します。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eco">
<title>Endpoint Copier Operator</title>
<para><link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier
Operator</link>は、Kubernetesサービスとエンドポイントのコピーを作成し、同期を維持することを目的としたKubernetesオペレータです。</para>
<section xml:id="id-how-does-suse-edge-use-endpoint-copier-operator">
<title>SUSE EdgeでのEndpoint Copier Operatorの用途</title>
<para>SUSE Edgeでは、Endpoint Copier
OperatorがK3s/RKE2クラスタの高可用性(HA)セットアップを実現するために重要な役割を果たします。これは、<literal>LoadBalancer</literal>タイプの<literal>kubernetes-vip</literal>サービスを作成することで達成され、そのエンドポイントがkubernetesエンドポイントと常に同期を維持するようにします。公開されているIPアドレスはクラスタに参加するために他のノードから使用されるため、MetalLB
(<xref linkend="components-metallb"/>)は
<literal>kubernetes-vip</literal>サービスを管理するために活用されます。</para>
</section>
<section xml:id="id-best-practices-7">
<title>ベストプラクティス</title>
<para>Endpoint Copier Operatorの使用に関する包括的なドキュメントについては、<link
xl:href="https://github.com/suse-edge/endpoint-copier-operator/blob/main/README.md">こちら</link>をご覧ください。</para>
<para>また、Endpoint Copier OperatorとMetalLBを使用してK3s/RKE2HA
セットアップを実行するする方法については、SUSEのガイド(<xref
linkend="guides-metallb-k3s"/>)を参照してください。</para>
</section>
<section xml:id="id-known-issues-7">
<title>既知の問題</title>
<para>現在、Endpoint Copier
Operatorは1つのサービス/エンドポイントにのみ対応しています。今後、複数のサービス/エンドポイントをサポートする機能拡張が予定されています。</para>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>このセクションでは、Edge Virtualizationを使用してエッジノードで仮想マシンを実行する方法について説明します。Edge
Virtualizationは、軽量な仮想化ユースケース向けに設計されており、仮想化およびコンテナ化されたアプリケーションのデプロイメントと管理に共通のワークフローが利用されることが想定されています。</para>
<para>SUSE Edge Virtualizationでは、仮想マシンの実行方法として次の2つをサポートしています。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ホストレベルでlibvirt+qemu-kvmを介して仮想マシンを手動でデプロイする(Kubernetesは関与しない)</para>
</listitem>
<listitem>
<para>KubeVirtオペレータをデプロイし、Kubernetesベースで仮想マシンを管理する</para>
</listitem>
</orderedlist>
<para>どちらのオプションも有効ですが、以下では2番目のオプションのみを説明しています。SUSE Linux
Microで提供されている、すぐに使用できる標準の仮想化メカニズムを使用する場合は、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/chap-virtualization-introduction.html">こちら</link>で包括的なガイドを参照してください。このガイドは主にSUSE
Linux Enterprise Server用に記載されていますが、概念はほぼ同じです。</para>
<para>このガイドではまず、事前にデプロイ済みのシステムに追加の仮想化コンポーネントをデプロイする方法について説明しますが、その後に続くセクションでは、Edge
Image
Builderを使用してこの設定を最初のデプロイメントに組み込む方法を説明しています。基本手順を実行して環境を手動で設定する必要がない場合は、そちらのセクションに進んでください。</para>
<section xml:id="id-kubevirt-overview">
<title>KubeVirtの概要</title>
<para>KubeVirtでは、仮想マシンと他のコンテナ化ワークロードを併せてKubernetesで管理できます。これを実現するために、Linux仮想化スタックのユーザスペース部分をコンテナ内で実行します。これにより、ホストシステムの要件が最小限に抑えられ、セットアップと管理が容易になります。</para>
<informalexample>
<para>KubeVirtのアーキテクチャの詳細については、<link
xl:href="https://kubevirt.io/user-guide/architecture/">アップストリームドキュメント</link>を参照してください。</para>
</informalexample>
</section>
<section xml:id="id-prerequisites-5">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro
6.1がインストールされ、BIOSで仮想化拡張機能が有効になっている少なくとも1台の物理ホスト(詳細については、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">こちら</link>を参照してください)。</para>
</listitem>
<listitem>
<para>ノード全体で、K3s/RKE2
Kubernetesクラスタがすでにデプロイされており、クラスタへのスーパーユーザアクセスを可能にする適切な<literal>kubeconfig</literal>が設定されている。</para>
</listitem>
<listitem>
<para>ルートユーザへのアクセス —
以下の説明では、自身がルートユーザであり、<literal>sudo</literal>を使用して特権を昇格して<emphasis>いない</emphasis>ことを想定しています。</para>
</listitem>
<listitem>
<para><link
xl:href="https://helm.sh/docs/intro/install/">Helm</link>がローカルで利用可能で、適切なネットワーク接続を備えていて、Kubernetesクラスタに設定をプッシュし、必要なイメージをダウンロードできる。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Edge Virtualizationの手動インストール</title>
<para>このガイドでは、Kubernetesのデプロイメント手順については説明しませんが、SUSE Edgeに適したバージョンの<link
xl:href="https://k3s.io/">K3s</link>または<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>がインストールされていること、およびkubeconfigが適切に設定されていて標準の<literal>kubectl</literal>コマンドをスーパーユーザとして実行できることを想定しています。また、シングルノードクラスタを形成することを想定していますが、マルチノードのデプロイメントでも大きな違いはないと考えられます。</para>
<para>SUSE Edge Virtualizationは、3つの別個のHelmチャートを使用してデプロイします。具体的には次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>:
中心的な仮想化コンポーネント。つまり、Kubernetesが仮想マシンをデプロイおよび管理できるようにするために必要なKubernetes
CRD、オペレータ、およびその他のコンポーネント。</para>
</listitem>
<listitem>
<para><emphasis role="strong">KubeVirtダッシュボード拡張機能</emphasis>:
仮想マシンの起動/停止やコンソールへのアクセスなど、基本的な仮想マシン管理を実行できるオプションのRancher UI拡張機能。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>:
KubeVirtの永続ストレージの統合を可能にする追加コンポーネント。仮想マシンが既存のKubernetesストレージバックエンドをデータ用に使用する機能を提供するだけでなく、ユーザが仮想マシンのデータボリュームのインポートまたはクローンの作成を行うことも可能にします。</para>
</listitem>
</itemizedlist>
<para>これらの各Helmチャートは、現在使用しているSUSE
Edgeのリリースに従ってバージョン管理されています。運用での使用/サポートされる使用のためには、SUSEレジストリにあるアーティファクトを使用してください。</para>
<para>まず、 <literal>kubectl</literal>のアクセスが機能していることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.30.5+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1</screen>
<para>これで、<emphasis role="strong">KubeVirt</emphasis>および<emphasis
role="strong">Containerized Data Importer
(CDI)</emphasis>のHelmチャートのインストールに進むことができます。</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/charts/kubevirt --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/charts/cdi --namespace cdi-system --create-namespace</screen>
<para>数分ですべてのKubeVirtおよびCDIコンポーネントがデプロイされるはずです。これを検証するには、<literal>kubevirt-system</literal>および<literal>cdi-system</literal>のネームスペース内にデプロイされたすべてのリソースを確認します。</para>
<para>KubeVirtリソースを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>CDIリソースを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para><literal>VirtualMachine</literal>カスタムリソース定義(CRD)がデプロイされていることを確認するには、次のコマンドで検証できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para><literal>VirtualMachine</literal>オブジェクトの定義が出力され、次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>仮想マシンのデプロイ</title>
<para>KubeVirtとCDIがデプロイされたので、<link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE
Tumbleweed</link>に基づくシンプルな仮想マシンを定義してみましょう。この仮想マシンは最もシンプルな設定であり、標準の「Podネットワーキング」を使用して、他のPodと同じネットワーキング設定を行います。また、非永続ストレージを使用するため、<link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>を持たないコンテナと同様に、ストレージは一時的なものになります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>これにより、<literal>VirtualMachine</literal>が作成されたことを示すメッセージが出力されます。</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>この<literal>VirtualMachine</literal>定義は最小限であり、設定はほとんど指定されていません。この定義は単に、この仮想マシンが、一時的な<literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>に基づくディスクイメージ(つまり、リモートイメージリポジトリからのコンテナイメージに保存されるディスクイメージ)を使用する、2GBのメモリを備えたマシンタイプ「<link
xl:href="https://wiki.qemu.org/Features/Q35">q35</link>」であることを示しています。また、base64でエンコードされたcloudInitディスクを指定しており、このディスクはブート時にユーザを作成してパスワードを適用する目的にのみ使用します(デコードには<literal>base64
-d</literal>を使用します)。</para>
<blockquote>
<note>
<para>この仮想マシンイメージはテスト専用です。このイメージは公式にサポートされておらず、ドキュメントの例としてのみ使用されています。</para>
</note>
</blockquote>
<para>このマシンは、openSUSE
Tumbleweedのディスクイメージをダウンロードする必要があるためブートに数分かかりますが、ブートが完了したら、次のコマンドで仮想マシンの情報をチェックして、仮想マシンの詳細を確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>これにより、仮想マシンが起動されたノードと、仮想マシンのIPアドレスが出力されます。Podネットワーキングを使用しているため、報告されるIPアドレスは他のPodと同様であり、ルーティング可能であることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>これらのコマンドをKubernetesクラスタノード自体で実行する場合は、トラフィックをPodに直接ルーティングするCNI
(Ciliumなど)を使用して、マシン自体に直接<literal>ssh</literal>で接続できるはずです。次のIPアドレスを、仮想マシンに割り当てられているIPアドレスに置き換えます。</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>この仮想マシンに接続すると、さまざまな操作を試すことができますが、リソースの点で制限があり、ディスク容量は1GBしかないことに注意してください。終了したら、<literal>Ctrl-D</literal>または<literal>exit</literal>でSSHセッションを切断します。</para>
<para>仮想マシンプロセスは、依然として標準のKubernetes
Podでラップされています。<literal>VirtualMachine</literal>
CRDは目的の仮想マシンを表していますが、仮想マシンが実際に起動されるプロセスは、他のアプリケーションと同様に、標準のKubernetes
Podである<literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>
Podを介して行われます。起動されたすべての仮想マシンに対して、<literal>virt-launcher</literal>
Podが存在することがわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>次に、定義したTumbleweedマシンの1つの<literal>virt-launcher</literal> Podが表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>この<literal>virt-launcher</literal>
Podを調べてみると、<literal>libvirt</literal>プロセスと<literal>qemu-kvm</literal>プロセスを実行していることがわかります。このPod自体を起動して詳細を確認できます。次のコマンドは、使用しているPodの名前に合わせて調整する必要があることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Podが起動したら、<literal>virsh</literal>コマンドを実行するのと併せて、プロセスを確認してみます。<literal>qemu-system-x86_64</literal>バイナリに加え、仮想マシンを監視するための特定のプロセスも実行されていることがわかります。また、ディスクイメージの場所と、ネットワーキングが(タップデバイスとして)どのように接続されているかもわかります。</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>最後に、この仮称マシンを削除して、クリーンアップしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>virtctlの使用</title>
<para>KubeVirtには、標準のKubernetes
CLIツールである<literal>kubectl</literal>とともに、仮想化の世界とKubernetesが設計された世界との間のギャップを埋める方法でクラスタとのインタフェースを可能にするCLIユーティリティが付属しています。たとえば、
<literal>virtctl</literal>ツールは、APIやCRDを直接使用することなく、仮想マシンのライフサイクル(起動、停止、再起動など)の管理、仮想コンソールへのアクセスの提供、仮想マシンイメージのアップロード、サービスなどのKubernetesコンストラクトとのインタフェースを行う機能を提供します。</para>
<para><literal>virtctl</literal>ツールの最新の安定バージョンをダウンロードしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.5.2
$ wget https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64</screen>
<para>別のアーキテクチャまたはLinux以外のマシンを使用している場合は、他のリリースを<link
xl:href="https://github.com/kubevirt/kubevirt/releases">こちら</link>で見つけることができます。続行する前に、この実行可能ファイルを作成する必要があります。また、実行可能ファイルを<literal>$PATH</literal>内の特定の場所に移動すると便利な場合があります。</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-$VERSION-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>その後、<literal>virtctl</literal>コマンドラインツールを使用して、仮想マシンを作成できます。出力を<literal>kubectl
apply</literal>に直接パイプしていることに注意して、前の仮想マシンを複製してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>これで、仮想マシンが実行されているのがわかります(コンテナイメージがキャッシュされるため、今回はかなり早く起動するはずです)。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>これで、 <literal>virtctl</literal>を使用して仮想マシンに直接接続できるようになりました。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para><literal>virtctl</literal>で使用可能なコマンドはほかにも多数あります。たとえば、 <literal>virtctl
console</literal>を使用すると、ネットワーキングが機能していない場合にシリアルコンソールにアクセスでき、<literal>virtctl
guestosinfo</literal>を使用すると、ゲストに<literal>qemu-guest-agent</literal>がインストールされていて実行されていれば、包括的なOS情報を取得できます。</para>
<para>最後に、仮想マシンを一時停止し、再開してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para><literal>VirtualMachine</literal>オブジェクトが「<emphasis
role="strong">Paused</emphasis>」と表示され、<literal>VirtualMachineInstance</literal>オブジェクトは「<emphasis
role="strong">Running</emphasis>」ですが「<emphasis
role="strong">READY=False</emphasis>」と表示されているのがわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>また、仮想マシンに接続できなくなっていることもわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>仮想マシンを再開して、もう一度試してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>これで、接続を再確立できるはずです。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>最後に、仮想マシンを削除しましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>シンプルなIngressネットワーキング</title>
<para>このセクションでは、仮想マシンを標準のKubernetesサービスとして公開し、<link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINXとRKE2</link>、<link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">TraefikとK3s</link>などのKubernetes
Ingressサービスを介して利用可能にする方法を示します。このドキュメントでは、これらのコンポーネントがすでに適切に設定されていること、およびKubernetesサーバノードまたはIngress仮想IPを指す適切なDNSポインタが設定されていて(ワイルドカードを使用するなど)、Ingressを適切に解決できることを前提としています。</para>
<blockquote>
<note>
<para>SUSE Edge
3.1以降では、マルチサーバノード設定でK3sを使用している場合、Ingress用にMetalLBベースのVIPを設定する必要がある場合があります。RKE2では、これは不要です。</para>
</note>
</blockquote>
<para>この例の環境では、別のopenSUSE
Tumbleweed仮想マシンをデプロイし、cloud-initを使用して、ブート時にNGINXをシンプルなWebサーバとしてインストールしています。また、呼び出しの実行時に期待どおりに動作することを確認するためにシンプルなメッセージを返すように設定しています。この処理を確認するには、以下の出力のcloud-initのセクションに対して<literal>base64
-d</literal>を実行するだけです。</para>
<para>では、この仮想マシンを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>この仮想マシンが正常に起動したら、<literal>virtctl</literal>コマンドを使用して、外部ポート<literal>8080</literal>とターゲットポート<literal>80</literal>
(NGINXがデフォルトでリスンするポート)で<literal>VirtualMachineInstance</literal>を公開できます。<literal>virtctl</literal>コマンドは、仮想マシンオブジェクトとPodのマッピングを理解しているため、ここではこのコマンドを使用します。これにより、新しいサービスが作成されます。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>これで、適切なサービスが自動的に作成されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>次に、<literal>kubectl create
ingress</literal>を使用すると、このサービスを指すIngressオブジェクトを作成できます。ここでURL (<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">Ingress</link>オブジェクトの「ホスト」として知られている)をDNS設定に合わせて調整し、ポート<literal>8080</literal>を指すようにします。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>DNSが正しく設定されたら、URLに対してすぐにcurlを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>この仮想マシンとそのサービス、およびIngressリソースを削除して、クリーンアップしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Rancher UI拡張機能の使用</title>
<para>SUSE Edge VirtualizationはRancher Manager用のUI拡張機能を提供しており、Rancher Dashboard
UIを使用して基本的な仮想マシン管理を行うことができます。</para>
<section xml:id="id-installation-4">
<title>インストール</title>
<para>インストールのガイダンスについては、Rancher Dashboard拡張機能(<xref
linkend="components-rancher-dashboard-extensions"/>)を参照してください。</para>
</section>
<section xml:id="kubevirt-dashboard-extension-usage">
<title>KubeVirt Rancher Dashboard拡張機能の使用</title>
<para>この拡張機能により、Cluster Explorerに新たに<emphasis
role="strong">［KubeVirt］</emphasis>セクションが導入されます。このセクションは、KubeVirtがインストールされている管理対象クラスタに追加されます。</para>
<para>この拡張機能を使用すると、KubeVirt仮想マシンリソースを直接操作し、仮想マシンのライフサイクルを管理できます。</para>
<section xml:id="id-creating-a-virtual-machine">
<title>仮想マシンの作成</title>
<orderedlist numeration="arabic">
<listitem>
<para>左側のナビゲーションでKubeVirtが有効な管理対象クラスタをクリックして、<emphasis role="strong">［Cluster
Explorer］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［KubeVirt］ </emphasis> &gt; ［Virtual Machines
(仮想マシン)］ページで、画面の右上にある［ <literal>Create from YAML
(YAMLから作成)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>仮想マシンの定義を入力するか貼り付けて、［<literal>Create (作成)</literal>］を押します。「仮想マシンのデプロイ
」セクションの仮想マシンの定義を参考にしてください。</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width="100%"/>
</imageobject>
<textobject><phrase>［Virtual Machines (仮想マシン)］ページ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-virtual-machine-actions">
<title>仮想マシンのアクション</title>
<para>各仮想マシンの右側にある <emphasis role="strong">⋮</emphasis>
ドロップダウンリストからアクセスできるアクションメニューを使用して、起動、停止、一時停止、またはソフト再起動のアクションを実行できます。または、アクションを実行する仮想マシンを選択して、リストの一番上にあるグループアクションを使用することもできます。</para>
<para>アクションを実行すると、仮想マシン実行戦略に影響する場合があります。詳細については<link
xl:href="https://kubevirt.io/user-guide/compute/run_strategies/#virtctl">KubeVirtドキュメントの表</link>を参照
してください。</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>仮想マシンコンソールへのアクセス</title>
<para>［Virtual Machines (仮想マシン)］リストには［<literal>Console</literal>
(コンソール)］ドロップダウンリストがあり、ここから<emphasis
role="strong">VNCまたはシリアルコンソール</emphasis>を使用してマシンに接続できます。このアクションは、実行中のマシンでのみ使用できます。</para>
<para>新しく起動した仮想マシンでは、コンソールにアクセスできるようになるまでにしばらく時間がかかることがあります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width="100%"/>
</imageobject>
<textobject><phrase>VNCコンソールUI</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、SUSE Linux Micro OSのゴールデンイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBでプロビジョニングしたKubernetesクラスタ上にKubeVirtとCDIの両方をエアギャップインストールするには、
<xref linkend="kubevirt-install"/>に従ってください。</para>
</section>
</chapter>
<chapter xml:id="components-system-upgrade-controller">
<title>System Upgrade Controller</title>
<para><link xl:href="https://github.com/rancher/system-upgrade-controller">System
Upgrade Controllerのドキュメント</link>を参照してください。</para>
<blockquote>
<para>System Upgrade Controller
(SUC)は、汎用のKubernetesネイティブアップグレードコントローラ(ノード用)を提供することを目的としています。あらゆるアップグレードポリシー/要件を定義するための新しいCRDであるPlanが導入されています。Planは、クラスタ内のノードを変更する明確な意図です。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-system-upgrade-controller">
<title>SUSE EdgeでのSystem Upgrade Controllerの用途</title>
<para>SUSE
Edgeでは、<literal>SUC</literal>を使用して、管理クラスタとダウンストリームクラスタでのOSとKubernetesのバージョンアップグレードに関連するさまざまな「Day
2」操作を容易にします。</para>
<para>「Day 2」操作は、<literal>SUC
Plan</literal>によって定義されます。これらのプランに基づき、<literal>SUC</literal>は各ノードにワークロードをデプロイし、それぞれの「Day
2」操作を実行します。</para>
<para><literal>SUC</literal>は、<xref
linkend="components-upgrade-controller"/>内でも使用されます。SUCとUpgrade
Controllerの主な違いに関する詳細については、 <xref
linkend="components-upgrade-controller-uc-vs-suc"/>を参照してください。</para>
</section>
<section xml:id="components-system-upgrade-controller-install">
<title>System Upgrade Controllerのインストール</title>
<important>
<para>Rancher <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.10.0">v2.10.0</link>以降、<literal>System
Upgrade Controller</literal>は自動的にインストールされます。</para>
<para>ご使用の環境がRancherによって管理<emphasis
role="strong">されていない</emphasis>場合、またはRancherのバージョンが<literal>v2.10.0</literal>より前の場合に<emphasis
role="strong">のみ</emphasis>、以下の手順に従ってください。</para>
</important>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあるFleet
(<xref linkend="components-fleet"/>)からSUCをインストールすることをお勧めします。</para>
<note>
<para><literal>suse-edge/fleet-examples</literal>リポジトリで提供されるリソースは常に有効な<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">fleet-examples
release</link>から使用される<emphasis
role="strong">必要があります</emphasis>。使用する必要のあるリリースを確認するには、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
</note>
<para>SUCのインストールにFleetを使用できない場合は、RancherのHelmチャートリポジトリからインストールするか、独自のサードパーティGitOpsワークフローにRancherのHelmチャートを組み込むことができます。</para>
<para>このセクションでは以下の内容を取り上げます。</para>
<itemizedlist>
<listitem>
<para>Fleetのインストール(<xref linkend="components-system-upgrade-controller-fleet"/>)</para>
</listitem>
<listitem>
<para>Helmのインストール(<xref linkend="components-system-upgrade-controller-helm"/>)</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet">
<title>System Upgrade Controller Fleetのインストール</title>
<para>Fleetを使用して、SUCをデプロイするために使用できるリソースは2つあります。</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソース -
外部/ローカルGitサーバが利用できるユースケース用。インストール手順については、「System Upgrade Controllerのインストール –
GitRepo」(<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>)を参照してください。</para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソース -
ローカルGitサーバオプションをサポートしないエアギャップ環境のユースケース用。インストール手順については、「System Upgrade
Controllerのインストール - バンドル」(<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>)を参照してください。</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet-gitrepo">
<title>System Upgrade Controllerのインストール - GitRepo</title>
<note>
<para>このプロセスは、使用できる場合はRancher UIから実行することもできます。詳細については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UIでのFleetへのアクセス</link>」を参照してください。</para>
</note>
<para><emphasis role="strong">管理</emphasis>クラスタで、次の操作を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUCをデプロイするクラスタを決定します。これは、<emphasis
role="strong">管理</emphasis>クラスタ内の適切なFleetワークスペースにSUC
<literal>GitRepo</literal>リソースをデプロイすることで実行されます。デフォルトでは、Fleetには2つのワークスペースがあります。</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - <emphasis
role="strong">管理</emphasis>クラスタにデプロイする必要があるリソース用。</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - <emphasis
role="strong">ダウンストリーム</emphasis>クラスタにデプロイする必要があるリソース用。</para>
<para>Fleetワークスペースの詳細については、<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">アップストリーム</link>ドキュメントを参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>GitRepo</literal>リソースをデプロイします。</para>
<itemizedlist>
<listitem>
<para>管理クラスタにSUCをデプロイするには:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-local -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF</screen>
</listitem>
<listitem>
<para>ダウンストリームクラスタにSUCをデプロイするには:</para>
<note>
<para>以下のリソースをデプロイする前に、有効な<literal>ターゲット</literal>設定を提供する<emphasis
role="strong">必要があります</emphasis>。Fleetがリソースをデプロイするダウンストリームクラスタを認識できるようにするためです。ダウンストリームクラスタへのマッピング方法については、「
<link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters (ダウンストリームクラスタへのマッピング) </link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>GitRepo</literal>リソースがデプロイされていることを検証します。</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get gitrepo system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.4.0   1/1</screen>
</listitem>
<listitem>
<para>System Upgrade Controllerのデプロイメントを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-fleet-bundle">
<title>System Upgrade Controllerのインストール - バンドル</title>
<para>このセクションでは、<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>を使用して、標準のFleet設定から<literal>バンドル</literal>リソースをビルドしてデプロイする方法を説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ネットワークにアクセスできるマシンで、<literal>fleet-cli</literal>をダウンロードします。</para>
<note>
<para>ダウンロードしたfleet-cliのバージョンが、クラスタにデプロイされているFleetのバージョンと一致していることを確認してください。</para>
</note>
<itemizedlist>
<listitem>
<para>Macユーザの場合、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</listitem>
<listitem>
<para>LinuxおよびWindowsユーザの場合、Fleet<link
xl:href="https://github.com/rancher/fleet/releases">リリース</link>ごとに<emphasis
role="strong">アセット</emphasis>としてバイナリが存在します。</para>
<itemizedlist>
<listitem>
<para>Linux AMD:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>fleet-cli</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x fleet-cli</screen>
</listitem>
<listitem>
<para>使用する<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のクローンを作成します。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para><literal>fleet-examples</literal>リポジトリにある、SUC Fleetに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller</screen>
</listitem>
<listitem>
<para>SUCをデプロイするクラスタを決定します。これは、管理クラスタ内の適切な
FleetワークスペースにSUCバンドルをデプロイすることで実行されます。デフォルトでは、Fleetには2つのワークスペースがあります。</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - <emphasis
role="strong">管理</emphasis>クラスタにデプロイする必要があるリソース用。</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - <emphasis
role="strong">ダウンストリーム</emphasis>クラスタにデプロイする必要があるリソース用。</para>
<para>Fleetワークスペースの詳細については、<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">アップストリーム</link>ドキュメントを参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ダウンストリームクラスタにのみSUCをデプロイする場合は、特定のクラスタに一致する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>ダウンストリームクラスタへのマッピング方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
<listitem>
<para>バンドルの構築に進みます。</para>
<note>
<para><literal>fleet-examples/fleets/day2/system-upgrade-controller</literal>ディレクトリのfleet-cliをダウンロード<emphasis
role="strong">していない</emphasis>ことを確認してください。これをダウンロードすると、バンドルでパッケージ化され、これは推奨されません。</para>
</note>
<itemizedlist>
<listitem>
<para>管理クラスタにSUCをデプロイするには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>ダウンストリームクラスタにSUCをデプロイするには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
<para>このプロセスの詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートをバンドルに変換する)</link>」を参照してください。</para>
<para><literal>fleet-cli apply</literal>コマンドの詳細については、「<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>system-upgrade-controller-bundle.yaml</literal>バンドルを管理クラスタマシンに転送します。</para>
<screen language="bash" linenumbering="unnumbered">scp system-upgrade-controller-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para>管理クラスタで、<literal>system-upgrade-controller-bundle.yaml</literal>バンドルをデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>管理クラスタで、バンドルがデプロイされていることを検証します。</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get bundle system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1</screen>
</listitem>
<listitem>
<para>バンドルをデプロイしたFleetワークスペースに基づいて、クラスタに移動し、SUCデプロイメントを検証します。</para>
<note>
<para>SUCは常に、<emphasis role="strong">cattle-system</emphasis>ネームスペースにデプロイされます。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-helm">
<title>System Upgrade Controller Helmのインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>Rancherチャートリポジトリを追加します。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>SUCチャートをデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 107.0.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace</screen>
<para>これにより、Edge 3.4プラットフォームに必要なSUCバージョン0.16.0がインストールされます。</para>
</listitem>
<listitem>
<para>SUCデプロイメントを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans">
<title>System Upgrade Controller Planのモニタリング</title>
<para>SUC Planは次の方法で確認できます。</para>
<itemizedlist>
<listitem>
<para>Rancher UI (<xref
linkend="components-system-upgrade-controller-monitor-plans-rancher"/>)を通じて</para>
</listitem>
<listitem>
<para>クラスタ内の手動モニタリング(<xref
linkend="components-system-upgrade-controller-monitor-plans-manual"/>)を通じて</para>
</listitem>
</itemizedlist>
<important>
<para>SUC Plan用にデプロイされたPodは正常実行後、<emphasis role="strong">15</emphasis>分間維持されます
。その後、作成元の対応するJobによって削除されます。この時間後もPodのログにアクセスできるようにするには、クラスタのログ記録を有効にする必要があります。Rancherでこれを実行する方法については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/logging">Rancher
Integration with Logging Services (Rancherとログ記録サービスの統合)</link>」を参照してください。</para>
</important>
<section xml:id="components-system-upgrade-controller-monitor-plans-rancher">
<title>System Upgrade Controller Planのモニタリング - Rancher UI</title>
<para>特定のSUC PlanのPodログを確認するには、次の手順を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">［☰］→［&lt;クラスタ名&gt;］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>［Workloads (ワークロード)］→［Pods］を選択します。</para>
</listitem>
<listitem>
<para>［<literal>Only User Namespaces
(ユーザネームスペースのみ)</literal>］ドロップダウンメニューを選択し、［<literal>cattle-system</literal>］ネームスペースを追加します。</para>
</listitem>
<listitem>
<para>Podフィルタバーに、SUC Plan Podの名前を入力します。名前は次のテンプレート形式になります:
<literal>apply-&lt;plan_name&gt;-on-&lt;node_name&gt;</literal>。</para>
<note>
<para>特定のSUC Planに対して［<literal>Completed (完了)</literal>］Podと［<literal>Unknown
(不明)</literal>］Podの両方が存在する場合があります。これは予期されており、一部のアップグレードの性質により発生します。</para>
</note>
</listitem>
<listitem>
<para>ログを確認するPodを選択し、<emphasis role="strong">［⋮］ → ［View Logs
(ログの表示)］</emphasis>に移動します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans-manual">
<title>System Upgrade Controller Planのモニタリング - 手動</title>
<note>
<para>以下の手順は、 <literal>kubectl</literal>が、<emphasis role="strong">SUC
Plan</emphasis>がデプロイされたクラスタに接続するように設定されていることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>デプロイした<emphasis role="strong">SUC</emphasis> Planを一覧にします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get plans -n cattle-system</screen>
</listitem>
<listitem>
<para><emphasis role="strong">SUC</emphasis> Plan用Podを入手します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l upgrade.cattle.io/plan=&lt;plan_name&gt; -n cattle-system</screen>
<note>
<para>特定のSUC Planに対して［<literal>Completed (完了)</literal>］Podと［<literal>Unknown
(不明)</literal>］Podの両方が存在する場合があります。これは予期されており、一部のアップグレードの性質により発生します。</para>
</note>
</listitem>
<listitem>
<para>Podのログを取得します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n cattle-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="components-upgrade-controller">
<title>Upgrade Controller</title>
<para>次のSUSE Edgeプラットフォームコンポーネントに対してアップグレードを実行できるKubernetesコントローラ:</para>
<itemizedlist>
<listitem>
<para>オペレーティングシステム(SUSE Linux Micro)</para>
</listitem>
<listitem>
<para>Kubernetes (K3sとRKE2)</para>
</listitem>
<listitem>
<para>追加のコンポーネント(Rancher、Elemental、SUSE Securityなど)</para>
</listitem>
</itemizedlist>
<para><link xl:href="https://github.com/suse-edge/upgrade-controller">Upgrade
Controller</link>は、アップグレードの<emphasis
role="strong">トリガ</emphasis>として機能する単一の<literal>ユーザ向け</literal>リソース内に上記コンポーネントの複雑さをカプセル化することで、アップグレードプロセスを効率化します。ユーザはこのリソースを設定する必要があるのみで、<literal>Upgrade
Controller</literal>が残りの作業を行います。</para>
<note>
<para><literal>Upgrade Controller</literal>は現在、<emphasis
role="strong">非エアギャップ管理</emphasis>クラスタに対してのみSUSE
Edgeプラットフォームのアップグレードをサポートしています。詳細については、<xref
linkend="components-upgrade-controller-known-issues"/>セクションを参照してください。</para>
</note>
<section xml:id="id-how-does-suse-edge-use-upgrade-controller">
<title>SUSE EdgeでのUpgrade Controllerの用途</title>
<para><emphasis role="strong">Upgrade Controller</emphasis>は、管理クラスタをあるSUSE
Edgeリリースバージョンから次のバージョンにアップグレードするために必要な(以前は手動の)「Day 2」操作を自動化するために必要不可欠です。</para>
<para>この自動化のために、Upgrade ControllerはSystem Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>)や<link
xl:href="https://github.com/k3s-io/helm-controller/">Helm
Controller</link>などのツールを利用します。</para>
<para>Upgrade Controllerの仕組みの詳細については、<xref
linkend="components-upgrade-controller-how"/>を参照してください。</para>
<para>Upgrade Controllerの既知の制限事項については、<xref
linkend="components-upgrade-controller-known-issues"/>を参照してください。</para>
<para>Upgrade ControllerとSystem Upgrade Controllerの違いについては、<xref
linkend="components-upgrade-controller-uc-vs-suc"/>を参照してください。</para>
</section>
<section xml:id="components-upgrade-controller-uc-vs-suc">
<title>Upgrade ControllerとSystem Upgrade Controller</title>
<para>System Upgrade Controller (SUC) (<xref
linkend="components-system-upgrade-controller"/>)は、アップグレード手順を特定のKubernetesノードに伝播する汎用ツールです。</para>
<para>System Upgrade ControllerはSUSE Edgeプラットフォームの一部の「Day
2」操作をサポートしていますが、すべてをカバー<emphasis
role="strong">しているわけではありません</emphasis>。また、サポートされている操作であっても、ユーザは複数の<literal>SUC
Plan</literal>を手動で設定、維持、およびデプロイする必要があります。これはエラーが発生しやすいプロセスであり、予期しない問題が発生する可能性があります。</para>
<para>これにより、SUSE Edgeプラットフォームのさまざまな「Day 2」操作の複雑さを<emphasis
role="strong">自動化</emphasis>および<emphasis
role="strong">抽象化</emphasis>するツールが必要になりました。こうして<literal>Upgrade
Controller</literal>が 開発されました。Upgrade
Controllerは、アップグレードを推進する単一の<literal>ユーザ向けリソース</literal>を導入することで、アップグレードプロセスをシンプルにします。ユーザはこのリソースを管理する必要があるのみで、
<literal>Upgrade Controller</literal>が残りの作業を行います。</para>
</section>
<section xml:id="components-upgrade-controller-installation">
<title>Upgrade Controllerのインストール</title>
<section xml:id="id-prerequisites-6">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://helm.sh/docs/intro/install/">Helm</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://cert-manager.io/v1.15-docs/installation/helm/#installing-with-helm">cert-manager</link></para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller-install"/>)</para>
</listitem>
<listitem>
<para>Kubernetesクラスタ: K3sまたはRKE2のいずれか</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-steps">
<title>手順</title>
<orderedlist numeration="arabic">
<listitem>
<para>管理クラスタにUpgrade Controller Helmチャートをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install upgrade-controller oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1 --create-namespace --namespace upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Upgrade Controllerのデプロイメントを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Upgrade Controller Podを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Upgrade Controller Podログを検証します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n upgrade-controller-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-how">
<title>Upgrade Controllerの仕組み</title>
<para>Edgeリリースのアップグレードを実行するため、Upgrade Controllerでは2つの新しいKubernetes<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>が導入されました。</para>
<itemizedlist>
<listitem>
<para>UpgradePlan (<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>) -
ユーザが作成、Edgeリリースアップグレードに関する設定を保持。</para>
</listitem>
<listitem>
<para>ReleaseManifest (<xref
linkend="components-upgrade-controller-extensions-release-manifest"/>) -
Upgrade Controllerが作成。特定のEdgeリリースバージョンに固有のコンポーネントバージョンを保持する。<emphasis
role="strong">このファイルはユーザが編集する必要はありません。</emphasis></para>
</listitem>
</itemizedlist>
<para>Upgrade Controllerは、
<literal>UpgradePlan</literal>リソースの<literal>releaseVersion</literal>プロパティでユーザによって指定されたEdgeリリースバージョンのコンポーネントデータを保持する<literal>ReleaseManifest</literal>リソースの作成に進みます。</para>
<para>Upgrade
Controllerは、<literal>ReleaseManifest</literal>のコンポーネントデータを使用して、次の順序のEdgeリリースコンポーネントのアップグレードに進みます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>オペレーティングシステム(OS) (<xref linkend="components-upgrade-controller-how-os"/>)</para>
</listitem>
<listitem>
<para>Kubernetes (<xref linkend="components-upgrade-controller-how-k8s"/>)</para>
</listitem>
<listitem>
<para>追加のコンポーネント(<xref linkend="components-upgrade-controller-how-additional"/>)</para>
</listitem>
</orderedlist>
<note>
<para>アップグレードプロセス中に、Upgrade
Controllerは、作成した<literal>UpgradePlan</literal>にアップグレード情報を絶えず出力します。アップグレードプロセスを追跡する方法の詳細については、
「アップグレードプロセスの追跡」(<xref
linkend="components-upgrade-controller-how-track"/>)を参照してください。</para>
</note>
<section xml:id="components-upgrade-controller-how-os">
<title>オペレーティングシステムのアップグレード</title>
<para>オペレーティングシステムをアップグレードするため、Upgrade Controllerは、次の命名テンプレートを持つSUC (<xref
linkend="components-system-upgrade-controller"/>) Planを作成します。</para>
<itemizedlist>
<listitem>
<para>コントロールプレーンノードのOSアップグレードに関連するSUC Planの場合 -
<literal>control-plane-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal></para>
</listitem>
<listitem>
<para>ワーカーノードのOSアップグレードに関連するSUC Planの場合 -
<literal>workers-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal></para>
</listitem>
</itemizedlist>
<para>これらのプランに基づいて、SUCは、実際のOSアップグレードを実行するクラスタの各ノードにワークロードを作成します。</para>
<para><literal>ReleaseManifest</literal>によって、OSアップグレードには次のものが含まれる場合があります。</para>
<itemizedlist>
<listitem>
<para>Package only updates (パッケージのみの更新) - OSバージョンがEdgeリリース間で変更されないユースケース用。</para>
</listitem>
<listitem>
<para>Full OS migration (完全なOSマイグレーション) - OSバージョンがEdgeリリース間で変更されるユースケース用。</para>
</listitem>
</itemizedlist>
<para>アップグレードは、コントロールプレーンノードから順に一度に<emphasis
role="strong">1台</emphasis>のノードが実行されます。コントロールプレーンノードのアップグレードが終了した場合にのみ、ワーカーノードのアップグレードが開始されます。</para>
<note>
<para>クラスタに特定のタイプの<emphasis role="strong">1台</emphasis>を超えるノードがある場合、 Upgrade
Controllerは、クラスタノードの<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link>を実行するようにOS
SUC Planを設定します。</para>
<para>コントロールノードが1台を<emphasis role="strong">超え</emphasis> 、 <emphasis
role="strong">1台のみ</emphasis>ワーカーノードがあるクラスタでは、コントロールプレーンノードに対してのみdrainが実行されます。その逆も同様です。</para>
<para>ノードのdrainを完全に無効にする方法については、「UpgradePlan」(<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>)のセクションを参照してください。</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-k8s">
<title>Kubernetesのアップグレード</title>
<para>クラスタのKubernetesディストリビューションをアップグレードするため、Upgrade Controllerは、次の命名テンプレートを持つSUC
(<xref linkend="components-system-upgrade-controller"/>) Planを作成します。</para>
<itemizedlist>
<listitem>
<para>コントロールプレーンノードのKubernetesアップグレードに関連するSUC Planの場合 -
<literal>control-plane-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
<listitem>
<para>ワーカーノードのKubernetesアップグレードに関連するSUC Planの場合 -
<literal>workers-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
</itemizedlist>
<para>これらのプランに基づいて、SUCは、実際のKubernetesアップグレードを実行するクラスタの各ノードにワークロードを作成します。</para>
<para>Kubernetesアップグレードは、コントロールプレーンノードから順に一度に<emphasis
role="strong">1台</emphasis>のノードが実行されます。コントロールプレーンノードのアップグレードが終了した場合にのみ、ワーカーノードのアップグレードが開始されます。</para>
<note>
<para>クラスタに特定のタイプの<emphasis role="strong">1台</emphasis>を超えるノードがある場合、 Upgrade
Controllerは、クラスタノードの<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link>を実行するようにKubernetes
SUC Planを設定します。</para>
<para>コントロールノードが1台を<emphasis role="strong">超え</emphasis> 、 <emphasis
role="strong">1台のみ</emphasis>ワーカーノードがあるクラスタでは、コントロールプレーンノードに対してのみdrainが実行されます。その逆も同様です。</para>
<para>ノードのdrainを完全に無効にする方法については、<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>を参照してください。</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-additional">
<title>追加のコンポーネントのアップグレード</title>
<para>現在、追加のコンポーネントはすべてHelmチャートを介してインストールされます。特定のリリースのコンポーネントの全リストについては、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
<para>EIB (<xref linkend="components-eib"/>)を通じてデプロイされたHelmチャートの場合、Upgrade
Controllerは各コンポーネントの既存の<link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart
CR</link>を更新します。</para>
<para>EIB以外でデプロイされたHelmチャートの場合、Upgrade
Controllerは、コンポーネントごとに<literal>HelmChart</literal>リソースを作成します。</para>
<para><literal>HelmChart</literal>リソースの作成/更新後、 Upgrade Controllerは、<link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>に依存してこの変更を取得し、実際のコンポーネントのアップグレードを続行します。</para>
<para>チャートは<literal>ReleaseManifest</literal>の順序に基づいて順次アップグレードされます。追加の値は<literal>UpgradePlan</literal>を通じて渡すこともできます。新しいSUSE
Edgeリリースでチャートのバージョンが変更されない場合、アップグレードされません。これに関する詳細については、<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>を参照してください。</para>
</section>
</section>
<section xml:id="components-upgrade-controller-extensions">
<title>Kubernetes API拡張機能</title>
<para>Upgrade Controllerによって導入されたKubernetes APIの拡張機能。</para>
<section xml:id="components-upgrade-controller-extensions-upgrade-plan">
<title>UpgradePlan</title>
<para>Upgrade Controllerは、<literal>UpgradePlan</literal>と呼ばれる新たなKubernetes <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>を導入しました。</para>
<para><literal>UpgradePlan</literal>は、Upgrade
Controllerの指示メカニズムとして機能し、次の設定をサポートします。</para>
<itemizedlist>
<listitem>
<para><literal>releaseVersion</literal> -
クラスタをアップグレースする必要があるEdgeリリースバージョン。リリースバージョンは、<link
xl:href="https://semver.org">セマンティック</link>バージョン管理に従う必要があり、リリースノート(<xref
linkend="release-notes"/>)から取得する必要があります。</para>
</listitem>
<listitem>
<para><literal>disableDrain</literal> - <emphasis
role="strong">オプション</emphasis>。Upgrade Controllerにノードの<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drains</link>を無効にするかどうかを指示します。<link
xl:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Disruption
Budgets</link>を含むワークロードがある場合に役立ちます。</para>
<itemizedlist>
<listitem>
<para>コントロールプレーンノードのdrain無効化の例:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true</screen>
</listitem>
<listitem>
<para>コントロールプレーンとワーカーノードのdrain無効化の例:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true
    worker: true</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>helm</literal> - <emphasis
role="strong">オプション</emphasis>。Helmを介してインストールされたコンポーネントの追加の値を指定します。</para>
<warning>
<para>アップグレードに重要な値についてのみこのフィールドを使用することをお勧めします。標準のチャート値の更新は、各チャートが次のバージョンにアップグレードされた後に実行する必要があります。</para>
</warning>
<itemizedlist>
<listitem>
<para>例:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  helm:
  - chart: foo
    values:
      bar: baz</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-upgrade-controller-extensions-release-manifest">
<title>ReleaseManifest</title>
<para>Upgrade
Controllerは、<literal>ReleaseManifest</literal>と呼ばれる新たなKubernetes<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>を導入しました。</para>
<para><literal>ReleaseManifest</literal>リソースは、Upgrade Controllerによって作成され、<emphasis
role="strong">1つ</emphasis>の特定のEdgeリリースバージョンのコンポーネントデータを保持します。つまり、各Edgeリリースバージョンのアップグレードは、異なる<literal>ReleaseManifest</literal>リソースによって表されます。</para>
<warning>
<para>Release Manifestは常に、Upgrade Controllerによって作成される必要があります。</para>
<para><literal>ReleaseManifest</literal>リソースを手動で作成または編集することはお勧めしていません。ユーザが手動で作成または編集することを決めた場合は、<emphasis
role="strong">自己責任</emphasis>で行う必要があります。</para>
</warning>
<para>Release Manifestが提供するコンポーネントデータには、以下が含まれますが、これに制限されません。</para>
<itemizedlist>
<listitem>
<para>オペレーティングシステムデータ - バージョン、サポートされているアーキテクチャ、追加のアップグレードデータなど</para>
</listitem>
<listitem>
<para>Kubernetesディストリビューションデータ - <link
xl:href="https://docs.rke2.io">RKE2</link>/<link
xl:href="https://k3s.io">K3s</link>でサポートされているバージョン</para>
</listitem>
<listitem>
<para>追加のコンポーネントデータ - SUSE Helmチャートデータ(場所、バージョン、名前など)</para>
</listitem>
</itemizedlist>
<para>ReleaseManifestの例については、<link
xl:href="https://github.com/suse-edge/upgrade-controller/blob/main/config/samples/lifecycle_v1alpha1_releasemanifest.yaml">アップストリーム</link>ドキュメントを参照してください。<emphasis>これは単なる例であって、有効な<literal>ReleaseManifest</literal>リソースとして作成することを目的としたものではないことに注意してください。</emphasis></para>
</section>
</section>
<section xml:id="components-upgrade-controller-how-track">
<title>アップグレードプロセスの追跡</title>
<para>このセクションは、ユーザが<literal>UpgradePlan</literal>リソースを作成すると、Upgrade
Controllerが開始するアップグレードプロセスを追跡およびデバッグする手段として機能します。</para>
<section xml:id="components-upgrade-controller-how-track-general">
<title>一般</title>
<para>アップグレードプロセスの状態に関する一般情報は、Upgrade Planのステータス状況で確認できます。</para>
<para>Upgrade Planリソースのステータスは、次の方法で確認できます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get upgradeplan &lt;upgradeplan_name&gt; -n upgrade-controller-system -o yaml</screen>
<formalpara>
<title>Upgrade Planの実行例:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Control plane nodes are being upgraded
    reason: InProgress
    status: "False"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Kubernetes upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Rancher upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Longhorn upgrade is not yet started
    reason: Pending
    status: Unknown
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: MetalLB upgrade is not yet started
    reason: Pending
    status: Unknown
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: CDI upgrade is not yet started
    reason: Pending
    status: Unknown
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: KubeVirt upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: NeuVector upgrade is not yet started
    reason: Pending
    status: Unknown
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: EndpointCopierOperator upgrade is not yet started
    reason: Pending
    status: Unknown
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Elemental upgrade is not yet started
    reason: Pending
    status: Unknown
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: SRIOV upgrade is not yet started
    reason: Pending
    status: Unknown
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Metal3 upgrade is not yet started
    reason: Pending
    status: Unknown
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: RancherTurtles upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherTurtlesUpgraded
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
<para>ここで、 Upgrade
Controllerがアップグレードをスケジュールしようとするすべてのコンポーネントを確認できます。各状態は、次のテンプレートに従います。</para>
<itemizedlist>
<listitem>
<para><literal>lastTransitionTime</literal> -
このコンポーネントの状態が、あるステータスから別のステータスに遷移した最後の時刻。</para>
</listitem>
<listitem>
<para><literal>message (メッセージ)</literal> - 特定のコンポーネントの状態の現在のアップグレード状態を示すメッセージ。</para>
</listitem>
<listitem>
<para><literal>reason (理由)</literal> -
特定のコンポーネントの状態の現在のアップグレード状態。考えられる<literal>理由</literal>には次のものが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>Succeeded (成功)</literal> - 特定のコンポーネントのアップグレードが成功しました。</para>
</listitem>
<listitem>
<para><literal>Failed (失敗)</literal> - 特定のコンポーネントアップグレードが失敗しました。</para>
</listitem>
<listitem>
<para><literal>InProgress (進行中)</literal> - 特定のコンポーネントのアップグレードは現在進行中です。</para>
</listitem>
<listitem>
<para><literal>Pending (保留中)</literal> - 特定のコンポーネントのアップグレードは、まだスケジュールされていません。</para>
</listitem>
<listitem>
<para><literal>Skipped (スキップ)</literal> -
特定のコンポーネントがクラスタで見つからないため、アップグレードはスキップされます。</para>
</listitem>
<listitem>
<para><literal>Error (エラー)</literal> - 特定のコンポーネントで一時的なエラーが発生しました。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>status (ステータス)</literal> - 現在の状態
<literal>タイプ</literal>のステータス。<literal>True</literal>、<literal>False</literal>、<literal>Unknown
(不明)</literal>のいずれか。</para>
</listitem>
<listitem>
<para><literal>type (タイプ)</literal> - 現在アップグレードされたコンポーネントのインジケータ。</para>
</listitem>
</itemizedlist>
<para>Upgrade
Controllerは、<literal>OSUpgraded</literal>および<literal>KubernetesUpgraded</literal>タイプのコンポーネント状態のSUC
Planを作成します。これらのコンポーネント用に作成されたSUC Planを詳細に追跡するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<para>他のすべてのコンポーネント状態のタイプは、<link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>によって作成されたリソースを表示することで詳細に追跡できます。詳細については、<xref
linkend="components-upgrade-controller-how-track-helm"/>を参照してください。</para>
<para>Upgrade ControllerによってスケジュールされたUpgrade Planは、次の場合に<literal>successful
(成功)</literal>とマーク付けすることができます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Pending (保留中)</literal>または<literal>InProgress
(進行中)</literal>のコンポーネントの状態がありません。</para>
</listitem>
<listitem>
<para><literal>lastSuccessfulReleaseVersion</literal>プロパティがUpgrade
Planの設定で指定された<literal>releaseVersion</literal>を指しています。<emphasis>このプロパティは、
アップグレードプロセスが成功すると、Upgrade ControllerによってUpgrade
Planのステータスに追加されます。</emphasis></para>
</listitem>
</orderedlist>
<formalpara>
<title>成功した<literal>UpgradePlan</literal>の例:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:48Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:59Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart rancher upgrade succeeded
    reason: Succeeded
    status: "True"
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart longhorn is not installed
    reason: Skipped
    status: "False"
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Specified version of chart metallb is already installed
    reason: Skipped
    status: "False"
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart cdi is not installed
    reason: Skipped
    status: "False"
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart kubevirt is not installed
    reason: Skipped
    status: "False"
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart neuvector-crd is not installed
    reason: Skipped
    status: "False"
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Specified version of chart endpoint-copier-operator is already installed
    reason: Skipped
    status: "False"
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Chart elemental-operator upgrade succeeded
    reason: Succeeded
    status: "True"
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:27:15Z"
    message: Chart sriov-crd is not installed
    reason: Skipped
    status: "False"
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:27:19Z"
    message: Chart metal3 is not installed
    reason: Skipped
    status: "False"
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:27:27Z"
    message: Chart rancher-turtles is not installed
    reason: Skipped
    status: "False"
    type: RancherTurtlesUpgraded
  lastSuccessfulReleaseVersion: 3.4
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
</section>
<section xml:id="components-upgrade-controller-how-track-helm">
<title>Helm Controller</title>
<para>このセクションでは、<link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>によって作成されたリソースを追跡する方法について説明します。</para>
<note>
<para>以下の手順は、<literal>kubectl</literal>がUpgrade
Controllerが導入されているクラスタに接続するように設定されていることを前提としています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>特定のコンポーネントの<literal>HelmChart</literal>リソースを見つけます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmcharts -n kube-system</screen>
</listitem>
<listitem>
<para><literal>HelmChart</literal>リソースの名前を使用して、
<literal>helm-controller</literal>によって作成されたアップグレードPodを見つけます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n kube-system

# Example for Rancher
kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n kube-system
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-tv9wn   0/1     Completed   0          16m</screen>
</listitem>
<listitem>
<para>コンポーネント固有のPodのログを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n kube-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-known-issues">
<title>既知の制限事項</title>
<itemizedlist>
<listitem>
<para>ダウンストリームクラスタのアップグレードはUpgrade
Controllerによってまだ管理されていません。ダウンストリームクラスタをアップグレードする方法については、<xref
linkend="day2-downstream-clusters"/>を参照してください。</para>
</listitem>
<listitem>
<para>Upgrade Controllerは、 EIB (<xref
linkend="components-eib"/>)を通じてデプロイされる追加のSUSE Edge Helmチャートについて、その<link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart
CR</link>が<literal>kube-system</literal>ネームスペースにデプロイされていることを想定しています。これを実行するには、EIB定義ファイルで<literal>installationNamespace</literal>プロパティを設定します。詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">アップストリーム</link>ドキュメントを参照してください。</para>
</listitem>
<listitem>
<para>現在、Upgrade
Controllerには、管理クラスタで現在実行中のEdgeリリースバージョンを判断する方法がありません。クラスタで現在実行しているEdgeリリースバージョンより大きいEdgeリリースバージョンを指定するようにしてください。</para>
</listitem>
<listitem>
<para>現在、Upgrade Controllerは、<emphasis
role="strong">非エアギャップ</emphasis>環境のアップグレードのみをサポートしています。<emphasis
role="strong">エアギャップ</emphasis>アップグレードは、まだ可能ではありません。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Multi-Linux ManagerはSUSE Edgeに含まれており、エッジデプロイメントのすべてのノードでSUSE Linux
Microを基盤となるオペレーティングシステムとして常に最新の状態に保つための自動化と制御を提供します。</para>
<para>詳細については、<xref linkend="quickstart-suma"/>と<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUSE
Multi-Linux Managerのドキュメント</link>を参照してください。</para>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>ハウツーガイド</title>
<partintro>
<para>ハウツーガイドとベストプラクティス</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>K3s上のMetalLB (レイヤ2モードを使用)</title>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>このガイドでは、MetalLBをレイヤ2 (L2)モードでデプロイする方法について説明します。</para>
<section xml:id="id-why-use-metallb">
<title>MetalLBを使用する理由</title>
<para>MetalLBは、いくつかの理由により、ベアメタルKubernetesクラスタでの負荷分散に最適な選択肢です。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Kubernetesとのネイティブ統合:
MetalLBはKubernetesとシームレスに統合されており、使い慣れたKubernetesツールとプラクティスを使用して簡単にデプロイおよび管理できます。</para>
</listitem>
<listitem>
<para>ベアメタルとの互換性:
クラウドベースのロードバランサとは異なり、MetalLBは、従来のロードバランサが利用できない、または実現できないオンプレミスデプロイメント向けに特別に設計されています。</para>
</listitem>
<listitem>
<para>複数のプロトコルをサポート: MetalLBはレイヤ2モードとBGP (Border Gateway
Protocol)モードの両方をサポートし、さまざまなネットワークアーキテクチャと要件に柔軟に対応します。</para>
</listitem>
<listitem>
<para>高可用性: MetalLBは、負荷分散の責任を複数のノードに分散することで、サービスの高可用性と信頼性を保証します。</para>
</listitem>
<listitem>
<para>スケーラビリティ: MetalLBは大規模なデプロイメントに対応し、Kubernetesクラスタに合わせてスケーリングして需要の増加に対応します。</para>
</listitem>
</orderedlist>
<para>レイヤ2モードでは、1つのノードがローカルネットワークにサービスをアドバタイズする責任を負います。ネットワークの視点からは、マシンのネットワークインタフェースに複数のIPアドレスが割り当てられているように見えます。</para>
<para>レイヤ2モードの主な利点は、その汎用性です。あらゆるEthernetネットワークで動作し、特別なハードウェアも、高価なルータも必要ありません。</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>K3s上のMetalLB (L2を使用)</title>
<para>このクイックスタートではL2モードを使用します。つまり、特別なネットワーク機器は必要ありませんが、ネットワーク範囲内の空きIPが3つ必要です。</para>
</section>
<section xml:id="id-prerequisites-7">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>MetalLBがデプロイされるK3sクラスタ。</para>
</listitem>
</itemizedlist>
<warning>
<para>K3sには、Klipperという名前の独自のサービスロードバランサが付属しています。<link
xl:href="https://metallb.universe.tf/configuration/k3s/">MetalLBを実行するにはKlipperを無効にする必要があります</link>。Klipperを無効にするには、<literal>--disable=servicelb</literal>フラグを使用してK3sをインストールする必要があります。</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>ネットワーク範囲内の3つの空きIPアドレス。この例では <literal>192.168.122.10-192.168.122.12</literal></para>
</listitem>
</itemizedlist>
<important>
<para>これらのIPアドレスが未割り当てであることを確認する必要があります。DHCP環境では、二重割り当てを避けるため、これらのアドレスをDHCPプールに含めないでください。</para>
</important>
</section>
<section xml:id="id-deployment-2">
<title>デプロイメント</title>
<para>SUSEでは、SUSE Edgeソリューションの一部として公開されているMetalLB Helmチャートを使用します。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>設定</title>
<para>この時点でインストールは完了しています。次に、サンプル値を使用して<link
xl:href="https://metallb.universe.tf/configuration/">設定</link>を行います。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>これで、MetalLBを使用する準備ができました。L2モードでは、次のようにさまざまな設定をカスタマイズできます。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">IPv6とデュアルスタックサービス</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">アドレスの自動割り当てを制御する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">アドレス割り当ての範囲を特定のネームスペースとサービスに縮小する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">サービスをアナウンスできるノードのセットを制限する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">LBのIPをアナウンスできるネットワークインタフェースを指定する</link></para>
</listitem>
</itemizedlist>
<para>また、<link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>についても、さらに多くのカスタマイズが可能です。</para>
<section xml:id="traefik-and-metallb">
<title>TraefikとMetalLB</title>
<para>Traefikは、デフォルトではK3sとともにデプロイされます(<literal>--disable=traefik</literal>を使用して<link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">K3sを無効にできます</link>)。また、デフォルトで<literal>LoadBalancer</literal>として公開されます(Klipperで使用するため)。ただし、Klipperを無効にする必要があるため、Ingress用Traefikサービスは<literal>LoadBalancer</literal>タイプのままです。そのため、MetalLBをデプロイした時点では、最初のIPは自動的にTraefik
Ingressに割り当てられます。</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>これは、このプロセスの後半(<xref linkend="ingress-with-metallb"/>)で適用されます。</para>
</section>
</section>
<section xml:id="id-usage">
<title>使用法</title>
<para>デプロイメントの例を作成してみましょう。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>最終的にサービスは次のようになります。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>実際の動作を見てみましょう。</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<section xml:id="ingress-with-metallb">
<title>IngressとMetalLB</title>
<para>TraefikはすでにIngressコントローラとして機能しているため、次のような<literal>Ingress</literal>オブジェクトを介してHTTP/HTTPSトラフィックを公開できます。</para>
<screen language="bash" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>これにより、次のような結果が返されます。</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>MetalLBが正しく動作していることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">% arping hellok3s.${IP}.sslip.io

ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>上記の例では、トラフィックは次のように流れます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal>が実際のIPに解決されます。</para>
</listitem>
<listitem>
<para>続いて、トラフィックが<literal>metallb-speaker</literal> Podによって処理されます。</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal>がトラフィックを<literal>traefik</literal>コントローラにリダイレクトします。</para>
</listitem>
<listitem>
<para>最後に、Traefikが要求を<literal>hello-kubernetes</literal>サービスに転送します。</para>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="guides-metallb-k3s-l3">
<title>K3s上のMetalLB (レイヤ3モードを使用)</title>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>このガイドでは、MetalLBをレイヤ3 (L3) BGPモードでデプロイする方法について説明します。</para>
<section xml:id="id-why-use-metallb-2">
<title>MetalLBを使用する理由</title>
<para>MetalLBは、いくつかの理由により、ベアメタルKubernetesクラスタでの負荷分散に最適な選択肢です。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Kubernetesとのネイティブ統合:
MetalLBはKubernetesとシームレスに統合されており、使い慣れたKubernetesツールとプラクティスを使用して簡単にデプロイおよび管理できます。</para>
</listitem>
<listitem>
<para>ベアメタルとの互換性:
クラウドベースのロードバランサとは異なり、MetalLBは、従来のロードバランサが利用できない、または実現できないオンプレミスデプロイメント向けに特別に設計されています。</para>
</listitem>
<listitem>
<para>複数のプロトコルをサポート: MetalLBはレイヤ2モードとレイヤ3 BGP (Border Gateway
Protocol)モードの両方をサポートし、さまざまなネットワークアーキテクチャと要件に柔軟に対応します。</para>
</listitem>
<listitem>
<para>高可用性: MetalLBは、負荷分散の責任を複数のノードに分散することで、サービスの高可用性と信頼性を保証します。</para>
</listitem>
<listitem>
<para>スケーラビリティ: MetalLBは大規模なデプロイメントに対応し、Kubernetesクラスタに合わせてスケーリングして需要の増加に対応します。</para>
</listitem>
</orderedlist>
<para>レイヤ2モードでは、1つのノードがローカルネットワークにサービスをアドバタイズする責任を負います。ネットワークの視点からは、マシンのネットワークインタフェースに複数のIPアドレスが割り当てられているように見えます。</para>
<para>レイヤ2モードの主な利点は、その汎用性です。あらゆるEthernetネットワークで動作し、特別なハードウェアも、高価なルータも必要ありません。</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l3">
<title>K3s上のMetalLB (L3を使用)</title>
<para>このクイックスタートではL3モードを使用します。つまり、ネットワーク範囲内にBGP機能を備えた近隣ルータが必要です。</para>
</section>
<section xml:id="id-prerequisites-8">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>MetalLBがデプロイされるK3sクラスタ。</para>
</listitem>
<listitem>
<para>BGPプロトコルをサポートするネットワーク上のルータ。</para>
</listitem>
<listitem>
<para>サービス用のネットワーク範囲内の空きIPアドレス。この例では <literal>192.168.10.100</literal></para>
</listitem>
</itemizedlist>
<important>
<para>このIPアドレスが未割り当てであることを確認する必要があります。DHCP環境では、二重割り当てを避けるため、このアドレスをDHCPプールに含めないでください。</para>
</important>
</section>
<section xml:id="id-configuration-to-advertise-service-ip-addresses">
<title>サービスIPアドレスをアドバタイズするための設定</title>
<para>すぐに利用可能なBGPは設定済みのすべてのピアにサービスIPアドレスをアドバタイズします。これらのピア(通常はルータ)は、各サービスIPアドレスに対して32ビットのネットワークマスクを持つルートを受け取ります。この例では、クラスタと同じネットワーク上にあるFRRベースのルータを使用します。その後、MetalLBのBGP機能を使用して、そのFRRベースのルータにサービスをアドバタイズします。</para>
</section>
<section xml:id="id-deployment-3">
<title>デプロイメント</title>
<para>SUSEでは、SUSE Edgeソリューションの一部として公開されているMetalLB Helmチャートを使用します。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration-2">
<title>設定</title>
<orderedlist numeration="arabic">
<listitem>
<para>この時点でインストールは完了します。<literal>IPAddressPool</literal>を作成します。</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: bgp-pool
  namespace: metallb-system
  labels:
    app: httpd
spec:
  addresses:
  - 192.168.10.100/32
  autoAssign: true
  avoidBuggyIPs: false
  serviceAllocation:
    namespaces:
    - metallb-system
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - key: serviceType
        operator: In
        values:
        - httpd
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para><literal>BGPPeer</literal>を設定します。</para>
</listitem>
</orderedlist>
<note>
<para>FRRルータのASNは1000であり、<literal>BGPPeer</literal>は1001です。また、FRRルータのIPアドレスは192.168.3.140であることも確認できます。</para>
</note>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: mypeertest
spec:
  peerAddress: 192.168.3.140
  peerASN: 1000
  myASN: 1001
  routerID: 4.4.4.4
EOF</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>BGPAdvertisement (L3)を作成します。</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-test
  namespace: metallb-system
spec:
  ipAddressPools:
  - bgp-pool
EOF</screen>
</section>
<section xml:id="id-usage-2">
<title>使用法</title>
<orderedlist numeration="arabic">
<listitem>
<para>サービスを含むサンプルアプリケーションを作成します。この場合、そのサービスに対する
<literal>IPAddressPool</literal>からのIPアドレスは<literal>192.168.10.100</literal>です。</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment
  namespace: metallb-system
  labels:
    app: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      pod-label: httpd
  template:
    metadata:
      labels:
        pod-label: httpd
    spec:
      containers:
      - name: httpdcontainer
        image: image: docker.io/library/httpd:2.4
        ports:
          - containerPort: 80
            protocol: TCP
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: http-service
  namespace: metallb-system
  labels:
    serviceType: httpd
spec:
  selector:
    pod-label: httpd
  type: LoadBalancer
  ports:
  - protocol: TCP
    port: 8080
    name: 8080-tcp
    targetPort: 80
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>確認するには、FRRルータにログオンし、BGPアドバタイズメントから作成されたルートを確認できます。</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered">42178089cba5# show ip bgp all

For address family: IPv4 Unicast
BGP table version is 3, local router ID is 2.2.2.2, vrf id 0
Default local pref 100, local AS 1000
Status codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
* i172.16.0.0/24    1.1.1.1                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
* i172.17.0.0/24    3.3.3.3                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
*= 192.168.10.100/32
                    192.168.3.162                          0 1001 i
*=                  192.168.3.163                          0 1001 i
*&gt;                  192.168.3.161                          0 1001 i

Displayed  3 routes and 7 total paths
kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>このルータがネットワークのデフォルトゲートウェイである場合、そのネットワーク上のボックスから<literal>curl</literal>コマンドを実行し、httpdサンプルアプリに到達できることを確認できます。</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered"># curl http://192.168.10.100:8080
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
#</screen>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>Kubernetes APIサーバの前面のMetalLB</title>
<para>このガイドでは、MetalLBサービスを使用して、3つのコントロールプレーンノードを持つHAクラスタ上でRKE2/K3s
APIを外部に公開する方法を示します。これを実現するために、<literal>LoadBalancer</literal>タイプのKubernetes
Serviceを手動で作成します。その後、<literal>EndpointSlices</literal>オブジェクトが自動的に作成され、クラスタで使用可能なすべてのコントロールプレーンノードのIPが保持されます。EndpointSlicesをクラスタで発生するイベント(ノードの追加/削除やノードのオフライン化)と継続的に同期するため、Endpoint
Copier Operator (<xref
linkend="components-eco"/>)がデプロイされます。このオペレータはデフォルトの<literal>kubernetes</literal>
EndpointSlicesで発生するイベントを監視し、管理対象を自動的に更新して同期を維持します。管理対象のServiceは<literal>LoadBalancer</literal>タイプであるため、MetalLBは静的な<literal>ExternalIP</literal>を割り当てます。この<literal>ExternalIP</literal>はAPIサーバとの通信に使用されます。</para>
<section xml:id="id-prerequisites-9">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>RKE2/K3sをデプロイするための3つのホスト。</para>
<itemizedlist>
<listitem>
<para>ホスト名は各ホストで違う名前にしてください。</para>
</listitem>
<listitem>
<para>テストの場合は仮想マシンを使用できます。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ネットワーク内で2つ以上のIPが使用可能(Traefik/Nginx用に1つ、管理対象サービス用に1つ)。</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-rke2k3s">
<title>RKE2/K3sのインストール</title>
<note>
<para>新しいクラスタを使用せず、既存のクラスタを使用する場合は、この手順をスキップして次の手順に進んでください。</para>
</note>
<para>まず、ネットワーク内の空きIPを、後で管理対象のServiceの<literal>ExternalIP</literal>で使用できるように予約する必要があります。</para>
<para>最初のホストにSSHで接続して、クラスタモードで必要なディストリビューションをインストールします。</para>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC="server \
 --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" sh -

systemctl enable rke2-server.service
systemctl start rke2-server.service

# Fetch the cluster token:
RKE2_TOKEN=$(tr -d '\n' &lt; /var/lib/rancher/rke2/server/node-token)</screen>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>必ず、<literal>k3s
server</literal>コマンドで<literal>--disable=servicelb</literal>フラグを指定してください。</para>
</note>
<important>
<para>これ以降、コマンドはローカルマシンで実行する必要があります。</para>
</important>
<para>APIサーバに外部からアクセスするには、RKE2/K3s VMのIPを使用します。</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
export KUBE_DISTRIBUTION=&lt;k3s/rke2&gt;

scp ${NODE_IP}:/etc/rancher/${KUBE_DISTRIBUTION}/${KUBE_DISTRIBUTION}.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-cluster">
<title>既存のクラスタの設定</title>
<note>
<para>この手順は、既存のRKE2/K3sクラスタを使用する 場合にのみ有効です。</para>
</note>
<para>既存のクラスタを使用するには、<literal>tls-san</literal>フラグを修正する必要があります。また、K3sでは<literal>servicelb</literal>
LBを無効にする必要があります。</para>
<para>RKE2またはK3sサーバのフラグを変更するには、ディストリビューションに応じて、クラスタのすべてのVM上で<literal>/etc/systemd/system/rke2.service</literal>または<literal>/etc/systemd/system/k3s.service</literal>ファイルを変更する必要があります。</para>
<para>フラグは<literal>ExecStart</literal>に挿入する必要があります。例:</para>
<para>RKE2の場合:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/rke2 \
    server \
        '--write-kubeconfig-mode=644' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>K3sの場合:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>次に、次のコマンドを実行して、新しい設定をロードする必要があります。</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart ${KUBE_DISTRIBUTION}</screen>
</section>
<section xml:id="id-installing-metallb">
<title>MetalLBのインストール</title>
<para><literal>MetalLB</literal>をデプロイするには、K3s上のMetalLB (<xref
linkend="guides-metallb-k3s"/>)のガイドを使用できます。</para>
<para><emphasis role="strong">メモ:</emphasis> <literal>VIP_SERVICE_IP</literal>
IPアドレスがクラスタ内の既存の<literal>IPAddressPools</literal>と重複していないことを確認してください。</para>
<para>管理対象のServiceにのみ使用される個別の<literal>IpAddressPool</literal>と<literal>L2Advertisement</literal>を作成します。</para>
<para><emphasis role="strong">メモ:</emphasis>
以下のIPAddressPoolは、<literal>default</literal>ネームスペースの<literal>LoadBalancer</literal>タイプのServiceに割り当てられます。複数の<literal>LoadBalancer</literal>サービスがそこに存在する場合、追加の<link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">ServiceSelectors</link>をこのVIPサービスと明示的に一致するように設定できます。</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: kubernetes-vip-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Endpoint Copier Operatorのインストール</title>
<screen language="bash" linenumbering="unnumbered">helm install \
endpoint-copier-operator oci://registry.suse.com/edge/charts/endpoint-copier-operator \
--namespace endpoint-copier-operator \
--create-namespace</screen>
<para>上記のコマンドは2つのレプリカを持つ<literal>endpoint-copier-operator</literal>オペレータのデプロイメントをデプロイします。一方がリーダーとなり、他方は必要に応じてリーダーの役割を引き継ぎます。</para>
<para>これで、<literal>kubernetes-vip</literal>
Serviceがデプロイされ、オペレータによって調整され、設定されたポートとIPを持つEndpointSlicesが作成されます。</para>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  type: LoadBalancer
EOF</screen>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer
EOF</screen>
<para><literal>kubernetes-vip</literal>サービスのIPアドレスが正しいことを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para><literal>default</literal>ネームスペースの<literal>kubernetes-vip-*</literal>および<literal>kubernetes</literal>のEndpointSlicesリソースが同じIPを指していることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpointslices | grep kubernetes</screen>
<para>すべて問題なければ、最後に<literal>Kubeconfig</literal>で<literal>VIP_SERVICE_IP</literal>を使用します。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>これ以降、<literal>kubectl</literal>はすべて<literal>kubernetes-vip</literal>サービスを経由するようになります。</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>コントロールプレーンノードの追加</title>
<para>プロセス全体を監視するため、端末タブを2つ以上開くことができます。</para>
<para>最初の端末:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>2つ目の端末:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpointslices</screen>
<para>次に、以下のコマンドを2つ目のノードと3つ目のノードで実行します。</para>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
systemctl enable rke2-server.service


mkdir -p /etc/rancher/rke2/
cat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml
server: https://${VIP_SERVICE_IP}:9345
token: ${RKE2_TOKEN}
EOF

systemctl start rke2-server.service</screen>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Edge Image Builderを使用したエアギャップデプロイメント</title>
<section xml:id="id-intro">
<title>概要</title>
<para>このガイドでは、Edge Image Builder (EIB) (<xref
linkend="components-eib"/>)を使用し、完全にエアギャップされた環境で複数のSUSE EdgeコンポーネントをSUSE
Linux Micro 6.1上にデプロイする方法を示します。これにより、EIBで作成したCustomized, Ready to Boot
(CRB)イメージでブートし、指定したコンポーネントをインターネット接続や手動手順なしにRKE2クラスタまたはK3sクラスタにデプロイできます。この設定は、デプロイメントに必要なアーティファクトをすべてOSイメージにプリベイクし、ブート後すぐに利用できるようにしたいお客様にとって非常に便利です。</para>
<para>ここでは、以下のエアギャップインストールについて説明します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-security"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-storage"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIBは、指定したHelmチャートとKubernetesマニフェストで参照されているイメージをすべて解析し、事前にダウンロードします。ただし、その一部がコンテナイメージをプルし、そのイメージに基づいて実行時にKubernetesリソースを作成しようとする場合があります。このような場合、完全なエアギャップ環境を設定するには、必要なイメージを定義ファイルに手動で指定する必要があります。</para>
</warning>
</section>
<section xml:id="id-prerequisites-10">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、すでにEIB (<xref
linkend="components-eib"/>)に精通していることを想定しています。まだEIBに精通していない場合は、クイックスタートガイド(<xref
linkend="quickstart-eib"/>)に従って、以下の演習で示されている概念の理解を深めてください。</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Libvirtのネットワーク設定</title>
<note>
<para>エアギャップデプロイメントのデモを示すため、このガイドはシミュレートされたエアギャップ<literal>libvirt</literal>ネットワークを使用して実施し、それに合わせて以下の設定を調整します。ご自身のデプロイメントでは、<literal>host1.local.yaml</literal>の設定の変更が必要になる場合があります。これについては、次の手順で説明します。</para>
</note>
<para>同じ<literal>libvirt</literal>ネットワーク設定を使用する場合は、このまま読み進めてください。そうでない場合は、<xref
linkend="config-dir-creation"/>までスキップしてください。</para>
<para>DHCPのIPアドレス範囲<literal>192.168.100.2/24</literal>で、分離されたネットワーク設定を作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>あとはネットワークを作成して起動するだけです。</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>ベースディレクトリの設定</title>
<para>ベースディレクトリの設定は、各種のコンポーネントすべてで同じであるため、ここで設定します。</para>
<para>まず、必要なサブディレクトリを作成します。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>必ず、使用する予定のゴールデンイメージを<literal>base-images</literal>ディレクトリに追加してください。このガイドでは、<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあるセルフインストールISOに焦点を当てて説明します。</para>
<para>ダウンロードしたイメージをコピーしましょう。</para>
<screen language="shell" linenumbering="unnumbered">cp SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIBは、ゴールデンイメージの入力を変更することはありません。</para>
</note>
<para>目的のネットワーク設定を含むファイルを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>この設定により、プロビジョニングされたシステムに以下が確実に存在するようになります(指定されたMACアドレスを使用)。</para>
<itemizedlist>
<listitem>
<para>静的IPアドレスを持つEthernetインタフェース</para>
</listitem>
<listitem>
<para>ルーティング</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>ホスト名(<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>結果のファイル構造は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>ベース定義ファイル</title>
<para>Edge Image Builderでは、<emphasis>定義ファイル</emphasis>を使用してSUSE Linux
Microイメージを変更します。定義ファイルには、設定可能なオプションの大部分が含まれています。これらのオプションの多くは、異なるコンポーネントのセクションで繰り返し使用されるため、ここで一覧にして説明します。</para>
<tip>
<para>定義ファイルのカスタマイズオプションの全リストについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#image-definition-file">アップストリームドキュメント</link>を参照してください。</para>
</tip>
<para>すべての定義ファイルに存在する次のフィールドを見てみましょう。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para><literal>image</literal>セクションは必須であり、入力イメージ、そのアーキテクチャとタイプ、および出力イメージの名前を指定します。</para>
<para><literal>operatingSystem</literal>セクションはオプションであり、プロビジョニングされたシステムに<literal>root/eib</literal>のユーザ名/パスワードでログインできるようにするための設定が含まれます。</para>
<para><literal>kubernetes</literal>セクションはオプションであり、Kubernetesタイプとバージョンを定義しています。RKE2ディストリビューションを使用します。代わりにK3sが必要な場合は、<literal>kubernetes.version:
v1.33.3+k3s1</literal>を使用します。<literal>kubernetes.nodes</literal>フィールドを介して明示的に設定しない限り、このガイドでブートストラップするすべてのクラスタは、シングルノードクラスタになります。</para>
<para><literal>embeddedArtifactRegistry</literal>セクションには、実行時に特定のコンポーネントでのみ参照されてプルされるイメージがすべて含まれます。</para>
</section>
<section xml:id="rancher-install">
<title>Rancherのインストール</title>
<note>
<para>デモで示すRancher (<xref
linkend="components-rancher"/>)のデプロイメントは、デモのために非常にスリム化されています。実際のデプロイメントでは、設定に応じて追加のアーティファクトが必要な場合があります。</para>
</note>
<para><link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">Rancher
2.12.1</link>リリースアセットには、エアギャップインストールに必要なすべてのイメージをリストする<literal>rancher-images.txt</literal>ファイルが含まれています。</para>
<para>コンテナイメージは合計で600個以上あり、結果として得られるCRBイメージは約30GBになります。Rancherのインストールでは、そのリストを最小の動作設定にまで削減します。そこから、デプロイメントに必要なイメージを追加し直すことができます。</para>
<para>定義ファイルを作成し、必要最小限のイメージリストを含めます。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.18.2
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v8.0.0
    - name: registry.rancher.com/rancher/compliance-operator:v1.1.0
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.23-build20250612
    - name: registry.rancher.com/rancher/hardened-calico:v3.30.2-build20250711
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.26.0-build20250611
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.21-k3s1-build20250612
    - name: registry.rancher.com/rancher/hardened-flannel:v0.27.1-build20250710
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.33.3-rke2r1-build20250716
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-multus-dynamic-networks-controller:v0.3.7-build20250711
    - name: registry.rancher.com/rancher/hardened-multus-thick:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.9.1-build20250704
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.13
    - name: registry.rancher.com/rancher/kubectl:v1.33.1
    - name: registry.rancher.com/rancher/kuberlr-kubectl:v5.0.0
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.31
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher131
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.10.2
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.rancher.com/rancher/prom-prometheus:v3.2.1
    - name: registry.rancher.com/rancher/prometheus-federator:v4.1.0
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.5-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.5-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.12.1
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v7.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/remotedialer-proxy:v0.5.0
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.33.1-0.20250516163953-99d91538b132-build20250612
    - name: registry.rancher.com/rancher/rke2-runtime:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/scc-operator:v0.1.1
    - name: registry.rancher.com/rancher/security-scan:v0.7.1
    - name: registry.rancher.com/rancher/shell:v0.5.0
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.13-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.rancher.com/rancher/ui-plugin-catalog:4.0.3
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.1
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.2
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.3
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.6.0</screen>
<para>600個以上のコンテナイメージの全リストと比較すると、このスリム化されたバージョンには約60個しか含まれておらず、新しいCRBイメージは約7GBになります。</para>
<para>RancherのHelm値も作成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para><literal>systemDefaultRegistry</literal>を<literal>registry.rancher.com</literal>に設定することで、Rancherは、ブート時にCRBイメージ内で起動される組み込みのアーティファクトレジストリ内でイメージを自動的に検索できます。このフィールドを省略すると、ノードでコンテナイメージを見つけられない場合があります。</para>
</warning>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Downloading file: dl-manifest-1.yaml 100% |██████████████████████████████████████████████████████████████████████████████| (583/583 kB, 12 MB/s)
Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████| (56/56, 8 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% |███████████████████████████████████████████████████████████| (644/644 MB, 29 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% |█████████████████████████████████████████████████████████| (400/400 MB, 29 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% |███████████████████████████████████████████████████████████████████████████| (36/36 MB, 30 MB/s)
Downloading file: sha256sum-amd64.txt 100% |█████████████████████████████████████████████████████████████████████████████| (4.3/4.3 kB, 29 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、Rancherのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cattle-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/helm-operation-6l6ld                        0/2     Completed   0          107s
pod/helm-operation-8tk2v                        0/2     Completed   0          2m2s
pod/helm-operation-blnrr                        0/2     Completed   0          2m49s
pod/helm-operation-hdcmt                        0/2     Completed   0          3m19s
pod/helm-operation-m74c7                        0/2     Completed   0          97s
pod/helm-operation-qzzr4                        0/2     Completed   0          2m30s
pod/helm-operation-s9jh5                        0/2     Completed   0          3m
pod/helm-operation-tq7ts                        0/2     Completed   0          2m41s
pod/rancher-99d599967-ftjkk                     1/1     Running     0          4m15s
pod/rancher-webhook-79798674c5-6w28t            1/1     Running     0          2m27s
pod/system-upgrade-controller-56696956b-trq5c   1/1     Running     0          104s

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/rancher           ClusterIP   10.43.255.80   &lt;none&gt;        80/TCP,443/TCP   4m15s
service/rancher-webhook   ClusterIP   10.43.7.238    &lt;none&gt;        443/TCP          2m27s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rancher                     1/1     1            1           4m15s
deployment.apps/rancher-webhook             1/1     1            1           2m27s
deployment.apps/system-upgrade-controller   1/1     1            1           104s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/rancher-99d599967                     1         1         1       4m15s
replicaset.apps/rancher-webhook-79798674c5            1         1         1       2m27s
replicaset.apps/system-upgrade-controller-56696956b   1         1         1       104s</screen>
<para>また、<literal>https://192.168.100.50.sslip.io</literal>に移動し、以前に設定した<literal>adminadminadmin</literal>パスワードでログインすると、Rancherダッシュボードが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width="100%"/>
</imageobject>
<textobject><phrase>エアギャップRancher</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="suse-security-install">
<title>SUSE Securityのインストール</title>
<para>Rancherのインストールとは異なり、SUSE
SecurityのインストールではEIBで特別な処理を行う必要はありません。EIBは基盤となるコンポーネントNeuVectorに必要なすべてのイメージを自動的にエアギャップ化します。</para>
<para>定義ファイルを作成します。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>NeuVector用のHelm値ファイルも作成します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 4 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████| (5/5, 13 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、SUSE Securityのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/neuvector-cert-upgrader-job-bxbnz           0/1     Completed   0          3m39s
pod/neuvector-controller-pod-7d854bfdc7-nhxjf   1/1     Running     0          3m44s
pod/neuvector-enforcer-pod-ct8jm                1/1     Running     0          3m44s

NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.234.241   &lt;none&gt;        443/TCP                         3m44s
service/neuvector-svc-controller          ClusterIP   None            &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   3m44s
service/neuvector-svc-crd-webhook         ClusterIP   10.43.50.190    &lt;none&gt;        443/TCP                         3m44s

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          3m44s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           3m44s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-7d854bfdc7   1         1         1       3m44s

NAME                                        SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-cert-upgrader-pod   0 0 1 1 *   &lt;none&gt;     True      0        &lt;none&gt;          3m44s
cronjob.batch/neuvector-updater-pod         0 0 * * *   &lt;none&gt;     False     0        &lt;none&gt;          3m44s

NAME                                    STATUS     COMPLETIONS   DURATION   AGE
job.batch/neuvector-cert-upgrader-job   Complete   1/1           7s         3m39s</screen>
</section>
<section xml:id="suse-storage-install">
<title>SUSE Storageのインストール</title>
<para>Longhornの<link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/airgap/">公式ドキュメント</link>には、エアギャップインストールに必要なすべてのイメージをリストした<literal>longhorn-images.txt</literal>ファイルが含まれています。定義ファイルには、Rancherコンテナレジストリからのミラー化された対応するイメージを含めます。作成してみましょう。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: [reg-code]
    packageList:
      - open-iscsi
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 107.0.0+up1.9.1
      - name: longhorn-crd
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        version: 107.0.0+up1.9.1
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.52
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1</screen>
<note>
<para>定義ファイルには<literal>open-iscsi</literal>パッケージがリストされていることに気づくでしょう。これは、LonghornがKubernetesに永続ボリュームを提供するために、さまざまなノードで実行されている<literal>iscsiadm</literal>デーモンに依存しているために必要です。</para>
</note>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 20956 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、Longhornのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/csi-attacher-787fd9c6c8-sf42d                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-tb82p                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-zhc6s                       1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-b2v9s                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-hwllt                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-mlrpk                    1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-t54zk                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-vdt5d                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-x9kh4                        1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-r62gr                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-vrwjn                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-z65nb                    1/1     Running   0          2m28s
pod/engine-image-ei-4623b511-9vhkb                      1/1     Running   0          3m13s
pod/instance-manager-6f95fd57d4a4cd0459e469d75a300552   1/1     Running   0          2m43s
pod/longhorn-csi-plugin-gx98x                           3/3     Running   0          2m28s
pod/longhorn-driver-deployer-55f9c88499-fbm6q           1/1     Running   0          3m28s
pod/longhorn-manager-dpdp7                              2/2     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-gg5hq                        1/1     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-s49jc                        1/1     Running   0          3m28s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.77.89    &lt;none&gt;        9502/TCP   3m28s
service/longhorn-backend              ClusterIP   10.43.56.17    &lt;none&gt;        9500/TCP   3m28s
service/longhorn-conversion-webhook   ClusterIP   10.43.54.73    &lt;none&gt;        9501/TCP   3m28s
service/longhorn-frontend             ClusterIP   10.43.22.82    &lt;none&gt;        80/TCP     3m28s
service/longhorn-recovery-backend     ClusterIP   10.43.45.143   &lt;none&gt;        9503/TCP   3m28s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-4623b511   1         1         1       1            1           &lt;none&gt;          3m13s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          2m28s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          3m28s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           2m28s
deployment.apps/csi-provisioner            3/3     3            3           2m28s
deployment.apps/csi-resizer                3/3     3            3           2m28s
deployment.apps/csi-snapshotter            3/3     3            3           2m28s
deployment.apps/longhorn-driver-deployer   1/1     1            1           3m28s
deployment.apps/longhorn-ui                2/2     2            2           3m28s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-787fd9c6c8               3         3         3       2m28s
replicaset.apps/csi-provisioner-74486b95c6            3         3         3       2m28s
replicaset.apps/csi-resizer-859d4557fd                3         3         3       2m28s
replicaset.apps/csi-snapshotter-6f69c6c8cc            3         3         3       2m28s
replicaset.apps/longhorn-driver-deployer-55f9c88499   1         1         1       3m28s
replicaset.apps/longhorn-ui-59c85fcf94                2         2         2       3m28s</screen>
</section>
<section xml:id="kubevirt-install">
<title>KubeVirtとCDIのインストール</title>
<para>KubeVirtとCDIの両方のHelmチャートでインストールされるのは、それぞれのオペレータのみです。残りのシステムのデプロイはオペレータに任されています。つまり、必要なコンテナイメージすべてを定義ファイルに含める必要があります。作成してみましょう。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: kubevirt
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/virt-api:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-controller:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-handler:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2-150700.3.5.2</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 48 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 4 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、KubeVirtとCDIの両方のインストールを確認できます。</para>
<para>KubeVirtを確認します。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                  READY   STATUS    RESTARTS   AGE
pod/virt-api-59cb997648-mmt67         1/1     Running   0          2m34s
pod/virt-controller-69786b785-7cc96   1/1     Running   0          2m8s
pod/virt-controller-69786b785-wq2dz   1/1     Running   0          2m8s
pod/virt-handler-2l4dm                1/1     Running   0          2m8s
pod/virt-operator-7c444cff46-nps4l    1/1     Running   0          3m1s
pod/virt-operator-7c444cff46-r25xq    1/1     Running   0          3m1s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.167.109   &lt;none&gt;        443/TCP   2m36s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m36s
service/virt-api                      ClusterIP   10.43.18.202    &lt;none&gt;        443/TCP   2m36s
service/virt-exportproxy              ClusterIP   10.43.142.188   &lt;none&gt;        443/TCP   2m36s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   2m8s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m34s
deployment.apps/virt-controller   2/2     2            2           2m8s
deployment.apps/virt-operator     2/2     2            2           3m1s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-59cb997648         1         1         1       2m34s
replicaset.apps/virt-controller-69786b785   2         2         2       2m8s
replicaset.apps/virt-operator-7c444cff46    2         2         2       3m1s

NAME                            AGE    PHASE
kubevirt.kubevirt.io/kubevirt   3m1s   Deployed</screen>
<para>CDIを確認します。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-5598c9bf47-pqfxw     1/1     Running   0          3m44s
pod/cdi-deployment-7cbc5db7f8-g46z7    1/1     Running   0          3m44s
pod/cdi-operator-777c865745-2qcnj      1/1     Running   0          3m48s
pod/cdi-uploadproxy-646f4cd7f7-fzkv7   1/1     Running   0          3m44s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.2.224    &lt;none&gt;        443/TCP    3m44s
service/cdi-prometheus-metrics   ClusterIP   10.43.237.13   &lt;none&gt;        8080/TCP   3m44s
service/cdi-uploadproxy          ClusterIP   10.43.114.91   &lt;none&gt;        443/TCP    3m44s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m44s
deployment.apps/cdi-deployment    1/1     1            1           3m44s
deployment.apps/cdi-operator      1/1     1            1           3m48s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m44s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-5598c9bf47     1         1         1       3m44s
replicaset.apps/cdi-deployment-7cbc5db7f8    1         1         1       3m44s
replicaset.apps/cdi-operator-777c865745      1         1         1       3m48s
replicaset.apps/cdi-uploadproxy-646f4cd7f7   1         1         1       3m44s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>トラブルシューティング</title>
<para>イメージの構築中に問題が発生した場合、またはプロセスをさらにテストおよびデバッグしたい場合は、<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">アップストリームドキュメント</link>を参照してください。</para>
</section>
</chapter>
<chapter xml:id="guides-kiwi-builder-images">
<title>Kiwiを使用したSUSE Linux Microの更新イメージの構築</title>
<para>このセクションでは、SUSE Linux Microの更新イメージを生成して、Edge Image BuilderまたはCluster API
(CAPI) +
Metal<superscript>3</superscript>で使用するか、あるいはディスクイメージをブロックデバイスに直接書き込む方法について説明します。このプロセスは、初期のシステムブートイメージに最新のパッチを含める必要がある状況や(インストール後のパッチ転送を最小化するため)、CAPIを使用するシナリオにおいて、ホストをインプレースでアップグレードするのではなく、新しいイメージを使用してオペレーティングシステムを再インストールする場合に役立ちます。</para>
<para>このプロセスでは、<link
xl:href="https://osinside.github.io/kiwi/">Kiwi</link>を利用してイメージの構築を実行します。SUSE
Edgeには、組み込みのヘルパーユーティリティによって全体的なプロセスが簡素化されるコンテナ化バージョンが付属しており、必要なターゲット<emphasis
role="strong">プロファイル</emphasis>を指定できます。このプロファイルで、必要な出力イメージのタイプを定義します。以下に一般的なものを示します。</para>
<itemizedlist>
<listitem>
<para>「<emphasis role="strong">Base</emphasis>」 - 縮小版のパッケージセットが含まれるSUSE Linux
Microディスクイメージ(podmanを含む)。</para>
</listitem>
<listitem>
<para>「<emphasis role="strong">Base-SelfInstall</emphasis>」 -
上の「Base」に基づくセルフインストールイメージ。</para>
</listitem>
<listitem>
<para>「<emphasis role="strong">Base-RT</emphasis>」 -
上の「Base」と同じですが、代わりにリアルタイム(rt)カーネルを使用します。</para>
</listitem>
<listitem>
<para>「<emphasis role="strong">Base-RT-SelfInstall</emphasis>」 -
上の「Base-RT」に基づくセルフインストールイメージ。</para>
</listitem>
<listitem>
<para>「<emphasis role="strong">Default</emphasis>」 - 上の「Base」に基づくSUSE Linux
Microディスクイメージですが、仮想化スタック、Cockpit、salt-minionなどのツールがいくつか追加されています。</para>
</listitem>
<listitem>
<para>「<emphasis role="strong">Default-SelfInstall</emphasis>」 -
上の「Default」に基づくセルフインストールイメージ。</para>
</listitem>
</itemizedlist>
<para>詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-deployment-images/index.html#alp-images-installer-type">SUSE
Linux Micro 6.1</link>のドキュメントを参照してください。</para>
<para>このプロセスはAMD64/Intel
64およびAArch64のどちらのアーキテクチャでも動作しますが、両方のアーキテクチャですべてのイメージプロファイルが利用できるわけではありません。たとえば、SUSE
Edge 3.4では、SUSE Linux Micro
6.1が使用されており、リアルタイムカーネルが含まれるプロファイル(つまり「Base-RT」または「Base-RT-SelfInstall」)は現在のところAArch64では利用できません。</para>
<note>
<para>構築するイメージと同じアーキテクチャの構築ホストを使用する必要があります。つまり、AArch64のイメージを構築するにはAArch64の構築ホストを使用する必要があり、AMD64/Intel 64の場合も同様です。クロス構築は現在のところサポートされていません。</para>
</note>
<section xml:id="id-prerequisites-11">
<title>前提条件</title>
<para>Kiwi Image Builderには以下が必要です。</para>
<itemizedlist>
<listitem>
<para>構築するイメージと同じアーキテクチャのSUSE Linux Micro 6.1ホスト(「構築システム」)。</para>
</listitem>
<listitem>
<para>構築システムは<literal>SUSEConnect</literal>を介して登録済みである必要があります(この登録を使用してSUSEリポジトリから最新のパッケージをプルします)。</para>
</listitem>
<listitem>
<para>必要なパッケージをプルするために使用できるインターネット接続。プロキシ経由で接続している場合、構築ホストを再設定する必要があります。</para>
</listitem>
<listitem>
<para>構築ホスト上でSELinuxを無効化する必要があります(コンテナ内でSELinuxのラベル付けが実行され、これがホストのポリシーと競合する可能性があるためです)。</para>
</listitem>
<listitem>
<para>コンテナイメージ、構築ホスト、および作成される出力イメージを格納するための10GB以上の空きディスク容量。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started-2">
<title>はじめに</title>
<para>特定の制限のため、現在はSELinuxを無効化する必要があります。SUSE Linux Micro
6.1イメージ構築ホストに接続し、SELinuxを確実に無効化します。</para>
<screen language="console" linenumbering="unnumbered"># setenforce 0</screen>
<para>作成されるイメージを保存するためにKiwi構築コンテナと共有する出力ディレクトリを作成します。</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/output</screen>
<para>SUSEレジストリから最新のKiwi Builderイメージをプルします。</para>
<screen language="console" linenumbering="unnumbered"># podman pull registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
(...)</screen>
</section>
<section xml:id="id-building-the-default-image">
<title>デフォルトイメージの構築</title>
<para>コンテナイメージの実行時に引数が指定されていない場合、これがKiwiイメージコンテナのデフォルトの動作です。以下のコマンドは、次の2つのディレクトリをコンテナにマップした状態で<literal>podman</literal>を実行します。</para>
<itemizedlist>
<listitem>
<para>基になるホストのSUSE Linux
Microパッケージリポジトリディレクトリである<literal>/etc/zypp/repos.d</literal>。</para>
</listitem>
<listitem>
<para>上記で作成される出力の<literal>~/output</literal>ディレクトリ。</para>
</listitem>
</itemizedlist>
<para>Kiwiイメージコンテナでは、<literal>build-image</literal>ヘルパースクリプトを次のように実行する必要があります。</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<note>
<para>このスクリプトを初めて実行すると、開始直後にスクリプトが<emphasis
role="strong">失敗</emphasis>し、「<emphasis role="strong">ERROR: Early loop
device test failed, please retry the container run. (エラー:
ループデバイスの早期テストが失敗しました。もう一度コンテナの実行を試してください。)</emphasis>」というエラーが発生すると予想されます。これは、基になるホストシステム上に作成されるループデバイスがすぐにはコンテナイメージ内に表示されないことを示す症状です。単にコマンドを再実行すれば、問題なく続行されるはずです。</para>
</note>
<para>数分後、ローカルのoutputディレクトリ内にイメージを確認できます。</para>
<screen language="console" linenumbering="unnumbered">(...)
INFO: Image build successful, generated images are available in the 'output' directory.

# ls -1 output/
SLE-Micro.x86_64-6.1.changes
SLE-Micro.x86_64-6.1.packages
SLE-Micro.x86_64-6.1.raw
SLE-Micro.x86_64-6.1.verified
build
kiwi.result
kiwi.result.json</screen>
</section>
<section xml:id="id-building-images-with-other-profiles">
<title>他のプロファイルを使用したイメージの構築</title>
<para>異なるイメージプロファイルを構築するには、Kiwiコンテナイメージのヘルパースクリプトで「<emphasis
role="strong">-p</emphasis>」コマンドオプションを使用します。たとえば、「<emphasis
role="strong">Default-SelfInstall</emphasis>」のISOイメージを構築するには、次のコマンドを使用します。</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall
(...)</screen>
<note>
<para>データの消失を防ぐため、Kiwiは、<literal>output</literal>ディレクトリにイメージが存在する場合、実行を拒否します。<literal>rm
-f output/*</literal>を使用して続行する前に、outputディレクトリの内容を削除する必要があります。</para>
</note>
<para>または、リアルタイムカーネル(「<emphasis
role="strong">kernel-rt</emphasis>」)を使用してSelfInstall
ISOイメージを構築するには、次のコマンドを使用します。</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Base-RT-SelfInstall
(...)</screen>
</section>
<section xml:id="id-building-images-with-large-sector-sizes">
<title>大きいセクタサイズを使用したイメージの構築</title>
<para>ハードウェアによっては、大きいセクタサイズ、つまり標準の512バイトではなく<emphasis
role="strong">4096バイト</emphasis>のイメージが必要になることがあります。コンテナ化されたKiwi
Builderでは、「<emphasis
role="strong">-b</emphasis>」パラメータを指定することで、大きいブロックサイズのイメージを生成できます。たとえば、大きいセクタサイズを使用して「<emphasis
role="strong">Default-SelfInstall</emphasis>」イメージを構築するには、次のコマンドを使用します。</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall -b
(...)</screen>
</section>
<section xml:id="id-using-a-custom-kiwi-image-definition-file">
<title>カスタムのKiwiイメージ定義ファイルの使用</title>
<para>高度なユースケースでは、カスタムのKiwiイメージ定義ファイル(<literal>SL-Micro.kiwi</literal>)を、必要な構築後スクリプトとともに使用できます。このためには、SUSE
Edgeチームによって事前にパッケージ化されているデフォルトの定義を上書きする必要があります。</para>
<para>新しいディレクトリを作成し、ヘルパースクリプトが参照するコンテナイメージにマップします(<literal>/micro-sdk/defs</literal>)。</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/mydefs/
# cp /path/to/SL-Micro.kiwi ~/mydefs/
# cp /path/to/config.sh ~/mydefs/
# podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output -v ~/mydefs/:/micro-sdk/defs/ \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<warning>
<para>これは、高度なユースケースにおいて、サポート性に問題が生じる可能性がある場合にのみ必要です。詳細なアドバイスとガイダンスについては、SUSEの担当者にお問い合わせください。</para>
</warning>
<para>コンテナに含まれるデフォルトのKiwiイメージ定義ファイルを取得するには、次のコマンドを使用できます。</para>
<screen language="console" linenumbering="unnumbered">$ podman create --name kiwi-builder registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi .
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi.4096 .
$ podman rm kiwi-builder
$ ls ./SL-Micro.*
(...)</screen>
</section>
</chapter>
<chapter xml:id="guides-clusterclass-example">
<title>ClusterClassを使用したダウンストリームクラスタのデプロイ</title>
<section xml:id="id-introduction">
<title>はじめに</title>
<para>Kubernetesクラスタのプロビジョニングは、クラスタコンポーネントの設定に深い専門知識が求められる複雑な作業です。設定が複雑化したり、さまざまなプロバイダからの要求により多数のプロバイダ固有のリソース定義が導入されたりすると、クラスタの作成が困難に感じられるかもしれません。幸いなことに、Kubernetes
Cluster API
(CAPI)は、ClusterClassによってさらに強化された、より洗練された宣言型アプローチを提供します。この機能はテンプレート駆動型モデルを導入し、複雑さをカプセル化して一貫性を促進する再利用可能なクラスタクラスを定義できます。</para>
</section>
<section xml:id="id-what-is-clusterclass">
<title>ClusterClassとは</title>
<para>CAPIプロジェクトは、クラスタのインスタンス化にテンプレートベースの手法を採用することで、Kubernetesクラスタのライフサイクル管理におけるパラダイムシフトとしてClusterClass機能を導入しました。ユーザがクラスタごとにリソースを個別に定義する代わりに、ClusterClassを定義することで、ClusterClassは包括的で再利用可能なブループリントとして機能します。この抽象データ型表現は、Kubernetesクラスタの望ましい状態と設定をカプセル化し、定義された仕様に準拠した複数のクラスタを迅速かつ一貫して作成できるようにします。この抽象データ型により、設定の負担が軽減され、より管理しやすいデプロイメントマニフェストが実現されます。つまり、ワークロードクラスタのコアコンポーネントはクラスレベルで定義されるため、ユーザはこれらのテンプレートをKubernetesクラスタのフレーバーとして使用し、クラスタのプロビジョニングに1回または複数回再利用できるようになります。ClusterClassの実装は、従来の大規模なCAPI管理に固有の課題に対処するいくつかの重要な利点をもたらします。</para>
<itemizedlist>
<listitem>
<para>複雑さとYAMLの冗長性を大幅に削減</para>
</listitem>
<listitem>
<para>最適化された保守と更新プロセス</para>
</listitem>
<listitem>
<para>デプロイメント全体における一貫性と標準化の強化</para>
</listitem>
<listitem>
<para>スケーラビリティと自動化機能の向上</para>
</listitem>
<listitem>
<para>宣言型管理と堅牢なバージョン管理</para>
</listitem>
</itemizedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="clusterclass.png" width="100%"/>
</imageobject>
<textobject><phrase>ClusterClass</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-example-of-current-capi-provisioning-file">
<title>現在のCAPIプロビジョニングファイルの例</title>
<para>Cluster API
(CAPI)とRKE2プロバイダを活用するKubernetesクラスタのデプロイメントには、いくつかのカスタムリソースの定義が必要です。これらのリソースは、クラスタとその基盤となるインフラストラクチャの望ましい状態を定義し、CAPIがプロビジョニングと管理のライフサイクルをオーケストレーションできるようにします。以下のコードスニペットは設定する必要のあるリソースタイプを示しています。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Cluster</emphasis>:
このリソースは、ノード間の通信とサービスディスカバリを制御するネットワークトポロジを含む、高レベルな設定をカプセル化します。さらに、コントロールプレーン仕様と指定されたインフラストラクチャプロバイダリソースとの重要なリンクを確立し、これにより望ましいクラスタアーキテクチャと、そのプロビジョニング対象となる基盤インフラストラクチャについてCAPIに通知します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3Cluster</emphasis>:
このリソースは、Metal3に固有のインフラストラクチャレベルの属性を定義します。たとえば、Kubernetes
APIサーバにアクセスできる外部エンドポイントなど。</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlane</emphasis>:
RKE2ControlPlaneリソースは、クラスタのコントロールプレーンノードの特性と動作を定義します。この仕様では、コントロールプレーンレプリカの必要な数(高可用性と耐障害性を確保するために重要)、特定のKubernetesディストリビューションのバージョン(選択したRKE2リリースと一致)、コントロールプレーンコンポーネントへの更新展開戦略などのパラメータが設定されます。さらに、このリソースはクラスタ内で使用されるContainer
Network Interface
(CNI)を指定し、エージェント固有の設定の注入を容易にします。これには多くの場合、コントロールプレーンノード上のRKE2エージェントのシームレスで自動化されたプロビジョニングのためにIgnitionが活用されます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3MachineTemplate</emphasis>:
このリソースは、使用されるイメージを定義するKubernetesクラスタのワーカーノードを形成する、個々のコンピューティングインスタンス作成のブループリントとして機能します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3DataTemplate</emphasis>:
Metal3MachineTemplateを補完するMetal3DataTemplateリソースは、追加のメタデータを新たにプロビジョニングされたマシンインスタンスに指定できるようにします。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: emea-spa-cluster-3
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: emea-spa-cluster-3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  controlPlaneEndpoint:
    host: 192.168.122.203
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: emea-spa-cluster-3
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  registrationMethod: "control-plane-endpoint"
  registrationAddress: 192.168.122.203
  serverConfig:
    cni: cilium
    cniMultusEnable: true
    tlsSan:
      - 192.168.122.203
      - https://192.168.122.203.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - 192.168.122.203/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          node: group-3
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
</section>
<section xml:id="id-transforming-the-capi-provisioning-file-to-clusterclass">
<title>CAPIプロビジョニングファイルのClusterClassへの変換</title>
<section xml:id="id-clusterclass-definition">
<title>ClusterClass定義</title>
<para>以下のコードは、特定のタイプのKubernetesクラスタを一貫してデプロイするための宣言型テンプレートである、ClusterClassリソースを定義します。この仕様には、共通のインフラストラクチャとコントロールプレーンの設定が含まれており、クラスタフリート全体にわたる効率的なプロビジョニングと統一されたライフサイクル管理が可能になります。以下のClusterClassの例にはいくつかの変数があり、実際の値を使用してクラスタインスタンス化プロセス中に置き換えられます。例では次の変数が使用されています。</para>
<itemizedlist>
<listitem>
<para><literal>controlPlaneMachineTemplate</literal>: 使用されるControlPlane Machine
Template参照を定義する名前です</para>
</listitem>
<listitem>
<para><literal>controlPlaneEndpointHost</literal>:
コントロールプレーンエンドポイントのホスト名またはIPアドレスです</para>
</listitem>
<listitem>
<para><literal>tlsSan</literal>: コントロールプレーンエンドポイントのTLSサブジェクト代替名です</para>
</listitem>
</itemizedlist>
<para>ClusterClass定義ファイルは、次の3つのリソースに基づいて定義されます。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ClusterClass</emphasis>:
このリソースは、コントロールプレーンとインフラストラクチャテンプレートを含むClusterClass定義全体をカプセル化します。また、インスタンス化プロセス中に置き換えられる変数のリストも含まれます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlaneTemplate</emphasis>:
このリソースは、コントロールプレーンテンプレートを定義し、コントロールプレーンノードの目的の設定を指定します。レプリカ数、Kubernetesバージョン、使用するCNIなどのパラメータが含まれます。また、一部のパラメータは、インスタンス化プロセス中に適切な値に置き換えられます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3ClusterTemplate</emphasis>:
このリソースは、インフラストラクチャテンプレートを定義し、基盤となるインフラストラクチャの目的の設定を指定します。コントロールプレーンエンドポイントやnoCloudProviderフラグなどのパラメータが含まれます。また、一部のパラメータは、インスタンス化プロセス中に適切な値に置き換えられます。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlaneTemplate
metadata:
  name: example-controlplane-type2
  namespace: emea-spa
spec:
  template:
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: example-controlplane    # This will be replaced by the patch applied in each cluster instances
        namespace: emea-spa
      replicas: 1
      version: v1.33.3+rke2r1
      rolloutStrategy:
        type: "RollingUpdate"
        rollingUpdate:
          maxSurge: 1
      registrationMethod: "control-plane-endpoint"
      registrationAddress: "default"  # This will be replaced by the patch applied in each cluster instances
      serverConfig:
        cni: cilium
        cniMultusEnable: true
        tlsSan:
          - "default"  # This will be replaced by the patch applied in each cluster instances
      agentConfig:
        format: ignition
        additionalUserData:
          config: |
            default
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3ClusterTemplate
metadata:
  name: example-cluster-template-type2
  namespace: emea-spa
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: "default"  # This will be replaced by the patch applied in each cluster instances
        port: 6443
      noCloudProvider: true
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: example-clusterclass-type2
  namespace: emea-spa
spec:
  variables:
    - name: controlPlaneMachineTemplate
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: controlPlaneEndpointHost
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: tlsSan
      required: true
      schema:
        openAPIV3Schema:
          type: array
          items:
            type: string
  infrastructure:
    ref:
      kind: Metal3ClusterTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: example-cluster-template-type2
  controlPlane:
    ref:
      kind: RKE2ControlPlaneTemplate
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      name: example-controlplane-type2
  patches:
    - name: setControlPlaneMachineTemplate
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/infrastructureRef/name"
              valueFrom:
                variable: controlPlaneMachineTemplate
    - name: setControlPlaneEndpoint
      definitions:
        - selector:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: Metal3ClusterTemplate
            matchResources:
              infrastructureCluster: true  # Added to select InfraCluster
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/controlPlaneEndpoint/host"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setRegistrationAddress
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/registrationAddress"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setTlsSan
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/serverConfig/tlsSan"
              valueFrom:
                variable: tlsSan
    - name: updateAdditionalUserData
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/agentConfig/additionalUserData"
              valueFrom:
                template: |
                  config: |
                    variant: fcos
                    version: 1.4.0
                    storage:
                      files:
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: endpoint-copier-operator
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                                targetNamespace: endpoint-copier-operator
                                version: 304.0.1+up0.3.0
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: metallb
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/metallb
                                targetNamespace: metallb-system
                                version: 304.0.0+up0.14.9
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: metallb.io/v1beta1
                              kind: IPAddressPool
                              metadata:
                                name: kubernetes-vip-ip-pool
                                namespace: metallb-system
                              spec:
                                addresses:
                                  - {{ .controlPlaneEndpointHost }}/32
                                serviceAllocation:
                                  priority: 100
                                  namespaces:
                                    - default
                                  serviceSelectors:
                                    - matchExpressions:
                                      - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                              ---
                              apiVersion: metallb.io/v1beta1
                              kind: L2Advertisement
                              metadata:
                                name: ip-pool-l2-adv
                                namespace: metallb-system
                              spec:
                                ipAddressPools:
                                  - kubernetes-vip-ip-pool
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: v1
                              kind: Service
                              metadata:
                                name: kubernetes-vip
                                namespace: default
                                labels:
                                  serviceType: kubernetes-vip
                              spec:
                                ports:
                                - name: rke2-api
                                  port: 9345
                                  protocol: TCP
                                  targetPort: 9345
                                - name: k8s-api
                                  port: 6443
                                  protocol: TCP
                                  targetPort: 6443
                                type: LoadBalancer
                    systemd:
                      units:
                        - name: rke2-preinstall.service
                          enabled: true
                          contents: |
                            [Unit]
                            Description=rke2-preinstall
                            Wants=network-online.target
                            Before=rke2-install.service
                            ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                            [Service]
                            Type=oneshot
                            User=root
                            ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                            ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                            ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                            ExecStartPost=/bin/sh -c "umount /mnt"
                            [Install]
                            WantedBy=multi-user.target</screen>
</section>
<section xml:id="id-cluster-instance-definition">
<title>クラスタインスタンス定義</title>
<para>ClusterClassのコンテキストにおいて、クラスタインスタンスとは、定義されているClusterClassに基づいて作成されたクラスタの特定の実行中のインスタンスを指します。これは、ClusterClassで指定されたブループリントから直接派生した固有の設定、リソース、および運用状態を持つ具体的なデプロイメントを表します。これには、アクティブに実行されている特定のマシンのセット、ネットワーキング設定、および関連するKubernetesコンポーネントが含まれます。クラスタインスタンスを理解することは、ClusterClassフレームワークを使用してプロビジョニングされた特定のデプロイ済みクラスタのライフサイクルの管理、アップグレードの実行、スケーリング操作の実行、および監視の実行に必要不可欠です。</para>
<para>クラスタインスタンスを定義するには、次のリソースを定義する必要があります。</para>
<itemizedlist>
<listitem>
<para>Cluster</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate</para>
</listitem>
<listitem>
<para>Metal3DataTemplate</para>
</listitem>
</itemizedlist>
<para>テンプレート(ClusterClass定義ファイル)で以前に定義された変数は、クラスタのインスタンス化の最終値に置き換えられます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  topology:
    class: example-clusterclass-type2  # Correct way to reference ClusterClass
    version: v1.33.3+rke2r1
    controlPlane:
      replicas: 1
    variables:                         # Variables to be replaced for this cluster instance
      - name: controlPlaneMachineTemplate
        value: emea-spa-cluster-3-machinetemplate
      - name: controlPlaneEndpointHost
        value: 192.168.122.203
      - name: tlsSan
        value:
          - 192.168.122.203
          - https://192.168.122.203.sslip.io
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3-machinetemplate
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          cluster-type: type2
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine</screen>
<para>このアプローチにより、ClusterClassを定義したら、3つのみのリソースでクラスタをデプロイする、より効率的なプロセスが可能になります。</para>
</section>
</section>
</chapter>
</part>
<part xml:id="tips-and-tricks">
<title>ヒントとコツ</title>
<partintro>
<para>Edgeコンポーネントのヒントとコツ</para>
</partintro>
<chapter xml:id="tips-edge-image-builder">
<title>Edge Image Builder</title>
<section xml:id="id-common">
<title>一般</title>
<itemizedlist>
<listitem>
<para>Linux以外の環境でこれらの手順に従ってイメージを構築する場合、仮想マシン経由で<literal>Podman</literal>を実行している可能性があります。デフォルトでは、この仮想マシンにはシステムリソースが少量割り当てられるように設定され、RPM解決プロセスなどのリソース集約的な操作を実行する際に、<literal>Edge
Image
Builder</literal>が不安定な動作を引き起こす可能性があります。その場合は、Podmanマシンのリソースを調整する必要があります。調整するには、Podman
Desktop
(設定の歯車→Podmanマシン編集アイコン)を使用するか、直接<literal>podman-machine-set</literal><link
xl:href="https://docs.podman.io/en/stable/markdown/podman-machine-set.1.html">コマンド</link>を実行してください。</para>
</listitem>
<listitem>
<para>現時点では、<literal>Edge Image
Builder</literal>はクロスアーキテクチャセットアップでイメージを構築できません。すなわち、以下の環境で実行する必要があります。</para>
<itemizedlist>
<listitem>
<para>SL Micro <literal>aarch64</literal>イメージを構築する場合は、AArch64システム(Apple Siliconなど)</para>
</listitem>
<listitem>
<para>SL Micro <literal>x86_64</literal>イメージを構築する場合は、AMD64/Intel 64システム</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-kubernetes">
<title>Kubernetes</title>
<itemizedlist>
<listitem>
<para>マルチノードのKubernetesクラスタを作成するには、定義ファイルの<literal>kubernetes</literal>セクションを以下を実行するように調整する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>kubernetes.nodes</literal>の下にすべてのサーバおよびエージェントノードをリストする。</para>
</listitem>
<listitem>
<para><literal>kubernetes.network.apiVIP</literal>の下に、すべてのnon-initializerノードがクラスタに参加するために使用する仮想IPアドレスを設定する。</para>
</listitem>
<listitem>
<para>オプションで、<literal>kubernetes.network.apiHost</literal>の下に、クラスタにアクセスするためのドメインアドレスを指定するようにAPIホストを設定する。この設定の詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">Kubernetesセクションのドキュメント</link>を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Edge Image
Builder</literal>は、異なるノードのホスト名に基づいてそのKubernetesタイプ(<literal>サーバ</literal>または<literal>エージェント</literal>)を決定します。この設定は定義ファイルで管理されていますが、マシンの一般的なネットワーキングセットアップには、<xref
linkend="components-nmc"/>で説明されているDHCP設定を使用できます。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="tips-elemental">
<title>Elemental</title>
<section xml:id="id-common-2">
<title>一般</title>
<section xml:id="id-expose-rancher-service">
<title>Rancherサービスの公開</title>
<para>RKE2またはK3sを使用する場合、サービス(このコンテキストではRancher)がデフォルトでは公開されていないため、管理クラスタから公開する必要があります。RKE2ではNGINX
Ingressコントローラが使用されており、k3sではTraefikが使用されています。現在のワークフローでは、サービスの通知(L2またはBGPアドバタイズメント経由)にはMetalLBを使用し、それぞれのIngressコントローラを使用して<literal>HelmChartConfig</literal>経由でIngressを作成することが推奨されています。これは、新しいIngressオブジェクトを作成すると既存のセットアップが上書きされるためです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Rancher Prime (Helm経由)をインストールし、必要な値を設定します。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-192.168.64.101.sslip.io
replicas: 1
bootstrapPassword: Admin
global.cattle.psp.enabled: "false"</screen>
<tip>
<para>詳細については、<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Rancherのインストール</link>のドキュメントに従ってください。</para>
</tip>
</listitem>
<listitem>
<para>Rancherを公開するためのLoadBalancerサービスを作成します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local
EOF</screen>
</listitem>
<listitem>
<para>Helm値で以前に設定したIPアドレスを使用して、サービスのIPアドレスプールを作成します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.64.101/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}
EOF</screen>
</listitem>
<listitem>
<para>IPアドレスプール用のL2アドバタイズメントを作成します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ingress-ippool
EOF</screen>
</listitem>
<listitem>
<para>Elementalが適切にインストールされていることを確認します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>管理ノードにElemental OperatorとElemental UIをインストールします。</para>
</listitem>
<listitem>
<para>ダウンストリームノードにElementalの設定を登録コードとともに追加します。これによりEdge Image
Builderにマシンのリモート登録オプションを含めるように要求されます。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<tip>
<para>追加情報と例については、<xref linkend="install-elemental"/>と<xref
linkend="configure-elemental"/>を確認してください。</para>
</tip>
</section>
</section>
<section xml:id="id-hardware-specific">
<title>ハードウェア固有</title>
<section xml:id="id-trusted-platform-module">
<title>Trusted Platform Module</title>
<para><link xl:href="https://elemental.docs.rancher.com/tpm/">Trusted Platform
Module</link> (TPM)設定を適切に行う必要があります。適切に設定しないと、次のようなエラーが発生します。</para>
<screen language="console" linenumbering="unnumbered">Nov 25 18:17:06 eled elemental-register[4038]: Error: registering machine: cannot generate authentication token: opening tpm for getting attestation data: TPM device not available</screen>
<para>このエラーは、以下のアプローチのいずれかで軽減できます。</para>
<itemizedlist>
<listitem>
<para>仮想マシン設定でTPMを有効にする</para>
</listitem>
</itemizedlist>
<para><emphasis>MacOSでのUTMの例</emphasis></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="tpm.png" width="100%"/> </imageobject>
<textobject><phrase>TPM</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><literal>MachineRegistration</literal>リソースでTPMシードに負の値を使用してTPMをエミュレートする</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: true
        emulated-tpm-seed: -1</screen>
<itemizedlist>
<listitem>
<para><literal>MachineRegistration</literal>リソースでTPMを無効にする</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: false</screen>
</section>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>サードパーティの統合</title>
<partintro>
<para>サードパーティツールの統合方法</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link
xl:href="https://nats.io/">NATS</link>は、ますますハイパーコネクテッド化が進む世界のために構築された接続テクノロジです。NATSは、クラウドベンダ、オンプレミス、エッジ、Web、モバイルデバイスがどのように組み合わさっていてもアプリケーションが安全に通信することを可能にする単一のテクノロジです。NATSはオープンソース製品ファミリで構成されており、各製品は緊密に統合されている一方で、簡単に個別にデプロイできます。NATSは世界中で数千社もの企業で使用されており、マイクロサービス、エッジコンピューティング、モバイル、IoTなどのユースケースに幅広く対応しているため、NATSを使用して従来のメッセージングの強化や置き換えを図ることができます。</para>
<section xml:id="id-architecture">
<title>アーキテクチャ</title>
<para>NATSは、メッセージの形式でアプリケーション間のデータ交換を可能にするインフラストラクチャです。</para>
<section xml:id="id-nats-client-applications">
<title>NATSクライアントアプリケーション</title>
<para>NATSクライアントライブラリを使用すると、アプリケーションが異なるインスタンス間でパブリッシュ、サブスクライブ、要求、および応答できるようになります。このようなアプリケーションを一般的に<literal>クライアントアプリケーション</literal>と呼びます。</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>NATSサービスインフラストラクチャ</title>
<para>NATSサービスは、相互接続されてNATSサービスインフラストラクチャを提供するように設定された1つ以上のNATSサーバプロセスによって提供されます。NATSサービスインフラストラクチャは、1つのエンドデバイスで動作する単一のNATSサーバプロセスから、すべての主要クラウドプロバイダと世界のあらゆる地域にまたがる多数のクラスタからなるパブリックなグローバルスーパークラスタまで拡張可能です。</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>シンプルなメッセージングデザイン</title>
<para>NATSを使用すると、アプリケーションはメッセージを送受信して簡単に通信できます。これらのメッセージはサブジェクト文字列によってアドレス指定および識別され、ネットワークの場所には依存しません。データはエンコードされてメッセージとしてフレーム化され、パブリッシャによって送信されます。メッセージは1人以上のサブスクライバによって受信、デコード、処理されます。</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATSにはJetStreamと呼ばれる分散型の永続化システムが組み込まれています。JetStreamは、今日のテクノロジにおけるストリーミングで明らかになった問題、すなわち複雑性、脆弱性、スケーラビリティの欠如を解決するために作成されました。また、JetStreamは、パブリッシャとサブスクライバのカップリングに関する問題(パブリッシュされたメッセージを受信するにはサブスクライバが稼働している必要がある)も解決します。NATS
JetStreamの詳細については、<link
xl:href="https://docs.nats.io/nats-concepts/jetstream">こちら</link>を参照してください。</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>インストール</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>K3s上へのNATSのインストール</title>
<para>NATSは複数のアーキテクチャ向けに構築されているため、K3s (<xref
linkend="components-k3s"/>)上に簡単にインストールできます。</para>
<para>NATSのデフォルト値を上書きするvaluesファイルを作成しましょう。</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>では、Helmを介してNATSをインストールしてみましょう。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>上記の<literal>values.yaml</literal>ファイルでは、次のコンポーネントが<literal>nats</literal>ネームスペースに配置されます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>NATS StatefulsetのHAバージョン。3つのコンテナ(NATSサーバ + ConfigリローダとMetricsサイドカー)が含まれます。</para>
</listitem>
<listitem>
<para>NATS boxコンテナ。セットアップの確認に使用できる一連の<literal>NATS</literal>ユーティリティが付属します。</para>
</listitem>
<listitem>
<para>JetStreamは、Podにバインドされた<literal>PVC</literal>が付属するKey-Valueバックエンドも利用します。</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>セットアップのテスト</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>テストサブジェクトのサブスクリプションを作成します。</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>テストサブジェクトにメッセージを送信します。</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>クリーンアップ</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>K3sのバックエンドとしてのNATS</title>
<para>K3sが利用するコンポーネントの1つが<link
xl:href="https://github.com/k3s-io/kine">KINE</link>です。KINEは、最初からリレーショナルデータベースをターゲットとした代替ストレージバックエンドでetcdを置き換えることを可能にするシムです。JetStreamはKey
Value APIを備えているので、NATSをK3sクラスタのバックエンドとして利用することが可能です。</para>
<para>K3sのビルトインNATSが容易になるマージ済みのPRがありますが、この変更はまだK3sリリースに<link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">含まれていません</link>。</para>
<para>このため、K3sのバイナリを手動で構築する必要があります。</para>
<section xml:id="id-building-k3s">
<title>K3sの構築</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>次のコマンドは、ビルドタグに<literal>nats</literal>を追加して、K3sでNATSビルトイン機能を有効にします。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>&lt;node-ip&gt;は、K3sを起動するノードの実際のIPに置き換えます。</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>K3sをローカルで構築するには、buildx Docker CLIプラグインが必要です。<literal>$ make
local</literal>が失敗する場合は、<link
xl:href="https://github.com/docker/buildx#manual-download">手動でインストール</link>できます。</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>NATS CLIのインストール</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>K3sのバックエンドとしてのNATSの実行</title>
<para>ノードで<literal>ssh</literal>を実行し、<literal>--datastore-endpoint</literal>フラグで<literal>nats</literal>を指してK3sを実行しましょう。</para>
<note>
<para>次のコマンドでは、K3sをフォアグランドプロセスとして起動するので、ログを簡単に追跡して問題がないかどうかを確認できます。現在の端末をブロックしないようにするには、コマンドの前に<literal>&amp;</literal>フラグを追加して、バックグラウンドプロセスとして起動できます。</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>NATSバックエンドを使用するK3sサーバを<literal>slemicro</literal>
VM上で永続化するには、次のスクリプトを実行して、必要な設定で<literal>systemd</literal>サービスを作成します。</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>トラブルシューティング</title>
<para>次のコマンドをノード上で実行して、ストリームのすべてが適切に動作していることを確認できます。</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-suse-linux-micro">
<title>SUSE Linux Micro上のNVIDIA GPU</title>
<section xml:id="id-intro-2">
<title>概要</title>
<para>このガイドでは、事前構築済みの<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">オープンソースドライバ</link>を使用してホストレベルのNVIDIA
GPUサポートをSUSE Linux Micro 6.1に実装する方法を説明します。これらのドライバは、NVIDIAの <link
xl:href="https://github.com/NVIDIA/gpu-operator">GPU
Operator</link>によって動的にロードされるのではなく、オペレーティングシステムにベイクされているドライバです。この設定は、デプロイメントに必要なすべてのアーティファクトをあらかじめイメージにベイクしておき、ドライバのバージョンを動的に選択する必要がない(つまり、ユーザがKubernetesを介してドライバのバージョンを選択する必要がない)お客様に非常に適しています。このガイドでは最初に、すでに事前にデプロイされているシステムに追加コンポーネントをデプロイする方法を説明しますが、その後のセクションでは、Edge
Image
Builderを使用してこの設定を初期デプロイメントに組み込む方法について説明します。基本的な操作を読む必要がない場合や、手動でセットアップしたくない場合は、スキップしてそちらのセクションに進んでください。</para>
<para>これらのドライバのサポートは、SUSEとNVIDIAの両社が緊密に連携して提供しており、ドライバはパッケージリポジトリの一部としてSUSEによって構築および出荷されている点を強調しておくことが重要です。ただし、ドライバを使用する組み合わせについて不安や質問がある場合は、SUSEまたはNVIDIAのアカウントマネージャに問い合わせてサポートを受けてください。<link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE)を使用する予定の場合は、<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">NVAIE認定GPU</link>を使用していることを確認してください。NVAIE認定GPUでは、独自のNVIDIAドライバを使用する必要がある<emphasis>場合があります</emphasis>。不明な点がある場合は、NVIDIAの担当者に問い合わせてください。</para>
<para>NVIDIA GPU
Operatorの統合の詳細は、このガイドでは説明<emphasis>しません</emphasis>。Kubernetes用のNVIDIA GPU
Operatorの統合についてはここでは説明しませんが、このガイドのほとんどの手順に従って、基礎となるオペレーティングシステムをセットアップできます。そして、NVIDIA
GPU
OperatorのHelmチャートの<literal>driver.enabled=false</literal>フラグを使用して<emphasis>プリインストール</emphasis>されたドライバをGPU
Operatorが使用できるようにするだけで、ホスト上にインストールされたドライバが取得されます。より包括的な手順については、 NVIDIA
(<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">こちら</link>)で参照できます。</para>
</section>
<section xml:id="id-prerequisites-12">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro 6.1がインストールされている少なくても1台のホスト。物理でも仮想でも構いません。</para>
</listitem>
<listitem>
<para>パッケージへのアクセスにはサブスクリプションが必要であるため、ホストがサブスクリプションに接続されていること。評価版は<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>から入手できます。</para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">互換性のあるNVIDIA
GPU</link>がインストールされていること(またはSUSE Linux
Microが実行されている仮想マシンに<emphasis>完全に</emphasis>パススルーされていること)。</para>
</listitem>
<listitem>
<para>ルートユーザへのアクセス —
以下の説明では、自身がルートユーザであり、<literal>sudo</literal>を使用して特権を昇格して<emphasis>いない</emphasis>ことを想定しています。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>手動インストール</title>
<para>このセクションでは、NVIDIAドライバをSUSE Linux
Microオペレーティングシステムに直接インストールします。これはopen版NVIDIAドライバがSUSE Linux
Microのコアパッケージリポジトリの一部となったためであり、必須のRPMパッケージをインストールするのと同じように簡単にインストールできるようになりました。実行可能パッケージのコンパイルやダウンロードは必要ありません。以下では、最新のGPUをサポートする「G06」世代ドライバのデプロイについて手順を追って説明します(詳細については<link
xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">こちら</link>を参照してください)。ご使用のシステムに搭載されているNVIDIA
GPUに適切なドライバ世代を選択してください。最新のGPUでは、「G06」ドライバが最も一般的な選択肢です。</para>
<para>始める前に、SUSEがSUSE Linux
Microの一部として出荷するopen版NVIDIAドライバのほかに、ご自身のセットアップに追加のNVIDIAコンポーネントも必要な場合があることを認識しておくことが重要です。たとえば、OpenGLライブラリ、CUDAツールキット、
<literal>nvidia-smi</literal>などのコマンドラインユーティリティ、<literal>nvidia-container-toolkit</literal>などのコンテナ統合コンポーネントです。これらのコンポーネントの多くはNVIDIA独自のソフトウェアであるため、SUSEからは出荷されません。また、NVIDIAの代わりにSUSEが出荷しても意味がありません。そのため、説明の一環として、これらのコンポーネントにアクセスできるようにする追加のリポジトリを設定し、これらのツールの使用方法の例をいくつか説明し、完全に機能するシステムを作成します。SUSEのリポジトリとNVIDIAのリポジトリを区別することが重要です。これは、NVIDIAが提供するパッケージのバージョンとSUSEが構築したものが一致しない場合があるためです。これは通常、SUSEがopen版ドライバの新バージョンを利用可能にしたときに発生し、NVIDIAのリポジトリで同等のパッケージが利用可能になるまでに数日かかります。</para>
<para>以下をチェックして、選択するドライババージョンがGPUと互換性があり、CUDAの要件を満たしていることを確認することをお勧めします。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">CUDAリリースノート</link></para>
</listitem>
<listitem>
<para>デプロイを計画しているドライババージョンが、<link
xl:href="https://download.nvidia.com/suse/sle15sp6/x86_64/">NVIDIAリポジトリ</link>に一致するバージョンがあり、サポートコンポーネントの同等のパッケージバージョンが利用可能であることを確認します。</para>
</listitem>
</itemizedlist>
<tip>
<para>NVIDIAオープンドライババージョンを確認するには、ターゲットマシンで<literal>zypper se -s
nvidia-open-driver</literal>を実行するか、<emphasis>または</emphasis>SUSE Customer
Centerで<link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.1&amp;arch=x86_64">SUSE
Linux Micro 6.1 for AMD64/Intel 64</link>の「nvidia-open-driver」を検索します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width="100%"/>
</imageobject>
<textobject><phrase>SUSE Customer Centre</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>NVIDIAリポジトリで同等のバージョンが利用可能であることを確認したら、ホストオペレーティングシステムにパッケージをインストールできます。そのためには、<literal>transactional-update</literal>セッションを開く必要があります。これにより、基礎となるオペレーティングシステムの読み込み/書き込みスナップショットが新しく作成され、イミュータブルプラットフォームに変更を加えることが可能になります(<literal>transactional-update</literal>の詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-transactional-updates/transactional-updates.html">こちら</link>を参照してください)。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para><literal>transactional-update</literal>シェルを起動したら、NVIDIAからパッケージリポジトリを追加します。これにより、<literal>nvidia-smi</literal>などの追加ユーティリティをプルできます。</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-suse-main
zypper --gpg-auto-import-keys refresh</screen>
<para>その後、ドライバと、追加ユーティリティの<literal>nvidia-compute-utils</literal>をインストールできます。ユーティリティが不要の場合は省略できますが、テスト目的の場合は、この段階でインストールする価値があります。</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>インストールが失敗する場合、選択したドライババージョンとNVIDIAがリポジトリで配布しているバージョンとの間で依存関係の不一致があることを示している可能性があります。前のセクションを参照して、バージョンが一致していることを確認してください。また、別のドライババージョンをインストールしてみてください。たとえば、NVIDIAリポジトリに以前のバージョンがある場合、インストールコマンドに<literal>nvidia-open-driver-G06-signed-kmp=550.54.14</literal>を指定して、一致するバージョンを指定してみることができます。</para>
</note>
<para>次に、サポートされているGPUを使用して<emphasis>いない</emphasis>場合は(<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">こちら</link>でリストを確認できます)、モジュールレベルでサポートを有効にすることで、ドライバが動作するかどうかを確認できますが、結果はユーザによって異なります。<emphasis>サポートされている</emphasis>GPUを使用している場合は、この手順はスキップしてください。</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>これらのパッケージをインストールしたので、<literal>transactional-update</literal>セッションを終了します。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>次に進む前に、<literal>transactional-update</literal>セッションを終了していることを確認してください。</para>
</note>
<para>ドライバをインストールしたら、再起動します。SUSE Linux
Microはイミュータブルオペレーティングシステムであるため、前の手順で作成した新しいスナップショットで再起動する必要があります。ドライバはこの新しいスナップショットにのみインストールされるため、この新しいスナップショットで再起動しないとドライバをロードできません(新しいスナップショットでの再起動は自動的に実行されます)。準備ができたらrebootコマンドを発行します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>システムが正常に再起動したら、ログインし直し、
<literal>nvidia-smi</literal>ツールを使用して、ドライバが正常にロードされていて、GPUへのアクセスと列挙をどちらも実行できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>このコマンドの出力は次のような出力になります。以下の例では、GPUが2つあることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>これで、SUSE Linux MicroシステムへのNVIDIAドライバのインストールと検証プロセスは完了です。</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>手動インストールの追加検証</title>
<para>この段階で確認できるのは、ホストレベルでNVIDIAデバイスにアクセスできること、およびドライバが正常にロードされていることだけです。ただし、それが機能していることを確認したい場合は、簡単なテストを実施して、GPUがユーザスペースアプリケーションから、理想的にはコンテナ経由で命令を受け取れること、および実際のワークロードが通常使用するものであるCUDAライブラリを通じて命令を受け取れることを検証します。このためには、<literal>nvidia-container-toolkit</literal>
(<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container
Toolkit</link>)をインストールしてホストOSにさらに変更を加えることができます。まず、別の<literal>transactional-update</literal>シェルを開きます。前の手順ではこれを1つのトランザクションで実行できたことに注目し、後のセクションでこれを完全に自動的に実行する方法を確認します。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>次に、NVIDIA Container
Toolkitリポジトリから<literal>nvidia-container-toolkit</literal>パッケージをインストールします。</para>
<itemizedlist>
<listitem>
<para>次の<literal>nvidia-container-toolkit.repo</literal>には、安定版(<literal>nvidia-container-toolkit</literal>)と実験版(<literal>nvidia-container-toolkit-experimental</literal>)のリポジトリが含まれています。運用環境での使用には、安定版リポジトリをお勧めします。実験版リポジトリはデフォルトで無効になっています。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>準備ができたら、<literal>transactional-update</literal>シェルを終了できます。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>…​そして新しいスナップショットでマシンを再起動します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>前述同様に、変更を有効にするには、必ず<literal>transactional-shell</literal>を終了し、マシンを再起動する必要があります。</para>
</note>
<para>マシンが再起動したら、システムがNVIDIA Container
Toolkitを使用してデバイスを正常に列挙できることを確認できます。出力は詳細で、INFOとWARNのメッセージがありますが、ERRORのメッセージはありません。</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>これにより、そのマシンで起動するコンテナはすべて、検出されたNVIDIA
GPUデバイスを使用できることが確認されます。準備ができたら、podmanベースのコンテナを実行できます。これを<literal>podman</literal>を介して行うことで、コンテナ内からNVIDIAデバイスへのアクセスを効果的に検証することができ、後の段階でKubernetesで同じ操作をするための自信が得られます。<literal>podman</literal>に対し、前のコマンドで<link
xl:href="https://registry.suse.com/repositories/bci-bci-base-15sp6">SLE
BCI</link>に基づいて処理したラベル付きのNVIDIAデバイスへのアクセス権を与え、バッシュコマンドを実行します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>続いて、一時的なpodmanコンテナ内からコマンドを実行します。このコンテナは基盤となるシステムにはアクセスできず一時的であるため、ここで行う操作は永続せず、基盤となるホスト上にあるものを壊すことは一切できないはずです。現在はコンテナ内で作業しているため、必要なCUDAライブラリをインストールできます。ここでもう一度、ご使用のドライバにあったCUDAバージョンを<link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">こちら</link>で確認してください。ただし、必要なCUDAバージョンは<literal>nvidia-smi</literal>の前の出力に表示されているはずです。以下の例では、<emphasis>CUDA
12.3</emphasis>をインストールして多数の例、デモ、開発キットをプルし、GPUを完全に検証できるようにしています。</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-suse
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>これが正常にインストールされた後にコンテナを終了しないでください。<literal>deviceQuery</literal>
CUDAの例を実行し、CUDAを介して、およびコンテナ自体からGPUアクセスを包括的に検証します。</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>成功すると、次のような出力が表示されます。コマンドの最後にある「<literal>Result =
PASS</literal>」というメッセージに注意してください。また、次の出力では2つのGPUが正しく識別されていますが、ご使用の環境では1つしかない場合があることにも注意してください。</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>ここから、続いて他のCUDAワークロードを実行できます。コンパイラやCUDAエコシステムの他の側面を使用して、さらにテストを実行できます。完了したら、コンテナを終了できます。コンテナにインストールしたものはすべて一時的なものであるため(したがって失われるため)、基盤となるオペレーティングシステムには影響がないことに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Kubernetesを使用した実装</title>
<para>open版NVIDIAドライバをSUSE Linux
Microにインストールして使用できることが証明されたので、同じマシンにKubernetesを設定してみましょう。このガイドでは、Kubernetesのデプロイについては説明しませんが、<link
xl:href="https://k3s.io/">K3s</link>または<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>をインストール済みで、kubeconfigが適宜設定されており、標準の<literal>kubectl</literal>コマンドをスーパーユーザとして実行できることを前提としています。ここではノードがシングルノードクラスタを形成していることを想定していますが、中心となる手順はマルチノードクラスタでも同様です。まず、<literal>kubectl</literal>のアクセスが機能していることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.33.3+rke2r1</screen>
<para>k3s/rke2のインストールによってホスト上のNVIDIA Container
Toolkitが検出され、NVIDIAランタイム統合が<literal>containerd</literal>
(k3s/rke2が使用するContainer Runtime
Interface)に自動設定されていることを確認する必要があります。確認するには、containerdの<literal>config.toml</literal>ファイルをチェックします。</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>次のような画面が表示される必要があります。K3sの場合に相当する場所は<literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>です。</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>これらのエントリが存在しない場合は、検出が失敗している可能性があります。この原因として考えられるのは、マシンまたはKubernetesサービスを再起動していないことです。必要に応じて、上記のようにこれらを手動で追加してください。</para>
</note>
<para>次に、NVIDIA
<literal>RuntimeClass</literal>を追加のKubernetesランタイムとしてデフォルト値に設定する必要があります。これにより、GPUへのアクセスが必要なPodに対するユーザ要求が、
<literal>containerd</literal>の設定に従って、NVIDIA Container
Toolkitを使用して<literal>nvidia-container-runtime</literal>を介してアクセスできるようにします。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>次の手順は、<link xl:href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA
Device Plugin</link>を設定することです。これにより、NVIDIA Container
Toolkitと連携して、クラスタ内で使用可能なリソースとしてNVIDIA
GPUを利用するようにKubernetesを設定します。このツールはまず、基盤となるホスト上のすべての機能(GPU、ドライバ、その他の機能(GLなど)を含む)を検出し、その後、ユーザがGPUリソースを要求してアプリケーションの一部として使用できるようにします。</para>
<para>まず、NVIDIA Device Plugin用のHelmリポジトリを追加して更新する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>これで、NVIDIA Device Pluginをインストールできます。</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>数分後、新しいPodが実行されているのがわかります。これで、利用可能なノード上での検出は完了し、検出されたGPUの数を示すタグがノードに付けられます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>これで、このGPUを使用するNVIDIA Podを作成する準備ができました。CUDA Benchmarkコンテナで試してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>すべて問題なければ、ログを見て、ベンチマーク情報を確認できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
        -fullscreen       (run n-body simulation in fullscreen mode)
        -fp64             (use double precision floating point values for simulation)
        -hostmem          (stores simulation data in host memory)
        -benchmark        (run benchmark to measure performance)
        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
        -compare          (compares simulation results running once on the default GPU and once on the CPU)
        -cpu              (run n-body simulation on the CPU)
        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>最後に、アプリケーションでOpenGLが必要な場合は、必要なNVIDIA OpenGLライブラリをホストレベルでインストールし、NVIDIA
Device PluginとNVIDIA Container
Toolkitを使用してそのライブラリをコンテナで利用できるようにすることができます。これを行うには、次のようにパッケージをインストールします。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>このパッケージをアプリケーションで使用できるようにするには再起動が必要です。NVIDIA Device Pluginは、NVIDIA Container
Toolkitを介してこれを自動的に再検出します。</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Edge Image Builderを使用した統合</title>
<para>さて、SUSE Linux Micro上のアプリケーションとGPUの全機能をデモで示したので、 <xref
linkend="components-eib"/>を使用してすべてをまとめ、デプロイ可能/使用可能なISOまたはRAWディスクイメージで提供したいと思います。このガイドでは、Edge
Image
Builderの使用方法は説明せずに、このようなイメージを構築するために必要な設定について説明します。以下に、必要なすべてのコンポーネントを追加設定なしにデプロイするためのイメージ定義の例と、必要なKubernetes設定ファイルを示します。以下に示す例では、Edge
Image Builderディレクトリは次のようなディレクトリ構造になっています。</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>これらのファイルを調べてみましょう。まず、K3sを実行するシングルノードクラスタのサンプルイメージ定義を次に示します。このイメージ定義では、ユーティリティとOpenGLパッケージもデプロイします(<literal>eib-config-iso.yaml</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: [snip]
kubernetes:
  version: v1.33.3+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>これは単なる例です。要件や期待に合うようにカスタマイズする必要がある場合があります。また、SUSE Linux
Microを使用する場合は、パッケージの依存関係を解決してNVIDIAドライバをプルするために、独自の
<literal>sccRegistrationCode</literal>を指定する必要があります。</para>
</note>
<para>これに加えて、他のコンポーネントを追加して、ブート時にKubernetesによってロードされるようにする必要があります。EIBディレクトリにはまず<literal>kubernetes</literal>ディレクトリが必要で、その下に設定、Helmチャート値、必要な追加のマニフェスト用のサブディレクトリが必要です。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>CNIを選択し(選択しない場合はデフォルトでCiliumになります)、SELinuxを有効にして、(オプションの)Kubernetes設定を行いましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>続いて、NVIDIA RuntimeClassがKubernetesクラスタ上に作成されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>ビルトインHelmコントローラを使用して、Kubernetes自体を使用してNVIDIA Device
Pluginをデプロイします。チャートの値ファイルでランタイムクラスを指定しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>次に進む前に、NVIDIA Container Toolkit RPMの公開鍵を取得する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>Kubernetesバイナリ、コンテナイメージ、Helmチャート(および参照イメージ)など、必要なアーティファクトがすべて自動的にエアギャップ化されます。つまり、デプロイ時のシステムにはデフォルトでインターネット接続は不要です。ここで必要なのは<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からSUSE
Linux Micro
ISOを取得する(そしてそれを<literal>base-images</literal>ディレクトリに配置する)ことだけです。そうすれば、Edge
Image Builderツールを呼び出してISOを生成できます。この例を完了するために、イメージの構築に使用したコマンドを次に示します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-config-iso.yaml</screen>
<para>Edge Image Builderの詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">ドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-resolving-issues">
<title>問題の解決</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smiでGPUが検出されない</title>
<para><literal>dmesg</literal>を使用してカーネルメッセージを確認します。<literal>NvKMSKapDevice</literal>を割り当てることができないことを示している場合は、サポート対象外のGPUの回避策を適用します。</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>メモ</emphasis>:
上記の手順でカーネルモジュールの設定を変更した場合は、変更を有効にするために、カーネルモジュールを再ロードするか、再起動する必要があります。</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="day-2-operations">
<title>Day 2操作</title>
<partintro>
<para>このセクションでは、管理者がさまざまな「Day 2」操作タスクを管理クラスタとダウンストリームクラスタの両方で処理する方法について説明します。</para>
</partintro>
<chapter xml:id="day2-migration">
<title>Edge 3.4のマイグレーション</title>
<para>このセクションでは、<literal>管理</literal>クラスタと<literal>ダウンストリーム</literal>クラスタを<literal>Edge
3.3</literal>から<literal>Edge 3.4.0</literal>に移行する方法について説明します。</para>
<important>
<para>クラスタマイグレーションは常に、<literal>Edge 3.3</literal>の
<literal>最新のZ-stream</literal>リリースから実行してください。</para>
<para>常に、<literal>Edge
3.4.0</literal>リリースに移行してください。マイグレーション後のアップグレードについては、管理クラスタ(<xref
linkend="day2-mgmt-cluster"/>)およびダウンストリームクラスタ(<xref
linkend="day2-downstream-clusters"/>)のセクションを参照してください。</para>
</important>
<section xml:id="day2-migration-mgmt">
<title>管理クラスタ</title>
<para>このセクションでは、次のトピックについて説明します。</para>
<para><xref linkend="day2-migration-mgmt-prereq"/> -
マイグレーションを開始する前に完了する必要がある前提条件の手順。</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller"/> - <xref
linkend="components-upgrade-controller"/>を使用して<literal>管理</literal>クラスタのマイグレーションを実行する方法。</para>
<para><xref linkend="day2-migration-mgmt-fleet"/> - <xref
linkend="components-fleet"/>を使用して<literal>管理</literal>クラスタのマイグレーションを実行する方法。</para>
<section xml:id="day2-migration-mgmt-prereq">
<title>前提条件</title>
<section xml:id="id-upgrade-the-bare-metal-operator-crds">
<title>Bare Metal Operator CRDのアップグレード</title>
<note>
<para><xref linkend="components-metal3"/>チャートのアップグレードが必要なクラスタにのみ適用されます。</para>
</note>
<para><literal>Metal<superscript>3</superscript></literal> Helmチャートには、<link
xl:href="https://book.metal3.io/bmo/introduction.html">Bare Metal Operator
(BMO)</link> CRDが含まれており、これはHelmの<link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you">CRD</link>ディレクトリを活用することで提供されています。</para>
<para>ただし、このアプローチには特定の制限があり、特にHelmを使用してこのディレクトリ内のCRDをアップグレードできません。詳細については、<link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations">Helmのドキュメント</link>を参照してください。</para>
<para>そのため、Metal<superscript>3</superscript>を<literal>Edge
3.4.0</literal>互換のバージョンにアップグレードする前に、ユーザは基盤となるBMO CRDを手動でアップグレードする必要があります。</para>
<para><literal>Helm</literal>がインストールされ、<literal>kubectl</literal>が<literal>管理</literal>
クラスタを指すように設定されているマシンで、次の手順を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>BMO CRDを手動で適用します。</para>
<screen language="bash" linenumbering="unnumbered">helm show crds oci://registry.suse.com/edge/charts/metal3 --version 304.0.16+up0.12.6 | kubectl apply -f -</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller">
<title>Upgrade Controller</title>
<important>
<para><literal>Upgrade Controller</literal>は現在、<emphasis
role="strong">非エアギャップ管理</emphasis>クラスタに対してのみEdgeリリースのマイグレーションをサポートしています。</para>
</important>
<para>次のトピックは、このセクションの一部として説明します。</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-prereq"/> -
<literal>Upgrade Controller</literal>に固有の前提条件。</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-migration"/> -
<literal>Upgrade
Controller</literal>を使用して、<literal>管理</literal>クラスタを新しいEdgeバージョンに移行するための手順。</para>
<section xml:id="day2-migration-mgmt-upgrade-controller-prereq">
<title>前提条件</title>
<section xml:id="id-edge-3-4-upgrade-controller">
<title>Edge 3.4 Upgrade Controller</title>
<para><literal>Upgrade
Controller</literal>を使用する前に、まず目的のEdgeリリースに移行できるバージョンが実行されていることを確認する必要があります。</para>
<para>このためには:</para>
<orderedlist numeration="arabic">
<listitem>
<para>以前のEdgeリリースから<literal>Upgrade
Controller</literal>がすでにデプロイされている場合は、そのチャートをアップグレードします。</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade upgrade-controller -n upgrade-controller-system oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1</screen>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal>がデプロイされて<emphasis
role="strong">いない</emphasis>場合は、<xref
linkend="components-upgrade-controller-installation"/>に従います。</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller-migration">
<title>マイグレーション手順</title>
<para><literal>Upgrade
Controller</literal>を使用した<literal>管理</literal>クラスタのマイグレーションの実行は、アップグレードの実行と基本的に類似しています。</para>
<para>唯一の違いは、<literal>UpgradePlan</literal>
が<literal>3.4.0</literal>リリースバージョンを指定する<emphasis
role="strong">必要がある</emphasis>ことです。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  # Change to the namespace of your Upgrade Controller
  namespace: CHANGE_ME
spec:
  releaseVersion: 3.4.0</screen>
<para>上記の<literal>UpgradePlan</literal>を使用してマイグレーションを実行する方法については、Upgrade
Controllerのアップグレードプロセス(<xref
linkend="management-day2-upgrade-controller"/>)を参照してください。</para>
</section>
</section>
<section xml:id="day2-migration-mgmt-fleet">
<title>Fleet</title>
<note>
<para>可能な限り、マイグレーションには<xref
linkend="day2-migration-mgmt-upgrade-controller"/>を使用してください。</para>
<para><literal>Upgrade
Controller</literal>でカバーされていないユースケースの場合にのみ、このセクションを参照してください。</para>
</note>
<para><literal>Fleet</literal>を使用した<literal>管理</literal>クラスタのマイグレーションの実行は、基本的にアップグレードの実行と類似しています。</para>
<para><emphasis role="strong">主な</emphasis>違いは次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>フリートは<literal>suse-edge/fleet-examples</literal>リポジトリの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link>リリースから<emphasis
role="strong">使用される必要があります</emphasis>。</para>
</listitem>
<listitem>
<para>アップグレードがスケジュールされているチャートは、<literal>Edge
3.4.0</literal>リリースと互換性のあるバージョンにアップグレードする <emphasis
role="strong">必要があります</emphasis>。<literal>Edge
3.4.0</literal>コンポーネントのリストについては、<xref
linkend="release-notes-3-4-0"/>を参照してください。</para>
</listitem>
</orderedlist>
<important>
<para><literal>Edge 3.4.0</literal>へのマイグレーションを成功させるには、ユーザが上記の点に従うことが重要です。</para>
</important>
<para>上記の点を踏まえて、ユーザはマイグレーションの実行に必要な手順に関する包括的なガイドとして、<literal>管理</literal>
クラスタFleet (<xref linkend="management-day2-fleet"/>)のドキュメントに従うことができます。</para>
</section>
</section>
<section xml:id="day2-migration-downstream">
<title>ダウンストリームクラスタ</title>
<para><xref linkend="day2-migration-downstream-fleet"/> - <xref
linkend="components-fleet"/>を使用して<literal>ダウンストリーム</literal>クラスタのマイグレーションを実行する方法。</para>
<section xml:id="day2-migration-downstream-fleet">
<title>Fleet</title>
<para><literal>Fleet</literal>を使用した<literal>ダウンストリーム</literal>クラスタのマイグレーションの実行は、アップグレードの実行と基本的に類似しています。</para>
<para><emphasis role="strong">主な</emphasis>違いは次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>フリートは<literal>suse-edge/fleet-examples</literal>リポジトリの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link>リリースから<emphasis
role="strong">使用される必要があります</emphasis>。</para>
</listitem>
<listitem>
<para>アップグレードがスケジュールされているチャートは、<literal>Edge
3.4.0</literal>リリースと互換性のあるバージョンにアップグレードする <emphasis
role="strong">必要があります</emphasis>。<literal>Edge
3.4.0</literal>コンポーネントのリストについては、<xref
linkend="release-notes-3-4-0"/>を参照してください。</para>
</listitem>
</orderedlist>
<important>
<para><literal>Edge 3.4.0</literal>へのマイグレーションを成功させるには、ユーザが上記の点に従うことが重要です。</para>
</important>
<para>上記の点を踏まえて、ユーザはマイグレーションの実行に必要な手順に関する包括的なガイドとして、
<literal>ダウンストリーム</literal>クラスタFleet (<xref
linkend="downstream-day2-fleet"/>)のドキュメントに従うことができます。</para>
</section>
</section>
</chapter>
<chapter xml:id="day2-mgmt-cluster">
<title>管理クラスタ</title>
<para>現在、<literal>管理</literal>クラスタで「Day 2」操作を実行するには、次の2つの方法があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="components-upgrade-controller"/> - <xref
linkend="management-day2-upgrade-controller"/>を通じて</para>
</listitem>
<listitem>
<para><xref linkend="components-fleet"/> - <xref
linkend="management-day2-fleet"/>を通じて</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-upgrade-controller">
<title>Upgrade Controller</title>
<important>
<para><literal>Upgrade Controller</literal>は現在、<emphasis
role="strong">非エアギャップ管理</emphasis>クラスタの<literal>Day
2</literal>操作のみをサポートしています。</para>
</important>
<para>このセクションでは、あるEdgeプラットフォームバージョンから別のバージョンに
<literal>管理</literal>クラスタをアップグレードする方法に関連する さまざまな<literal>Day
2</literal>操作を実行する方法について説明します。</para>
<para><literal>Day 2</literal>操作は、Upgrade Controller (<xref
linkend="components-upgrade-controller"/>)によって自動化され、以下のものが含まれます。</para>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro (<xref linkend="components-slmicro"/>) OSのアップグレード</para>
</listitem>
<listitem>
<para><xref linkend="components-rke2"/>または<xref linkend="components-k3s"/>
Kubernetesのアップグレード</para>
</listitem>
<listitem>
<para>SUSE追加コンポーネント(SUSE Rancher Prime、SUSE Securityなど)のアップグレード</para>
</listitem>
</itemizedlist>
<section xml:id="id-prerequisites-13">
<title>前提条件</title>
<para><literal>管理</literal>クラスタをアップグレードする前に、次の前提条件が満たされている必要があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>SCC登録ノード</literal> - クラスタのノードのOSが、アップグレードするEdgeリリース(<xref
linkend="release-notes"/>)で指定されているOSバージョンをサポートするサブスクリプションキーで登録されていることを確認してください。</para>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal> - <literal>Upgrade
Controller</literal>が<literal>管理</literal>クラスタにデプロイされていることを確認します。インストール手順については、<xref
linkend="components-upgrade-controller-installation"/>を参照してください。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade">
<title>アップグレード</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>管理</literal>クラスタをアップグレードするEdgeリリース(<xref
linkend="release-notes"/>)バージョンを決定します。</para>
</listitem>
<listitem>
<para><literal>管理</literal>クラスタに、目的の<literal>リリースバージョン</literal>を指定する<literal>UpgradePlan</literal>をデプロイします。<literal>UpgradePlan</literal>は<literal>Upgrade
Controller</literal>のネームスペースにデプロイされる必要があります。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n &lt;upgrade_controller_namespace&gt; -f - &lt;&lt;EOF
apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
spec:
  # Version retrieved from release notes
  releaseVersion: 3.X.Y
EOF</screen>
<note>
<para><literal>UpgradePlan</literal>に対して追加の設定を行いたいユースケースがある場合があります。すべての可能な設定については、<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>を参照してください。</para>
</note>
</listitem>
<listitem>
<para><literal>Upgrade
Controller</literal>のネームスペースに<literal>UpgradePlan</literal>をデプロイすると、<literal>アップグレードプロセス</literal>が開始されます。</para>
<note>
<para>実際の<literal>アップグレードプロセス</literal>の詳細については、<xref
linkend="components-upgrade-controller-how"/>を参照してください。</para>
<para><literal>アップグレードプロセス</literal>を追跡する方法については、<xref
linkend="components-upgrade-controller-how-track"/>を参照してください。</para>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet">
<title>Fleet</title>
<para>このセクションでは、Fleet (<xref linkend="components-fleet"/>)コンポーネントを使用して「Day
2」操作を実行する方法について説明します。</para>
<para>次のトピックは、このセクションの一部として説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-components"/> - すべての「Day
2」操作に使用されるデフォルトコンポーネント。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-determine-use-case"/> -
使用されるFleetカスタムリソースの概要と、これらのリソースがさまざまな「Day 2」操作のユースケースに適しているかどうかを説明します。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-upgrade-workflow"/> - Fleetを使用して「Day
2」操作を実行するためのワークフローガイドを提供します。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade"/> -
Fleetを使用してOSアップグレードを実行する方法を説明します。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade"/> -
Fleetを使用してKubernetesバージョンのアップグレードを実行する方法を説明します。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade"/> -
Fleetを使用してHelmチャートのアップグレードを実行する方法を説明します。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-components">
<title>コンポーネント</title>
<para>以下に、Fleetを使用して「Day
2」操作を正常に実行できるように<literal>管理</literal>クラスタに設定する必要があるデフォルトコンポーネントの説明を記載しています。</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<para><emphasis role="strong">オプション</emphasis>:
<literal>ダウンストリームクラスタ</literal>を管理し、<literal>System Upgrade
Controller</literal>を<literal>管理クラスタ</literal>上にデプロイする責任。</para>
<para>詳細については、<xref linkend="components-rancher"/>を参照してください。</para>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System Upgrade Controller (SUC)</title>
<para><emphasis role="strong">System Upgrade Controller</emphasis>は、
<literal>Plan</literal>と呼ばれるカスタムリソースを通じて提供される設定データに基づいて指定されたノードでタスクを実行する責任を負います。</para>
<para><emphasis
role="strong">SUC</emphasis>は、オペレーティングシステムとKubernetesディストリビューションのアップグレードに積極的に活用されます。</para>
<para><emphasis
role="strong">SUC</emphasis>コンポーネントとそれがEdgeスタックにどのように適合するかに関する詳細については、<xref
linkend="components-system-upgrade-controller"/>を参照してください。</para>
</section>
</section>
<section xml:id="management-day2-fleet-determine-use-case">
<title>ユースケースの決定</title>
<para>Fleetは2種類の<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>を使用してKubernetesおよびHelmリソースの管理を有効にします。</para>
<para>以下に、これらのリソースの目的と、「Day 2」操作のコンテキストに最適なユースケースに関する情報を示します。</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para><literal>GitRepo</literal>は、<literal>Fleet</literal>が<literal>バンドル</literal>の作成元として使用できるGitリポジトリを表すFleet
(<xref
linkend="components-fleet"/>)リソースです。各<literal>バンドル</literal>は、<literal>GitRepo</literal>リソースの内部で定義された設定パスに基づいて作成されます。詳細については、<link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>のドキュメントを参照してください。</para>
<para>「Day 2」操作のコンテキストでは、<literal>GitRepo</literal>リソースは通常、<emphasis>Fleet
GitOps</emphasis>アプローチを利用する<emphasis
role="strong">非エアギャップ</emphasis>環境に<literal>SUC</literal>または<literal>SUC
Plan</literal>をデプロイするために使用されます。</para>
<para>または、<emphasis role="strong">リポジトリのセットアップをローカルGitサーバ経由でミラーリングする場合</emphasis>
、<literal>GitRepo</literal>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
<section xml:id="id-bundle">
<title>バンドル</title>
<para><literal>バンドル</literal>は、ターゲットクラスタにデプロイする<emphasis role="strong">
生</emphasis>のKubernetesリソースを保持します。バンドルは通常、<literal>GitRepo</literal>リソースから作成されますが、手動でデプロイできるユースケースもあります。詳細については、<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>のドキュメントを参照してください。</para>
<para>「Day
2」操作のコンテキストでは、<literal>バンドル</literal>リソースは通常、何らかの形態の<emphasis>ローカルGitOps</emphasis>手法を使用しない<emphasis
role="strong">エアギャップ</emphasis>環境(<emphasis
role="strong">ローカルGitサーバ</emphasis>など
)で<literal>SUC</literal>または<literal>SUC Plan</literal>をデプロイするために使用されます。</para>
<para>または、ご自身のユースケースで<emphasis>GitOps</emphasis>ワークフローを使用できない場合は(Gitリポジトリを使用する場合など)、<literal>バンドル</literal>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">非エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
</section>
<section xml:id="management-day2-upgrade-workflow">
<title>Day 2ワークフロー</title>
<para>以下に、管理クラスタを特定のEdgeリリースにアップグレードする際に従う必要がある「Day 2」ワークフローを示します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>OSアップグレード(<xref linkend="management-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Kubernetesバージョンアップグレード(<xref linkend="management-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helmチャートアップグレード(<xref linkend="management-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade">
<title>OSアップグレード</title>
<para>このセクションでは、<xref linkend="components-fleet"/>と<xref
linkend="components-system-upgrade-controller"/>を使用してオペレーティングシステムのアップグレードを行う方法について説明します。</para>
<para>次のトピックは、このセクションの一部として説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-components"/> -
アップグレードプロセスで使用される追加のコンポーネント。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-overview"/> -
アップグレードプロセスの概要。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-requirements"/> -
アップグレードプロセスの要件。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-plan-deployment"/> -
<literal>SUC Plan</literal>のデプロイ方法に関する情報。このプランはアップグレードプロセスをトリガする責任を負います。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-os-upgrade-components">
<title>コンポーネント</title>
<para>このセクションでは、 <literal>OSアップグレード</literal>プロセスがデフォルトの「Day 2」コンポーネント(<xref
linkend="management-day2-fleet-components"/>)よりも使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="management-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>特定のノードでのOSアップグレードは、<link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>によって処理されます。</para>
<para>あるEdgeバージョンから別のEdgeバージョンにOSが必要とするアップグレードのタイプに応じて、異なるサービスが作成されます。</para>
<itemizedlist>
<listitem>
<para>同じOSバージョン(例:
<literal>6.0</literal>)を必要とするEdgeバージョンの場合、<literal>os-pkg-update.service</literal>が作成されます。このサービスは<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>を使用して、<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常のパッケージアップグレード</link>を実行します。</para>
</listitem>
<listitem>
<para>OSバージョンのマイグレーション(例: <literal>6.0</literal> →
<literal>6.1</literal>)が必要なEdgeバージョンの場合、<literal>os-migration.service</literal>が作成されます。このサービスは、<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>を使用して以下の処理を実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常のパッケージのアップグレード</link>。このアップグレードにより、すべてのパッケージを最新にすることで、古いパッケージバージョンに関連するマイグレーション時の障害を軽減します。</para>
</listitem>
<listitem>
<para><literal>zypper migration</literal>コマンドを利用したOSマイグレーション。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>上記サービスは、OSアップグレードが必要な管理クラスタ上に配置されている必要がある<literal>SUC
Plan</literal>を通じて各ノードに配布されます。</para>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-overview">
<title>概要</title>
<para>管理クラスタノードのオペレーティングシステムのアップグレードは、<literal>Fleet</literal>と<literal>System
Upgrade Controller (SUC)</literal>を利用して実行されます。</para>
<para><emphasis role="strong">Fleet</emphasis>は、<literal>SUC
Plan</literal>を目的のクラスタにデプロイおよび管理するために使用されます。</para>
<note>
<para><literal>SUC
Plan</literal>は、特定のタスクをノードセットで実行するために<literal>SUC</literal>が従う必要がある手順を記述した<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>です。<literal>SUC
Plan</literal>の例については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリームリポジトリ</link>を参照してください。</para>
</note>
<para><literal>OS SUC Plan</literal>は、<link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>または<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースを特定のFleet<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ワークスペース</link>にデプロイすることで、各クラスタに配布されます。Fleetはデプロイされた<literal>GitRepo/Bundle</literal>を取得し、その内容
(<literal>OS SUC plan</literal>)を目的のクラスタにデプロイします。</para>
<note>
<para><literal>GitRepo/バンドル</literal>リソースは常に、<literal>管理クラスタ</literal>上にデプロイされます。<literal>GitRepo</literal>リソースを使用するか<literal>バンドル</literal>リソースを使用するかは、ユースケースによって異なります。詳細については、<xref
linkend="management-day2-fleet-determine-use-case"/>をご確認ください。</para>
</note>
<para><literal>OS SUC Plan</literal>は、次のワークフローを記述します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>常に、OSをアップグレードする前に、ノードを<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>します。</para>
</listitem>
<listitem>
<para>常に、
<literal>ワーカー</literal>ノードの前に<literal>コントロールプレーン</literal>ノードをアップグレードします。</para>
</listitem>
<listitem>
<para>常に、一度に <emphasis role="strong">1</emphasis>ノードずつクラスタをアップグレードします。</para>
</listitem>
</orderedlist>
<para><literal>OS SUC Plan</literal>がデプロイされると、ワークフローは次のようになります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUCは、デプロイされた<literal>OS SUC Plan</literal>を照合し、<literal>Kubernetes
Job</literal>を<emphasis role="strong">各ノード</emphasis>に作成します。</para>
</listitem>
<listitem>
<para><literal>Kubernetes
Job</literal>は、パッケージのアップグレードまたはOSマイグレーションのためにsystemd.service (<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>)を作成します。</para>
</listitem>
<listitem>
<para>作成された <literal>systemd.service</literal>は、特定のノードでOSアップグレードプロセスをトリガします。</para>
<important>
<para>OSアップグレードプロセスが終了すると、対応するノードが <literal>再起動</literal>され、システムに更新が適用されます。</para>
</important>
</listitem>
</orderedlist>
<para>上記の説明を以下に図示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>fleet day2管理osアップグレード</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-os-upgrade-requirements">
<title>要件</title>
<para><emphasis>全般:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC登録マシン</emphasis> - すべての管理クラスタノードを<literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>に登録する必要があります。これは、各<literal>systemd.service</literal>が目的のRPMリポジトリに正常に接続できるようにするために必要です。</para>
<important>
<para>OSバージョンのマイグレーション (例: <literal>6.0</literal> →
<literal>6.1</literal>)が必要なEdgeリリースの場合、SCCキーが新しいバージョンへのマイグレーションをサポートしていることを確認してください。</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">コントロールプレーン</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。OSアップグレードに関連する<emphasis
role="strong">SUC
Plan</emphasis>は、<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>の下の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。<emphasis
role="strong">有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para><emphasis role="strong">control-plane</emphasis> SUC
Planに対してカスタムのTolerationを定義する例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>エアギャップ:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SUSE RPMリポジトリのミラーリング</emphasis> - OS
RPMリポジトリをローカルにミラーリングし、<literal>systemd.service</literal>がそのリポジトリにアクセスできるようにする必要があります。このためには、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>または<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>のいずれかを使用します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment">
<title>OSアップグレード - SUC Planのデプロイメント</title>
<important>
<para>この手順を使用して以前にアップグレードした環境の場合、ユーザは次の手順の<emphasis
role="strong">いずれか</emphasis>を完了していることを確認する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>管理クラスタから古いEdgeリリースバージョンに関連する以前にデプロイしたSUC Planを削除する</literal> -
これは、既存の<literal>GitRepo/バンドル</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">ターゲット設定</link>から目的のクラスタを削除するか、<literal>GitRepo/バンドル</literal>リソースを完全に削除することで、実行できます。</para>
</listitem>
<listitem>
<para><literal>既存のGitRepo/バンドルリソースを再利用する</literal> -
目的の<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>の正しいフリートを保持する新しいタグにリソースのリビジョンをポイントすることで実行できます。</para>
</listitem>
</itemizedlist>
<para>これは古いEdgeリリースバージョンの<literal>SUC Plan</literal>間のクラッシュを回避するために実行されます。</para>
<para>ユーザがアップグレードを試す場合、管理クラスタに既存の<literal>SUC
Plan</literal>がある場合は、次のフリートエラーが表示されます。</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para><xref
linkend="management-day2-fleet-os-upgrade-overview"/>で説明したように、OSアップグレードは、次のいずれかの方法を使用して<literal>SUC
Plan</literal>を目的のクラスタに配布することで実行されます。</para>
<itemizedlist>
<listitem>
<para>Fleet <literal>GitRepo</literal>リソース - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>。</para>
</listitem>
<listitem>
<para>Fleet<literal>バンドル</literal>リソース - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle"/>。</para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref
linkend="management-day2-fleet-determine-use-case"/>を参照してください。</para>
<para><literal>OS SUC Plan</literal>をサードパーティのGitOpsツールからデプロイするユースケースの場合は、<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-third-party"/>を参照してください。</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>OS SUC Plan</literal>を配布する、<emphasis
role="strong">GitRepo</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>を通じて(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<para>Rancher UIを通じて<literal>GitRepo</literal>リソースを作成するには、公式の<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">ドキュメント</link>に従ってください。</para>
<para>Edgeチームは、すぐに使用できる<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">フリート</link>を維持しています。環境によっては、このフリートを直接使用することも、テンプレートとして使用することもできます。</para>
<important>
<para>常に、このフリートは有効なEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>フリートが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要がないユースケースでは、ユーザは<literal>suse-edge/fleet-examples</literal>リポジトリから
<literal>os-upgrade</literal>フリートを直接参照できます。</para>
<para>カスタム変更(カスタム許容値の追加など)が必要な場合、ユーザは別のリポジトリから<literal>os-upgrade</literal>フリートを参照して、必要に応じてSUC
Planに変更を追加できるようにする必要があります。</para>
<para><literal>GitRepo</literal>を<literal>suse-edge/fleet-examples</literal>リポジトリのフリートを使用するように設定する方法の例については、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">こちら</link>を参照してください。</para>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>設定を編集します。</para>
<itemizedlist>
<listitem>
<para><literal>spec.targets</literal>セクションを削除します。ダウンストリームクラスタにのみ必要です。</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>GitRepo</literal>のネームスペースを<literal>fleet-local</literal>ネームスペースにポイントします。これは、リソースを管理クラスタにデプロイするために実行します。</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-local</literal>ネームスペースの下に、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-local

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>OS SUC Plan</literal>を配布する<emphasis
role="strong">バンドル</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>を通じて(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<para>Edgeチームは、以下の手順で使用可能な、すぐに使用できる<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">バンドル</link>を維持しています。</para>
<important>
<para>このバンドルは常に有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>RancherのUIを通じてバンドルを作成するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">［☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<note>
<para>バンドルが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要があるユースケースがある場合があります(カスタム許容値の追加など)。これらの変更を、以下の手順で生成されるバンドルに必ず含めてください。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">バンドルコンテンツ</link>を<literal>suse-edge/fleet-examples</literal>から<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページに手動でコピーする。</para>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>のクローンを作成し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。ここからバンドルの場所(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)に移動して、バンドルファイルを選択できます。これにより、バンドルコンテンツを持つ<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページが自動入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Rancher UIでバンドルを編集します。</para>
<itemizedlist>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ネームスペース</emphasis>を<literal>fleet-local</literal>ネームスペースを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを<literal>ローカル</literal>(管理)クラスタを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>一部のユースケースでは、<literal>ローカル</literal>クラスタに異なる名前を付けることができます。</para>
<para><literal>ローカル</literal>クラスタ名を取得するには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>設定を編集します。</para>
<itemizedlist>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを<literal>ローカル</literal>(管理)クラスタを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>一部のユースケースでは、<literal>ローカル</literal>クラスタに異なる名前を付けることができます。</para>
<para><literal>ローカル</literal>クラスタ名を取得するには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ネームスペース</emphasis>を<literal>fleet-local</literal>ネームスペースを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>作成した<emphasis
role="strong">バンドル</emphasis>リソースを<literal>fleet-local</literal>ネームスペースの下に表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-local</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザが<literal>OS SUC Plan</literal>を独自のサードパーティGitOpsワークフロー(例:
<literal>Flux</literal>)に組み込むユースケースがある場合があります。</para>
<para>必要なOSアップグレードリソースを取得するには、まず使用する<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリのEdge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを決定します。</para>
<para>その後、リソースは<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>で見つかります。ここでは次のようになります。</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal>は、<emphasis
role="strong">コントロールプレーン</emphasis>ノードのSUC Planリソースです。</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal>は、 <emphasis
role="strong">ワーカー</emphasis>ノードのSUC Planリソースです。</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal>は、systemd.service (<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>)の作成を担当する、<literal>upgrade.sh</literal>スクリプトを含むSecretです。</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal>は、<literal>upgrade.sh</literal>スクリプトによって使用される設定を保持するConfigMapです。</para>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、 <literal>System Upgrade
Controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。SUCデプロイメントの情報については、<xref
linkend="components-system-upgrade-controller-install"/>を参照してください。</para>
</important>
<para>GitOpsワークフローをOSアップグレードの<emphasis role="strong">SUC
Plan</emphasis>をデプロイするために使用する方法をよりよく理解するために、概要(<xref
linkend="management-day2-fleet-os-upgrade-overview"/>)を確認すると役立つ場合があります。</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade">
<title>Kubernetesバージョンアップグレード</title>
<para>このセクションでは、<xref linkend="components-fleet"/>と<xref
linkend="components-system-upgrade-controller"/>を使用してKubernetesアップグレードを実行する方法について説明します。</para>
<para>次のトピックは、このセクションの一部として説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-components"/> -
アップグレードプロセスで使用される追加のコンポーネント。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-overview"/> -
アップグレードプロセスの概要。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-requirements"/> -
アップグレードプロセスの要件。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment"/> -
<literal>SUC Plan</literal>のデプロイ方法に関する情報。このプランはアップグレードプロセスをトリガする責任を負います。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-k8s-upgrade-components">
<title>コンポーネント</title>
<para>このセクションでは、 <literal>K8sアップグレード</literal>プロセスがデフォルトの「Day 2」コンポーネント(<xref
linkend="management-day2-fleet-components"/>)よりも使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>特定のノードのRKE2バージョンのアップグレードを担当するコンテナイメージ。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、RKE2のアップグレードが必要な各<emphasis
role="strong">クラスタ</emphasis>に配置する必要があります。</para>
<para><literal>rke2-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>特定のノードのK3sバージョンのアップグレードを担当するコンテナイメージ。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、K3sのアップグレードが必要な各<emphasis
role="strong">クラスタ</emphasis>に配置する必要があります。</para>
<para><literal>k3s-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/k3s-io/k3s-upgrade">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-overview">
<title>概要</title>
<para>管理クラスタノードのKubernetesディストリビューションのアップグレードは、<literal>Fleet</literal>と<literal>System
Upgrade Controller (SUC)</literal>を利用して実行されます。</para>
<para><literal>Fleet</literal>は、<literal>SUC
Plan</literal>を目的のクラスタにデプロイし、管理するために使用されます。</para>
<note>
<para><literal>SUC Plan</literal>は、 <emphasis
role="strong">SUC</emphasis>が特定のタスクをノードのセットで実行するために従う必要がある手順を記述した<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>です。<literal>SUC
Plan</literal>の例については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリームリポジトリ</link>を参照してください。</para>
</note>
<para><literal>K8s SUC Plan</literal>は、 <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>または<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースを特定のFleet<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ワークスペース</link>にデプロイすることで各クラスタに配布されます。Fleetはデプロイされた<literal>GitRepo/バンドル</literal>を取得し、その内容(<literal>K8s
SUC Plan</literal>)を目的のクラスタにデプロイします。</para>
<note>
<para><literal>GitRepo/バンドル</literal>リソースは常に、<literal>管理クラスタ</literal>上にデプロイされます。<literal>GitRepo</literal>リソースを使用するか<literal>バンドル</literal>リソースを使用するかは、ユースケースによって異なります。詳細については、<xref
linkend="management-day2-fleet-determine-use-case"/>をご確認ください。</para>
</note>
<para><literal>K8s SUC Plan</literal>は、次のワークフローを記述します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>常に、K8sをアップグレードする前に、ノードを<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>します。</para>
</listitem>
<listitem>
<para>常に、
<literal>ワーカー</literal>ノードの前に<literal>コントロールプレーン</literal>ノードをアップグレードします。</para>
</listitem>
<listitem>
<para>常に、一度に <emphasis
role="strong">1</emphasis>ノードずつ<literal>コントロールプレーン</literal>ノードをアップグレードし、一度に
<emphasis
role="strong">2</emphasis>ノードずつ<literal>ワーカー</literal>ノードをアップグレードします。</para>
</listitem>
</orderedlist>
<para><literal>K8s SUC Plan</literal>がデプロイされたら、ワークフローは次のようになります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUCは、デプロイ済みの<literal>K8s SUC Plan</literal>を照合し、 <literal>Kubernetes
Job</literal>を<emphasis role="strong">各ノード</emphasis>に作成します。</para>
</listitem>
<listitem>
<para>Kubernetesディストリビューションによって、Jobはrke2-upgrade (<xref
linkend="management-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>)またはk3s-upgrade
(<xref
linkend="management-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>)のコンテナイメージを実行するPodを作成します。</para>
</listitem>
<listitem>
<para>作成されたPodは次のワークフローを実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>ノード上の既存の<literal>rke2/k3s</literal>バイナリを<literal>rke2-upgrade/k3s-upgrade</literal>イメージのバイナリで置き換えます。</para>
</listitem>
<listitem>
<para>実行中の<literal>rke2/k3s</literal>プロセスを強制終了します。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>rke2/k3s</literal>プロセスを強制終了すると、再起動がトリガされ、更新されたバイナリを実行する新しいプロセスが起動し、Kubernetesディストリビューションバージョンがアップグレードされます。</para>
</listitem>
</orderedlist>
<para>上記の説明を以下に図示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>fleet day2管理k8sアップグレード</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-requirements">
<title>要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Kubernetesディストリビューションをバックアップします。</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">RKE2クラスタ</emphasis>については、<link
xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2のバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">K3sクラスタ</emphasis>については、<link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3sのバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">control-plane</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。Kubernetesバージョンアップグレードに関する<emphasis
role="strong">SUC Plan</emphasis>は、次の場所の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">必ず、有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para>RKE2 <emphasis role="strong">control-plane</emphasis> SUC
PlanのカスタムTolerationの定義例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment">
<title>K8sアップグレード - SUC Planのデプロイメント</title>
<important>
<para>この手順を使用して以前にアップグレードした環境の場合、ユーザは次の手順の<emphasis
role="strong">いずれか</emphasis>を完了していることを確認する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>管理クラスタから古いEdgeリリースバージョンに関連する以前にデプロイしたSUC Planを削除する</literal> -
これは、既存の<literal>GitRepo/バンドル</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">ターゲット設定</link>から目的のクラスタを削除するか、<literal>GitRepo/バンドル</literal>リソースを完全に削除することで、実行できます。</para>
</listitem>
<listitem>
<para><literal>既存のGitRepo/バンドルリソースを再利用する</literal> -
目的の<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>の正しいフリートを保持する新しいタグにリソースのリビジョンをポイントすることで実行できます。</para>
</listitem>
</itemizedlist>
<para>これは古いEdgeリリースバージョンの<literal>SUC Plan</literal>間のクラッシュを回避するために実行されます。</para>
<para>ユーザがアップグレードを試す場合、管理クラスタに既存の<literal>SUC
Plan</literal>がある場合は、次のフリートエラーが表示されます。</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para><xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>で説明したように、Kubernetesアップグレードは、次のいずれかの方法を使用して<literal>SUC
Plan</literal>を目的のクラスタに配布することで実行されます。</para>
<itemizedlist>
<listitem>
<para>Fleet GitRepoリソース(<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Fleetバンドルリソース(<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref
linkend="management-day2-fleet-determine-use-case"/>を参照してください。</para>
<para><literal>K8s SUC Plan</literal>をサードパーティのGitOpsツールからデプロイするユースケースの場合は、<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>を参照してください。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>K8s SUC Plan</literal>を配布する、<emphasis
role="strong">GitRepo</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>を通じて(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<para>Rancher UIを通じて<literal>GitRepo</literal>リソースを作成するには、公式の<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">ドキュメント</link>に従ってください。</para>
<para>Edgeチームは、<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>
Kubernetesディストリビューションの両方のすぐに使用できるフリートを維持しています。環境によっては、このフリートを直接使用することも、テンプレートとして使用することもできます。</para>
<important>
<para>常に、これらのフリートは有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>これらのフリートが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要がないユースケースの場合、ユーザは<literal>suse-edge/fleet-examples</literal>リポジトリからフリートを直接参照できます。</para>
<para>カスタム変更(カスタム許容値の追加など)が必要な場合、ユーザは別のリポジトリからフリートを参照して、必要に応じてSUC
Planに変更を追加できるようにする必要があります。</para>
<para><literal>suse-edge/fleet-examples</literal>リポジトリからフリートを使用した<literal>GitRepo</literal>リソースの設定例は、次のとおりです。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>設定を編集します。</para>
<itemizedlist>
<listitem>
<para><literal>spec.targets</literal>セクションを削除します。ダウンストリームクラスタにのみ必要です。</para>
<itemizedlist>
<listitem>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>GitRepo</literal>のネームスペースを<literal>fleet-local</literal>ネームスペースにポイントします。これは、リソースを管理クラスタにデプロイするために実行します。</para>
<itemizedlist>
<listitem>
<para>RKE2の場合:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>K3sの場合:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-local</literal>ネームスペースの下に、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-local

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-local

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>KubernetesアップグレードSUC Plan</literal>を配布する、<emphasis
role="strong">バンドル</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>を通じて(
<literal>Rancher</literal> が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<para>Edgeチームは、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>
Kubernetesディストリビューションの両方のすぐに使用できるバンドルを維持しています。環境によっては、これらのバンドルを直接使用することも、テンプレートとして使用することもできます。</para>
<important>
<para>このバンドルは常に有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>RancherのUIを通じてバンドルを作成するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">［☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<note>
<para>バンドルが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要があるユースケースがある場合があります(カスタム許容値の追加など)。これらの変更を、以下の手順で生成されるバンドルに必ず含めてください。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>suse-edge/fleet-examples</literal>から<emphasis role="strong">［Create
from YAML (YAMLから作成)］</emphasis>ページに <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>または<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>のバンドルコンテンツを手動でコピーする。</para>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのクローンを作成し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。そこから、必要なバンドルに移動します(RKE2の場合は<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>、K3sの場合は<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>)。<emphasis
role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページにバンドルコンテンツが自動的に入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Rancher UIでバンドルを編集します。</para>
<itemizedlist>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ネームスペース</emphasis>を<literal>fleet-local</literal>ネームスペースを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを<literal>ローカル</literal>(管理)クラスタを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>一部のユースケースでは、<literal>ローカル</literal>クラスタに異なる名前を付けることができます。</para>
<para><literal>ローカル</literal>クラスタ名を取得するには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>設定を編集します。</para>
<itemizedlist>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを<literal>ローカル</literal>(管理)クラスタを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>一部のユースケースでは、<literal>ローカル</literal>クラスタに異なる名前を付けることができます。</para>
<para><literal>ローカル</literal>クラスタ名を取得するには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ネームスペース</emphasis>を<literal>fleet-local</literal>ネームスペースを指すように変更します。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>作成した<emphasis
role="strong">バンドル</emphasis>リソースを<literal>fleet-local</literal>ネームスペースの下に表示します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-local

# For K3s
kubectl get bundles k3s-upgrade -n fleet-local

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザが<literal>KubernetesアップグレードSUC Plan</literal>を独自のサードパーティGitOpsワークフロー(例:
<literal>Flux</literal>)に組み込むユースケースがある場合があります。</para>
<para>必要なK8sアップグレードリソースを取得するには、まず使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを決定します。</para>
<para>その後、次の場所でリソースを確認できます。</para>
<itemizedlist>
<listitem>
<para>RKE2クラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>K3sクラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、 <literal>System Upgrade
Controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。SUCデプロイメントの情報については、<xref
linkend="components-system-upgrade-controller-install"/>を参照してください。</para>
</important>
<para>GitOpsワークフローをKubernetesバージョンアップグレードの<emphasis role="strong">SUC
Plan</emphasis>をデプロイするために使用する方法をよりよく理解するために、<literal>Fleet</literal>を使用する更新手順の概要(<xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>)を確認すると役立つ場合があります。</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade">
<title>Helmチャートのアップグレード</title>
<para>このセクションでは、次の内容について説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-air-gap"/> -
Edgeに関連するOCIチャートとイメージをプライベートレジストリに配布する方法に関する情報を保持します。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure"/> -
異なるHelmチャートアップグレードのユースケースとそのアップグレード手順に関する情報を保持します。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-air-gap">
<title>エアギャップ環境の準備</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet">
<title>HelmチャートFleetにアクセスできることの確認</title>
<para>環境でサポートする内容によって、次のオプションのいずれかを選択できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>管理クラスタ</literal>からアクセス可能なローカルGitサーバでチャートのFleetリソースをホストします。</para>
</listitem>
<listitem>
<para>FleetのCLIを使用して、直接使用可能でどこかにホストする必要のない<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">バンドルにHelmチャートを変換</link>します。FleetのCLIは、<link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">リリース</link>ページから取得できます。Macユーザの場合は、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Edgeリリースバージョンに必要なアセットの検索</title>
<orderedlist numeration="arabic">
<listitem>
<para>「Day 2」<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のページに移動し、チャートのアップグレード先のEdgeリリースを見つけ、<emphasis
role="strong">［Assets (アセット)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Assets (アセット)］</emphasis>セクションから、次のファイルをダウンロードします。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">リリースファイル</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">説明</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para><literal>edge-release-images.txt</literal>ファイルで指定されたイメージを取得し、それらを「.tar.gz」アーカイブにパッケージ化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するOCIチャートイメージを取得し、それらを「.tar.gz」アーカイブにパッケージ化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>「.tar.gz」アーカイブからイメージをロードし、再タグ付けして、プライベートレジストリにプッシュします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Edge OCI「.tgz」チャートパッケージを含むディレクトリを取得し、プライベートレジストリにロードします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するOCIチャートイメージのリストを含みます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するイメージのリストを含みます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive">
<title>Edgeリリースイメージアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>イメージアーカイブを生成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>これにより、<literal>edge-images.tar.gz</literal>という名前のすぐにロードできるアーカイブが作成されます。</para>
<note>
<para><literal>-i|--images</literal>オプションが指定される場合、アーカイブの名前は異なる場合があります。</para>
</note>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします。</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive">
<title>Edge OCIチャートイメージアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>OCIチャートイメージアーカイブを生成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>これにより、 <literal>oci-artefacts.tar.gz</literal>という名前のアーカイブが作成されます。</para>
<note>
<para><literal>-a|--archive</literal>オプションが指定される場合、アーカイブの名前は異なる場合があります。</para>
</note>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします。</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine">
<title>Edgeリリースイメージをエアギャップマシンにロード</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>以前に<emphasis role="strong">コピーした</emphasis>
<literal>edge-images.tar.gz</literal>アーカイブを渡して、スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>これにより、<literal>edge-images.tar.gz</literal>からすべてのイメージがロードされ、再タグ付けされて、それらを<literal>--registry</literal>オプションで指定されているレジストリにプッシュします。</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine">
<title>Edge OCIチャートイメージのエアギャップマシンへのロード</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>コピーした<literal>oci-artefacts.tar.gz</literal>アーカイブをuntarします。</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>命名テンプレート<literal>edge-release-oci-tgz-&lt;date&gt;</literal>を含むディレクトリが生成されます。</para>
</listitem>
<listitem>
<para>このディレクトリを<literal>edge-load-oci-artefacts.sh</literal>スクリプトに渡し、Edge
OCIチャートイメージをプライベートレジストリにロードします。</para>
<note>
<para>このスクリプトは、<literal>Helm</literal>
CLIが環境にプリインストールされていることを想定しています。Helmのインストール手順については、「<link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm
(Helmのインストール)</link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution">
<title>Kubernetesディストリビューションでプライベートレジストリを設定する</title>
<para>RKE2の場合は、「<link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration (プライベートレジストリの設定)</link>」を参照してください。</para>
<para>K3sの場合は、「<link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration (プライベートレジストリの設定)</link>」を参照してください。</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure">
<title>アップグレード手順</title>
<para>このセクションでは、Helmアップグレード手順の次のユースケースを中心に説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>手動でデプロイしたHelmチャートを確実にアップグレードすることはできません。<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/>で説明する方法を使用してHelmチャートを再デプロイすることをお勧めします。</para>
</important>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>新しいクラスタがあり、Edge Helmチャートをデプロイして管理したい</title>
<para>このセクションでは、以下の実行方法について説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>チャートのFleetリソースの準備</title>
<orderedlist numeration="arabic">
<listitem>
<para>チャートのFleetリソースを、使用するEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから取得します。</para>
</listitem>
<listitem>
<para>HelmチャートのFleet
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)に移動します。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">GitOpsワークフローを使用する場合は</emphasis>、チャートFleetディレクトリをGitOpsを実行するGitリポジトリにコピーします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis> Helmチャートで<emphasis
role="strong">値</emphasis>を設定する必要がある場合は、コピーしたディレクトリの<literal>fleet.yaml</literal>ファイルに含まれる設定<literal>.helm.values</literal>を編集します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis>
環境に合わせて、チャートのFleetにリソースを追加しなければならないユースケースがあります。Fleetディレクトリを拡張する方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリのコンテンツ)</link>」を参照してください。</para>
</listitem>
</orderedlist>
<note>
<para>場合によっては、Fleet がHelm操作に使用するデフォルトのタイムアウトが不十分で、次のエラーが発生する可能性があります。</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>このような場合は、<literal>fleet.yaml</literal>ファイルの<literal>helm</literal>設定の下に、<link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>プロパティを追加してください。</para>
</note>
<para><literal>longhorn</literal> Helmチャートの<emphasis
role="strong">例</emphasis>は次のようになります。</para>
<itemizedlist>
<listitem>
<para>ユーザGitリポジトリ構造:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>ユーザの<literal>Longhorn</literal>データが入力された<literal>fleet.yaml</literal>の内容:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>これらは値の例であり、<literal>longhorn</literal>チャートのカスタム設定を示すために使用しているだけです。<literal>longhorn</literal>チャートのデプロイメントのガイドラインと<emphasis
role="strong">みなさない</emphasis>でください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>チャートのFleetのデプロイ</title>
<para>チャートのFleetをデプロイするには、GitRepo (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>)またはバンドル(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>)のいずれかを使用できます。</para>
<note>
<para>Fleetをデプロイする場合に、<literal>Modified</literal>メッセージを取得した場合、Fleetの<literal>diff</literal>セクションに対応する<literal>comparePatches</literal>エントリを追加してください。詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore
Modified GitRepos (変更されたGitReposを無視する差分を生成する)</link> 」を参照してください。</para>
</note>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>Fleetの<link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソースは、チャートのFleetリソースへのアクセス方法、それらのリソースを適用するのに必要なクラスタに関する情報を保持しています。</para>
<para><literal>GitRepo</literal>リソースは、<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UI</link>を通じて、または手動で<literal>管理クラスタ</literal>にリソースを<link
xl:href="https://fleet.rancher.io/tut-deployment">デプロイ</link>することで、デプロイできます。</para>
<para><emphasis role="strong">手動</emphasis>デプロイメントの<emphasis
role="strong">Longhorn</emphasis> <literal>GitRepo</literal>リソースの例:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-local
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url</screen>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>バンドル</title>
<para><link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースは、Fleetによってデプロイされる必要がある生のKubernetesリソースを保持しています。通常、<literal>GitRepo</literal>アプローチを使用することを推奨されますが、
環境がエアギャップされ、ローカルGitサーバをサポートできないユースケース用に、<literal>バンドル</literal>がHelmチャートFleetをターゲットクラスタに伝播するのに役立ちます。</para>
<para><literal>バンドル</literal>は、Rancher UI (<literal>［Continuous Delivery
(継続的デリバリ)］ → ［Advanced (詳細) ］→ ［Bundles (バンドル)］ → ［Create from YAML
(YALMから作成)］</literal>)を介して、または<literal>バンドル</literal>リソースを正しいFleetネームスペースに手動でデプロイして、デプロイできます。Fleetネームスペースについては、アップストリーム<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ドキュメント</link>を参照してください。</para>
<para>Edge Helmチャートの<literal>バンドル</literal>は、Fleetの 「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートをバンドルに変換する)</link>」 アプローチを利用して作成できます。</para>
<para>以下に、<literal>バンドル</literal>リソースを<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
HelmチャートFleetテンプレートから作成し、このバンドルを<literal>管理クラスタ</literal>に手動でデプロイする方法の例を示します。</para>
<note>
<para>ワークフローを説明するため、以下の例では<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>ディレクトリ構造を使用しています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>チャートFleetテンプレートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>FleetにHelmチャートのデプロイ先クラスタを指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
<note>
<para>ローカルクラスタの名前が異なる場合があるユースケースがいくつかあります。</para>
<para>ローカルクラスタ名を取得するには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para><link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>を使用して、<literal>Longhorn</literal>
HelmチャートFleetをバンドルリソースに変換します。</para>
<note>
<para>FleetのCLI は、<link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">リリース</link><emphasis
role="strong">アセット</emphasis>ページから取得できます(<literal>fleet-linux-amd64</literal>)。</para>
<para>Macユーザの場合、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>チャートFleetテンプレートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>FleetにHelmチャートのデプロイ先クラスタを指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para><link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>を使用して、<literal>Longhorn
CRD</literal> HelmチャートFleetをバンドルリソースに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>longhorn-bundle.yaml</literal>と<literal>longhorn-crd-bundle.yaml</literal>ファイルを<literal>管理クラスタ</literal>にデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>これらの手順に従うと、<literal>SUSE Storage</literal>が指定された管理クラスタのすべてにデプロイされます。</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>デプロイされたHelmチャートの管理</title>
<para>Fleetでデプロイした後のHelmチャートのアップグレードについては、<xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>を参照してください。</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>Fleetで管理されているHelmチャートをアップグレードしたい場合</title>
<orderedlist numeration="arabic">
<listitem>
<para>目的のEdgeリリースと互換性を持つように、チャートをアップブレードする必要があるバージョンを決定します。EdgeリリースごとのHelmチャートバージョンはリリースノート(<xref
linkend="release-notes"/>)から表示できます。</para>
</listitem>
<listitem>
<para>Fleetで監視されているGitリポジトリで、Helmチャートの<literal>fleet.yaml</literal>ファイルを、リリースノート(<xref
linkend="release-notes"/>)から取得した正しいチャートの<emphasis
role="strong">バージョン</emphasis>と<emphasis
role="strong">リポジトリ</emphasis>で編集します。</para>
</listitem>
<listitem>
<para>変更をコミットしてリポジトリにプッシュした後で、目的のHelmチャートのアップグレードがトリガされます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>EIBを介してデプロイされたHelmチャートをアップグレードしたい</title>
<para><xref linkend="components-eib"/>は、<literal>HelmChart</literal>リソースを作成し、<link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>
Helm統合機能によって導入された<literal>helm-controller</literal>を利用して、Helmチャートをデプロイします。</para>
<para><literal>EIB</literal>を介してデプロイされたHelmチャートが正常にアップグレードされるようにするには、ユーザは各<literal>HelmChart</literal>リソースに対してアップグレードを実行する必要があります。</para>
<para>以下に関する情報が提供されています。</para>
<itemizedlist>
<listitem>
<para>アップグレードプロセスの一般的な概要(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>)。</para>
</listitem>
<listitem>
<para>必要なアップグレード手順(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>)。</para>
</listitem>
<listitem>
<para>説明した方法を使用した<link
xl:href="https://longhorn.io">Longhorn</link>チャートアップグレードを示す例(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>)。</para>
</listitem>
<listitem>
<para>異なるGitOpsツールを使用したアップグレードプロセスを使用する方法(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>)。</para>
</listitem>
</itemizedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>概要</title>
<para><literal>EIB</literal>を介してデプロイされたHelmチャートは、<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>と呼ばれる<literal>フリート</literal>を通じてアップグレードされます。</para>
<para>この<literal>フリート</literal>は、<emphasis
role="strong">ユーザが提供した</emphasis>データを処理し、特定のHelmChartリソースセットを<emphasis
role="strong">更新</emphasis>します。</para>
<para>これらのリソースを更新すると、 <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>がトリガされ、変更された<literal>HelmChart</literal>リソースに関連付けられているHelmチャートが<emphasis
role="strong">アップグレード</emphasis>されます。</para>
<para>ユーザは以下を行うことのみ求められます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>アップグレードが必要な各Helmチャートのアーカイブをローカルで<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
</listitem>
<listitem>
<para>これらのアーカイブを<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal>スクリプトに渡します。これにより、これらのアーカイブのデータが
<literal>eib-charts-upgrader</literal>フリートに含められます。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal>
Fleetを<literal>管理クラスタ</literal>にデプロイします。これは<literal>GitRepo</literal>リソースまたは<literal>バンドル</literal>リソースのいずれか通じて実行されます。</para>
</listitem>
</orderedlist>
<para>デプロイされたら、<literal>eib-charts-upgrader</literal>がFleetのサポートによって、そのリソースを目的の管理クラスタに配布します。</para>
<para>これらのリソースには、次のものが含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis
role="strong">ユーザが提供する</emphasis>Helmチャートデータを保持する<literal>Secret</literal>のセット。</para>
</listitem>
<listitem>
<para>前述の<literal>Secret</literal>をマウントし、それらに基づいて対応するHelmChartリソースに<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">パッチを適用する</link>
<literal>Pod</literal>をデプロイする<literal>Kubernetes Job</literal>。</para>
</listitem>
</orderedlist>
<para>前述のとおり、これにより、<literal>helm-controller</literal>がトリガされ、実際の
Helmチャートアップグレードが実行されます。</para>
<para>上記の説明を以下に図示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-management-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2管理helm eibアップグレード</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>アップグレード手順</title>
<orderedlist numeration="arabic">
<listitem>
<para>正しいリリース <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">タグ</link>から<literal>suse-edge/fleet-examples</literal>リポジトリのクローンを作成します。</para>
</listitem>
<listitem>
<para>取得したHelmチャートアーカイブを保存するディレクトリを作成します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>新規に作成されたアーカイブディレクトリ内で、アップグレードするHelmチャートのアーカイブを<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">リリースタグ</link>の<emphasis
role="strong">アセット</emphasis>から、<literal>generate-chart-upgrade-data.sh</literal>スクリプトをダウンロードします。</para>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para><literal>--archive-dir</literal>ディレクトリ内のチャートアーカイブごとに、スクリプトにより、チャートアップグレードデータを含む<literal>Kubernetes
Secret
YAML</literal>ファイルが生成され、<literal>--fleet-path</literal>によって指定されたフリートの<literal>base/secrets</literal>ディレクトリに保存されます。</para>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトは、フリートに追加の変更も適用し、生成された<literal>Kubernetes
Secret YAML</literal>ファイルが、フリートによってデプロイされたワークロードによって正しく利用されるようにします。</para>
<important>
<para>ユーザは、<literal>generate-chart-upgrade-data.sh</literal>スクリプトが生成する内容に変更を加えてはなりません。</para>
</important>
</listitem>
</orderedlist>
<para>以下の手順は、実行している環境によって異なります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>GitOpsをサポートする環境の場合(例: エアギャップされていない、エアギャップされたがローカルGitサーバサポートが可能):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>fleets/day2/eib-charts-upgrader</literal>
Fleetを、GitOpsに使用するリポジトリにコピーします。</para>
<note>
<para>Fleetに<literal>generate-chart-upgrade-data.sh</literal>スクリプトによって加えられた変更が含まれていることを確認してください。</para>
</note>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal>
Fleetのすべてのリソースを配布するために使用される<literal>GitRepo</literal>リソースを設定します。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Rancher UIを通じた<literal>GitRepo</literal>の設定およびデプロイメントについては、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI (Rancher UIでのFleetへのアクセス)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>GitRepo</literal>手動設定およびデプロイメントの場合、「<link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a Deployment
(デプロイメントの作成)</link>」を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>GitOpsをサポートしていない環境の場合 (例: エアギャップされ、ローカルGitサーバの使用が許可されていない):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>rancher/fleet</literal> <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">リリース</link>ページから<literal>fleet-cli</literal>バイナリをダウンロードします(Linuxの場合は<literal>fleet-linux-amd64</literal>)。Macユーザの場合、使用可能なHomebrew
Formulaeがあります(<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>)。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal> Fleetに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>リソースをデプロイする場所をFleetに指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To map the local(management) cluster
- clusterName: local
EOF</screen>
<note>
<para>一部のユースケースでは、<literal>ローカル</literal>クラスタに異なる名前を付けることができます。</para>
<para><literal>ローカル</literal>クラスタ名を取得するには、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para><literal>fleet-cli</literal>を使用して、Fleetを <literal>バンドル</literal>リソースに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>これにより、<literal>eib-charts-upgrader</literal>
Fleetからのすべてのテンプレート化されたリソースを保持するバンドル(<literal>bundle.yaml</literal>)が作成されます。</para>
<para><literal>fleet apply</literal>コマンドに関する詳細については、「<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>」を参照してください。</para>
<para>Fleetをバンドルに変換する方法に関する詳細については、「 <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートのバンドルへの変換)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>バンドル</literal>をデプロイします。これは次の2つの方法のいずれかで実行できます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>RancherのUIを通じて - <emphasis role="strong">［Continuous Delivery
(継続的デリバリ)］→［Advanced (詳細)］→［Bundles (バンドル)］ → ［Create from YAML
(YAMLから作成)］</emphasis>に移動して、<literal>bundle.yaml</literal>
コンテンツを解析するか、［<literal>Read from File
(ファイルから読み取り)</literal>］オプションをクリックして、ファイル自体を渡します。</para>
</listitem>
<listitem>
<para>手動 -
<literal>管理クラスタ</literal>内に<literal>bundle.yaml</literal>ファイルを手動でデプロイします。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>これらの手順を実行すると、正常に<literal>GitRepo/バンドル</literal>リソースがデプロイされます。リソースはFleetによって取得され、そのコンテンツはユーザが以前の手順で指定したターゲットクラスタにデプロイされます。プロセスの概要については、<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>を参照してください。</para>
<para>アップグレードプロセスの追跡方法については、<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>を参照してください。</para>
<important>
<para>チャートアップグレードが正常に確認されたら、<literal>バンドル/GitRepo</literal>リソースを削除します。</para>
<para>これにより、<literal>管理</literal>クラスタから不要になったアップグレードリソースが削除され、今後バージョンクラッシュが発生しないようになります。</para>
</important>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>例</title>
<note>
<para>以下の例は、<literal>管理</literal>クラスタ上で、<literal>EIB</literal>を介してデプロイされたHelmチャートを、あるバージョンから別のバージョンにアップグレードする方法を示しています。この例で使用されているバージョンは、推奨バージョン<emphasis
role="strong">ではない</emphasis>ことに注意してください。Edgeリリースに固有のバージョン推奨事項については、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
</note>
<para><emphasis>ユースケース:</emphasis></para>
<itemizedlist>
<listitem>
<para><literal>管理</literal>クラスタで、古いバージョンの<link
xl:href="https://longhorn.io">Longhorn</link>が実行されています。</para>
</listitem>
<listitem>
<para>クラスタは、次のイメージ定義<emphasis>スニペット</emphasis>を使用して、EIBを通じてデプロイされました。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal>は、Edge
3.4リリースと互換性のあるバージョンにアップグレードする必要があります。つまり、<literal>107.0.0+up1.9.1</literal>にアップグレードする必要があります。</para>
</listitem>
<listitem>
<para><literal>管理クラスタ</literal>がローカルGitサーバのサポートなしで<emphasis
role="strong">エアギャップ</emphasis>されており、Rancherセットアップが動作していることを前提としています。</para>
</listitem>
</itemizedlist>
<para>アップグレード手順(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>)に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>release-3.4.0</literal>タグから<literal>suse-edge/fleet-example</literal>リポジトリのクローンを作成します。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para><literal>Longhorn</literal>アップグレードアーカイブが保存されるディレクトリを作成します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>目的の<literal>Longhorn</literal>チャートアーカイブバージョンを取得します。</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para><literal>archives</literal>ディレクトリ以外で、<literal>suse-edge/fleet-examples</literal>リリース<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">タグ</link>から<literal>generate-chart-upgrade-data.sh</literal>スクリプトをダウンロードします。</para>
</listitem>
<listitem>
<para>ディレクトリセットアップは次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>スクリプト実行後のディレクトリ構造は次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Gitで変更されたファイルは次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal> Fleetの<literal>バンドル</literal>を作成します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>まず、Fleet自体に移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>次に、<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>次に、<literal>fleet-cli</literal>バイナリを使用して、Fleetをバンドルに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Rancher UIを通じてバンドルをデプロイします。</para>
<figure>
<title>Rancher UIを通じたバンドルのデプロイ</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例1</phrase></textobject>
</mediaobject></figure>
<para>ここから、<emphasis role="strong">［Read from File
(ファイルから読み取り)］</emphasis>を選択し、システムで<literal>bundle.yaml</literal>ファイルを見つけます。</para>
<para>これにより、Rancher UI内に<literal>バンドル</literal>が自動入力されます。</para>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>正常にデプロイされたら、バンドルは次のようになります。</para>
<figure>
<title>正常にデプロイされたバンドル</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para><literal>バンドル</literal>のデプロイメントが成功した後で、アップグレードプロセスを監視するには次のようにします。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>アップグレードPod</literal>のログを確認します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_management.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例3管理</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>ここで、helm-controllerによってアップグレードに作成されたPodのログを確認します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Pod名は次のテンプレートを使用します -
<literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>Podは、<literal>HelmChart</literal>リソースがデプロイされたネームスペースにあります。この場合、このネームスペースは<literal>kube-system</literal>です。</para>
<figure>
<title>正常にアップグレードされたLonghornチャートのログ</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_management.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例4管理</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Rancherの<literal>HelmCharts</literal>セクション(<literal>［More Resources
(その他のリソース)］ →
［HelmCharts］</literal>)に移動して、<literal>HelmChart</literal>のバージョンが更新されていることを確認します。チャートがデプロイされたネームスペースを選択します。この例では、<literal>kube-system</literal>です。</para>
</listitem>
<listitem>
<para>最後に、 Longhorn Podが実行されていることを確認します。</para>
</listitem>
</orderedlist>
<para>上記の検証後、Longhorn
Helmチャートが<literal>107.0.0+up1.9.1</literal>バージョンにアップグレードされていると仮定しても問題ないでしょう。</para>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>サードパーティのGitOpsツールを使用したHelmチャートのアップグレード</title>
<para>ユーザがこのアップグレード手順をFleet以外のGitOpsワークフロー(<literal>Flux</literal>など)で実行したいユースケースが存在する場合があります。</para>
<para>アップグレード手順に必要なリソースを生成するには、
<literal>generate-chart-upgrade-data.sh</literal>スクリプトを使用して、ユーザが提供するデータを<literal>eib-charts-upgrader</literal>
Fleetに入力することができます。この実行方法の詳細については、<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>を参照してください。</para>
<para>完全なセットアップ後、<link
xl:href="https://kustomize.io">kustomize</link>を使用して、クラスタにデプロイ可能な完全な動作ソリューションを生成することができます。</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>GitOpsワークフローにソリューションを含める場合は、<literal>fleet.yaml</literal>ファイルを削除して、残ったものを有効な<literal>Kustomize</literal>セットアップとして使用できます。まず、<literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行し、アップグレードするHelmチャートのデータを<literal>Kustomize</literal>セットアップに読み込ませることを忘れないでください。</para>
<para>このワークフローの使用方法を理解するには、<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>と<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>を参照すると役立つでしょう。</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="day2-downstream-clusters">
<title>ダウンストリームクラスタ</title>
<important>
<para>以下の手順は、SUSE Telco Cloud (<xref
linkend="atip"/>)によって管理される<literal>ダウンストリーム</literal>クラスタには適用されません。ダウンストリームクラスタのアップグレードに関するガイダンスについては、<xref
linkend="atip-lifecycle-downstream"/>を参照してください。</para>
</important>
<para>このセクションでは、 <literal>ダウンストリーム</literal>クラスタのさまざまな部分に対して「Day
2」操作を実行可能な方法について説明します。</para>
<section xml:id="downstream-day2-fleet">
<title>Fleet</title>
<para>このセクションでは、Fleet (<xref linkend="components-fleet"/>)コンポーネントを使用して「Day
2」操作を実行する方法について説明します。</para>
<para>次のトピックは、このセクションの一部として説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-components"/> - すべての「Day
2」操作に使用されるデフォルトコンポーネント。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-determine-use-case"/> -
使用されるFleetカスタムリソースの概要と、これらのリソースがさまざまな「Day 2」操作のユースケースに適しているかどうかを説明します。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-upgrade-workflow"/> - Fleetを使用して「Day
2」操作を実行するためのワークフローガイドを提供します。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade"/> -
Fleetを使用してOSアップグレードを実行する方法を説明します。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade"/> -
Fleetを使用してKubernetesバージョンアップグレードを実行する方法を説明します。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade"/> -
Fleetを使用してHelmチャートアップグレードを実行する方法を説明します。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-components">
<title>コンポーネント</title>
<para>以下に、Fleetを使用して「Day
2」操作を正常に実行できるように、<literal>ダウンストリーム</literal>クラスタで設定する必要があるデフォルトコンポーネントについて説明します。</para>
<section xml:id="id-system-upgrade-controller-suc-2">
<title>System Upgrade Controller (SUC)</title>
<note>
<para>各ダウンストリームクラスタ上にデプロイする<emphasis role="strong">必要があります</emphasis>。</para>
</note>
<para><emphasis role="strong">System Upgrade Controller</emphasis>は、
<literal>Plan</literal>と呼ばれるカスタムリソースを通じて提供される設定データに基づいて指定されたノードでタスクを実行する責任を負います。</para>
<para><emphasis
role="strong">SUC</emphasis>は、オペレーティングシステムとKubernetesディストリビューションのアップグレードに積極的に活用されます。</para>
<para><emphasis
role="strong">SUC</emphasis>コンポーネントとそれがEdgeスタックにどのように適合するかに関する詳細については、<xref
linkend="components-system-upgrade-controller"/>を参照してください。</para>
<para><emphasis role="strong">SUC</emphasis>をデプロイする方法については、まずユースケースを決定し(<xref
linkend="downstream-day2-fleet-determine-use-case"/>)、次に「System Upgrade
Controllerのインストール - GitRepo」(<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>)、または「System
Upgrade Controller のインストール - バンドル」(<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>)を参照してください。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-determine-use-case">
<title>ユースケースの決定</title>
<para>Fleetは2種類の<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>を使用してKubernetesおよびHelmリソースの管理を有効にします。</para>
<para>以下に、これらのリソースの目的と、「Day 2」操作のコンテキストに最適なユースケースに関する情報を示します。</para>
<section xml:id="id-gitrepo-2">
<title>GitRepo</title>
<para><literal>GitRepo</literal>は、<literal>Fleet</literal>が<literal>バンドル</literal>の作成元として使用できるGitリポジトリを表すFleet
(<xref
linkend="components-fleet"/>)リソースです。各<literal>バンドル</literal>は、<literal>GitRepo</literal>リソースの内部で定義された設定パスに基づいて作成されます。詳細については、<link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>のドキュメントを参照してください。</para>
<para>「Day 2」操作のコンテキストでは、<literal>GitRepo</literal>リソースは通常、<emphasis>Fleet
GitOps</emphasis>アプローチを利用する<emphasis
role="strong">非エアギャップ</emphasis>環境に<literal>SUC</literal>または<literal>SUC
Plan</literal>をデプロイするために使用されます。</para>
<para>または、<emphasis role="strong">リポジトリのセットアップをローカルGitサーバ経由でミラーリングする場合</emphasis>
、<literal>GitRepo</literal>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
<section xml:id="id-bundle-2">
<title>バンドル</title>
<para><literal>バンドル</literal>は、ターゲットクラスタにデプロイする<emphasis role="strong">
生</emphasis>のKubernetesリソースを保持します。バンドルは通常、<literal>GitRepo</literal>リソースから作成されますが、手動でデプロイできるユースケースもあります。詳細については、<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>のドキュメントを参照してください。</para>
<para>「Day
2」操作のコンテキストでは、<literal>バンドル</literal>リソースは通常、何らかの形態の<emphasis>ローカルGitOps</emphasis>手法を使用しない<emphasis
role="strong">エアギャップ</emphasis>環境(<emphasis
role="strong">ローカルGitサーバ</emphasis>など
)で<literal>SUC</literal>または<literal>SUC Plan</literal>をデプロイするために使用されます。</para>
<para>または、ご自身のユースケースで<emphasis>GitOps</emphasis>ワークフローを使用できない場合は(Gitリポジトリを使用する場合など)、<literal>バンドル</literal>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">非エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
</section>
<section xml:id="downstream-day2-upgrade-workflow">
<title>Day 2ワークフロー</title>
<para>以下に、ダウンストリームクラスタを特定のEdgeリリースにアップグレードする際に従う必要のある「Day 2」ワークフローを示します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>OSアップグレード(<xref linkend="downstream-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Kubernetesバージョンアップグレード(<xref linkend="downstream-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helmチャートアップグレード(<xref linkend="downstream-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade">
<title>OSアップグレード</title>
<para>このセクションでは、<xref linkend="components-fleet"/>と<xref
linkend="components-system-upgrade-controller"/>を使用してオペレーティングシステムのアップグレードを行う方法について説明します。</para>
<para>次のトピックは、このセクションの一部として説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-components"/> -
アップグレードプロセスによって使用される追加のコンポーネント。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-overview"/> -
アップグレードプロセスの概要。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-requirements"/> -
アップグレードプロセスの要件。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment"/> -
アップグレードプロセスのトリガを担当する、<literal>SUC Plan</literal>をデプロイする方法に関する情報。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-os-upgrade-components">
<title>コンポーネント</title>
<para>このセクションでは、<literal>OSアップグレード</literal>プロセスがデフォルトの「Day 2」コンポーネント(<xref
linkend="downstream-day2-fleet-components"/>)よりも使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>特定のノードでのOSアップグレードは、<link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>によって処理されます。</para>
<para>あるEdgeバージョンから別のEdgeバージョンにOSが必要とするアップグレードのタイプに応じて、異なるサービスが作成されます。</para>
<itemizedlist>
<listitem>
<para>同じOSバージョン(例:
<literal>6.0</literal>)を必要とするEdgeバージョンの場合、<literal>os-pkg-update.service</literal>が作成されます。このサービスは<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>を使用して、<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常のパッケージアップグレード</link>を実行します。</para>
</listitem>
<listitem>
<para>OSバージョンのマイグレーション(例: <literal>6.0</literal> →
<literal>6.1</literal>)が必要なEdgeバージョンの場合、<literal>os-migration.service</literal>が作成されます。このサービスは、<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>を使用して以下の処理を実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常のパッケージのアップグレード</link>。このアップグレードにより、すべてのパッケージを最新にすることで、古いパッケージバージョンに関連するマイグレーション時の障害を軽減します。</para>
</listitem>
<listitem>
<para><literal>zypper migration</literal>コマンドを利用したOSマイグレーション。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>上記サービスは、OSアップグレードが必要なダウンストリームクラスタ上に配置する必要がある<literal>SUC
Plan</literal>を通じて各ノードに配布されます。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-overview">
<title>概要</title>
<para>ダウンストリームクラスタノードのオペレーティングシステムのアップグレードは、<literal>Fleet</literal>と<literal>System
Upgrade Controller (SUC)</literal>を利用して実行されます。</para>
<para><emphasis role="strong">Fleet</emphasis>は、<literal>SUC
Plan</literal>を目的のクラスタにデプロイおよび管理するために使用されます。</para>
<note>
<para><literal>SUC
Plan</literal>は、特定のタスクをノードセットで実行するために<literal>SUC</literal>が従う必要がある手順を記述した<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>です。<literal>SUC
Plan</literal>の例については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリームリポジトリ</link>を参照してください。</para>
</note>
<para><literal>OS SUC Plan</literal>は、<link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>または<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースを特定のFleet<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ワークスペース</link>にデプロイすることで、各クラスタに配布されます。Fleetはデプロイされた<literal>GitRepo/Bundle</literal>を取得し、その内容
(<literal>OS SUC plan</literal>)を目的のクラスタにデプロイします。</para>
<note>
<para><literal>GitRepo/バンドル</literal>リソースは常に、<literal>管理クラスタ</literal>上にデプロイされます。<literal>GitRepo</literal>リソースを使用するか<literal>バンドル</literal>リソースを使用するかは、ユースケースによって異なります。詳細については、<xref
linkend="downstream-day2-fleet-determine-use-case"/>をご確認ください。</para>
</note>
<para><literal>OS SUC Plan</literal>は、次のワークフローを記述します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>常に、OSをアップグレードする前に、ノードを<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>します。</para>
</listitem>
<listitem>
<para>常に、
<literal>ワーカー</literal>ノードの前に<literal>コントロールプレーン</literal>ノードをアップグレードします。</para>
</listitem>
<listitem>
<para>常に、一度に <emphasis role="strong">1</emphasis>ノードずつクラスタをアップグレードします。</para>
</listitem>
</orderedlist>
<para><literal>OS SUC Plan</literal>がデプロイされると、ワークフローは次のようになります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUCは、デプロイされた<literal>OS SUC Plan</literal>を照合し、<literal>Kubernetes
Job</literal>を<emphasis role="strong">各ノード</emphasis>に作成します。</para>
</listitem>
<listitem>
<para><literal>Kubernetes
Job</literal>は、パッケージのアップグレードまたはOSマイグレーションのためにsystemd.service (<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>)を作成します。</para>
</listitem>
<listitem>
<para>作成された <literal>systemd.service</literal>は、特定のノードでOSアップグレードプロセスをトリガします。</para>
<important>
<para>OSアップグレードプロセスが終了すると、対応するノードが <literal>再起動</literal>され、システムに更新が適用されます。</para>
</important>
</listitem>
</orderedlist>
<para>上記の説明を以下に図示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>fleet day2ダウンストリームosアップグレード</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-requirements">
<title>要件</title>
<para><emphasis>全般:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC登録マシン</emphasis> -
すべてのダウンストリームクラスタノードを<literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>に登録する必要があります。これは、各<literal>systemd.service</literal>が目的のRPMリポジトリに正常に接続できるようにするために必要です。</para>
<important>
<para>OSバージョンのマイグレーション (例: <literal>6.0</literal> →
<literal>6.1</literal>)が必要なEdgeリリースの場合、SCCキーが新しいバージョンへのマイグレーションをサポートしていることを確認してください。</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">コントロールプレーン</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。OSアップグレードに関連する<emphasis
role="strong">SUC
Plan</emphasis>は、<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>の下の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。<emphasis
role="strong">有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para><emphasis role="strong">control-plane</emphasis> SUC
Planに対してカスタムのTolerationを定義する例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>エアギャップ:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SUSE RPMリポジトリのミラーリング</emphasis> - OS
RPMリポジトリをローカルにミラーリングし、<literal>systemd.service</literal>がそのリポジトリにアクセスできるようにする必要があります。このためには、<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>または<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>のいずれかを使用します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment">
<title>OSアップグレード - SUC Planのデプロイメント</title>
<important>
<para>この手順を使用して以前にアップグレードした環境の場合、ユーザは次の手順の<emphasis
role="strong">いずれか</emphasis>を完了していることを確認する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>ダウンストリームクラスタから古いEdgeリリースバージョンに関連する以前にデプロイしたSUC Planを削除する</literal>
- これは、既存の<literal>GitRepo/バンドル</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">ターゲット設定</link>から目的のクラスタを削除するか、<literal>GitRepo/バンドル</literal>リソースを完全に削除することで、実行できます。</para>
</listitem>
<listitem>
<para><literal>既存のGitRepo/バンドルリソースを再利用する</literal> -
目的の<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>の正しいフリートを保持する新しいタグにリソースのリビジョンをポイントすることで実行できます。</para>
</listitem>
</itemizedlist>
<para>これは古いEdgeリリースバージョンの<literal>SUC Plan</literal>間のクラッシュを回避するために実行されます。</para>
<para>ユーザがアップグレードを試す場合、ダウンストリームクラスタに既存の<literal>SUC
Plan</literal>がある場合は、次のフリートエラーが表示されます。</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para><xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>で説明したように、OSアップグレードは、次の方法のいずれかの方法を使用して、<literal>SUC
Plan</literal>を目的のクラスタに配布することで実行されます。</para>
<itemizedlist>
<listitem>
<para>Fleet <literal>GitRepo</literal>リソース - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>。</para>
</listitem>
<listitem>
<para>Fleet <literal>バンドル</literal>リソース - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle"/>。</para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref
linkend="downstream-day2-fleet-determine-use-case"/>を参照してください。</para>
<para><literal>OS SUC Plan </literal>をサードパーティのGitOpsツールからデプロイするユースケースの場合は、<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-third-party"/>を参照してください。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>OS SUC Plan</literal>を配布する、<emphasis
role="strong">GitRepo</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>を通じて(<literal>Rancher</literal>が使用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<para>Rancher UIを通じて<literal>GitRepo</literal>リソースを作成するには、公式の<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">ドキュメント</link>に従ってください。</para>
<para>Edgeチームは、すぐに使用できる<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">フリート</link>を維持しています。環境によっては、このフリートを直接使用することも、テンプレートとして使用することもできます。</para>
<important>
<para>常に、このフリートは有効なEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>フリートが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要がないユースケースでは、ユーザは<literal>suse-edge/fleet-examples</literal>リポジトリから
<literal>os-upgrade</literal>フリートを直接参照できます。</para>
<para>カスタム変更(カスタム許容値の追加など)が必要な場合、ユーザは別のリポジトリから<literal>os-upgrade</literal>フリートを参照して、必要に応じてSUC
Planに変更を追加できるようにする必要があります。</para>
<para><literal>GitRepo</literal>を<literal>suse-edge/fleet-examples</literal>リポジトリのフリートを使用するように設定する方法の例については、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">こちら</link>を参照してください。</para>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>設定を編集し、<literal>spec.targets</literal>で目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはどのダウンストリームクラスタにもマップ<emphasis
role="strong">されません</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタ変更に一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>OS SUC Plan</literal>を配布する<emphasis
role="strong">バンドル</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>を通じて(<literal>Rancher</literal>が使用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<para>Edgeチームは、以下の手順で使用可能な、すぐに使用できる<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">バンドル</link>を維持しています。</para>
<important>
<para>このバンドルは常に有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>RancherのUIを通じてバンドルを作成するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">［☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<note>
<para>バンドルが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要があるユースケースがある場合があります(カスタム許容値の追加など)。これらの変更を、以下の手順で生成されるバンドルに必ず含めてください。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">バンドルコンテンツ</link>を<literal>suse-edge/fleet-examples</literal>から<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページに手動でコピーする。</para>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>のクローンを作成し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。ここからバンドルの場所(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)に移動して、バンドルファイルを選択できます。これにより、バンドルコンテンツを持つ<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページが自動入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">されません</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-default</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザが<literal>OS SUC Plan</literal>を独自のサードパーティGitOpsワークフロー(例:
<literal>Flux</literal>)に組み込むユースケースがある場合があります。</para>
<para>必要なOSアップグレードリソースを取得するには、まず使用する<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリのEdge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを決定します。</para>
<para>その後、リソースは<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>で見つかります。ここでは次のようになります。</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal>は、<emphasis
role="strong">コントロールプレーン</emphasis>ノードのSUC Planリソースです。</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal>は、 <emphasis
role="strong">ワーカー</emphasis>ノードのSUC Planリソースです。</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal>は、systemd.service (<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>)の作成を担当する、<literal>upgrade.sh</literal>スクリプトを含むSecretです。</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal>は、<literal>upgrade.sh</literal>スクリプトによって使用される設定を保持するConfigMapです。</para>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、 <literal>System Upgrade
Controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。SUCデプロイメントの情報については、<xref
linkend="components-system-upgrade-controller-install"/>を参照してください。</para>
</important>
<para>GitOpsワークフローをOSアップグレードの<emphasis role="strong">SUC
Plan</emphasis>をデプロイするために使用する方法をよりよく理解するために、概要(<xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>)を確認すると役立つ場合があります。</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade">
<title>Kubernetesバージョンアップグレード</title>
<important>
<para>このセクションでは、Rancher (<xref
linkend="components-rancher"/>)インスタンスを使用して作成されて<emphasis
role="strong">いない</emphasis>ダウンストリームクラスタのKubernetesアップグレードについて説明します。<literal>Rancher</literal>で作成したクラスタのKubernetesバージョンをアップグレードする方法については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes (Kubernetesのアップグレードとロールバック)</link>」を参照してください。</para>
</important>
<para>このセクションでは、<xref linkend="components-fleet"/>と<xref
linkend="components-system-upgrade-controller"/>を使用してKubernetesアップグレードを実行する方法について説明します。</para>
<para>次のトピックは、このセクションの一部として説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-components"/> -
アップグレードプロセスで使用される追加のコンポーネント。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/> -
アップグレードプロセスの概要。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-requirements"/> -
アップグレードプロセスの要件。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment"/> -
アップグレードプロセスのトリガを担当する、 <literal>SUC Plan</literal>をデプロイする方法に関する情報。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components">
<title>コンポーネント</title>
<para>このセクションでは、 <literal>K8sアップグレード</literal>プロセスがデフォルトの「Day 2」コンポーネント(<xref
linkend="downstream-day2-fleet-components"/>)よりも使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>特定のノードのRKE2バージョンのアップグレードを担当するコンテナイメージ。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、RKE2のアップグレードが必要な各<emphasis
role="strong">クラスタ</emphasis>に配置する必要があります。</para>
<para><literal>rke2-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>特定のノードのK3sバージョンのアップグレードを担当するコンテナイメージ。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、K3sのアップグレードが必要な各<emphasis
role="strong">クラスタ</emphasis>に配置する必要があります。</para>
<para><literal>k3s-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/k3s-io/k3s-upgrade">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-overview">
<title>概要</title>
<para>ダウンストリームクラスタのKubernetesディストリビューションアップグレードは、<literal>Fleet</literal>と<literal>System
Upgrade Controller (SUC)</literal>を利用して実行されます。</para>
<para><literal>Fleet</literal>は、<literal>SUC
Plan</literal>を目的のクラスタにデプロイし、管理するために使用されます。</para>
<note>
<para><literal>SUC Plan</literal>は、 <emphasis
role="strong">SUC</emphasis>が特定のタスクをノードのセットで実行するために従う必要がある手順を記述した<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">カスタムリソース</link>です。<literal>SUC
Plan</literal>の例については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリームリポジトリ</link>を参照してください。</para>
</note>
<para><literal>K8s SUC Plan</literal>は、 <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>または<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースを特定のFleet<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ワークスペース</link>にデプロイすることで各クラスタに配布されます。Fleetはデプロイされた<literal>GitRepo/バンドル</literal>を取得し、その内容(<literal>K8s
SUC Plan</literal>)を目的のクラスタにデプロイします。</para>
<note>
<para><literal>GitRepo/バンドル</literal>リソースは常に、<literal>管理クラスタ</literal>上にデプロイされます。<literal>GitRepo</literal>リソースを使用するか<literal>バンドル</literal>リソースを使用するかは、ユースケースによって異なります。詳細については、<xref
linkend="downstream-day2-fleet-determine-use-case"/>をご確認ください。</para>
</note>
<para><literal>K8s SUC Plan</literal>は、次のワークフローを記述します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>常に、K8sをアップグレードする前に、ノードを<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>します。</para>
</listitem>
<listitem>
<para>常に、
<literal>ワーカー</literal>ノードの前に<literal>コントロールプレーン</literal>ノードをアップグレードします。</para>
</listitem>
<listitem>
<para>常に、一度に <emphasis
role="strong">1</emphasis>ノードずつ<literal>コントロールプレーン</literal>ノードをアップグレードし、一度に
<emphasis
role="strong">2</emphasis>ノードずつ<literal>ワーカー</literal>ノードをアップグレードします。</para>
</listitem>
</orderedlist>
<para><literal>K8s SUC Plan</literal>がデプロイされたら、ワークフローは次のようになります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUCは、デプロイ済みの<literal>K8s SUC Plan</literal>を照合し、 <literal>Kubernetes
Job</literal>を<emphasis role="strong">各ノード</emphasis>に作成します。</para>
</listitem>
<listitem>
<para>Kubernetesディストリビューションによって、Jobはrke2-upgrade (<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>)またはk3s-upgrade
(<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>)のコンテナイメージを実行するPodを作成します。</para>
</listitem>
<listitem>
<para>作成されたPodは次のワークフローを実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>ノード上の既存の<literal>rke2/k3s</literal>バイナリを<literal>rke2-upgrade/k3s-upgrade</literal>イメージのバイナリで置き換えます。</para>
</listitem>
<listitem>
<para>実行中の<literal>rke2/k3s</literal>プロセスを強制終了します。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>rke2/k3s</literal>プロセスを強制終了すると、再起動がトリガされ、更新されたバイナリを実行する新しいプロセスが起動し、Kubernetesディストリビューションバージョンがアップグレードされます。</para>
</listitem>
</orderedlist>
<para>上記の説明を以下に図示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>fleet day2ダウンストリームk8sアップグレード</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-requirements">
<title>要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Kubernetesディストリビューションをバックアップします。</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">RKE2クラスタ</emphasis>については、<link
xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2のバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">K3sクラスタ</emphasis>については、<link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3sのバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">control-plane</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。Kubernetesバージョンアップグレードに関する<emphasis
role="strong">SUC Plan</emphasis>は、次の場所の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">必ず、有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para>RKE2 <emphasis role="strong">control-plane</emphasis> SUC
PlanのカスタムTolerationの定義例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment">
<title>K8sアップグレード - SUC Planのデプロイメント</title>
<important>
<para>この手順を使用して以前にアップグレードした環境の場合、ユーザは次の手順の<emphasis
role="strong">いずれか</emphasis>を完了していることを確認する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>ダウンストリームクラスタから古いEdgeリリースバージョンに関連する以前にデプロイしたSUC Planを削除する</literal>
- これは、既存の<literal>GitRepo/バンドル</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">ターゲット設定</link>から目的のクラスタを削除するか、<literal>GitRepo/バンドル</literal>リソースを完全に削除することで、実行できます。</para>
</listitem>
<listitem>
<para><literal>既存のGitRepo/バンドルリソースを再利用する</literal> -
目的の<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>の正しいフリートを保持する新しいタグにリソースのリビジョンをポイントすることで実行できます。</para>
</listitem>
</itemizedlist>
<para>これは古いEdgeリリースバージョンの<literal>SUC Plan</literal>間のクラッシュを回避するために実行されます。</para>
<para>ユーザがアップグレードを試す場合、ダウンストリームクラスタに既存の<literal>SUC
Plan</literal>がある場合は、次のフリートエラーが表示されます。</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para><xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>で説明したように、Kubernetesアップグレードは、次のいずれかの方法を使用して<literal>SUC
Plan</literal>を目的のクラスタに配布することで実行されます。</para>
<itemizedlist>
<listitem>
<para>Fleet GitRepoリソース(<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Fleetバンドルリソース(<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref
linkend="downstream-day2-fleet-determine-use-case"/>を参照してください。</para>
<para><literal>K8s SUC Plan</literal>をサードパーティのGitOpsツールからデプロイするユースケースの場合は、<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>を参照してください。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>K8s SUC Plan</literal>を配布する、<emphasis
role="strong">GitRepo</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>を通じて(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<para>Rancher UIを通じて<literal>GitRepo</literal>リソースを作成するには、公式の<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">ドキュメント</link>に従ってください。</para>
<para>Edgeチームは、<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>
Kubernetesディストリビューションの両方のすぐに使用できるフリートを維持しています。環境によっては、このフリートを直接使用することも、テンプレートとして使用することもできます。</para>
<important>
<para>常に、これらのフリートは有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>これらのフリートが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要がないユースケースの場合、ユーザは<literal>suse-edge/fleet-examples</literal>リポジトリからフリートを直接参照できます。</para>
<para>カスタム変更(カスタム許容値の追加など)が必要な場合、ユーザは別のリポジトリからフリートを参照して、必要に応じてSUC
Planに変更を追加できるようにする必要があります。</para>
<para><literal>suse-edge/fleet-examples</literal>リポジトリからフリートを使用した<literal>GitRepo</literal>リソースの設定例は、次のとおりです。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>設定を編集し、<literal>spec.targets</literal>で目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはどのダウンストリームクラスタにもマップ<emphasis
role="strong">されません</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタ変更に一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>KubernetesアップグレードSUC Plan</literal>を配布する、<emphasis
role="strong">バンドル</emphasis>リソースは、次の方法のいずれかでデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal> - <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>を通じて(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesアップグレードプロセスを監視するには、<xref
linkend="components-system-upgrade-controller-monitor-plans"/>を参照してください。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<para>Edgeチームは、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>
Kubernetesディストリビューションの両方のすぐに使用できるバンドルを維持しています。環境によっては、これらのバンドルを直接使用することも、テンプレートとして使用することもできます。</para>
<important>
<para>このバンドルは常に有効なEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから使用してください。</para>
</important>
<para>RancherのUIを通じてバンドルを作成するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">［☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<note>
<para>バンドルが配布する<literal>SUC
Plan</literal>にカスタム変更を含める必要があるユースケースがある場合があります(カスタム許容値の追加など)。これらの変更を、以下の手順で生成されるバンドルに必ず含めてください。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>suse-edge/fleet-examples</literal>から<emphasis role="strong">［Create
from YAML (YAMLから作成)］</emphasis>ページに <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>または<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>のバンドルコンテンツを手動でコピーする。</para>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのクローンを作成し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。そこから、必要なバンドルに移動します(RKE2の場合は<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>、K3sの場合は<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>)。<emphasis
role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページにバンドルコンテンツが自動的に入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">されません</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザが<literal>KubernetesアップグレードSUC Plan</literal>を独自のサードパーティGitOpsワークフロー(例:
<literal>Flux</literal>)に組み込むユースケースがある場合があります。</para>
<para>必要なK8sアップグレードリソースを取得するには、まず使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを決定します。</para>
<para>その後、次の場所でリソースを確認できます。</para>
<itemizedlist>
<listitem>
<para>RKE2クラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>K3sクラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>ワーカー</literal>ノードの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、 <literal>System Upgrade
Controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。SUCデプロイメントの情報については、<xref
linkend="components-system-upgrade-controller-install"/>を参照してください。</para>
</important>
<para>GitOpsワークフローをKubernetesバージョンアップグレードの<emphasis role="strong">SUC
Plan</emphasis>をデプロイするために使用する方法をよりよく理解するために、<literal>Fleet</literal>を使用する更新手順の概要(<xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>)を確認すると役立つ場合があります。</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade">
<title>Helmチャートのアップグレード</title>
<para>このセクションでは、次の内容について説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-air-gap"/> -
Edgeに関連するOCIチャートとイメージをプライベートレジストリに配布する方法に関する情報を保持します。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure"/> -
異なるHelmチャートアップグレードのユースケースとそのアップグレード手順に関する情報を保持します。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-air-gap">
<title>エアギャップ環境の準備</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet-2">
<title>HelmチャートFleetにアクセスできることの確認</title>
<para>環境でサポートする内容によって、次のオプションのいずれかを選択できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>管理クラスタ</literal>からアクセス可能なローカルGitサーバでチャートのFleetリソースをホストします。</para>
</listitem>
<listitem>
<para>FleetのCLIを使用して、直接使用可能でどこかにホストする必要のない<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">バンドルにHelmチャートを変換</link>します。FleetのCLIは、<link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">リリース</link>ページから取得できます。Macユーザの場合は、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version-2">
<title>Edgeリリースバージョンに必要なアセットの検索</title>
<orderedlist numeration="arabic">
<listitem>
<para>「Day 2」<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のページに移動し、チャートのアップグレード先のEdgeリリースを見つけ、<emphasis
role="strong">［Assets (アセット)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Assets (アセット)］</emphasis>セクションから、次のファイルをダウンロードします。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">リリースファイル</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">説明</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para><literal>edge-release-images.txt</literal>ファイルで指定されたイメージを取得し、それらを「.tar.gz」アーカイブにパッケージ化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するOCIチャートイメージを取得し、それらを「.tar.gz」アーカイブにパッケージ化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>「.tar.gz」アーカイブからイメージをロードし、再タグ付けして、プライベートレジストリにプッシュします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Edge OCI「.tgz」チャートパッケージを含むディレクトリを取得し、プライベートレジストリにロードします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するOCIチャートイメージのリストを含みます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>特定のEdgeリリースに関連するイメージのリストを含みます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive-2">
<title>Edgeリリースイメージアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>イメージアーカイブを生成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>これにより、<literal>edge-images.tar.gz</literal>という名前のすぐにロードできるアーカイブが作成されます。</para>
<note>
<para><literal>-i|--images</literal>オプションが指定される場合、アーカイブの名前は異なる場合があります。</para>
</note>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします。</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive-2">
<title>Edge OCIチャートイメージアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>OCIチャートイメージアーカイブを生成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>これにより、 <literal>oci-artefacts.tar.gz</literal>という名前のアーカイブが作成されます。</para>
<note>
<para><literal>-a|--archive</literal>オプションが指定される場合、アーカイブの名前は異なる場合があります。</para>
</note>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします。</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine-2">
<title>Edgeリリースイメージをエアギャップマシンにロード</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>以前に<emphasis role="strong">コピーした</emphasis>
<literal>edge-images.tar.gz</literal>アーカイブを渡して、スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>これにより、<literal>edge-images.tar.gz</literal>からすべてのイメージがロードされ、再タグ付けされて、それらを<literal>--registry</literal>オプションで指定されているレジストリにプッシュします。</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine-2">
<title>Edge OCIチャートイメージのエアギャップマシンへのロード</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>コピーした<literal>oci-artefacts.tar.gz</literal>アーカイブをuntarします。</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>命名テンプレート<literal>edge-release-oci-tgz-&lt;date&gt;</literal>を含むディレクトリが生成されます。</para>
</listitem>
<listitem>
<para>このディレクトリを<literal>edge-load-oci-artefacts.sh</literal>スクリプトに渡し、Edge
OCIチャートイメージをプライベートレジストリにロードします。</para>
<note>
<para>このスクリプトは、<literal>Helm</literal>
CLIが環境にプリインストールされていることを想定しています。Helmのインストール手順については、「<link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm
(Helmのインストール)</link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution-2">
<title>Kubernetesディストリビューションでプライベートレジストリを設定する</title>
<para>RKE2の場合は、「<link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration (プライベートレジストリの設定)</link>」を参照してください。</para>
<para>K3sの場合は、「<link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration (プライベートレジストリの設定)</link>」を参照してください。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure">
<title>アップグレード手順</title>
<para>このセクションでは、Helmアップグレード手順の次のユースケースを中心に説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>手動でデプロイしたHelmチャートを確実にアップグレードすることはできません。<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/>で説明する方法を使用してHelmチャートを再デプロイすることをお勧めします。</para>
</important>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>新しいクラスタがあり、Edge Helmチャートをデプロイして管理したい</title>
<para>このセクションでは、以下の実行方法について説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>チャートのFleetリソースの準備</title>
<orderedlist numeration="arabic">
<listitem>
<para>チャートのFleetリソースを、使用するEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから取得します。</para>
</listitem>
<listitem>
<para>HelmチャートのFleet
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)に移動します。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">GitOpsワークフローを使用する場合は</emphasis>、チャートFleetディレクトリをGitOpsを実行するGitリポジトリにコピーします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis> Helmチャートで<emphasis
role="strong">値</emphasis>を設定する必要がある場合は、コピーしたディレクトリの<literal>fleet.yaml</literal>ファイルに含まれる設定<literal>.helm.values</literal>を編集します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis>
環境に合わせて、チャートのFleetにリソースを追加しなければならないユースケースがあります。Fleetディレクトリを拡張する方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリのコンテンツ)</link>」を参照してください。</para>
</listitem>
</orderedlist>
<note>
<para>場合によっては、Fleet がHelm操作に使用するデフォルトのタイムアウトが不十分で、次のエラーが発生する可能性があります。</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>このような場合は、<literal>fleet.yaml</literal>ファイルの<literal>helm</literal>設定の下に、<link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>プロパティを追加してください。</para>
</note>
<para><literal>longhorn</literal> Helmチャートの<emphasis
role="strong">例</emphasis>は次のようになります。</para>
<itemizedlist>
<listitem>
<para>ユーザGitリポジトリ構造:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>ユーザの<literal>Longhorn</literal>データが入力された<literal>fleet.yaml</literal>の内容:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>これらは値の例であり、<literal>longhorn</literal>チャートのカスタム設定を示すために使用しているだけです。<literal>longhorn</literal>チャートのデプロイメントのガイドラインと<emphasis
role="strong">みなさない</emphasis>でください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>チャートのFleetのデプロイ</title>
<para>チャートのFleetをデプロイするには、GitRepo (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>)またはバンドル(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>)のいずれかを使用できます。</para>
<note>
<para>Fleetをデプロイする場合に、<literal>Modified</literal>メッセージを取得した場合、Fleetの<literal>diff</literal>セクションに対応する<literal>comparePatches</literal>エントリを追加してください。詳細については、「<link
xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore
Modified GitRepos (変更されたGitReposを無視する差分を生成する)</link> 」を参照してください。</para>
</note>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>Fleetの<link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソースは、チャートのFleetリソースへのアクセス方法、それらのリソースを適用するのに必要なクラスタに関する情報を保持しています。</para>
<para><literal>GitRepo</literal>リソースは、<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UI</link>を通じて、または手動で<literal>管理クラスタ</literal>にリソースを<link
xl:href="https://fleet.rancher.io/tut-deployment">デプロイ</link>することで、デプロイできます。</para>
<para><emphasis role="strong">手動</emphasis>デプロイメントの<emphasis
role="strong">Longhorn</emphasis> <literal>GitRepo</literal>リソースの例:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>バンドル</title>
<para><link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>リソースは、Fleetによってデプロイされる必要がある生のKubernetesリソースを保持しています。通常、<literal>GitRepo</literal>アプローチを使用することを推奨されますが、
環境がエアギャップされ、ローカルGitサーバをサポートできないユースケース用に、<literal>バンドル</literal>がHelmチャートFleetをターゲットクラスタに伝播するのに役立ちます。</para>
<para><literal>バンドル</literal>は、Rancher UI (<literal>［Continuous Delivery
(継続的デリバリ)］ → ［Advanced (詳細) ］→ ［Bundles (バンドル)］ → ［Create from YAML
(YALMから作成)］</literal>)を介して、または<literal>バンドル</literal>リソースを正しいFleetネームスペースに手動でデプロイして、デプロイできます。Fleetネームスペースについては、アップストリーム<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">ドキュメント</link>を参照してください。</para>
<para>Edge Helmチャートの<literal>バンドル</literal>は、Fleetの 「<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートをバンドルに変換する)</link>」 アプローチを利用して作成できます。</para>
<para>以下に、<literal>バンドル</literal>リソースを<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>と<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
HelmチャートFleetテンプレートから作成し、このバンドルを<literal>管理クラスタ</literal>に手動でデプロイする方法の例を示します。</para>
<note>
<para>ワークフローを説明するため、以下の例では<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>ディレクトリ構造を使用しています。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>チャートFleetテンプレートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>FleetにHelmチャートのデプロイ先クラスタを指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>ダウンストリームクラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマッピング)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>を使用して、<literal>Longhorn</literal>
HelmチャートFleetをバンドルリソースに変換します。</para>
<note>
<para>FleetのCLI は、<link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">リリース</link><emphasis
role="strong">アセット</emphasis>ページから取得できます(<literal>fleet-linux-amd64</literal>)。</para>
<para>Macユーザの場合、<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulaeがあります。</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>チャートFleetテンプレートに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>FleetにHelmチャートのデプロイ先クラスタを指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
</listitem>
<listitem>
<para><link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>を使用して、<literal>Longhorn
CRD</literal> HelmチャートFleetをバンドルリソースに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>longhorn-bundle.yaml</literal>と<literal>longhorn-crd-bundle.yaml</literal>ファイルを<literal>管理クラスタ</literal>にデプロイします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>これらの手順に従うと、<literal>SUSE Storage</literal>が指定されたダウンストリームクラスタのすべてにデプロイされます。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>デプロイされたHelmチャートの管理</title>
<para>Fleetでデプロイした後のHelmチャートのアップグレードについては、<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>を参照してください。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>Fleetで管理されているHelmチャートをアップグレードしたい場合</title>
<orderedlist numeration="arabic">
<listitem>
<para>目的のEdgeリリースと互換性を持つように、チャートをアップブレードする必要があるバージョンを決定します。EdgeリリースごとのHelmチャートバージョンはリリースノート(<xref
linkend="release-notes"/>)から表示できます。</para>
</listitem>
<listitem>
<para>Fleetで監視されているGitリポジトリで、Helmチャートの<literal>fleet.yaml</literal>ファイルを、リリースノート(<xref
linkend="release-notes"/>)から取得した正しいチャートの<emphasis
role="strong">バージョン</emphasis>と<emphasis
role="strong">リポジトリ</emphasis>で編集します。</para>
</listitem>
<listitem>
<para>変更をコミットしてリポジトリにプッシュした後で、目的のHelmチャートのアップグレードがトリガされます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>EIBを介してデプロイされたHelmチャートをアップグレードしたい</title>
<para><xref linkend="components-eib"/>は、<literal>HelmChart</literal>リソースを作成し、<link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>
Helm統合機能によって導入された<literal>helm-controller</literal>を利用して、Helmチャートをデプロイします。</para>
<para><literal>EIB</literal>を介してデプロイされたHelmチャートが正常にアップグレードされるようにするには、ユーザは各<literal>HelmChart</literal>リソースに対してアップグレードを実行する必要があります。</para>
<para>以下に関する情報が提供されています。</para>
<itemizedlist>
<listitem>
<para>アップグレードプロセスの一般的な概要(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>)。</para>
</listitem>
<listitem>
<para>必要なアップグレード手順(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>)。</para>
</listitem>
<listitem>
<para>説明した方法を使用した<link
xl:href="https://longhorn.io">Longhorn</link>チャートアップグレードを示す例(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>)。</para>
</listitem>
<listitem>
<para>異なるGitOpsツールを使用したアップグレードプロセスを使用する方法(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>)。</para>
</listitem>
</itemizedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>概要</title>
<para><literal>EIB</literal>を介してデプロイされたHelmチャートは、<link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>と呼ばれる<literal>フリート</literal>を通じてアップグレードされます。</para>
<para>この<literal>フリート</literal>は、<emphasis
role="strong">ユーザが提供した</emphasis>データを処理し、特定のHelmChartリソースセットを<emphasis
role="strong">更新</emphasis>します。</para>
<para>これらのリソースを更新すると、 <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>がトリガされ、変更された<literal>HelmChart</literal>リソースに関連付けられているHelmチャートが<emphasis
role="strong">アップグレード</emphasis>されます。</para>
<para>ユーザは以下を行うことのみ求められます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>アップグレードが必要な各Helmチャートのアーカイブをローカルで<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
</listitem>
<listitem>
<para>これらのアーカイブを<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal>スクリプトに渡します。これにより、これらのアーカイブのデータが
<literal>eib-charts-upgrader</literal>フリートに含められます。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal>
Fleetを<literal>管理クラスタ</literal>にデプロイします。これは<literal>GitRepo</literal>リソースまたは<literal>バンドル</literal>リソースのいずれか通じて実行されます。</para>
</listitem>
</orderedlist>
<para>デプロイされたら、<literal>eib-charts-upgrader</literal>がFleetのサポートによって、そのリソースを目的のダウンストリームクラスタに配布します。</para>
<para>これらのリソースには、次のものが含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis
role="strong">ユーザが提供する</emphasis>Helmチャートデータを保持する<literal>Secret</literal>のセット。</para>
</listitem>
<listitem>
<para>前述の<literal>Secret</literal>をマウントし、それらに基づいて対応するHelmChartリソースに<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">パッチを適用する</link>
<literal>Pod</literal>をデプロイする<literal>Kubernetes Job</literal>。</para>
</listitem>
</orderedlist>
<para>前述のとおり、これにより、<literal>helm-controller</literal>がトリガされ、実際の
Helmチャートアップグレードが実行されます。</para>
<para>上記の説明を以下に図示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-downstream-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>fleet day2ダウンストリームhelm eibアップグレード</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>アップグレード手順</title>
<orderedlist numeration="arabic">
<listitem>
<para>正しいリリース <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">タグ</link>から<literal>suse-edge/fleet-examples</literal>リポジトリのクローンを作成します。</para>
</listitem>
<listitem>
<para>取得したHelmチャートアーカイブを保存するディレクトリを作成します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>新規に作成されたアーカイブディレクトリ内で、アップグレードするHelmチャートのアーカイブを<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">リリースタグ</link>の<emphasis
role="strong">アセット</emphasis>から、<literal>generate-chart-upgrade-data.sh</literal>スクリプトをダウンロードします。</para>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para><literal>--archive-dir</literal>ディレクトリ内のチャートアーカイブごとに、スクリプトにより、チャートアップグレードデータを含む<literal>Kubernetes
Secret
YAML</literal>ファイルが生成され、<literal>--fleet-path</literal>によって指定されたフリートの<literal>base/secrets</literal>ディレクトリに保存されます。</para>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトは、フリートに追加の変更も適用し、生成された<literal>Kubernetes
Secret YAML</literal>ファイルが、フリートによってデプロイされたワークロードによって正しく利用されるようにします。</para>
<important>
<para>ユーザは、<literal>generate-chart-upgrade-data.sh</literal>スクリプトが生成する内容に変更を加えてはなりません。</para>
</important>
</listitem>
</orderedlist>
<para>以下の手順は、実行している環境によって異なります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>GitOpsをサポートする環境の場合(例: エアギャップされていない、エアギャップされたがローカルGitサーバサポートが可能):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>fleets/day2/eib-charts-upgrader</literal>
Fleetを、GitOpsに使用するリポジトリにコピーします。</para>
<note>
<para>Fleetに<literal>generate-chart-upgrade-data.sh</literal>スクリプトによって加えられた変更が含まれていることを確認してください。</para>
</note>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal>
Fleetのすべてのリソースを配布するために使用される<literal>GitRepo</literal>リソースを設定します。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Rancher UIを通じた<literal>GitRepo</literal>の設定およびデプロイメントについては、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI (Rancher UIでのFleetへのアクセス)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>GitRepo</literal>手動設定およびデプロイメントの場合、「<link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a Deployment
(デプロイメントの作成)</link>」を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>GitOpsをサポートしていない環境の場合 (例: エアギャップされ、ローカルGitサーバの使用が許可されていない):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>rancher/fleet</literal> <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">リリース</link>ページから<literal>fleet-cli</literal>バイナリをダウンロードします(Linuxの場合は<literal>fleet-linux-amd64</literal>)。Macユーザの場合、使用可能なHomebrew
Formulaeがあります(<link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>)。</para>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal> Fleetに移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>リソースをデプロイする場所をFleetに指示する<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To match all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>ターゲットクラスタをマップする方法については、アップストリーム<link
xl:href="https://fleet.rancher.io/gitrepo-targets">ドキュメント</link>を参照してください。</para>
</listitem>
<listitem>
<para><literal>fleet-cli</literal>を使用して、Fleetを <literal>バンドル</literal>リソースに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>これにより、<literal>eib-charts-upgrader</literal>
Fleetからのすべてのテンプレート化されたリソースを保持するバンドル(<literal>bundle.yaml</literal>)が作成されます。</para>
<para><literal>fleet apply</literal>コマンドに関する詳細については、「<link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>」を参照してください。</para>
<para>Fleetをバンドルに変換する方法に関する詳細については、「 <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle (Helmチャートのバンドルへの変換)</link>」を参照してください。</para>
</listitem>
<listitem>
<para><literal>バンドル</literal>をデプロイします。これは次の2つの方法のいずれかで実行できます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>RancherのUIを通じて - <emphasis role="strong">［Continuous Delivery
(継続的デリバリ)］→［Advanced (詳細)］→［Bundles (バンドル)］ → ［Create from YAML
(YAMLから作成)］</emphasis>に移動して、<literal>bundle.yaml</literal>
コンテンツを解析するか、［<literal>Read from File
(ファイルから読み取り)</literal>］オプションをクリックして、ファイル自体を渡します。</para>
</listitem>
<listitem>
<para>手動 -
<literal>管理クラスタ</literal>内に<literal>bundle.yaml</literal>ファイルを手動でデプロイします。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>これらの手順を実行すると、正常に<literal>GitRepo/バンドル</literal>リソースがデプロイされます。リソースはFleetによって取得され、そのコンテンツはユーザが以前の手順で指定したターゲットクラスタにデプロイされます。プロセスの概要については、<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>を参照してください。</para>
<para>アップグレードプロセスの追跡方法については、 <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>を参照してください。</para>
<important>
<para>チャートアップグレードが正常に確認されたら、<literal>バンドル/GitRepo</literal>リソースを削除します。</para>
<para>これにより、<literal>ダウンストリーム</literal>クラスタから不要になったアップグレードリソースが削除され、今後バージョンクラッシュが発生しないようになります。</para>
</important>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>例</title>
<note>
<para>以下の例は、<literal>ダウンストリーム</literal>クラスタ上で、<literal>EIB</literal>を介してデプロイされたHelmチャートを、あるバージョンから別のバージョンにアップグレードする方法を示しています。この例で使用されているバージョンは、推奨バージョン<emphasis
role="strong">ではない</emphasis>ことに注意してください。Edgeリリースに固有のバージョン推奨事項については、リリースノート(<xref
linkend="release-notes"/>)を参照してください。</para>
</note>
<para><emphasis>ユースケース:</emphasis></para>
<itemizedlist>
<listitem>
<para>クラスタ名<literal>doc-example</literal>が<link
xl:href="https://longhorn.io">Longhorn</link>の古いバージョンを実行しています。</para>
</listitem>
<listitem>
<para>クラスタは、次のイメージ定義<emphasis>スニペット</emphasis>を使用して、EIBを通じてデプロイされました。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal>は、Edge
3.4リリースと互換性のあるバージョンにアップグレードする必要があります。つまり、<literal>107.0.0+up1.9.1</literal>にアップグレードする必要があります。</para>
</listitem>
<listitem>
<para><literal>doc-example</literal>の管理を担当する<literal>管理クラスタ</literal>がローカルGitサーバのサポートなしで<emphasis
role="strong">エアギャップ</emphasis>されており、Rancherセットアップが動作していることを前提としています。</para>
</listitem>
</itemizedlist>
<para>アップグレード手順(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>)に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>release-3.4.0</literal>タグから<literal>suse-edge/fleet-example</literal>リポジトリのクローンを作成します。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para><literal>Longhorn</literal>アップグレードアーカイブが保存されるディレクトリを作成します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>目的の<literal>Longhorn</literal>チャートアーカイブバージョンを取得します。</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para><literal>archives</literal>ディレクトリ以外で、<literal>suse-edge/fleet-examples</literal>リリース<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">タグ</link>から<literal>generate-chart-upgrade-data.sh</literal>スクリプトをダウンロードします。</para>
</listitem>
<listitem>
<para>ディレクトリセットアップは次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para><literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行します。</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>スクリプト実行後のディレクトリ構造は次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Gitで変更されたファイルは次のようになるはずです。</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para><literal>eib-charts-upgrader</literal> Fleetの<literal>バンドル</literal>を作成します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>まず、Fleet自体に移動します。</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>次に、<literal>targets.yaml</literal>ファイルを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: doc-example
EOF</screen>
</listitem>
<listitem>
<para>次に、<literal>fleet-cli</literal>バイナリを使用して、Fleetをバンドルに変換します。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
<listitem>
<para>ここで、 <literal>管理クラスタ</literal>マシンに<literal>bundle.yaml</literal>を転送します。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Rancher UIを通じてバンドルをデプロイします。</para>
<figure>
<title>Rancher UIを通じたバンドルのデプロイ</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例1</phrase></textobject>
</mediaobject></figure>
<para>ここから、<emphasis role="strong">［Read from File
(ファイルから読み取り)］</emphasis>を選択し、システムで<literal>bundle.yaml</literal>ファイルを見つけます。</para>
<para>これにより、Rancher UI内に<literal>バンドル</literal>が自動入力されます。</para>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>正常にデプロイされたら、バンドルは次のようになります。</para>
<figure>
<title>正常にデプロイされたバンドル</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para><literal>バンドル</literal>のデプロイメントが成功した後で、アップグレードプロセスを監視するには次のようにします。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>アップグレードPod</literal>のログを確認します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例3ダウンストリーム</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>ここで、helm-controllerによってアップグレードに作成されたPodのログを確認します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Pod名は次のテンプレートを使用します -
<literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>Podは、<literal>HelmChart</literal>リソースがデプロイされたネームスペースにあります。この場合、このネームスペースは<literal>kube-system</literal>です。</para>
<figure>
<title>正常にアップグレードされたLonghornチャートのログ</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例4ダウンストリーム</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Rancherの<literal>HelmCharts</literal>セクション(<literal>［More Resources
(その他のリソース)］ →
［HelmCharts］</literal>)に移動して、<literal>HelmChart</literal>のバージョンが更新されていることを確認します。チャートがデプロイされたネームスペースを選択します。この例では、<literal>kube-system</literal>です。</para>
</listitem>
<listitem>
<para>最後に、 Longhorn Podが実行されていることを確認します。</para>
</listitem>
</orderedlist>
<para>上記の検証後、Longhorn
Helmチャートが<literal>107.0.0+up1.9.1</literal>バージョンにアップグレードされていると仮定しても問題ないでしょう。</para>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>サードパーティのGitOpsツールを使用したHelmチャートのアップグレード</title>
<para>ユーザがこのアップグレード手順をFleet以外のGitOpsワークフロー(<literal>Flux</literal>など)で実行したいユースケースが存在する場合があります。</para>
<para>アップグレード手順に必要なリソースを生成するには、<literal>generate-chart-upgrade-data.sh</literal>スクリプトを使用して、ユーザが提供するデータを<literal>eib-charts-upgrader</literal>
Fleetに入力することができます。この実行方法の詳細については、<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>を参照してください。</para>
<para>完全なセットアップ後、<link
xl:href="https://kustomize.io">kustomize</link>を使用して、クラスタにデプロイ可能な完全な動作ソリューションを生成することができます。</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>GitOpsワークフローにソリューションを含める場合は、<literal>fleet.yaml</literal>ファイルを削除して、残ったものを有効な<literal>Kustomize</literal>セットアップとして使用できます。まず、<literal>generate-chart-upgrade-data.sh</literal>スクリプトを実行し、アップグレードするHelmチャートのデータを<literal>Kustomize</literal>セットアップに読み込ませることを忘れないでください。</para>
<para>このワークフローの使用方法を理解するには<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>と<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>を参照すると役立つでしょう。</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-suse-telco-cloud-documentation">
<title>SUSE Telco Cloudのドキュメント</title>
<partintro>
<para>SUSE Telco Cloudのドキュメントはこちらでご確認ください。</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Telco Cloud</title>
<para>SUSE Telco Cloud (旧称: SUSE Edge for
Telco)は、通信事業者向けに最適化されたコンピューティングプラットフォームであり、通信事業者や通信ネットワークベンダはそのネットワークを刷新し、その最新化を加速できます。</para>
<para>SUSE Telco Cloudは、CNFをホストするための完全な通信事業者対応のクラウドネイティブなスタックであり、すべての通信ドメイン(Packet
Core、RAN、OSS、BSS)をカバーします。</para>
<itemizedlist>
<listitem>
<para>エッジスタックの複雑な設定を通信事業者の規模で自動的にゼロタッチでロールアウトし、ライフサイクルを管理します。</para>
</listitem>
<listitem>
<para>通信事業者に固有の設定とワークロードを使用して、通信事業者グレードのハードウェアの品質を継続的に保証します。</para>
</listitem>
<listitem>
<para>エッジ専用に設計されたコンポーネントで構成されているため、フットプリントが小さく、ワットパフォーマンスが高くなっています。</para>
</listitem>
<listitem>
<para>ベンダに依存しないAPIを備え、100%オープンソースであるため、柔軟なプラットフォーム戦略を維持します。</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>コンセプトとアーキテクチャ</title>
<para>SUSE Telco
Cloudは、クラウドネイティブな最新の通信事業者向けアプリケーションをコアからエッジまで大規模にホストするために設計されたプラットフォームです。</para>
<para>このページでは、SUSE Telco Cloudで使用されるアーキテクチャとコンポーネントについて説明します。</para>
<section xml:id="id-suse-telco-cloud-architecture">
<title>SUSE Telco Cloudアーキテクチャ</title>
<para>次の図は、SUSE Telco Cloudの高レベルアーキテクチャを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.png"
width="100%"/> </imageobject>
<textobject><phrase>製品atipアーキテクチャ1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-2">
<title>コンポーネント</title>
<para>異なる2つのブロックがあります。管理スタックとランタイムスタックです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理スタック</emphasis>: SUSE Telco
Cloud内でランタイムスタックのプロビジョニングとライフサイクルを管理するために使用される部分です。次のコンポーネントが含まれます。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="components-rancher"/>)を使用した、パブリッククラウド環境とプライベートクラウド環境のマルチクラスタ管理</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>)、MetalLB (<xref
linkend="components-metallb"/>)、および<literal>CAPI</literal> (Cluster
API)インフラストラクチャプロバイダを使用したベアメタルサポート</para>
</listitem>
<listitem>
<para>包括的なテナント分離と<literal>IDP</literal> (IDプロバイダ)の統合</para>
</listitem>
<listitem>
<para>サードパーティ統合と拡張機能の大規模なマーケットプレイス</para>
</listitem>
<listitem>
<para>ベンダに依存しないAPIと充実したプロバイダエコシステム</para>
</listitem>
<listitem>
<para>SUSE Linux Microのトランザクション更新の制御</para>
</listitem>
<listitem>
<para>GitリポジトリとFleet (<xref
linkend="components-fleet"/>)を使用してクラスタのライフサイクルを管理するGitOpsエンジン</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">ランタイムスタック</emphasis>: SUSE Telco
Cloud内でワークロードを実行するために使用される部分です。</para>
<itemizedlist>
<listitem>
<para>RKE2 (<xref
linkend="components-rke2"/>)。エッジ環境およびコンプライアンス重視の通信環境向けに最適化された、セキュリティが強化された軽量のKubernetesディストリビューションとして機能します。</para>
</listitem>
<listitem>
<para>SUSE Security (<xref
linkend="components-suse-security"/>)。イメージの脆弱性スキャン、ディープパケットインスペクション、クラスタ内の自動トラフィック制御などのセキュリティ機能を実現します。</para>
</listitem>
<listitem>
<para>ブロックストレージとSUSE Storage (<xref
linkend="components-suse-storage"/>)。クラウドネイティブのストレージソリューションをシンプルかつ簡単に使用できます。</para>
</listitem>
<listitem>
<para>SUSE Linux Micro (<xref
linkend="components-slmicro"/>)で最適化されたオペレーティングシステム。コンテナ運用のための、安全、軽量でイミュータブルな(トランザクショナルファイルシステムを備えた)
OSを実現します。SUSE Linux Microは、AArch64アーキテクチャとAMD64/Intel
64アーキテクチャで利用でき、通信事業者およびエッジのユースケース向けに<literal>リアルタイムカーネル</literal>もサポートしています。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>デプロイメントフローの例</title>
<para>管理コンポーネントとランタイムコンポーネントの関係を理解できるように、以下にワークフローの概要の例を示します。</para>
<para>ダイレクトネットワークプロビジョニングは、すべてのコンポーネントを事前設定した状態で新しいダウンストリームクラスタをデプロイできるワークフローであり、手動操作なしですぐにワークロードを実行できます。</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>例1: すべてのコンポーネントがインストールされた新しい管理クラスタをデプロイする</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、管理スタックが含まれる新しい<literal>ISO</literal>イメージを作成します。その後、この<literal>ISO</literal>イメージを使用して、新しい管理クラスタをVMまたはベアメタルにインストールできます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png"
width="100%"/> </imageobject>
<textobject><phrase>製品atipアーキテクチャ2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>新しい管理クラスタをデプロイする方法の詳細については、SUSE Telco Cloudの管理クラスタに関するガイド(<xref
linkend="atip-management-cluster"/>)を参照してください。</para>
</note>
<note>
<para>Edge Image Builderの使用方法については、Edge Image Builderのガイド(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>例2: 通信事業者プロファイルを使用してシングルノードのダウンストリームクラスタをデプロイして通信ワークロードを実行可能にする</title>
<para>管理クラスタが稼働したら、その管理クラスタを使用して、ダイレクトネットワークプロビジョニングワークフローにより、すべての通信機能が有効化および設定された状態でシングルノードのダウンストリームクラスタをデプロイすることができます。</para>
<para>次の図に、これをデプロイするワークフローの概要を示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png"
width="100%"/> </imageobject>
<textobject><phrase>製品atipアーキテクチャ3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>ダウンストリームクラスタのデプロイ方法の詳細については、SUSE Telco Cloudの自動化されたプロビジョニングに関するガイド(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
</note>
<note>
<para>通信機能の詳細については、SUSE Telco Cloudの通信機能に関するガイド(<xref
linkend="atip-features"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>例3: MetalLBをロードバランサとして使用して高可用性ダウンストリームクラスタをデプロイする</title>
<para>管理クラスタが稼働したら、その管理クラスタを使用して、ダイレクトネットワークプロビジョニングワークフローにより、<literal>MetalLB</literal>をロードバランサとして使用する高可用性ダウンストリームクラスタをデプロイすることができます。</para>
<para>次の図に、これをデプロイするワークフローの概要を示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png"
width="100%"/> </imageobject>
<textobject><phrase>製品atipアーキテクチャ4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>ダウンストリームクラスタのデプロイ方法の詳細については、SUSE Telco Cloudの自動化されたプロビジョニングに関するガイド(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
</note>
<note>
<para><literal>MetalLB</literal>の詳細については、<xref
linkend="components-metallb"/>を参照してください。</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>要件と前提</title>
<section xml:id="id-hardware">
<title>ハードウェア</title>
<para>SUSE Telco Cloudのハードウェア要件は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理クラスタ</emphasis>: 管理クラスタには、<literal>SUSE Linux
Micro</literal>、<literal>RKE2</literal>、<literal>SUSE Rancher
Prime</literal>、<literal>Metal<superscript>3</superscript></literal>などのコンポーネントが含まれ、管理クラスタを使用して複数のダウンストリームクラスタを管理します。管理するダウンストリームクラスタの数によっては、サーバのハードウェア要件は変わる場合があります。</para>
<itemizedlist>
<listitem>
<para>サーバ(<literal>VM</literal>または<literal>ベアメタル</literal>)の最小要件は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>RAM: 8GB以上(16GB以上を推奨)</para>
</listitem>
<listitem>
<para>CPU: 2個以上(4個以上を推奨)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">ダウンストリームクラスタ</emphasis>:
ダウンストリームクラスタは、通信ワークロードを実行するためにデプロイされるクラスタです。<literal>SR-IOV</literal>、<literal>CPUパフォーマンス最適化</literal>などの特定の通信機能を有効にするには、固有の要件が必要になります。</para>
<itemizedlist>
<listitem>
<para>SR-IOV: VF
(仮想機能)をCNF/VNFにパススルーモードでアタッチするには、NICがSR-IOVをサポートしていて、BIOSでVT-d/AMD-Viが有効化されている必要があります。</para>
</listitem>
<listitem>
<para>CPUプロセッサ: 特定の通信ワークロードを実行するには、こちらの参照表(<xref
linkend="atip-features"/>)に記載されているほとんどの機能を利用できるようにCPUプロセッサモデルを適応させる必要があります。</para>
</listitem>
<listitem>
<para>仮想メディアでインストールするためのファームウェア要件:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">サーバハードウェア</entry>
<entry align="left" valign="top">BMCモデル</entry>
<entry align="left" valign="top">管理</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>Dell製ハードウェア</para></entry>
<entry align="left" valign="top"><para>第15世代</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Supermicro製ハードウェア</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>HPE製ハードウェア</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-network">
<title>ネットワーク</title>
<para>ネットワークアーキテクチャの参考として、次の図に、通信事業者環境の一般的なネットワークアーキテクチャを示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirements1.svg"
width="100%"/> </imageobject>
<textobject><phrase>製品atip要件1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>このネットワークアーキテクチャは次のコンポーネントに基づきます。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理ネットワーク</emphasis>:
このネットワークは、ダウンストリームクラスタノードの管理や帯域外管理に使用されます。通常は独立した管理スイッチに接続しますが、同じサービススイッチに接続し、VLANを使ってトラフィックを分離することもできます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コントロールプレーンネットワーク</emphasis>:
このネットワークは、ダウンストリームクラスタノードと、そこで実行されているサービスとの間の通信に使用されます。また、ダウンストリームクラスタノードと外部サービス(<literal>DHCP</literal>サーバや<literal>DNS</literal>サーバなど)との間の通信にも使用されます。接続環境では、スイッチやルータでインターネット経由のトラフィックを処理できる場合もあります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">その他のネットワーク</emphasis>:
場合によっては、特定の目的に合わせてノードを他のネットワークに接続できます。</para>
</listitem>
</itemizedlist>
<note>
<para>ダイレクトネットワークプロビジョニングワークフローを使用するには、管理クラスタがダウンストリームクラスタサーバのBaseboard Management
Controller (BMC)とネットワークで接続されていて、ホストの準備とプロビジョニングを自動化できる必要があります。</para>
</note>
</section>
<section xml:id="id-port-requirements">
<title>ポート要件</title>
<para>SUSE Telco
Cloudのデプロイメントが適切に動作するには、管理ノードおよびダウンストリームKubernetesクラスタノードにアクセス可能な複数のポートが必要です。</para>
<note>
<para>正確なリストは、デプロイされているオプションコンポーネントと選択したデプロイメントオプション(CNIプラグインなど)によって異なります。</para>
</note>
<section xml:id="id-management-nodes">
<title>管理ノード</title>
<para>次の表は、管理クラスタを実行しているノードで開いているポートをリストしています。</para>
<note>
<para>CNIプラグイン関連のポートについては、CNI固有のポート要件(<xref
linkend="cni-specific-port-requirements"/>)を参照してください。</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-nodes" frame="all" rowsep="1" colsep="1">
<title>管理ノードの受信ネットワークルール</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">プロトコル</entry>
<entry align="left" valign="top">ポート</entry>
<entry align="left" valign="top">ソース</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>SSHアクセスを必要とする任意のソース</para></entry>
<entry align="left" valign="top"><para>管理クラスタノードへのSSHアクセス</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>外部TLS終端を行うロードバランサ/プロキシ</para></entry>
<entry align="left" valign="top"><para>外部TLS終端が使用される場合はRancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Rancher UI/APIへのTLSアクセスを必要とする任意のソース</para></entry>
<entry align="left" valign="top"><para>Rancherエージェント、Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>RKE2 (管理クラスタ)サーバノード</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal>クライアントポート</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>RKE2 (管理クラスタ)サーバノード</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal>ピアポート</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6180</para></entry>
<entry align="left" valign="top"><para>この公開ポート(非TLS)からIPA<superscript>(2)</superscript> ramdiskイメージをプルするように
<literal>Metal<superscript>3</superscript>/ironic</literal>によって以前に指示された任意のBMC<superscript>(1)</superscript></para></entry>
<entry align="left" valign="top"><para>仮想メディアベースのブート用のIPA<superscript>(2)</superscript>
ISOイメージを提供する<literal>Ironic</literal> httpd非TLS Webサーバ。<?asciidoc-br?>
<?asciidoc-br?>このポートが有効な場合、機能的に同等だがTLS対応のポート(下記参照)は開かれません</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6185</para></entry>
<entry align="left" valign="top"><para>この公開ポート(TLS)からIPA<superscript>(2)</superscript>
ramdiskイメージをプルするように<literal>Metal<superscript>3</superscript>/ironic</literal>によって以前に指示された任意のBMC<superscript>(1)</superscript></para></entry>
<entry align="left" valign="top"><para><?asciidoc-br?>仮想メディアベースのブート用のIPA<superscript>(2)</superscript>
ISOイメージを提供する<literal>Ironic</literal> httpd TLS対応Webサーバ。<?asciidoc-br?>
<?asciidoc-br?>このポートが有効な場合、機能的には同等だがTLSが無効なポート(上記参照)は開かれません</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6385</para></entry>
<entry align="left" valign="top"><para>「登録済み」<literal>BareMetalHost</literal>インスタンスにデプロイされ実行されている任意の<literal>Metal<superscript>3</superscript>/ironic</literal>
IPA<superscript>(1)</superscript> ramdiskイメージ</para></entry>
<entry align="left" valign="top"><para>Ironic API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>任意の管理クラスタノード、(管理クラスタ)外部の任意のKubernetesクライアント</para></entry>
<entry align="left" valign="top"><para>Kubernetes API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6545</para></entry>
<entry align="left" valign="top"><para>任意の管理クラスタノード</para></entry>
<entry align="left" valign="top"><para>OCI準拠レジストリ(Hauler)からアーティファクトをプルします</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>RKE2サーバとエージェントノード(管理クラスタ)</para></entry>
<entry align="left" valign="top"><para>ノード登録用のRKE2スーパーバイザAPI (すべてのRKE2サーバノードで開かれたポート)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>任意の管理クラスタノード</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal>メトリクス</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000～32767</para></entry>
<entry align="left" valign="top"><para><literal>spec.type: NodePort</literal>または<literal>spec.type:
LoadBalancer</literal> <link
xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service
APIオブジェクト</link>を介してプライマリネットワークで公開されているサービスにアクセスする、(管理クラスタ)外部の任意のソース</para></entry>
<entry align="left" valign="top"><para>使用可能な <literal>NodePort</literal> ポート範囲</para></entry>
</row>
</tbody>
</tgroup>
</table>
<para><superscript>(1)</superscript> BMC: ベースボード管理コントローラ<?asciidoc-br?>
<superscript>(2)</superscript> IPA: Ironic Python Agent</para>
</section>
<section xml:id="id-downstream-nodes">
<title>ダウンストリームノード</title>
<para>SUSE Telco
Cloudでは、任意の(ダウンストリーム)サーバが実行中のダウンストリームKubernetesクラスタの一部となる前(または、それ自体が単一ノードのダウンストリームKubernetesクラスタとして実行される前)に、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/baremetalhost-states.md">BaremetalHostのプロビジョニング状態</link>のいくつかを経由する必要があります。</para>
<itemizedlist>
<listitem>
<para>宣言されたばかりのダウンストリームサーバのベースボード管理コントローラ(BMC)は、帯域外ネットワーク経由でアクセスできる必要があります。BMCは(管理クラスタ上で実行されているIronicサービスから)以下の初期実行手順について指示されます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>BMCで提供される<literal>仮想メディア</literal>内の指定されたIPA ramdiskイメージをプルしてロードします。</para>
</listitem>
<listitem>
<para>サーバの電源を入れます。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>以下のポートは、BMCから公開される予定です(実際のハードウェアによって異なる可能性があります)。</para>
<table xml:id="table-inbound-network-rules-for-baseboard-management-controllers" frame="all" rowsep="1" colsep="1">
<title>ベースボード管理コントローラの受信ネットワークルール</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">プロトコル</entry>
<entry align="left" valign="top">ポート</entry>
<entry align="left" valign="top">ソース</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Ironic Conductor (管理クラスタから)</para></entry>
<entry align="left" valign="top"><para>Redfish APIアクセス(HTTP)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Ironic Conductor (管理クラスタから)</para></entry>
<entry align="left" valign="top"><para>Redfish APIアクセス(HTTPS)</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>BMC <literal>仮想メディア</literal> にロードされたIPA
ramdiskイメージを使用してダウンストリームサーバイメージを起動すると、ハードウェア検査フェーズが開始されます。以下の表は、実行中のIPA
ramdiskイメージによって公開されるポートをリストしています。</para>
</listitem>
</itemizedlist>
<table xml:id="table-inbound-network-rules-for-downstream-nodes-provisioning-phase" frame="all" rowsep="1" colsep="1">
<title>ダウンストリームノードの受信ネットワークルール -
<literal>Metal<superscript>3</superscript>/Ironic</literal>プロビジョニングフェーズ</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">プロトコル</entry>
<entry align="left" valign="top">ポート</entry>
<entry align="left" valign="top">ソース</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>IPA ramdiskイメージへのSSHアクセスを必要とする任意のソース</para></entry>
<entry align="left" valign="top"><para>検査対象のダウンストリームクラスタノードへのSSHアクセス</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9999</para></entry>
<entry align="left" valign="top"><para>Ironic Conductor (管理クラスタから)</para></entry>
<entry align="left" valign="top"><para>実行中のramdiskイメージに対するIronicコマンド</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>ベアメタルホストが適切にプロビジョニングされ、ダウンストリームKubernetesクラスタに参加すると、以下のポートが公開されます。</para>
</listitem>
</itemizedlist>
<note>
<para>CNIプラグイン関連のポートについては、CNI固有のポート要件(<xref
linkend="cni-specific-port-requirements"/>)を参照してください。</para>
</note>
<table xml:id="table-inbound-network-rules-for-downstream-nodes" frame="all" rowsep="1" colsep="1">
<title>ダウンストリームノードの受信ネットワークルール</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">プロトコル</entry>
<entry align="left" valign="top">ポート</entry>
<entry align="left" valign="top">ソース</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>SSHアクセスを必要とする任意のソース</para></entry>
<entry align="left" valign="top"><para>ダウンストリームクラスタノードへのSSHアクセス</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>外部TLS終端を行うロードバランサ/プロキシ</para></entry>
<entry align="left" valign="top"><para>外部TLS終端が使用される場合はRancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Rancher UI/APIへのTLSアクセスを必要とする任意のソース</para></entry>
<entry align="left" valign="top"><para>Rancherエージェント、Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>RKE2 (ダウンストリームクラスタ)サーバノード</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal>クライアントポート</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>RKE2 (ダウンストリームクラスタ)サーバノード</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal>ピアポート</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>任意のダウンストリームクラスタノード、(ダウンストリームクラスタ)外部の任意のKubernetesクライアント</para></entry>
<entry align="left" valign="top"><para>Kubernetes API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>RKE2サーバおよびエージェントノード(ダウンストリームクラスタ)</para></entry>
<entry align="left" valign="top"><para>ノード登録用のRKE2スーパーバイザAPI (すべてのRKE2サーバノードで開かれたポート)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>任意のダウンストリームクラスタノード</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal>メトリクス</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10255</para></entry>
<entry align="left" valign="top"><para>任意のダウンストリームクラスタノード</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal>読み取り専用アクセス</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000～32767</para></entry>
<entry align="left" valign="top"><para><literal>spec.type: NodePort</literal>または<literal>spec.type:
LoadBalancer</literal> <link
xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service
APIオブジェクト</link>を介してプライマリネットワークで公開されているサービスにアクセスする、(ダウンストリームクラスタ)外部の任意のソース</para></entry>
<entry align="left" valign="top"><para>使用可能な <literal>NodePort</literal> ポート範囲</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="cni-specific-port-requirements">
<title>CNI固有のポート要件</title>
<para>サポートされている各CNIバリアントには、独自のポート要件が伴います。詳細については、RKE2ドキュメントの「<link
xl:href="https://docs.rke2.io/install/requirements#cni-specific-inbound-network-rules">CNI
Specific Inbound Network Rules (CNI固有の受信ネットワークルール)</link>」を参照してください。</para>
<para><literal>cilium</literal>がデフォルト/プライマリCNIプラグインとして設定されている場合、<literal>cilium-operator</literal>ワークロードが、デプロイされるKubernetesクラスタの外部にメトリクスを公開するように設定されると、以下のTCPポートが付加的に公開されます。これにより、そのKubernetesクラスタ外部で実行されている外部<literal>Prometheus</literal>サーバインスタンスが引き続きこれらのメトリクスを収集できるようになります。</para>
<note>
<para>これは、rke2-cilium Helmチャートを介して<literal>cilium</literal>をデプロイする際のデフォルトオプションです。</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-downstream-nodes-external-metrics-cilium-operator" frame="all" rowsep="1" colsep="1">
<title>管理/ダウンストリームノード用の受信ネットワークルール -
<literal>cilium-operator</literal>からの外部メトリクス公開が有効</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">プロトコル</entry>
<entry align="left" valign="top">ポート</entry>
<entry align="left" valign="top">ソース</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9963</para></entry>
<entry align="left" valign="top"><para>(Kubernetesクラスタ)外部のメトリクスコレクタ</para></entry>
<entry align="left" valign="top"><para>cilium-operatorメトリクス公開</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>サービス(DHCP、DNSなど)</title>
<para>デプロイ先の環境の種類によっては、<literal>DHCP</literal>、<literal>DNS</literal>などの外部サービスが必要な場合があります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">接続環境</emphasis>:
この場合、ノードはインターネットに接続され(L3ルーティングプロトコルを使用)、外部サービスはお客様が提供します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">非接続/エアギャップ環境</emphasis>:
この場合、ノードはインターネットにIPで接続されないため、サービスを追加して、ダイレクトネットワークプロビジョニングワークフローに必要なコンテンツをローカルにミラーリングする必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ファイルサーバ</emphasis>:
ファイルサーバは、ダイレクトネットワークプロビジョニングワークフローの中で、ダウンストリームクラスタノードにプロビジョニングするOSイメージを保存するために使用されます。<literal>Metal<superscript>3</superscript></literal>
HelmチャートでメディアサーバをデプロイしてOSイメージを保存できます。次のセクション(<xref
linkend="metal3-media-server"/>)を確認してください。ただし、既存のローカルWebサーバを使用することもできます。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-systemd-services">
<title>systemdサービスの無効化</title>
<para>通信ワークロードの場合、ノード上で実行されているワークロードのパフォーマンス(レイテンシ)に影響を及ぼさないように、ノード上で実行されている一部のサービスを無効にしたり、適切に設定することが重要です。</para>
<itemizedlist>
<listitem>
<para><literal>rebootmgr</literal>は、システムに保留中の更新がある場合の再起動方針を設定できるサービスです。通信ワークロードでは、システムによってスケジュールされた更新がある場合、<literal>rebootmgr</literal>サービスを無効にするか正しく設定してノードの再起動を回避し、ノードで実行中のサービスへの影響を避けることが非常に重要です。</para>
</listitem>
</itemizedlist>
<note>
<para><literal>rebootmgr</literal>の詳細については、<link
xl:href="https://github.com/SUSE/rebootmgr">rebootmgrのGitHubリポジトリ</link>を参照してください。</para>
</note>
<para>次のコマンドを実行して、使用する方針を検証します。</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>また、次のコマンドを実行すると無効にすることができます。</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>または、<literal>rebootmgrctl</literal>コマンドを次のように使用できます。</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para><literal>rebootmgr</literal>の方針を設定するこの設定は、ダイレクトネットワークプロビジョニングワークフローを使用して自動化できます。詳細については、自動化されたプロビジョニングに関するドキュメント(<xref
linkend="atip-automated-provisioning"/>)を確認してください。</para>
</note>
<itemizedlist>
<listitem>
<para><literal>transactional-update</literal>は、システムによって制御される自動更新を可能にするサービスです。通信ワークロードの場合、ノードで実行中のサービスに影響を及ぼさないように、自動更新を無効にすることが重要です。</para>
</listitem>
</itemizedlist>
<para>自動更新を無効にするには、次のコマンドを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable transactional-update.timer
systemctl --now disable transactional-update-cleanup.timer</screen>
<itemizedlist>
<listitem>
<para><literal>fstrim</literal>は、ファイルシステムを毎週自動的にトリミングできるサービスです。通信ワークロードでは、ノードで実行中のサービスに影響を及ぼさないように、自動トリミングを無効にすることが重要です。</para>
</listitem>
</itemizedlist>
<para>自動トリミングを無効にするには、次のコマンドを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable fstrim.timer</screen>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>管理クラスタの設定</title>
<section xml:id="id-introduction-2">
<title>はじめに</title>
<para>管理クラスタは、SUSE Telco
Cloud内でランタイムスタックのプロビジョニングとライフサイクルを管理するために使用されます。技術的観点からは、管理クラスタには次のコンポーネントが含まれています。</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro</literal>
(OS)。ユースケースに応じて、ネットワーキング、ストレージ、ユーザ、カーネル引数などの一部の設定をカスタマイズできます。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>
(Kubernetesクラスタ)。ユースケースに応じて、<literal>Multus</literal>、<literal>Cilium</literal>、<literal>Calico</literal>などの特定のCNIプラグインを使用するように設定できます。</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> (管理プラットフォーム)。クラスタのライフサイクルを管理します。</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>
(コンポーネント)。ベアメタルノードのライフサイクルを管理します。</para>
</listitem>
<listitem>
<para><literal>CAPI</literal>
(コンポーネント)。Kubernetesクラスタ(ダウンストリームクラスタ)のライフサイクルを管理します。<literal>RKE2 CAPI
Provider</literal>は、RKE2クラスタのライフサイクルを管理するために使用されます。</para>
</listitem>
</itemizedlist>
<para>上記のコンポーネントをすべて使用すると、管理クラスタは、宣言型アプローチを使用してインフラストラクチャやアプリケーションを管理し、ダウンストリームクラスタのライフサイクルを管理できます。</para>
<note>
<para><literal>SUSE Linux Micro</literal>の詳細については、「SUSE Linux Micro」(<xref
linkend="components-slmicro"/>)を参照してください。</para>
<para><literal>RKE2</literal>の詳細については、「RKE2」(<xref
linkend="components-rke2"/>)を参照してください。</para>
<para><literal>Rancher</literal>の詳細については、「Rancher」(<xref
linkend="components-rancher"/>)を参照してください。</para>
<para><literal>Metal<superscript>3</superscript></literal>の詳細については、「Metal3」(<xref
linkend="components-metal3"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>管理クラスタの設定手順</title>
<para>管理クラスタを設定するには、次の手順が必要です(シングルノードを使用)。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png"
width="100%"/> </imageobject>
<textobject><phrase>製品atip管理クラスタ1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>宣言型アプローチを使用して管理クラスタを設定するための主な手順は次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">接続環境のイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>:
最初の手順では、接続環境で使用する必要がある設定をすべて含むマニフェストとファイルを準備します。</para>
<itemizedlist>
<listitem>
<para>接続環境のディレクトリ構造(<xref linkend="mgmt-cluster-directory-structure"/>):
この手順では、Edge Image Builderで使用するディレクトリ構造を作成し、設定ファイルとイメージそのものを保存します。</para>
</listitem>
<listitem>
<para>管理クラスタ定義ファイル(<xref linkend="mgmt-cluster-image-definition-file"/>):
<literal>mgmt-cluster.yaml</literal>ファイルが管理クラスタのメイン定義ファイルです。このファイルには、作成するイメージに関する次の情報が含まれています。</para>
<itemizedlist>
<listitem>
<para>イメージ情報: ゴールデンイメージを使用して作成するイメージに関する情報。</para>
</listitem>
<listitem>
<para>オペレーティングシステム: イメージで使用するオペレーティングシステムの設定。</para>
</listitem>
<listitem>
<para>Kubernetes: Helmチャートとリポジトリ、Kubernetesのバージョン、ネットワーク設定、およびクラスタで使用するノード。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Customフォルダ(<xref linkend="mgmt-cluster-custom-folder"/>):
<literal>custom</literal>フォルダには設定ファイルとスクリプトが含まれ、Edge Image
Builderはこれらを使用して完全に機能する管理クラスタをデプロイします。</para>
<itemizedlist>
<listitem>
<para>ファイル: 管理クラスタが使用する設定ファイルが含まれています。</para>
</listitem>
<listitem>
<para>スクリプト: 管理クラスタが使用するスクリプトが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Kubernetesフォルダ(<xref linkend="mgmt-cluster-kubernetes-folder"/>):
<literal>kubernetes</literal>フォルダには、管理クラスタが使用する設定ファイルが含まれています。</para>
<itemizedlist>
<listitem>
<para>Manifests: 管理クラスタが使用するマニフェストが含まれています。</para>
</listitem>
<listitem>
<para>Helm: 管理クラスタによって使用されるHelm値ファイルが含まれます。</para>
</listitem>
<listitem>
<para>Config: 管理クラスタが使用する設定ファイルが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Networkフォルダ(<xref linkend="mgmt-cluster-network-folder"/>):
<literal>network</literal>フォルダには、管理クラスタノードが使用するネットワーク設定ファイルが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">エアギャップ環境でのイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>:
この手順では、エアギャップシナリオで使用するマニフェストとファイルを準備する際の相違点を示します。</para>
<itemizedlist>
<listitem>
<para>定義ファイルの変更(<xref linkend="mgmt-cluster-image-definition-file-airgap"/>):
<literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIBの出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。</para>
</listitem>
<listitem>
<para>customフォルダの変更(<xref linkend="mgmt-cluster-custom-folder-airgap"/>):
<literal>custom</literal>フォルダを変更し、管理クラスタをエアギャップ環境で実行するために必要なリソースを含める必要があります。</para>
<itemizedlist>
<listitem>
<para>登録スクリプト:
エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトを削除する必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>helm値フォルダ(<xref linkend="mgmt-cluster-helm-values-folder-airgap"/>)での変更:
<literal>helm/values</literal>フォルダはエアギャップ環境で管理クラスタを実行するために必要な設定を含むように変更する必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">イメージの作成(<xref
linkend="mgmt-cluster-image-creation"/>)</emphasis>: この手順では、Edge Image
Builderツールを使用してイメージを作成します(接続環境とエアギャップ環境の両方が対象です)。ご使用のシステムでEdge Image
Builderツールを実行するための前提条件(<xref linkend="components-eib"/>)を確認してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">管理クラスタのプロビジョニング(<xref
linkend="mgmt-cluster-provision"/>)</emphasis>:
この手順では、前の手順で作成したイメージを使用して管理クラスタをプロビジョニングする方法について説明します(接続シナリオとエアギャップシナリオの両方が対象です)。この手順は、ラップトップ、サーバ、VM、またはUSBポートを搭載した他の任意のAMD64/Intel
64システムを使用して実行できます。</para>
</listitem>
</orderedlist>
<note>
<para>Edge Image Builderの詳細については、「Edge Image Builder」(<xref
linkend="components-eib"/>)およびEdge Image Builderのクイックスタート(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>接続環境用のイメージの準備</title>
<para>Edge Image
Builderは、管理クラスタのイメージを作成するために使用されます。このドキュメントでは、管理クラスタのセットアップに必要な最小設定について説明します。</para>
<para>Edge Image Builderは、コンテナ内で実行されるため、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher
Desktop</link>などのコンテナランタイムが必要です。このガイドでは、Podmanが使用できることを前提としています。</para>
<para>また、高可用性管理クラスタをデプロイするための前提条件として、ネットワークで次の3つのIPを予約する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>apiVIP</literal> (API VIPアドレス用(Kubernetes APIサーバへのアクセスに使用))。</para>
</listitem>
<listitem>
<para><literal>ingressVIP</literal> (Ingress VIPアドレス(Rancher UIなどで使用))。</para>
</listitem>
<listitem>
<para><literal>metal3VIP</literal> (Metal3 VIPアドレス用)。</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure">
<title>ディレクトリ構造</title>
<para>EIBを実行する場合、ディレクトリはホストからマウントされます。したがって、最初に実行する手順は、EIBが設定ファイルとイメージ自体を保存するために使用するディレクトリ構造を作成することです。このディレクトリの構造は次のとおりです。</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── os-files
│ └── var
│   └── lib
│     └── rancher
│       └── rke2
│         └── server
│           └── manifests
│             └── rke2-ingress-config.yaml
├── kubernetes
│ ├── manifests
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── longhorn.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para><literal>SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso</literal>イメージは、<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があり、また、<literal>base-images</literal>フォルダの下に配置されている必要があります。</para>
<para>イメージのSHA256チェックサムを確認し、イメージが改ざんされていないことを確認する必要があります。このチェックサムは、イメージをダウンロードした場所と同じ場所にあります。</para>
<para>ディレクトリ構造の例は、 <link xl:href="https://github.com/suse-edge/atip">SUSE Edge
GitHubリポジトリの「telco-examples」フォルダ</link>にあります。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>管理クラスタ定義ファイル</title>
<para><literal>mgmt-cluster.yaml</literal>ファイルは管理クラスタのメイン定義ファイルで、次の情報が含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    - open-iscsi
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: $API_HOST
    apiVIP: $API_VIP
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para><literal>mgmt-cluster.yaml</literal>定義ファイルのフィールドと値について説明するために、ここではこのファイルを次のセクションに分割しています。</para>
<itemizedlist>
<listitem>
<para>イメージセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>ここで、<literal>baseImage</literal>は、SUSE Customer
CenterまたはSUSEダウンロードページからダウンロードした元のイメージです。<literal>outputImageName</literal>は、管理クラスタのプロビジョニングに使用する新しいイメージの名前です。</para>
<itemizedlist>
<listitem>
<para>オペレーティングシステムセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>ここで、<literal>installDevice</literal>はオペレーティングシステムのインストールに使用するデバイス、<literal>username</literal>および<literal>encryptedPassword</literal>はシステムへのアクセスに使用する資格情報、<literal>packageList</literal>はインストールするパッケージのリスト(<literal>jq</literal>はインストールプロセス中に内部的に必要)です。<literal>sccRegistrationCode</literal>は構築時にパッケージと依存関係を取得するために使用する登録コードで、SUSE
Customer
Centerから取得できます。暗号化パスワードは次のように<literal>openssl</literal>コマンドを使用して生成できます。</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>この出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Kubernetesセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para><literal>helm</literal>セクションには、インストールするHelmチャートのリスト、使用するリポジトリ、およびこれらすべてのバージョン設定が含まれます。</para>
<para><literal>network</literal>セクションには、<literal>RKE2</literal>コンポーネントが使用する<literal>apiHost</literal>や<literal>apiVIP</literal>などのネットワーク設定が含まれます。<literal>apiVIP</literal>は、ネットワーク内で使用されていないIPアドレスにし、DHCPを使用する場合はDHCPプールから除外してください。マルチノードクラスタでは、<literal>apiVIP</literal>がKubernetes
APIサーバへのアクセスに使用されます。<literal>apiHost</literal>は、<literal>RKE2</literal>コンポーネントが使用する<literal>apiVIP</literal>への名前解決として機能します。</para>
<para><literal>nodes</literal>セクションには、クラスタで使用するノードのリストが含まれています。この例では、シングルノードクラスタを使用していますが、リストにノードを追加する(行のコメントを解除する)ことによってマルチノードクラスタに拡張できます。</para>
<note>
<itemizedlist>
<listitem>
<para>ノードの名前はクラスタ内で固有である必要があります。</para>
</listitem>
<listitem>
<para>オプションで、<literal>initializer</literal>フィールドを使用してブートストラップ
ホストを指定します。これを指定しない場合、リストの最初のノードになります。</para>
</listitem>
<listitem>
<para>ネットワーク設定が必要な場合、ノードの名前はネットワークフォルダ(<xref
linkend="mgmt-cluster-network-folder"/>)で定義されたホスト名と同じである必要があります。</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Customフォルダ</title>
<para><literal>custom</literal>フォルダには次のサブフォルダが含まれます。</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para><literal>custom/files</literal>フォルダには、管理クラスタが使用する設定ファイルが含まれます。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>フォルダには、管理クラスタが使用するスクリプトが含まれます。</para>
</listitem>
</itemizedlist>
<para><literal>custom/files</literal>フォルダには、次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>:
<literal>Metal<superscript>3</superscript></literal>、<literal>Rancher</literal>、<literal>MetalLB</literal>の設定パラメータが含まれます。このファイルを変更するのは、使用するネームスペースを変更する場合のみにしてください。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>:
使用する<literal>Metal<superscript>3</superscript></literal>コンポーネントの設定が含まれます(変更不要)。今後のバージョンでは、このスクリプトは代わりに<literal>Rancher
Turtles</literal>を使用するように置き換えられて、使いやすくなる予定です。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
        EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
        EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
        apiVersion: management.cattle.io/v3
        kind: Feature
        metadata:
          name: embedded-cluster-api
        spec:
          value: false
        EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>:
使用する<literal>Rancher</literal>コンポーネントの設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>:
systemdサービスを作成して初回ブート時にスクリプトを実行するための設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single one
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para><literal>custom/scripts</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>99-alias.sh</literal>スクリプト:
管理クラスタが初回ブート時にkubeconfigファイルを読み込むために使用するエイリアスが含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para><literal>99-mgmt-setup.sh</literal>スクリプト:
初回ブート時にスクリプトをコピーするための設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
        cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para><literal>99-register.sh</literal>スクリプト:
SCC登録コードを使用してシステムを登録するための設定が含まれます。アカウントにシステムを登録するには、<literal>${SCC_ACCOUNT_EMAIL}</literal>および<literal>${SCC_REGISTRATION_CODE}</literal>が正しく設定されている必要があります。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
        zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Kubernetesフォルダ</title>
<para><literal>kubernetes</literal>フォルダには次のサブフォルダが含まれます。</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para><literal>kubernetes/config</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>:
デフォルトでは、デフォルトでインストールされている<literal>CNI</literal>プラグインは<literal>Cilium</literal>であるため、このフォルダとファイルを作成する必要はありません。<literal>CNI</literal>プラグインをカスタマイズする必要がある場合に備えて、<literal>kubernetes/config</literal>フォルダにある<literal>server.yaml</literal>ファイルを使用できます。このファイルには次の情報が含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
system-default-registry: registry.rancher.com</screen>
</listitem>
</itemizedlist>
<note>
<para>これは、使用するCNIプラグインなどの特定のKubernetesカスタマイズを定義する任意のファイルです。さまざまなオプションについては、<link
xl:href="https://docs.rke2.io/install/configuration">公式ドキュメント</link>で確認できます。</para>
</note>
<para><literal>os-files/var/lib/rancher/rke2/server/manifests</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>:
管理クラスタ用の<literal>Ingress</literal>サービスを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
</itemizedlist>
<note>
<para><literal>HelmChartConfig</literal>は、以前のリリースで説明している<literal>kubernetes/manifests</literal>経由ではなく、<literal>os-files</literal>経由で<literal>/var/lib/rancher/rke2/server/manifests</literal>ディレクトリに含める必要があります。</para>
</note>
<para><literal>kubernetes/manifests</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>:
<literal>NeuVector</literal>ネームスペースを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>:
<literal>MetalLB</literal>コンポーネントの<literal>L2Advertisement</literal>を作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>:
<literal>rke2-ingress-nginx</literal>コンポーネントの<literal>IPAddressPool</literal>を作成するための設定が含まれます。<literal>${INGRESS_VIP}</literal>を正しく設定し、<literal>rke2-ingress-nginx</literal>コンポーネントで使用するために予約するIPアドレスを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para><literal>kubernetes/helm/values</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>:
<literal>Rancher</literal>コンポーネントを作成する設定が含まれます。<literal>${INGRESS_VIP}</literal>を正しく設定して、<literal>Rancher</literal>コンポーネントで使用するIPアドレスを定義する必要があります。<literal>Rancher</literal>コンポーネントにアクセスするURLは、<literal>https://rancher-${INGRESS_VIP}.sslip.io</literal>です。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>:
<literal>NeuVector</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
registry: "registry.rancher.com"
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>longhorn.yaml</literal>:
<literal>Longhorn</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>:
<literal>Metal<superscript>3</superscript></literal>コンポーネントを作成するための設定が含まれます。<literal>${METAL3_VIP}</literal>を正しく設定して、<literal>Metal<superscript>3</superscript></literal>コンポーネントで使用するIPアドレスを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>メディアサーバは、Metal<superscript>3</superscript>に含まれるオプションの機能です(デフォルトでは無効になっています)。このMetal3の機能を使用するには、以前のマニフェストで設定する必要があります。Metal<superscript>3</superscript>メディアサーバを使用するには、次の変数を指定します。</para>
<itemizedlist>
<listitem>
<para>メディアサーバ機能を有効にするために、globalセクションに<literal>enable_metal3_media_server</literal>を追加して<literal>true</literal>に設定します。</para>
</listitem>
<listitem>
<para>メディアサーバ設定に次の内容を含めます。${MEDIA_VOLUME_PATH}はメディア内のメディアボリュームのパスです(例:
<literal>/home/metal3/bmh-image-cache</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>外部のメディアサーバを使用してイメージを保存できます。外部のメディアサーバをTLSで使用する場合は、次の設定を変更する必要があります。</para>
<itemizedlist>
<listitem>
<para>前の<literal>metal3.yaml</literal>ファイルで<literal>additionalTrustedCAs</literal>を<literal>true</literal>に設定し、外部のメディアサーバから、信頼できる追加のCAを有効にします。</para>
</listitem>
<listitem>
<para><literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal>フォルダに次のシークレット設定を含めて、外部のメディアサーバのCA証明書を保存します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para><literal>additional_ca_cert</literal>は、外部のメディアサーバのbase64エンコードCA証明書です。次のコマンドを使用し、証明書をエンコードして手動でシークレットを生成できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>:
<literal>Cert-Manager</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: true</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>ネットワーキングフォルダ</title>
<para><literal>network</literal>フォルダには、管理クラスタのノードと同じ数のファイルが含まれます。ここでは、ノードは1つのみであるため、<literal>mgmt-cluster-node1.yaml</literal>というファイルが1つあるだけです。ファイルの名前は、上述のnetwork/nodeセクションで<literal>mgmt-cluster.yaml</literal>定義ファイルに定義されているホスト名と一致させる必要があります。</para>
<para>ネットワーキング設定をカスタマイズして特定の静的IPアドレスを使用する必要がある場合(たとえばDHCPを使用しないシナリオの場合)、<literal>network</literal>フォルダにある<literal>mgmt-cluster-node1.yaml</literal>ファイルを使用できます。このファイルには次の情報が含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: ゲートウェイのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: DNSサーバのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: ネットワークインタフェースのMACアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: 管理クラスタのIPアドレス。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>DHCPを使用してIPアドレスを取得する場合、次の設定を使用できます(<literal>${MGMT_MAC}</literal>変数を使用して、<literal>MAC</literal>アドレスを正しく設定する必要があります)。</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>管理クラスタのノード数に応じて、<literal>mgmt-cluster-node2.yaml</literal>、<literal>mgmt-cluster-node3.yaml</literal>などのように追加のファイルを作成して残りのノードを設定できます。</para>
</listitem>
<listitem>
<para><literal>routes</literal>セクションは、管理クラスタのルーティングテーブルを定義するために使用します。</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>エアギャップ環境のイメージの準備</title>
<para>このセクションでは、エアギャップ環境のイメージを準備する方法について説明し、前の各セクションとの相違点のみを示します。エアギャップ環境のイメージを準備するには、前のセクション(接続環境のイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-connected"/>))を次のように変更する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIB出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。</para>
</listitem>
<listitem>
<para><literal>mgmt-cluster.yaml</literal>ファイルは、
<literal>rancher-turtles-airgap-resources</literal>
helmチャートを含むように変更される必要があります。</para>
</listitem>
<listitem>
<para>エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトは削除する必要があります。</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>定義ファイルの変更</title>
<para><literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含める必要があります。このセクションで、<literal>images</literal>フィールドには、出力イメージに含めるすべてのコンテナイメージのリストを含める必要があります。</para>
<note>
<para>次に、<literal>embeddedArtifactRegistry</literal>セクションが含まれる<literal>mgmt-cluster.yaml</literal>ファイルの例を示します。リストされているイメージに、必要なコンポーネントのバージョンが含まれていることを確認してください。</para>
</note>
<para><literal>rancher-turtles-airgap-resources</literal>
helmチャートは追加される必要もあり、これにより、<link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/v0.19/en/getting-started/air-gapped-environment.html">Rancher
Turtlesエアギャップドキュメント</link>で説明されているリソースが作成されます。また、必要な設定を指定するために、rancher-turtlesチャート用の
turtles.yaml値ファイルも必要です。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/mirrored-cilium-cilium:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-cilium-operator-generic:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0
    - name: registry.rancher.com/rancher/neuvector-compliance-config:1.0.6
    - name: registry.rancher.com/rancher/neuvector-controller:5.4.5
    - name: registry.rancher.com/rancher/neuvector-enforcer:5.4.5
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.suse.com/rancher/cluster-api-addon-provider-fleet:v0.11.0
    - name: registry.rancher.com/rancher/cluster-api-operator:v0.18.1
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.7-build20250425
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher/turtles:v0.24.0
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/shell:v0.4.1
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.suse.com/rancher/cluster-api-controller:v1.10.5
    - name: registry.suse.com/rancher/cluster-api-provider-metal3:v1.10.2
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1
    - name: registry.suse.com/rancher/hardened-sriov-network-operator:v1.5.0-build20250425
    - name: registry.suse.com/rancher/ip-address-manager:v1.10.2
    - name: registry.rancher.com/rancher/kubectl:v1.32.2
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.9.5</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>カスタムフォルダの変更</title>
<itemizedlist>
<listitem>
<para>エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトを削除する必要があります。ディレクトリ構造からわかるように、<literal>99-register.sh</literal>スクリプトは<literal>custom/scripts</literal>フォルダに含まれていません。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-helm-values-folder-airgap">
<title>Helm値フォルダの変更</title>
<itemizedlist>
<listitem>
<para><literal>turtles.yaml</literal>: Rancher
Turtlesのエアギャップ操作を指定するために必要な設定が含まれます。これはrancher-turtles-airgap-resourcesチャートのインストールによって異なることに注意してください。</para>
<screen language="yaml" linenumbering="unnumbered">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"</screen>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>イメージの作成</title>
<para>前の各セクションに従ってディレクトリ構造を準備したら(接続シナリオとエアギャップシナリオの両方が対象です)、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>ISO出力イメージファイルが作成されます。ここでは、上記のイメージ定義に基づく<literal>eib-mgmt-cluster-image.iso</literal>という名前のファイルです。</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>管理クラスタのプロビジョニング</title>
<para>前のイメージには、上述のコンポーネントがすべて含まれています。このイメージを使って、仮想マシンまたはベアメタルサーバを使用して(仮想メディア機能を使用して)管理クラスタをプロビジョニングできます。</para>
</section>
<section xml:id="mgmt-cluster-dualstack">
<title>デュアルスタックの考慮事項と設定</title>
<para>前のセクションで示した例では、シングルスタックIPv4管理クラスタの設定方法に関するガイダンスと例を示しています。このような管理クラスタは、ダウンストリームクラスタの動作状態とは無関係であり、デプロイ後にIPv4/IPv6シングルスタック設定またはデュアルスタック設定のいずれかで動作するように個別に設定できます。ただし、管理クラスタの設定方法は、プロビジョニングフェーズ中に使用可能な通信プロトコルに直接影響します。このフェーズでは、管理クラスタとダウンストリームホストがサポートするプロトコルに応じて、インバンド通信とアウトオブバンド通信の両方が行われる必要があります。BMCやダウンストリームクラスタノードの一部またはすべてがIPv6を使用することが想定される場合、管理クラスタにはデュアルスタックセットアップが必要となります。</para>
<note>
<para>シングルスタックIPv6管理クラスタはまだサポートされていません。</para>
</note>
<para>デュアルスタック機能を実現するには、KubernetesにPodとService用のIPv4とIPv6両方のCIDRを提供する必要があります。ただし、EIBを使用して管理クラスタイメージを構築する前に、他のコンポーネントについても特定の調整が必要です。Metal<superscript>3</superscript>プロビジョニングサービス(Ironic)は、インフラストラクチャや要件に応じて異なる方法で設定できます。</para>
<itemizedlist>
<listitem>
<para>Ironicサービスは、単一のIPアドレスではなくシステム上のすべてのインタフェースでリスンするよう設定できます。したがって、管理クラスタホストで関連インタフェースにIPv4とIPv6の両方のアドレスが割り当てられている限り、プロビジョニング中にいずれのアドレスも使用される可能性があります。この時点では、URL生成(Baremetal
OperatorやBMCなどの他のサービスが利用するため)には、これらのアドレスの1つしか選択できないことに注意してください。結果として、BMCとのIPv6通信を有効にするには、IPv6アドレスを含むBMH定義を処理する際に、Baremetal
OperatorはIPv6
URLを公開して渡すように指示できます。つまり、BMCがIPv6対応と識別された場合、プロビジョニングはIPv6経由でのみ実行され、それ以外の場合はすべてIPv4経由で実行されます。</para>
</listitem>
<listitem>
<para>IPv4とIPv6の両方に解決される単一のホスト名をMetal<superscript>3</superscript>で使用することで、IronicがこれらのアドレスをバインディングとURL作成に使用できるようになります。このアプローチにより、簡単な設定と柔軟な動作が可能になります(プロビジョニングの各ステップでIPv4とIPv6の両方が使用可能)。ただし、既存のDNSサーバ、IP割り当て、およびレコードがすでに設定されているインフラストラクチャが必要です。</para>
</listitem>
</itemizedlist>
<para>両方のケースで、KubernetesはIPv4とIPv6の両方に使用するCIDRを認識する必要があります。そのため、EIBディレクトリ内の<literal>kubernetes/config/server.yaml</literal>に次の行を追加できます。必ずIPv4を最初にリストしてください。</para>
<screen language="yaml" linenumbering="unnumbered">service-cidr: 10.96.0.0/12,fd12:4567:789c::/112
cluster-cidr: 193.168.0.0/18,fd12:4567:789b::/48</screen>
<para>一部のコンテナはホストネットワーキングを利用します。そのため、<literal>network</literal>ディレクトリの下でホストのネットワーク設定を変更して、IPv6接続を有効にします。</para>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    next-hop-address: ${MGMT_GATEWAY_V4}
    next-hop-interface: eth0
  - destination: ::/0
    next-hop-address: ${MGMT_GATEWAY_V6}
    next-hop-interface: eth0
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
    - 2001:4860:4860::8888
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP_V4}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    address:
    - ip: ${MGMT_CLUSTER_IP_V6}
      prefix-length: 128
    dhcp: false
    autoconf: false
    enabled: true</screen>
<para>プレースホルダーをゲートウェイIPアドレス、追加のDNSサーバ(必要な場合)、ネットワークインタフェースのMACアドレス、および管理クラスタのIPアドレスで置き換えてください。代わりにアドレス自動設定を希望する場合は、以下の抜粋を参照すれば、<literal>${MGMT_MAC}</literal>変数を設定するだけで済みます。</para>
<screen language="yaml" linenumbering="unnumbered">interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    enabled: true
    dhcp: true
  ipv6:
    enabled: false
    dhcp: true
    autoconf: true</screen>
<para>最初のオプションから開始し、<literal>kubernetes/helm/values/metal3.yaml</literal>を以下のように作成することで、単一ノード設定の残りのファイルを定義できます。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${MGMT_CLUSTER_IP_V4}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  listenOnAll: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort
metal3-baremetal-operator:
  baremetaloperator:
    externalHttpIPv6: ${MGMT_CLUSTER_IP_V6}</screen>
<para>また、<literal>kubernetes/helm/values/rancher.yaml</literal>を次のようにします。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_IP_V4}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>ここで、<literal>${MGMT_CLUSTER_IP_V4}</literal>と<literal>${MGMT_CLUSTER_IP_V6}</literal>は、ホストに以前に割り当てられたIPアドレスです。</para>
<para>または、IPアドレスの代わりにホスト名を使用するには、<literal>kubernetes/helm/values/metal3.yaml</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  provisioningHostname: `${MGMT_CLUSTER_HOSTNAME}`
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort</screen>
<para>また、<literal>kubernetes/helm/values/rancher.yaml</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_HOSTNAME}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>ここで、<literal>${MGMT_CLUSTER_HOSTNAME}</literal>は、ホストのIPアドレスに解決される完全修飾ドメイン名である必要があります。</para>
<para>詳細については、<link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/mgmt-cluster/dual-stack">SUSE
Edge GitHubリポジトリの「dual-stack」フォルダ</link>を参照してください。ここでディレクトリ構造の例を確認できます。</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>通信機能の設定</title>
<para>このセクションでは、SUSE Telco Cloudを介してデプロイされたクラスタの通信事業者固有の機能の設定について記述および説明します。</para>
<para>ダイレクトネットワークプロビジョニングのデプロイメント方法を使用します。この方法については、自動化されたプロビジョニング(<xref
linkend="atip-automated-provisioning"/>)に関するセクションで説明しています。</para>
<para>このセクションでは、次のトピックについて説明します。</para>
<itemizedlist>
<listitem>
<para>リアルタイム用のカーネルイメージ(<xref linkend="kernel-image-for-real-time"/>):
リアルタイムカーネルで使用するカーネルイメージ。</para>
</listitem>
<listitem>
<para>低レイテンシとハイパフォーマンスのためのカーネル引数(<xref linkend="kernel-args"/>):
通信ワークロードを最大のパフォーマンスと低レイテンシで実行するために、リアルタイムカーネルによって使用されるカーネル引数。</para>
</listitem>
<listitem>
<para>Tunedとカーネル引数によるCPUピニング(<xref linkend="cpu-tuned-configuration"/>):
カーネル引数とTunedプロファイルを使用してCPUを分離します。</para>
</listitem>
<listitem>
<para>CNI設定(<xref linkend="cni-configuration"/>): Kubernetesクラスタで使用するCNI設定。</para>
</listitem>
<listitem>
<para>SR-IOV設定(<xref linkend="sriov"/>): Kubernetesワークロードで使用するSR-IOV設定。</para>
</listitem>
<listitem>
<para>DPDK設定(<xref linkend="dpdk"/>): システムで使用するDPDK設定。</para>
</listitem>
<listitem>
<para>vRANアクセラレーションカード(<xref linkend="acceleration"/>):
Kubernetesワークロードで使用するアクセラレーションカードの設定。</para>
</listitem>
<listitem>
<para>Huge Page (<xref linkend="huge-pages"/>): Kubernetesワークロードで使用するHuge Pageの設定。</para>
</listitem>
<listitem>
<para>KubernetesでのCPUピニング(<xref linkend="cpu-pinning-kubernetes"/>):
CPUピニングを活用するようにKubernetesとアプリケーションを設定する。</para>
</listitem>
<listitem>
<para>NUMA対応のスケジューリング設定(<xref linkend="numa-aware-scheduling"/>):
Kubernetesワークロードで使用するNUMA対応のスケジューリング設定。</para>
</listitem>
<listitem>
<para>Metal LB設定(<xref linkend="metal-lb-configuration"/>):
Kubernetesワークロードで使用するMetal LB設定。</para>
</listitem>
<listitem>
<para>プライベートレジストリ設定(<xref linkend="private-registry"/>):
Kubernetesワークロードで使用するプライベートレジストリ設定。</para>
</listitem>
<listitem>
<para>Precision Time Protocolの設定(<xref linkend="ptp-configuration"/>):
実行中のPTP通信事業者プロファイルの設定ファイル。</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>リアルタイム用のカーネルイメージ</title>
<para>リアルタイムカーネルイメージは必ずしも標準カーネルより優れているとは限りません。リアルタイムカーネルは、特定のユースケース用に調整された別のカーネルです。低レイテンシを実現するために調整されていますが、その結果、スループットが犠牲になります。リアルタイムカーネルは一般的な用途には推奨されませんが、ここでは低レイテンシが重要な要因である通信ワークロード用のカーネルとして推奨されています。</para>
<para>主に4つの機能があります。</para>
<itemizedlist>
<listitem>
<para>決定論的実行:</para>
<para>予測可能性の向上 —
高負荷状態でも重要なビジネスプロセスが期限内に確実に完了し、常に高品質なサービスを提供します。高優先度プロセスのために重要なシステムリソースを保護することで、時間に依存するアプリケーションの予測可能性を向上できます。</para>
</listitem>
<listitem>
<para>低ジッター:</para>
<para>高度に決定論的な技術に基づいてジッターが低く抑えられているため、アプリケーションと実世界との同期を維持できます。これは、継続的に繰り返し計算を行う必要があるサービスで役立ちます。</para>
</listitem>
<listitem>
<para>優先度の継承:</para>
<para>優先度の継承とは、優先度の高いプロセスがある状況において、そのプロセスがタスクを完了するためには優先度の低いプロセスが完了するのを待つ必要がある場合に、優先度の低いプロセスが高優先度を一時的に引き受ける機能です。SUSE
Linux Enterprise Real Timeは、ミッションクリティカルなプロセスにおけるこのような優先度の逆転の問題を解決します。</para>
</listitem>
<listitem>
<para>スレッドの割り込み:</para>
<para>一般的なオペレーティングシステムでは、割り込みモードで実行中のプロセスはプリエンプト可能ではありません。SUSE Linux Enterprise
Real
Timeでは、このような割り込みをカーネルスレッドでカプセル化して割り込み可能にし、ユーザが定義した高優先度プロセスでハード割り込みとソフト割り込みをプリエンプトできます。</para>
<para>ここでは、<literal>SUSE Linux Micro
RT</literal>のようなリアルタイムイメージをインストール済みの場合、カーネルリアルタイムはすでにインストールされています。リアルタイムカーネルイメージは<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>からダウンロードできます。</para>
<note>
<para>リアルタイムカーネルの詳細については、<link
xl:href="https://www.suse.com/products/realtime/">SUSE Real
Time</link>を参照してください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="kernel-args">
<title>低レイテンシとハイパフォーマンスのためのカーネル引数</title>
<para>リアルタイムカーネルを適切に動作させ、通信ワークロードを実行する際には、最高のパフォーマンスと低レイテンシ実現できるようにカーネル引数を設定することが重要です。このユースケースのカーネル引数を設定する際には、いくつかの重要な概念を念頭に置く必要があります。</para>
<itemizedlist>
<listitem>
<para>SUSEリアルタイムカーネルを使用する際には、<literal>kthread_cpus</literal>を削除します。このパラメータは、カーネルスレッドが作成されるCPUを制御します。また、PID
1とカーネルモジュール(kmodユーザスペースヘルパー)のロードにどのCPUが許可されるかも制御します。このパラメータは認識されず、影響は何もありません。</para>
</listitem>
<listitem>
<para><literal>isolcpus</literal>、<literal>nohz_full</literal>、<literal>rcu_nocbs</literal>、および<literal>irqaffinity</literal>を使用して、CPUコアを分離します。CPUピニング技術の包括的なリストについては、「Tunedとカーネル引数によるCPUピニング」(<xref
linkend="cpu-tuned-configuration"/>)の章を参照してください。</para>
</listitem>
<listitem>
<para><literal>domain,nohz,managed_irq</literal>フラグを<literal>isolcpus</literal>カーネル引数に追加します。何もフラグを指定しない場合、<literal>isolcpus</literal>は<literal>domain</literal>フラグのみを指定するのと同等になります。これにより、指定したCPUがカーネルタスクを含むスケジューリングから分離されます。<literal>nohz</literal>フラグは指定されたCPUのスケジューラティックを停止し(CPUで実行できるタスクが1つのみの場合)、<literal>managed_irq</literal>フラグは指定したCPUの管理対象の外部(デバイス)割り込みのルーティングを回避します。NVMeデバイスのIRQラインはカーネルによって完全に管理されており、その結果、非分離(ハウスキーピング)コアにルーティングされることに注意してください。たとえば、このセクションの最後に示されたコマンドラインを実行すると、システムに割り当てられるキューは4つ(および管理/制御キュー)のみとなります。</para>
<screen language="shell" linenumbering="unnumbered">for I in $(grep nvme0 /proc/interrupts | cut -d ':' -f1); do cat /proc/irq/${I}/effective_affinity_list; done | column
39      0       19      20      39</screen>
<para>この動作は、分離されたコア上で実行中の時間的制約のあるアプリケーションに対するディスク I/O
による中断を防止します。ただし、ストレージに重点を置いたワークロードの場合、注意と慎重な設計が必要になる可能性があります。</para>
</listitem>
<listitem>
<para>ティック(カーネルの定期的なタイマー割り込み)を調整します。</para>
<itemizedlist>
<listitem>
<para><literal>skew_tick=1</literal>:
ティックは同時に発生する場合があります。<literal>skew_tick=1</literal>を指定すると、すべてのCPUがタイマーティックを正確に同じタイミングで受信するのではなく、わずかにオフセットされたタイミングで発生させます。これにより、システムジッターが軽減され、より一貫性があり低い割り込み応答時間(レイテンシに敏感なアプリケーションにとって不可欠な要件)が可能になります。</para>
</listitem>
<listitem>
<para><literal>nohz=on</literal>: アイドル状態のCPUにおける定期的なタイマーティックを停止します。</para>
</listitem>
<listitem>
<para><literal>nohz_full=&lt;cpu-cores&gt;</literal>:
リアルタイムアプリケーション専用の指定されたCPUにおける定期的なタイマーティックを停止します。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>mce=off</literal>を指定して、マシンチェック例外(MCE)の処理を無効にします。MCEはプロセッサによって検出されるハードウェアエラーであり、無効にすることで、ノイズの多いログを回避できます。</para>
</listitem>
<listitem>
<para><literal>nowatchdog</literal>を追加して、タイマーのハード割り込みコンテキストで実行されるタイマーとして実装されるソフトロックアップウォッチドッグを無効にします。有効期限が切れると(すなわち、ソフトロックアップが検出されると)、(ハード割り込みコンテキストで)警告が出力され、あらゆるレイテンシターゲットを実行します。有効期限が切れていない場合でも、タイマーリストに追加され、タイマー割り込みのオーバーヘッドがわずかに増加します。このオプションは、NMIウォッチドッグも無効にするため、NMIが干渉できなくなります。</para>
</listitem>
<listitem>
<para><literal>nmi_watchdog=0</literal>は、NMI (Non-Maskable
Interrupt)ウォッチドッグを無効にします。<literal>nowatchdog</literal>が使用される場合は省略できます。</para>
</listitem>
<listitem>
<para>RCU
(Read-Copy-Update)は、共有データに対して多数のリーダが同時にロックなしでアクセスできるようにするカーネルメカニズムです。RCUコールバックは、一定の「猶予期間」後にトリガされる関数で、すべての以前のリーダが終了していることを確認し、古いデータを安全に再利用できるようにします。SUSEでは、特に機密性の高いワークロード向けにRCUを微調整し、これらのコールバックを専用(ピニングされた)CPUからオフロードすることで、カーネル操作が重要な時間的制約のあるタスクと干渉しないようにしています。</para>
<itemizedlist>
<listitem>
<para><literal>rcu_nocbs</literal>でピニングされたCPUを指定し、RCUコールバックがそのCPU上で実行されないようにします。これにより、リアルタイムワークロードのジッターとレイテンシを軽減できます。</para>
</listitem>
<listitem>
<para><literal>rcu_nocb_poll</literal>は、コールバック処理が必要かどうかを確認するために、no-callback
CPUを定期的に「ポーリング」させます。これにより、割り込みのオーバーヘッドを削減できます。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_cpu_stall_suppress=1</literal>は、RCU
CPUストール警告を抑制します。これは、高負荷のリアルタイムシステムでは偽陽性となる場合があります。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_expedited=1</literal>は、RCU操作の猶予期間を短縮し、読み取り側のクリティカルセクションの応答性を向上させます。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_normal_after_boot=1</literal>:
rcu_expeditedとともに使用する場合、システム起動後にRCUが通常の(非迅速)動作に戻ることができます。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_task_stall_timeout=0</literal>は、RCUタスクのストール検出機能を無効にし、長時間実行されるRCUタスクによる潜在的な警告やシステム停止を防止します。</para>
</listitem>
<listitem>
<para><literal>rcutree.kthread_prio=99</literal>は、RCUコールバックカーネルスレッドの優先度を可能な限り最高(99)に設定し、必要に応じてRCUコールバックがスケジュールされ、迅速に処理されるようにします。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Metal3とCluster
APIに<literal>ignition.platform.id=openstack</literal>を追加し、クラスタのプロビジョニング/プロビジョニング解除が正常に実行されるようにします。これは、Openstack
Ironic由来のMetal3 Pythonエージェントによって使用されます。</para>
</listitem>
<listitem>
<para><literal>intel_pstate=passive</literal>を削除します。このオプションは、<literal>intel_pstate</literal>を汎用cpufreqガバナーと連携するように設定しますが、この連携を行うには、副作用として、ハードウェア管理P状態(<literal>HWP</literal>)を無効にします。ハードウェアのレイテンシを削減するため、このオプションはリアルタイムワークロードには推奨されません。</para>
</listitem>
<listitem>
<para><literal>intel_idle.max_cstate=0
processor.max_cstate=1</literal>を<literal>idle=poll</literal>に置き換えます。C-Stateの遷移を回避するには、<literal>idle=poll</literal>オプションを使用してC-Stateの遷移を無効にし、CPUを最高のC-Stateに維持します。<literal>intel_idle.max_cstate=0</literal>オプションは、<literal>intel_idle</literal>を無効にするため、<literal>acpi_idle</literal>が使用され、<literal>acpi_idle.max_cstate=1</literal>がacpi_idleの最大のC-stateを設定します。AMD64/Intel
64アーキテクチャでは、最初のACPI
C-Stateは常に<literal>POLL</literal>ですが、<literal>poll_idle()</literal>関数を使用しており、定期的にクロックを読み取ることで、タイムアウト後に
<literal>do_idle()</literal>でメインループを再起動する際に、若干のレイテンシが発生する可能性があります(これには<literal>TIF_POLL</literal>タスクフラグのクリアと設定も含まれます)。これに対して、<literal>idle=poll</literal>はタイトなループで実行され、タスクが再スケジュールされるのをビジーウェイトします。これにより、アイドル状態から抜け出すまでのレイテンシが最小化されますが、その代償としてCPUがアイドルスレッドでフルスピードで稼働し続けることになります。</para>
</listitem>
<listitem>
<para>BIOSのC1Eを無効にします。このオプションは、BIOSでC1E状態を無効にし、アイドル時にCPUがCIE状態になるのを回避します。C1E状態は、CPUがアイドル状態のときにレイテンシを発生される可能性のある低電力状態です。</para>
</listitem>
</itemizedlist>
<para>このドキュメントの残りの部分では、追加のパラメータ(Huge PageやIOMMUなど)について説明します。</para>
<para>以下は、前述の調整を含む32コアのIntelサーバのカーネル引数の例です。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
<para>以下は、64コアのAMDサーバのもう1つの設定例です。128個の論理プロセッサ(<literal>0-127</literal>)のうち、最初の8コア(<literal>0-7</literal>)はハウスキーピング用に指定され、残りの120コア(<literal>8-127</literal>)はアプリケーション用に固定されます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=575291cf-74e8-42cf-8f2c-408a20dc00b8 skew_tick=1 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack amd_iommu=on iommu=pt irqaffinity=0-7 isolcpus=domain,nohz,managed_irq,8-127 nohz_full=8-127 rcu_nocbs=8-127 mce=off nohz=on net.ifnames=0 nowatchdog nmi_watchdog=0 nosoftlockup quiet rcu_nocb_poll rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
</section>
<section xml:id="cpu-tuned-configuration">
<title>Tunedとカーネル引数によるCPUピニング</title>
<para><literal>tuned</literal>は、さまざまな事前定義済みプロファイルを使用して、システム状態を監視し、パフォーマンスを最適化するシステムチューニングツールです。主要な機能は、リアルタイムアプリケーションのなどの特定のワークロードに対してCPUコアを分離する機能です。この機能により、OSがこれらのコアを使用するのを防止し、潜在的にレイテンシが増加するのを回避します。</para>
<para>この機能を有効にして設定するには、まず、分離するCPUコアのプロファイルを作成します。この例では、64コアのうち、60コア(<literal>1-30、33-62</literal>)をアプリケーション専用にし、残りの4コアをハウスキーピングに使用します。分離されたCPUの設計は、リアルタイムアプリケーションに大きく依存することに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>次に、GRUBオプションを変更して、CPUコアと、CPUの使用法に関するその他の重要なパラメータを分離する必要があります。現在のハードウェア仕様で次のオプションをカスタマイズすることが重要です。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain、nohz、&#x200B;managed_irq、1-30、33-62</para></entry>
<entry align="left" valign="top"><para>コア1-30と3-62を分離します。<literal>domain</literal>は、これらのCPUが分離ドメインに属していることを示します。<literal>nohz</literal>は、これらの分離されたCPUがアイドル状態である場合にティックレス操作を有効にし、割り込みを減らします。<literal>managed_irq</literal>は、ピニングされたCPUがIRQの対象とならないように分離します。これは、ほとんどのIRQをハウスキーピングコアに指定する<literal>irqaffinity=0-7</literal>を考慮しています。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルは分離されたCPU全体でタイマー割り込みをずらすことができます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>有効にすると、カーネルの定期的なタイマー割り込み(「ティック」)は、アイドル状態のCPUコアで停止します。これにより、主にハウスキーピング用CPU
(<literal>0、31、32、63</literal>)にメリットをもたらします。これにより、電力が節約され、汎用コアでの不要なウェイクアップが削減されます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>分離されたコアの場合、これによりティックが停止され、CPUが単一のアクティブなタスクを実行している場合でも停止されます。つまり、CPUは完全なティックレスモード(または「dyntick」)で動作します。カーネルは、実際に必要な場合にのみタイマー割り込みを発生させます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>このオプションは、指定のCPUコアからRCUコールバック処理をオフロードします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocb_poll</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>このオプションを設定すると、no-RCU-callback
CPUは、他のCPUによって明示的にウェイクアップされるのではなく、コールバック処理が必要かどうかを確認するために定期的に「ポーリング」します。これにより、割り込みオーバーヘッドを削減できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルはハウスキーピング用コアに割り込みを実行できるようになります。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>これにより、アイドル状態から抜け出すまでのレイテンシが最小化されますが、その代償として、CPUがアイドルスレッドでフルスピードで稼働し続けることになります。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>このオプションはNMIウォッチドッグのみを無効にします。<literal>nowatchdog</literal>が設定されている場合は省略できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>このオプションは、タイマーのハード割り込みコンテキストで実行されるタイマーとして実装されるソフトロックアップウォッチドッグを無効にします。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>次のコマンドでGRUB設定を変更し、上記の変更を次回ブート時に適用します。</para>
<para>上記パラメータを使用して<literal>/etc/default/grub</literal>ファイルを編集します。ファイルは次のようになります。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>GRUB設定を更新します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを検証するには、次のコマンドを使用してカーネルコマンドラインを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<para>CPU設定を調整するために使用可能な別のスクリプトがあります。これは基本的には次の手順を実行します。</para>
<itemizedlist>
<listitem>
<para>CPUガバナーを<literal>パフォーマンス</literal>に設定します。</para>
</listitem>
<listitem>
<para>分離されたCPUへのタイマーのマイグレーションの設定を解除します。</para>
</listitem>
<listitem>
<para>kdaemonスレッドをハウスキーピング用CPUに移行します。</para>
</listitem>
<listitem>
<para>分離したCPUレイテンシを可能な限り低い値に設定します。</para>
</listitem>
<listitem>
<para>vmstatの更新を300秒に遅延させます。</para>
</listitem>
</itemizedlist>
<para>スクリプトは、<link
xl:href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.4/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh">SUSE
Telco Cloudサンプルリポジトリ</link>で入手できます。</para>
</section>
<section xml:id="cni-configuration">
<title>CNI設定</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal>はSUSE Telco
CloudのデフォルトのCNIプラグインです。RKE2クラスタでCiliumをデフォルトのプラグインとして有効にするには、<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに次の設定が必要です。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>これはコマンドライン引数でも指定できます。具体的には、<literal>/etc/systemd/system/rke2-server</literal>ファイルのサーバの行に<literal>--cni=cilium</literal>を追加します。</para>
<para>次のセクション(<xref linkend="option2-sriov-helm"/>)で説明する<literal>SR-IOV</literal>
Network
Operatorを使用するには、<literal>Multus</literal>とともに、<literal>Cilium</literal>や<literal>Calico</literal>などの別のCNIプラグインをセカンダリプラグインとして使用します。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</section>
<section xml:id="id-calico">
<title>Calico</title>
<para><literal>Calico</literal>は、SUSE Edge for
Telco用の別のCNIプラグインです。RKE2クラスタ上でデフォルトプラグインとしてCalicoを有効にするには、<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに次の設定が必要です。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- calico</screen>
<para>これは、コマンドライン引数で指定することもできます。つまり、<literal>--cni=calico</literal>を<literal>/etc/systemd/system/rke2-server</literal>ファイルのサーバ行に追加します。</para>
<para>次のセクション(<xref linkend="option2-sriov-helm"/>)で説明する<literal>SR-IOV</literal>
Network
Operatorを使用するには、<literal>Multus</literal>とともに、<literal>Cilium</literal>や<literal>Calico</literal>などの別のCNIプラグインをセカンダリプラグインとして使用します。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- calico</screen>
<note>
<para>CNIプラグインの詳細については、「<link
xl:href="https://docs.rke2.io/install/network_options">Network Options
(ネットワークオプション)</link>」を参照してください。</para>
</note>
</section>
<section xml:id="id-bond-cni">
<title>Bond CNI</title>
<para>一般的に、ボンディングは複数のネットワークインタフェースを単一の論理的な「ボンディング」インタフェースに集約する方法を提供します。これは通常、冗長なネットワーキングパスを導入することでサービスの可用性を向上させるために使用されますが、特定のボンディングモードで帯域幅を増やすためにも使用できます。multusと組み合わせてBond
CNIプラグインでは、以下のCNIプラグインがサポートされています。</para>
<itemizedlist>
<listitem>
<para>MACVLAN</para>
</listitem>
<listitem>
<para>ホストデバイス</para>
</listitem>
<listitem>
<para>SR-IOV</para>
</listitem>
</itemizedlist>
<section xml:id="id-bond-cni-with-macvlan">
<title>Bond CNIとMACVLAN</title>
<para>Bond
CNIプラグインとMACVLANを組み合わせて使用するには、コンテナ内に2つの空きインタフェースが必要です。以下の例では、「enp8s0」と「enp9s0」を使用しています。まず、これらのインタフェースに対するネットワークアタッチメント定義を作成します。</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp8s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp9s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para>この後で、ボンド自体のネットワークアタッチメント定義を追加します。</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>ここでのIPアドレス割り当ては静的であり、ボンドのアドレスを/24ネットワーク上の「192.168.200.100」と定義し、ゲートウェイはネットワークで最初に利用可能なアドレス上に配置されます。ボンドのネットワークアタッチメントでは、必要なボンドのタイプも定義します。この場合はactive-backupです。</para>
<para>このボンドを使用するには、ポッドがすべてのインタフェースについて認識している必要があります。ポッド定義例は次のようになります。</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-conf",
"interface": "net1"
},
{"name": "enp9s0-conf",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
<para>アノテーションがすべてのネットワークをどのように参照しているか、およびインタフェース「enp8s0 →
net1」と「enp9s0→net2」間のマッピングをどのように定義しているかに注意してください。</para>
</section>
<section xml:id="id-bond-cni-with-host-device">
<title>Bond CNIとホストデバイス</title>
<para>Bond
CNIプラグインとホストデバイスを組み合わせて使用するには、ホスト上に2つの空きインタフェースが必要です。これらのインタフェースはコンテナにマッピングされます。以下の例では、「enp8s0」と「enp9s0」を使用しています。まず、これらのインタフェースに対するネットワークアタッチメント定義を作成します。</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp8s0",
          "ipam": {}
        }]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp9s0",
          "ipam": {}
        }]
    }'</screen>
<para>この後で、ボンド自体のネットワークアタッチメント定義を追加します。これはMACVLANの使用例と同様です。</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>ここでのIPアドレス割り当ては静的であり、ボンドのアドレスを/24ネットワーク上の「192.168.200.100」と定義し、ゲートウェイはネットワークで最初に利用可能なアドレス上に配置されます。ボンドのネットワークアタッチメントでは、ボンドのタイプを定義します。この場合はactive-backupです。</para>
<para>このボンドを使用するには、ポッドがすべてのインタフェースについて認識している必要があります。ホストデバイスとのボンドのポッド定義例は次のようになります。</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-hostdev",
"interface": "net1"
},
{"name": "enp9s0-hostdev",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
</section>
<section xml:id="id-bond-cni-with-sr-iov">
<title>Bond CNIとSR-IOV</title>
<para>Bond CNIをSR-IOVと組み合わせて使用するのは非常に簡単です。SR-IOVの設定方法の詳細については、<xref
linkend="sriov"/>を参照してください。そこに記載されているとおりに、<literal>SriovNetworkNodePolicies</literal>を作成し、<literal>resourceNames</literal>や仮想機能の数などを定義する必要があります。<literal>resourceNames</literal>は、ポッド定義でインタフェースとして使用される<literal>SriovNetwork</literal>によって使用されます。ボンドの定義は、MACVLANとホストデバイスの場合とまったく同じです。</para>
<note>
<para>Bond
CNIとSR-IOVの組み合わせは、カーネルドライバを使用するSRIOV仮想機能(VF)にのみ適用されます。DPDKワークロードで使用されるようなユーザスペースドライバのVFは、Bond
CNIでボンディングできません。</para>
</note>
</section>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOVを使用すると、ネットワークアダプタなどのデバイスで、そのリソースへのアクセスをさまざまな<literal>PCIe</literal>ハードウェア機能の間で分離することができます。<literal>SR-IOV</literal>をデプロイするにはさまざまな方法がありますが、ここでは2つの方法を示します。</para>
<itemizedlist>
<listitem>
<para>オプション1: <literal>SR-IOV</literal> CNIデバイスプラグインと設定マップを使用して適切に設定する。</para>
</listitem>
<listitem>
<para>オプション2 (推奨): Rancher Primeから<literal>SR-IOV</literal>
Helmチャートを使用してこのデプロイメントを簡単に行えるようにする。</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">オプション1 - SR-IOV
CNIデバイスプラグインと設定マップをインストールして適切に設定する</emphasis></para>
<itemizedlist>
<listitem>
<para>デバイスプラグインの設定マップを準備する</para>
</listitem>
</itemizedlist>
<para>設定マップに入力する情報を<literal>lspci</literal>コマンドから取得します。</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>設定マップは<literal>JSON</literal>ファイルで構成され、このファイルで、フィルタを使用して検出を行うデバイスを記述し、インタフェースのグループを作成します。フィルタとグループを理解することが重要です。フィルタはデバイスを検出するために使用され、グループはインタフェースを作成するために使用されます。</para>
<para>フィルタを設定することもできます。</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (アクセラレータカード)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (ドライバ)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (物理インタフェース名)</para>
</listitem>
</itemizedlist>
<para>フィルタを設定して、より複雑なインタフェース構文に一致させることもできます。次に例を示します。</para>
<itemizedlist>
<listitem>
<para>pfNames:
<literal>["eth1#1,2,3,4,5,6"]</literal>または<literal>[eth1#1-6]</literal>
(物理インタフェース名)</para>
</listitem>
</itemizedlist>
<para>グループに関連して、<literal>FEC</literal>カード用のグループを1つと、<literal>Intel</literal>カード用のグループを1つ作成し、さらに、ユースケースに応じてプレフィックスを作成することもできます。</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>リソースグループを検出して作成し、一部の<literal>VF</literal>をPodに割り当てる組み合わせは多数あります。</para>
<note>
<para>フィルタとグループの詳細については、「<link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sriov-network-device-plugin
(sr-iovネットワークデバイスプラグイン)</link>」を参照してください。</para>
</note>
<para>フィルタとグループを設定して、ハードウェアとユースケースに応じたインタフェースに一致させると、使用する例が次の設定マップに表示されます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para><literal>daemonset</literal>ファイルを準備して、デバイスプラグインをデプロイします。</para>
</listitem>
</itemizedlist>
<para>このデバイスプラグインは、複数のアーキテクチャ(<literal>arm</literal>、<literal>amd</literal>、<literal>ppc64le</literal>)をサポートしています。したがって、同じファイルを異なるアーキテクチャに使用して、各アーキテクチャに複数の<literal>daemonset</literal>をデプロイできます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>設定マップと<literal>daemonset</literal>を適用すると、デバイスプラグインがデプロイされ、インタフェースが検出されてPodで使用できるようになります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Podで使用するノードでインタフェースが検出されて利用可能であることを確認します。</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para><literal>FEC</literal>は<literal>intel.com/intel_fec_5g</literal>で、値は1です。</para>
</listitem>
<listitem>
<para>Helmチャートを使用せずに、デバイスプラグインと設定マップを使用してデプロイした場合、<literal>VF</literal>は、<literal>intel.com/intel_sriov_odu</literal>または<literal>intel.com/intel_sriov_oru</literal>です。</para>
</listitem>
</itemizedlist>
<important>
<para>ここにインタフェースがない場合、そのインタフェースをPodで使用することはできないため、続行しても意味がありません。まず、設定マップとフィルタを確認して問題を解決してください。</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">オプション2 (推奨) - Rancherを使用し、SR-IOV
CNIおよびデバイスプラグイン用のHelmチャートを使用したインストール</emphasis></para>
<itemizedlist>
<listitem>
<para>Helmがない場合は入手します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>SR-IOVをインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">helm install sriov-crd oci://registry.suse.com/edge/charts/sriov-crd -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/charts/sriov-network-operator -n sriov-network-operator</screen>
<itemizedlist>
<listitem>
<para>デプロイしたリソースのcrdとPodを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>ノードのラベルを確認します。</para>
</listitem>
</itemizedlist>
<para>すべてのリソースが実行されていると、ラベルがノードに自動的に表示されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para><literal>daemonset</literal>を確認し、新しい<literal>sriov-network-config-daemon</literal>および<literal>sriov-rancher-nfd-worker</literal>がアクティブで準備できていることを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>数分後(更新に最大で10分かかる可能性があります)、ノードが検出されて、<literal>SR-IOV</literal>の機能が設定されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>検出されたインタフェースを確認します。</para>
</listitem>
</itemizedlist>
<para>検出されたインタフェースはネットワークデバイスのPCIアドレスである必要があります。この情報は、ホストで<literal>lspci</literal>コマンドを使用して確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>ここでインタフェースが検出されていない場合は、インタフェースが次の設定マップに存在することを確認してください。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>ここにデバイスがない場合は、設定マップを編集して、検出すべき適切な値を追加します(<literal>sriov-network-config-daemon</literal>デーモンセットの再起動が必要になります)。</para>
</note>
<itemizedlist>
<listitem>
<para><literal>NetworkNodeポリシー</literal>を作成して<literal>VF</literal>を設定します。</para>
</listitem>
</itemizedlist>
<para><literal>VF</literal>
(<literal>numVfs</literal>)がデバイス(<literal>rootDevices</literal>)から作成され、ドライバ<literal>deviceType</literal>と<literal>MTU</literal>が設定されます。</para>
<note>
<para><literal>resourceName</literal>フィールドには特殊文字を含めないでください。また、このフィールドはクラスタ全体で一意である必要があります。この例では、<literal>dpdk</literal>を<literal>sr-iov</literal>と組み合わせて使用するため、<literal>deviceType:
vfio-pci</literal>を使用しています。<literal>dpdk</literal>を使用しない場合は、deviceTypeを<literal>deviceType:
netdevice</literal> (デフォルト値)にする必要があります。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>設定を検証します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>sr-iovネットワークを作成します(別のネットワークが必要な場合のオプション)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>作成されたネットワークを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal>
(データプレーン開発キット)は、パケットの高速処理用の一連のライブラリとドライバです。DPDKは、広範なCPUアーキテクチャ上で実行されるパケット処理ワークロードを高速化するために使用されます。DPDKには、データプレーンライブラリと、以下のために最適化されたネットワークインタフェースコントローラ(<literal>NIC</literal>)ドライバが含まれています。</para>
<orderedlist numeration="arabic">
<listitem>
<para>キューマネージャはロックなしのキューを実装します。</para>
</listitem>
<listitem>
<para>バッファマネージャは固定サイズのバッファを事前割り当てします。</para>
</listitem>
<listitem>
<para>メモリマネージャは、メモリ内にオブジェクトのプールを割り当て、リングを使用してフリーオブジェクトを格納します。オブジェクトがすべての<literal>DRAM</literal>チャンネルに均等に分散されるようにします。</para>
</listitem>
<listitem>
<para>ポールモードドライバ(<literal>PMD</literal>)は、非同期通知なしで動作するように設計されているため、オーバーヘッドが軽減されます。</para>
</listitem>
<listitem>
<para>パケット処理を開発するためのヘルパーである一連のライブラリとしてのパケットフレームワーク。</para>
</listitem>
</orderedlist>
<para>次の手順では、<literal>DPDK</literal>を有効にする方法と、<literal>DPDK</literal>インタフェースが使用する<literal>NIC</literal>から<literal>VF</literal>を作成する方法を示します。</para>
<itemizedlist>
<listitem>
<para><literal>DPDK</literal>パッケージをインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>カーネルパラメータ:</para>
</listitem>
</itemizedlist>
<para>DPDKを使用するには、ドライバをいくつか使用して、カーネルの特定のパラメータを有効にします。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、DPDKインタフェースに<literal>vfio</literal>ドライバを使用できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommuまたはamd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、<literal>VF</literal>に<literal>vfio</literal>を使用できます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>これらのパラメータを有効にするには、各パラメータを<literal>/etc/default/grub</literal>ファイルに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para><literal>vfio-pci</literal>カーネルモジュールを読み込み、<literal>NIC</literal>で<literal>SR-IOV</literal>を有効にします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para><literal>NIC</literal>から仮想機能(<literal>VF</literal>)をいくつか作成します。</para>
</listitem>
</itemizedlist>
<para>たとえば、2つの異なる<literal>NIC</literal>に対して<literal>VF</literal>を作成するには、次のコマンドが必要です。</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>新しいVFを<literal>vfio-pci</literal>ドライバにバインドします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>設定が正しく適用されたことを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>vRANアクセラレーション(<literal>Intel ACC100/ACC200</literal>)</title>
<para>4Gから5Gネットワークへの移行に伴い、多くの通信サービスプロバイダが仮想化無線アクセスネットワーク(<literal>vRAN</literal>)アーキテクチャを採用して、チャンネル容量を増やし、エッジベースのサービスとアプリケーションのデプロイメントを容易にしようとしています。vRANソリューションは、ネットワーク上のリアルタイムのトラフィックと需要の量に応じて容量を柔軟に増減できるため、低レイテンシのサービスを提供するのに理想的です。</para>
<para>4Gおよび5Gで最も計算負荷が高いワークロードの1つがRANレイヤ1
(<literal>L1</literal>)の<literal>FEC</literal>です。これは、信頼性の低い通信チャンネルやノイズの多い通信チャンネルでのデータ伝送エラーを解消するものです。<literal>FEC</literal>技術は、4Gまたは5Gデータの一定数のエラーを検出して訂正することで、再送信の必要性を解消します。<literal>FEC</literal>アクセラレーショントランザクションにはセルの状態情報が含まれないため、簡単に仮想化でき、プールするメリットとセルの容易な移行が実現します。</para>
<itemizedlist>
<listitem>
<para>カーネルパラメータ</para>
</listitem>
</itemizedlist>
<para><literal>vRAN</literal>アクセラレーションを有効にするには、次のカーネルパラメータを有効にする必要があります(まだ存在しない場合)。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、DPDKインタフェースにvfioを使用できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommuまたはamd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、VFにvfioを使用できます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>GRUBファイル<literal>/etc/default/grub</literal>を変更して、これらのパラメータをカーネルコマンドラインに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを確認するには、コマンドラインを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>vfio-pciカーネルモジュールを読み込み、<literal>vRAN</literal>アクセラレーションを有効にします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>インタフェース情報Acc100を取得します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>物理インタフェース(<literal>PF</literal>)を<literal>vfio-pci</literal>ドライバにバインドします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>仮想機能(<literal>VF</literal>)を物理インタフェース(<literal>PF</literal>)から作成します。</para>
</listitem>
</itemizedlist>
<para>2つの<literal>VF</literal>を<literal>PF</literal>から作成し、次の手順に従って<literal>vfio-pci</literal>にバインドします。</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>提案された設定ファイルを使用してacc100を設定します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>FEC PFから作成した新しいVFを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>Huge Page</title>
<para>プロセスが<literal>RAM</literal>を使用すると、<literal>CPU</literal>はそのメモリ領域をプロセスが使用中であるとマークします。効率を高めるために、<literal>CPU</literal>は<literal>RAM</literal>をチャンクで割り当てます。多くのプラットフォームでは<literal>4K</literal>バイトがチャンクのデフォルト値です。これらのチャンクをページと呼び、ディスクなどにスワップできます。</para>
<para>プロセスのアドレススペースは仮想であるため、<literal>CPU</literal>とオペレーティングシステムは、どのページがどのプロセスに属していて、各ページがどこに保管されているかを覚えておく必要があります。ページ数が多いほど、メモリマッピングの検索に時間がかかります。プロセスが<literal>1GB</literal>のメモリを使用する場合、検索するエントリは262,144個になります(<literal>1GB</literal>
/ <literal>4K</literal>)。1つのページテーブルエントリが8バイトを消費する場合、<literal>2MB</literal>
(262,144 * 8)を検索することになります。</para>
<para>最新の<literal>CPU</literal>アーキテクチャはデフォルトより大きいページをサポートしているので、<literal>CPU/OS</literal>が検索するエントリが減少します。</para>
<itemizedlist>
<listitem>
<para>カーネルパラメータ</para>
</listitem>
</itemizedlist>
<para>Huge Pageを有効にするには、次のカーネルパラメータを追加する必要があります。この例では、40個の1Gページを設定していますが、Huge
Pageのサイズと正確な数は、アプリケーションのメモリ要件に合わせて調整する必要があります。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、Huge Pageを1Gに設定できます</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>前に定義したHuge Pageの数です</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageを取得するためのデフォルト値です</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>GRUBファイル<literal>/etc/default/grub</literal>を変更して、これらのパラメータを<literal>GRUB_CMDLINE_LINUX</literal>に追加します。</para>
<screen language="shell" linenumbering="unnumbered">default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを検証するには、次のコマンドラインを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Huge Pageの使用</para>
</listitem>
</itemizedlist>
<para>Huge Pageを使用するには、Huge Pageをマウントする必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Kubernetesワークロードをデプロイし、リソースとボリュームを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-kubernetes">
<title>KubernetesでのCPUピニング</title>
<section xml:id="id-prerequisite">
<title>前提条件</title>
<para>こちらのセクション(<xref
linkend="cpu-tuned-configuration"/>)で説明したパフォーマンスプロファイルに合わせて<literal>CPU</literal>が調整されていること。</para>
</section>
<section xml:id="id-configure-kubernetes-for-cpu-pinning">
<title>CPUピニング用のKubernetesの設定</title>
<para><literal>RKE2</literal>クラスタでCPU管理を実装するためにkubelet引数を設定します。次の例のような設定ブロックを<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに追加します。<literal>kubelet-reserved</literal>および
<literal>system-reserved</literal>引数でハウスキーピング用CPUコアを指定していることを確認します。</para>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=0,31,32,63"
- "system-reserved=cpu=0,31,32,63"</screen>
</section>
<section xml:id="id-leveraging-pinned-cpus-for-workloads">
<title>ワークロードにピニングされたCPUを活用する</title>
<para>kubeletで定義された<literal>静的ポリシー</literal>を使ってCPUピニング機能を使用する方法は、ワークロードに対して定義した要求と制限に応じて3つあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>BestEffort</literal> QoSクラス:
<literal>CPU</literal>に対して要求または制限を定義していない場合、Podはシステムで使用できる最初の<literal>CPU</literal>でスケジュールされます。</para>
<para><literal>BestEffort</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para><literal>Burstable</literal> QoSクラス:
CPUに対して要求を定義し、その要求が制限と同じではない場合、またはCPUの要求がない場合。</para>
<para><literal>Burstable</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>または</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para><literal>Guaranteed</literal> QoSクラス: CPUに対して要求を定義し、その要求が制限と同じである場合。</para>
<para><literal>Guaranteed</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="numa-aware-scheduling">
<title>NUMA対応のスケジューリング</title>
<para>Non-Uniform Memory AccessまたはNon-Uniform Memory Architecture
(<literal>NUMA</literal>)は、<literal>SMP</literal>
(マルチプロセッサ)アーキテクチャにおいて使用される物理メモリ設計であり、メモリアクセス時間がプロセッサからのメモリの相対的な位置によって異なります。<literal>NUMA</literal>では、プロセッサは専用のローカルメモリに、非ローカルメモリ、つまり別のプロセッサにローカルなメモリや複数のプロセッサで共有されているメモリよりも高速にアクセスできます。</para>
<section xml:id="id-identifying-numa-nodes">
<title>NUMAノードの特定</title>
<para><literal>NUMA</literal>ノードを特定するには、システムで次のコマンドを使用します。</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>この例では、<literal>NUMA</literal>ノードが1つだけあり、64個の<literal>CPU</literal>が表示されています。</para>
<para><literal>NUMA</literal>は<literal>BIOS</literal>で有効にする必要があります。<literal>dmesg</literal>にブート時のNUMA初期化レコードがない場合、カーネルリングバッファ内の<literal>NUMA</literal>関連のメッセージが上書きされた可能性があります。</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>MetalLB</title>
<para><literal>MetalLB</literal>は、ベアメタルKubernetesクラスタ用のロードバランサの実装であり、<literal>L2</literal>や<literal>BGP</literal>などの標準ルーティングプロトコルをアドバタイズプロトコルとして使用します。ベアメタル環境ではKubernetesサービスタイプ<literal>LoadBalancer</literal>を使用する必要があるため、Kubernetesクラスタ内のサービスを外部に公開するために使用できるのは、ネットワークロードバランサです。</para>
<para><literal>RKE2</literal>クラスタで<literal>MetalLB</literal>を有効にするには、次の手順を実行する必要があります。</para>
<itemizedlist>
<listitem>
<para>次のコマンドを使用して<literal>MetalLB</literal>をインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/metallb
  targetNamespace: metallb-system
  version: 304.0.0+up0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
  version: 304.0.1+up0.3.0
  createNamespace: true
EOF</screen>
<itemizedlist>
<listitem>
<para><literal>IpAddressPool</literal>および<literal>L2advertisement</literal>の設定を作成します。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para><literal>VIP</literal>を公開するためのエンドポイントサービスを作成します。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para><literal>VIP</literal>が作成され、<literal>MetalLB</literal>のPodが実行中であることを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>プライベートレジストリ設定</title>
<para><literal>Containerd</literal>をプライベートレジストリに接続するように設定し、そのプライベートレジストリを使用して各ノードにプライベートイメージをプルできます。</para>
<para>起動時に、<literal>RKE2</literal>は、<literal>registries.yaml</literal>ファイルが<literal>/etc/rancher/rke2/</literal>に存在するかどうかを確認し、このファイルで定義されたレジストリを使用するように<literal>containerd</literal>に指示します。プライベートレジストリを使用するには、このファイルを、レジストリを使用する各ノードにルートとして作成します。</para>
<para>プライベートレジストリを追加するには、ファイル<literal>/etc/rancher/rke2/registries.yaml</literal>を作成して次の内容を設定します。</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>または、認証を使用しない場合は次のように設定します。</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>レジストリの変更を有効にするには、ノード上でRKE2を起動する前にこのファイルを設定するか、または設定した各ノードでRKE2を再起動します。</para>
<note>
<para>この詳細については、「<link
xl:href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html#_registries_configuration_file">containerd
registry configuration rke2 (containerdレジストリ設定rke2)</link>」を確認してください。</para>
</note>
</section>
<section xml:id="ptp-configuration">
<title>Precision Time Protocol</title>
<para>Precision Time Protocol (PTP)は、
電気電子学会(IEEE)によって開発されたネットワークプロトコルで、コンピュータネットワークにおけるサブマイクロ秒単位の時間同期を可能にします。PTPは、その誕生以来数十年にわたり、多くの業界で利用されてきました。最近では、5Gネットワークの重要な要素として、通信ネットワークにおける採用が急増しています。比較的シンプルなプロトコルであるものの、アプリケーションに応じて設定が大幅に異なります。そのため、複数のプロファイルが定義され標準化されています。</para>
<para>このセクションでは、通信事業者固有のプロファイルのみについて説明します。そのため、NICにタイムスタンプ機能とPTPハードウェアクロック(PHC)が搭載されていることを前提とします。現在、すべての通信事業者グレードのネットワークアダプタにはハードウェアPTPサポートが付属していますが、以下のコマンドでその機能を確認できます。</para>
<screen language="console" linenumbering="unnumbered"># ethtool -T p1p1
Time stamping parameters for p1p1:
Capabilities:
        hardware-transmit
        software-transmit
        hardware-receive
        software-receive
        software-system-clock
        hardware-raw-clock
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
        off
        on
Hardware Receive Filter Modes:
        none
        all</screen>
<para><literal>p1p1</literal>を、PTPに使用されるインタフェースの名前に置き換えます。</para>
<para>以下のセクションでは、SUSE Telco
CloudにPTPをインストールして設定する方法について具体的に説明しますが、PTPの基本的な概念を理解していることが前提となります。PTPの概要と、SUSE
Telco Cloudに含まれる実装については、<link
xl:href="https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html">https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html</link>を参照してください。</para>
<section xml:id="id-install-ptp-software-components">
<title>PTPソフトウェアコンポーネントのインストール</title>
<para>SUSE Telco
Cloudでは、PTP実装が<literal>linuxptp</literal>パッケージで提供されています。このパッケージには次の2つのコンポーネントが含まれています。</para>
<itemizedlist>
<listitem>
<para><literal>ptp4l</literal>: NIC上のPHCを制御し、PTPプロトコルを実行するデーモン</para>
</listitem>
<listitem>
<para><literal>phc2sys</literal>: NIC上のPTP同期PHCとシステムクロックとの同期を維持するデーモン</para>
</listitem>
</itemizedlist>
<para>システム同期が完全に機能するには、両方のデーモンが必要であり、セットアップに従って正しく設定する必要があります。詳細については、<xref
linkend="ptp-telco-config"/>を参照してください。</para>
<para>ダウンストリームクラスタにPTPを統合する最も簡単で最良の方法は、Edge Image Builder
(EIB)定義ファイルの<literal>packageList</literal>の下に<literal>linuxptp</literal>パッケージを追加することです。これにより、クラスタプロビジョニング中にPTPコントロールプレーンソフトウェアが自動的にインストールされます。パッケージのインストールの詳細については、EIBのドキュメント(<xref
linkend="eib-configuring-rpm-packages"/>)を参照してください。</para>
<para>以下に、<literal>linuxptp</literal>を使用したEIBマニフェストのサンプルを示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: {micro-base-rt-image-raw}
  outputImageName: eibimage-slmicrort-telco.raw
operatingSystem:
  time:
    timezone: America/New_York
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
    enable:
      - ptp4l
      - phc2sys
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
      - open-iscsi
      - tuned
      - cpupower
      - linuxptp
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<note>
<para>SUSE Telco
Cloudに含まれる<literal>linuxptp</literal>パッケージでは、デフォルトで<literal>ptp4l</literal>および<literal>phc2sys</literal>が有効になりません。プロビジョニング時にシステム固有の設定ファイルがデプロイされている場合は(<xref
linkend="ptp-capi"/>を参照)、これらを有効にする必要があります。上記の例のように、マニフェストの<literal>systemd</literal>セクションに追加して有効にしてください。</para>
</note>
<para>EIBのドキュメントで説明されている通常のプロセスに従って、イメージを構築し(<xref
linkend="eib-how-to-build-image"/>)、そのイメージを使用してクラスタをデプロイします。EIBを初めて使用する場合は、<xref
linkend="components-eib"/>から開始してください。</para>
</section>
<section xml:id="ptp-telco-config">
<title>通信事業者のデプロイメント向けPTPの設定</title>
<para>多くの通信事業者向けアプリケーションでは、わずかな偏差で厳格な位相と時刻同期が求められるため、通信事業者向けの2つのプロファイルが定義されました(ITU-T
G.8275.1とITU-T G.8275.2)。これら両プロファイルは、同期メッセージの送信頻度が高く、代替Best Master Clock
Algorithm
(BMCA)の使用など、他の特徴的な特性を備えています。このような動作は、<literal>ptp4l</literal>が使用する設定ファイルに特定の設定を必要とします。これらの設定は以下のセクションで参照用に提供されています。</para>
<note>
<itemizedlist>
<listitem>
<para>両セクションは、タイムレシーバー設定の通常のクロックの場合のみを対象としてます。</para>
</listitem>
<listitem>
<para>このようなプロファイルは、適切に計画されたPTPインフラストラクチャで使用する必要があります。</para>
</listitem>
<listitem>
<para>特定のPTPネットワークには、追加の設定の調整が必要な場合があります。必要に応じて、提供された例を確認して調整してください。</para>
</listitem>
</itemizedlist>
</note>
<section xml:id="id-ptp-profile-itu-t-g-8275-1">
<title>PTPプロファイルITU-T G.8275.1</title>
<para>G.8275.1プロファイルには、次の特徴があります。</para>
<itemizedlist>
<listitem>
<para>Ethernet上で直接実行し、完全なネットワークサポートが必要です(隣接するノード/スイッチはPTPをサポートしている必要があります)。</para>
</listitem>
<listitem>
<para>デフォルトのドメイン設定は24です。</para>
</listitem>
<listitem>
<para>データセットの比較は、G.8275.xアルゴリズムと、<literal>priority2</literal>以降の
<literal>localPriority</literal>値に基づいています。</para>
</listitem>
</itemizedlist>
<para>次のコンテンツを<literal>/etc/ptp4l-G.8275.1.conf</literal>という名前のファイルにコピーします。</para>
<screen linenumbering="unnumbered"># Telecom G.8275.1 example configuration
[global]
domainNumber                    24
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             -3
logSyncInterval                 -4
logMinDelayReqInterval          -4
announceReceiptTimeout          3
serverOnly                      0
ptp_dst_mac                     01:80:C2:00:00:0E
network_transport               L2</screen>
<para>ファイルが作成されたら、デーモンが正常に起動するように<literal>/etc/sysconfig/ptp4l</literal>で参照される必要があります。これを行うには、
<literal>OPTIONS=</literal>行を次のように変更します。</para>
<screen linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"</screen>
<para>より正確な説明は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal>には、使用する設定ファイルのファイル名が必要です。この場合は<literal>/etc/ptp4l-G.8275.1.conf</literal>。</para>
</listitem>
<listitem>
<para><literal>-i</literal>には、使用するインタフェースの名前が必要です。<literal>$IFNAME</literal>を実際のインタフェース名に置き換えます。</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal>は、システムログ内のptp4l出力を適切に特定できるようにし、オプションです。</para>
</listitem>
</itemizedlist>
<para>上記の手順が完了したら、<literal>ptp4l</literal>デーモンを(再)起動する必要があります。</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>次のコマンドを使用してログを調べ、同期ステータスを確認します。</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-ptp-profile-itu-t-g-8275-2">
<title>PTPプロファイルITU-T G.8275.2</title>
<para>G.8275.2プロファイルには、次の特徴があります。</para>
<itemizedlist>
<listitem>
<para>IP上で実行し、完全なネットワークサポートは不要です(隣接するノード/スイッチはPTPをサポートしていない場合があります)。</para>
</listitem>
<listitem>
<para>デフォルトのドメイン設定は44です。</para>
</listitem>
<listitem>
<para>データセットの比較は、G.8275.xアルゴリズムと、<literal>priority2</literal>以降の
<literal>localPriority</literal>値に基づいています。</para>
</listitem>
</itemizedlist>
<para>次のコンテンツを<literal>/etc/ptp4l-G.8275.2.conf</literal>という名前のファイルにコピーします。</para>
<screen linenumbering="unnumbered"># Telecom G.8275.2 example configuration
[global]
domainNumber                    44
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             0
serverOnly                      0
hybrid_e2e                      1
inhibit_multicast_service       1
unicast_listen                  1
unicast_req_duration            60
logSyncInterval                 -5
logMinDelayReqInterval          -4
announceReceiptTimeout          2
#
# Customize the following for slave operation:
#
[unicast_master_table]
table_id                        1
logQueryInterval                2
UDPv4                           $PEER_IP_ADDRESS
[$IFNAME]
unicast_master_table            1</screen>
<para>次のプレースホルダーを必ず置き換えます。</para>
<itemizedlist>
<listitem>
<para><literal>$PEER_IP_ADDRESS</literal> -
同期を提供するマスタまたは境界クロックなど、通信する次のPTPノードのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>$IFNAME</literal> - <literal>ptp4l</literal>にPTPに使用するインタフェースを示します。</para>
</listitem>
</itemizedlist>
<para>ファイルが作成されると、デーモンが正常に起動するように、PTPに使用するインタフェースの名前とともに、<literal>/etc/sysconfig/ptp4l</literal>で参照される必要があります。これを行うには、<literal>OPTIONS=</literal>行を次のように変更します。</para>
<screen language="shell" linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.2.conf --message_tag ptp-8275.2"</screen>
<para>より正確な説明は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal>には、使用する設定ファイルのファイル名が必要です。この場合は、<literal>/etc/ptp4l-G.8275.2.conf</literal>です。</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal>は、システムログ内のptp4l出力を適切に特定できるようにし、オプションです。</para>
</listitem>
</itemizedlist>
<para>上記の手順が完了したら、<literal>ptp4l</literal>デーモンを(再)起動する必要があります。</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>次のコマンドを使用してログを調べ、同期ステータスを確認します。</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-configuration-of-phc2sys">
<title>phc2sysの設定</title>
<para>必須ではありませんが、<literal>phc2sys</literal>に移る前に、<literal>ptp4l</literal>の設定を完全に完了しておくことをお勧めします。<literal>phc2sys</literal>は設定ファイルを必要とせず、その実行パラメータは<literal>ptp4l</literal>と同様に、<literal>/etc/sysconfig/ptp4l</literal>に存在する<literal>OPTIONS=</literal>変数を通じてのみ制御できます。</para>
<screen linenumbering="unnumbered">OPTIONS="-s $IFNAME -w"</screen>
<para>ここで、<literal>$IFNAME</literal>はシステムクロックのソースとして使用される、ptp4lですでに設定されているインタフェースの名前です。これはソースPHCを識別するために使用されます。</para>
</section>
</section>
<section xml:id="ptp-capi">
<title>Cluster APIの統合</title>
<para>クラスタが管理クラスタおよびダイレクトネットワークプロビジョニングを通じてデプロイされるときは常に、プロビジョニング時に設定ファイルと<literal>/etc/sysconfig</literal>内の2つの設定変数の両方をホスト上にデプロイできます。以下は、同じG.8275.1設定ファイルをすべてのホスト上にデプロイする、変更された<literal>RKE2ControlPlane</literal>オブジェクトに焦点を当てたクラスタ定義の抜粋です。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  serverConfig:
    cni: canal
  agentConfig:
    format: ignition
    cisProfile: cis
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /etc/ptp4l-G.8275.1.conf
              overwrite: true
              contents:
                inline: |
                  # Telecom G.8275.1 example configuration
                  [global]
                  domainNumber                    24
                  priority2                       255
                  dataset_comparison              G.8275.x
                  G.8275.portDS.localPriority     128
                  G.8275.defaultDS.localPriority  128
                  maxStepsRemoved                 255
                  logAnnounceInterval             -3
                  logSyncInterval                 -4
                  logMinDelayReqInterval          -4
                  announceReceiptTimeout          3
                  serverOnly                      0
                  ptp_dst_mac                     01:80:C2:00:00:0E
                  network_transport               L2
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/ptp4l
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): ptp4l settings
                  ## Type:           string
                  ## Default:        "-i eth0 -f /etc/ptp4l.conf"
                  ## ServiceRestart: ptp4l
                  #
                  # Arguments when starting ptp4l(8).
                  #
                  OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/phc2sys
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): phc2sys settings
                  ## Type:           string
                  ## Default:        "-s eth0 -w"
                  ## ServiceRestart: phc2sys
                  #
                  # Arguments when starting phc2sys(8).
                  #
                  OPTIONS="-s $IFNAME -w"
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>上記の定義は、他の変数に加え、<xref
linkend="atip-automated-provisioning"/>で説明されているように、インタフェース名と他のCluster
APIオブジェクトで補完する必要があります。</para>
<note>
<itemizedlist>
<listitem>
<para>このアプローチは、クラスタのハードウェアが均一であり、インタフェース名を含め、すべてのホスト上で同じ設定が必要な場合にのみ便利です。</para>
</listitem>
<listitem>
<para>代替アプローチが可能であり、今後のリリースで取り上げる予定です。</para>
</listitem>
</itemizedlist>
</note>
<para>この時点で、ホストには機能していて実行中のPTPスタックがあり、PTP役割のネゴシエーションを開始するはずです。</para>
</section>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>完全に自動化されたダイレクトネットワークプロビジョニング</title>
<section xml:id="id-introduction-3">
<title>はじめに</title>
<para>ダイレクトネットワークプロビジョニングは、ダウンストリームクラスタのプロビジョニングを自動化できる機能です。この機能は、プロビジョニングするダウンストリームクラスタが多数あり、そのプロセスを自動化したい場合に便利です。</para>
<para>管理クラスタ(<xref linkend="atip-management-cluster"/>)は、次のコンポーネントのデプロイメントを自動化します。</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro RT</literal>
(OS)。ユースケースに応じて、ネットワーキング、ストレージ、ユーザ、カーネル引数などの設定をカスタマイズできます。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>
(Kubernetesクラスタ)。デフォルトの<literal>CNI</literal>プラグインは<literal>Cilium</literal>です。ユースケースに応じて、特定の<literal>CNI</literal>プラグイン(<literal>Cilium+Multus</literal>など)を使用できます。</para>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal></para>
</listitem>
<listitem>
<para><literal>SUSE Security</literal></para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>。高可用性マルチノードクラスタのロードバランサとして使用できます。</para>
</listitem>
</itemizedlist>
<note>
<para><literal>SUSE Linux Micro</literal>の詳細については、<xref
linkend="components-slmicro"/>を参照してください。<literal>RKE2</literal>の詳細については、<xref
linkend="components-rke2"/>を参照してください。<literal>SUSE
Storage</literal>の詳細については、<xref
linkend="components-suse-storage"/>を参照してください。<literal>SUSE
Security</literal>の詳細については、<xref
linkend="components-suse-security"/>を参照してください。</para>
</note>
<para>以降のセクションでは、さまざまなダイレクトネットワークプロビジョニングワークフローと、プロビジョニングプロセスに追加できる機能について説明します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
<note>
<para>次のセクションでは、SUSE Telco
Cloudを使用してダイレクトネットワークプロビジョニングワークフローの異なるシナリオを準備する方法について説明します。デプロイメントの異なる設定オプション(エアギャップ環境、DHCPおよびDHCPなしのネットワーク、プライベートコンテナレジストリなどを含む)の例については<link
xl:href="https://github.com/suse-edge/atip/tree/release-3.4/telco-examples/edge-clusters">SUSE
Telco Cloudリポジトリ</link>を参照してください。</para>
</note>
</section>
<section xml:id="eib-edge-image-connected">
<title>接続シナリオのダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroゴールデンイメージを準備します。</para>
<para>ほとんどの設定はEdge Image
Builderを使用して行うことができますが、このガイドではダウンストリームクラスタをセットアップするために必要な最小限の設定について説明します。</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>接続シナリオの前提条件</title>
<itemizedlist>
<listitem>
<para>Edge Image Builderを実行するには、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイムが必要です。</para>
</listitem>
<listitem>
<para>ゴールデンイメージは、プロファイル<literal>Base-SelfInstall</literal>
(リアルタイムカーネルには<literal>Base-RT-SelfInstall</literal>)を使用し、次のガイド<xref
linkend="guides-kiwi-builder-images"/>に従って構築されます。このプロセスは両方のアーキテクチャ(x86-64とaarch64)で同じです。</para>
</listitem>
</itemizedlist>
<note>
<para>構築するイメージと同じアーキテクチャの構築ホストを使用する必要があります。つまり、<literal>aarch64</literal>のイメージを構築するには<literal>aarch64</literal>の構築ホストを使用する必要があり、<literal>x86-64</literal>の場合も同様です(クロス構築は現在のところサポートされていません)。</para>
</note>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>接続シナリオのイメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ゴールデンイメージフォルダには、ガイド<xref
linkend="guides-kiwi-builder-images"/>に従って生成された出力RAWイメージが含まれます。プロファイル<literal>Base-SelfInstall</literal>
(または、リアルタイムカーネルの場合は<literal>Base-RT-SelfInstall</literal>)は、<literal>base-images</literal>フォルダにコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>ディレクトリには初回ブート時に実行するスクリプトが含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal>スクリプトは、デプロイメント時にOSルートパーティションをサイズ変更するために必要です。</para>
</listitem>
<listitem>
<para><literal>02-performance.sh</literal>スクリプトはオプションであり、パフォーマンス調整用にシステムを設定するために使用できます。</para>
</listitem>
<listitem>
<para><literal>03-sriov.sh</literal>スクリプトはオプションであり、SR-IOV用にシステムを設定するために使用できます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にイメージにコピーされる<literal>performance-settings.sh</literal>および<literal>sriov-auto-filler.sh</literal>ファイルが含まれます。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-config.yaml</literal>ファイルは、ダウンストリームクラスタイメージの主要な設定ファイルです。次に、Metal<superscript>3</superscript>を介したデプロイメントの最小例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>ここで、<literal>$SCC_REGISTRATION_CODE</literal>は <link
xl:href="https://scc.suse.com/">SUSE Customer
Center</link>からコピーした登録コードで、パッケージリストには必要な<literal>jq</literal>が含まれています。</para>
<para><literal>$ROOT_PASSWORD</literal>はルートユーザの暗号化パスワードで、テスト/デバッグに役立ちます。このパスワードは、<literal>openssl
passwd -6 PASSWORD</literal>コマンドで生成できます。</para>
<para>運用環境では、<literal>$USERKEY1</literal>を実際のSSHキーに置き換えて、usersブロックに追加できるSSHキーを使用することをお勧めします。</para>
<note>
<para><literal>arch:
x86_64</literal>は、イメージのアーキテクチャです。arm64アーキテクチャの場合は、<literal>arch:
aarch64</literal>を使用します。</para>
<para><literal>net.ifnames=1</literal>は、<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link>を有効にします。</para>
<para>これはmetal3チャートのデフォルト設定と一致しますが、この設定は、設定されたチャートの<literal>predictableNicNames</literal>の値と一致する必要があります。</para>
<para>また、<literal>ignition.platform.id=openstack</literal>は必須であり、この引数がないと、Metal<superscript>3</superscript>の自動化フローでIgnitionによるSLEMicroの設定が失敗することにも注意してください。</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト(<literal>custom/scripts/01-fix-growfs.sh</literal>)が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="add-custom-script-performance">
<title>パフォーマンススクリプト</title>
<para>次のオプションのスクリプト(<literal>custom/scripts/02-performance.sh</literal>)は、パフォーマンス調整用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para><literal>custom/files/performance-settings.sh</literal>のコンテンツは、パフォーマンス調整用にシステムを設定するために使用可能なスクリプトであり、次の<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">リンク</link>からダウンロードできます。</para>
</section>
<section xml:id="add-custom-script-sriov">
<title>SR-IOVスクリプト</title>
<para>次のオプションスクリプト(<literal>custom/scripts/03-sriov.sh</literal>)はSR-IOV用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para><literal>custom/files/sriov-auto-filler.sh</literal>のコンテンツは、SR-IOV用にシステムを設定するために使用可能なスクリプトであり、次の<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">リンク</link>からダウンロードできます。</para>
<note>
<para>同じアプローチを使用して、プロビジョニングプロセス中に実行する独自のカスタムスクリプトを追加します。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>通信ワークロードの追加設定</title>
<para><literal>dpdk</literal>、<literal>sr-iov</literal>、<literal>FEC</literal>などの通信機能を有効にするには、次の例に示すように追加のパッケージが必要な場合があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $user1Key1
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>ここで、<literal>$SCC_REGISTRATION_CODE</literal>は<link
xl:href="https://scc.suse.com/">SUSE Customer
Center</link>からコピーした登録コードです。また、パッケージリストには通信事業者プロファイル用の最小限のパッケージが含まれています。</para>
<note>
<para><literal>arch:
x86_64</literal>は、イメージのアーキテクチャです。arm64アーキテクチャの場合は、<literal>arch:
aarch64</literal>を使用します。</para>
</note>
</section>
<section xml:id="add-network-eib">
<title>高度なネットワーク設定のための追加スクリプト</title>
<para><xref
linkend="advanced-network-configuration"/>で説明されている静的IPや、より高度なネットワーキングシナリオを設定する必要がある場合、次の追加設定が必要です。</para>
<para><literal>network</literal>フォルダに、次の<literal>configure-network.sh</literal>ファイルを作成します。このファイルは、初回ブート時に設定ドライブデータを使用し、<link
xl:href="https://github.com/suse-edge/nm-configurator">NM
Configuratorツール</link>を使用してホストネットワーキングを設定します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>イメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>これにより、上記の定義に基づいた、<literal>eibimage-output-telco.raw</literal>という名前の出力ISOイメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、管理クラスタのドキュメントを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>エアギャップシナリオ用のダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroゴールデンイメージを準備します。</para>
<para>設定の多くはEdge Image
Builderを使用して行うことができますが、このガイドではエアギャップシナリオ用のダウンストリームクラスタの設定に必要な最小限の設定について説明します。</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>エアギャップシナリオの前提条件</title>
<itemizedlist>
<listitem>
<para>Edge Image Builderを実行するには、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイムが必要です。</para>
</listitem>
<listitem>
<para>ゴールデンイメージは、プロファイル<literal>Base-SelfInstall</literal>
(リアルタイムカーネルには<literal>Base-RT-SelfInstall</literal>)を使用し、次のガイド<xref
linkend="guides-kiwi-builder-images"/>に従って構築されます。このプロセスは両方のアーキテクチャ(x86-64とaarch64)で同じです。</para>
</listitem>
<listitem>
<para>コンテナイメージが必要なSR-IOVなどのワークロードを使用する場合、ローカルのプライベートレジストリをデプロイして設定済みである必要があります(TLSまたは認証、あるいはその両方を使用/不使用)。このレジストリを使用して、イメージとHelmチャートOCIイメージを保存します。</para>
</listitem>
</itemizedlist>
<note>
<para>構築するイメージと同じアーキテクチャの構築ホストを使用する必要があります。つまり、<literal>aarch64</literal>のイメージを構築するには<literal>aarch64</literal>の構築ホストを使用する必要があり、<literal>x86-64</literal>の場合も同様です(クロス構築は現在のところサポートされていません)。</para>
</note>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>エアギャップシナリオのイメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ゴールデンイメージフォルダには、ガイド<xref
linkend="guides-kiwi-builder-images"/>に従って生成された出力RAWイメージが含まれます。プロファイル<literal>Base-SelfInstall</literal>
(または、リアルタイムカーネルの場合は<literal>Base-RT-SelfInstall</literal>)は、<literal>base-images</literal>フォルダにコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>ディレクトリには初回ブート時に実行するスクリプトが含まれます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal>スクリプトは、デプロイメント時にOSルートパーティションをサイズ変更するために必要です。</para>
</listitem>
<listitem>
<para><literal>02-airgap.sh</literal>スクリプトは、エアギャップ環境でのイメージ作成プロセス中にイメージを適切な場所にコピーするために必要です。</para>
</listitem>
<listitem>
<para><literal>03-performance.sh</literal>スクリプトはオプションであり、パフォーマンス調整用にシステムを設定するために使用できます。</para>
</listitem>
<listitem>
<para><literal>04-sriov.sh</literal>スクリプトはオプションであり、SR-IOV用にシステムを設定するために使用できます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にイメージにコピーされる<literal>rke2</literal>および<literal>cni</literal>イメージが含まれています。また、オプションの<literal>performance-settings.sh</literal>および<literal>sriov-auto-filler.sh</literal>ファイルを含めることもできます。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-airgap-config.yaml</literal>ファイルは、ダウンストリームクラスタ用のメイン設定ファイルです。その内容については、前のセクション(<xref
linkend="add-telco-feature-eib"/>)で説明されています。</para>
</section>
<section xml:id="id-growfs-script">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト(<literal>custom/scripts/01-fix-growfs.sh</literal>)が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>エアギャップスクリプト</title>
<para>イメージ作成プロセス中にイメージを正しい場所にコピーするために、次のスクリプト(<literal>custom/scripts/02-airgap.sh</literal>)が必要です。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="add-custom-script-performance2">
<title>パフォーマンススクリプト</title>
<para>次のオプションのスクリプト(<literal>custom/scripts/03-performance.sh</literal>)は、パフォーマンス調整用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para><literal>custom/files/performance-settings.sh</literal>のコンテンツは、パフォーマンス調整用にシステムを設定するために使用可能なスクリプトであり、次の<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">リンク</link>からダウンロードできます。</para>
</section>
<section xml:id="add-custom-script-sriov2">
<title>SR-IOVスクリプト</title>
<para>次のオプションのスクリプト(<literal>custom/scripts/04-sriov.sh</literal>)は、SR-IOV用にシステムを設定するために使用できます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para><literal>custom/files/sriov-auto-filler.sh</literal>のコンテンツは、SR-IOV用にシステムを設定するために使用可能なスクリプトであり、次の<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">リンク</link>からダウンロードできます。</para>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>エアギャップシナリオのカスタムファイル</title>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にそのイメージにコピーする<literal>rke2</literal>イメージと<literal>cni</literal>イメージが含まれています。イメージを簡単に生成するには、次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">スクリプト</link>と、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-release-rke2-images.txt">こちら</link>にあるイメージのリストを使用してローカルでイメージを準備し、<literal>custom/files</literal>に含める必要があるアーティファクトを生成します。また、最新の<literal>rke2-install</literal>スクリプトを<link
xl:href="https://get.rke2.io/">こちら</link>からダウンロードすることもできます。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>イメージをダウンロードした後、ディレクトリ構造は次のようになるはずです。</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>エアギャップシナリオおよびSR-IOV (オプション)に必要なイメージのプライベートレジストリへのプリロード</title>
<para>エアギャップシナリオまたはその他のワークロードイメージでSR-IOVを使用する場合、次の各手順に従って、ローカルのプライベートレジストリにイメージをプリロードする必要があります。</para>
<itemizedlist>
<listitem>
<para>HelmチャートOCIイメージをダウンロードして抽出し、プライベートレジストリにプッシュする</para>
</listitem>
<listitem>
<para>必要な残りのイメージをダウンロードして抽出し、プライベートレジストリにプッシュする</para>
</listitem>
</itemizedlist>
<para>次のスクリプトを使用して、イメージをダウンロードして抽出し、プライベートレジストリにプッシュできます。これからSR-IOVイメージをプリロードする例を示しますが、その他のカスタムイメージも同じ方法でプリロードすることができます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SR-IOVのHelmチャートOCIイメージのプリロード:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>必要なHelmチャートOCIイメージが含まれるリストを作成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:304.0.2+up1.5.0
edge/sriov-crd-chart:304.0.2+up1.5.0
EOF</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-oci-artefacts.sh">スクリプト</link>と上記で作成したリストを使用してローカルtarballファイルを生成します。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/charts/sriov-network-operator:304.0.2+up1.5.0
Pulled: registry.suse.com/edge/charts/sriov-crd:304.0.2+up1.5.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-304.0.2+up1.5.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-304.0.2+up1.5.0.tgz</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-oci-artefacts.sh">スクリプト</link>を使用してtarballファイルをプライベートレジストリ(例:
<literal>myregistry:5000</literal>)にアップロードし、前の手順でダウンロードしたHelmチャートOCIイメージをレジストリにプリロードします。</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>SR-IOVに必要な残りのイメージをプリロードします。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>ここでは、通信ワークロードのために「sr-iov」コンテナイメージを含める必要があります(例: 参考として、これは <link
xl:href="https://github.com/suse-edge/charts/blob/main/charts/sriov-network-operator/1.5.0/values.yaml">helmチャート値</link>から取得できます)。</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">スクリプト</link>と上記で作成したリストを使用して、必要なイメージを含む、tarballファイルをローカルで生成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-images.sh">スクリプト</link>を使用してtarballファイルをプライベートレジストリ(例:
<literal>myregistry:5000</literal>)にアップロードし、前の手順でダウンロードしたイメージをプライベートレジストリにプリロードします。</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>エアギャップシナリオのイメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>これにより、上記の定義に基づいた、<literal>eibimage-output-telco.raw</literal>という名前の出力ISOイメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、管理クラスタドキュメントを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
</section>
</section>
<section xml:id="single-node">
<title>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード)</title>
<para>このセクションでは、ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化するために用いるワークフローについて説明します。これは、ダウンストリームクラスタのプロビジョニングを自動化する最もシンプルな方法です。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して、ダウンストリームクラスタを設定するための最小限の設定で生成されたイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上にある管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">ワークフロー</emphasis></para>
<para>次の図は、ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化するために用いるワークフローを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png"
width="100%"/> </imageobject>
<textobject><phrase>atip自動化シングルノード1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化する手順は2種類です。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ベアメタルホストを登録して、プロビジョニングプロセスで使用できるようにする。</para>
</listitem>
<listitem>
<para>ベアメタルホストをプロビジョニングして、オペレーティングシステムとKubernetesクラスタをインストールして設定する。</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">ベアメタルホストの登録</emphasis></para>
<para>最初の手順では、新しいベアメタルホストを管理クラスタに登録してプロビジョニングできるようにします。そのためには、ファイル<literal>bmh-example.yaml</literal>を管理クラスタ内に作成して、使用する<literal>BMC</literal>資格情報と登録する<literal>BaremetalHost</literal>オブジェクトを指定する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>各項目の内容は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal> —
新しいベアメタルホストの<literal>BMC</literal>のユーザ名。</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal> —
新しいベアメタルホストの<literal>BMC</literal>のパスワード。</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal> — 使用する新しいベアメタルホストの<literal>MAC</literal>アドレス。</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal> —
ベアメタルホストの<literal>BMC</literal>の<literal>URL</literal> (例:
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>)。ハードウェアプロバイダに応じて使用できる各種のオプションの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">こちらのリンク</link>を確認してください。</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>アーキテクチャは、登録するベアメタルホストのアーキテクチャに応じて、<literal>x86_64</literal>または<literal>aarch64</literal>のいずれかである必要があります。</para>
</listitem>
<listitem>
<para>ホストのネットワーク設定が、イメージのビルド時または<literal>BareMetalHost</literal>定義を通じて指定されていない場合、自動設定メカニズム(DHCP、DHCPv6、SLAAC)が使用されます。詳細または複雑な設定については、<xref
linkend="advanced-network-configuration"/>を参照してください。</para>
</listitem>
</itemizedlist>
</note>
<para>ファイルを作成したら、管理クラスタで次のコマンドを実行し、管理クラスタで新しいベアメタルホストの登録を開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>新しいベアメタルホストオブジェクトが登録され、その状態が「Registering (登録中)」から「Inspecting
(検査中)」に変わり、「Available (使用可能)」になります。この変更は次のコマンドを使用して確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para><literal>BaremetalHost</literal>オブジェクトは、<literal>BMC</literal>の資格情報が検証されるまでは「<literal>Registering
(登録中)</literal>」の状態です。資格情報の検証が完了すると、<literal>BaremetalHost</literal>オブジェクトの状態が「<literal>Inspecting
(検査中)</literal>」に変わります。この手順はハードウェアによっては多少時間がかかる可能性があります(最大で20分)。「Inspecting
(検査中)」のフェーズ中に、ハードウェア情報が取得されてKubernetesオブジェクトが更新されます。<literal>kubectl get bmh
-o yaml</literal>コマンドを使用して情報を確認してください。</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">プロビジョニング手順</emphasis></para>
<para>ベアメタルホストが登録されて使用可能になったら、次の手順で、ベアメタルホストをプロビジョニングしてオペレーティングシステムとKubernetesクラスタをインストールして設定します。そのためには、次の情報を使用して管理クラスタでファイル<literal>capi-provisioning-example.yaml</literal>を作成する必要があります(<literal>capi-provisioning-example.yaml</literal>は、以下のブロックを結合して生成できます)。</para>
<note>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</note>
<para>次のブロックはクラスタ定義です。ここで、<literal>pods</literal>ブロックと<literal>services</literal>ブロックを使用してネットワーキングを設定できます。また、ここにはコントロールプレーンオブジェクトと、使用するインフラストラクチャ(<literal>Metal<superscript>3</superscript></literal>プロバイダを使用)オブジェクトへの参照も含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:bad:cafe::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:bad:bad:cafe::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<note>
<itemizedlist>
<listitem>
<para>シングルスタックとデュアルスタック両方のデプロイメントが可能です。IPv4のみのクラスタの場合は、上記の定義からIPv6 CIDRを削除してください。</para>
</listitem>
<listitem>
<para>シングルスタックのIPv6デプロイメントは技術プレビュー状態であり、公式にはサポートされていません。</para>
</listitem>
</itemizedlist>
</note>
<para><literal>Metal3Cluster</literal>オブジェクトには、設定するコントロールプレーンのエンドポイント(<literal>${DOWNSTREAM_CONTROL_PLANE_IPV4}</literal>を置き換える)と、<literal>noCloudProvider</literal>(ベアメタルノードを使用しているため)を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>オブジェクトには、使用するコントロールプレーン設定を指定し、<literal>Metal3MachineTemplate</literal>オブジェクトには、使用するコントロールプレーンのイメージを指定します。また、ここには使用するレプリカの数(ここでは1)と、使用する<literal>CNI</literal>プラグイン(ここでは<literal>Cilium</literal>)に関する情報が含まれています。agentConfigブロックには、使用する
<literal>Ignition</literal>形式と、使用する<literal>additionalUserData</literal>が含まれます。これを使用して<literal>RKE2</literal>ノードに<literal>rke2-preinstall.service</literal>という名前のsystemdサービスを設定し、プロビジョニングプロセス中にIronicの情報を使用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換えます。ciliumでmultusを有効にするには、使用する設定を含むファイルを
<literal>rke2</literal>サーバマニフェストディレクトリに<literal>rke2-cilium-config.yaml</literal>という名前で作成します。最後の情報ブロックには、使用するKubernetesバージョンが含まれています。<literal>${RKE2_VERSION}</literal>は使用する<literal>RKE2</literal>のバージョンであり、この値は置き換えます(例:
<literal>v1.33.3+rke2r1</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには次の情報を指定します。</para>
<itemizedlist>
<listitem>
<para>テンプレートへの参照として使用する<literal>dataTemplate</literal>。</para>
</listitem>
<listitem>
<para>登録プロセス中に作成されたラベルとの一致に使用する<literal>hostSelector</literal>。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で<literal>EIB</literal>を使用して生成されたイメージへの参照として使用する<literal>image</literal>、およびそのイメージを検証するために使用する<literal>checksum</literal>と<literal>checksumType</literal>。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>前のブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行して、新しいベアメタルホストのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード)</title>
<para>このセクションでは、ダイレクトネットワークプロビジョニングと<literal>MetalLB</literal>をロードバランサ戦略として使用して、マルチノードのダウンストリームクラスタのプロビジョニングを自動化するために使用するワークフローについて説明します。これはダウンストリームクラスタのプロビジョニングを自動化する最もシンプルな方法です。次の図は、ダイレクトネットワークプロビジョニングと<literal>MetalLB</literal>を使用してマルチノードのダウンストリームクラスタのプロビジョニングを自動化するためのワークフローを示しています。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して、ダウンストリームクラスタを設定するための最小限の設定で生成されたイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上にある管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">ワークフロー</emphasis></para>
<para>次の図は、ダイレクトネットワークプロビジョニングを使用してマルチノードのダウンストリームクラスタのプロビジョニングを自動化するために使用するワークフローを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png"
width="100%"/> </imageobject>
<textobject><phrase>atip自動化マルチノード1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>3つのベアメタルホストを登録し、プロビジョニングプロセスで使用できるようにする。</para>
</listitem>
<listitem>
<para>3つのベアメタルホストをプロビジョニングし、オペレーティングシステムと、<literal>MetalLB</literal>を使用するKubernetesクラスタをインストールして設定する。</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">ベアメタルホストの登録</emphasis></para>
<para>最初の手順では、管理クラスタに3つのベアメタルホストを登録してプロビジョニングできるようにします。そのためには、管理クラスタにファイル<literal>bmh-example-node1.yaml</literal>、<literal>bmh-example-node2.yaml</literal>、および<literal>bmh-example-node3.yaml</literal>を作成して、使用する<literal>BMC</literal>資格情報と、管理クラスタに登録する<literal>BaremetalHost</literal>オブジェクトを指定する必要があります。</para>
<note>
<itemizedlist>
<listitem>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</listitem>
<listitem>
<para>1つのホストのプロセスについてのみ説明します。他の2つのノードにも同じ手順が適用されます。</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>各項目の内容は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal> — 最初のベアメタルホストのBMCのユーザ名。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal> — 最初のベアメタルホストのBMCのパスワード。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal> — 使用する最初のベアメタルホストのMACアドレス。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal> — 最初のベアメタルホストのBMCのURL (例:
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>)。URLのホスト部分は、既存のインフラストラクチャで許可されている場合、IPアドレス(v4またはv6)またはドメイン名とすることができます。ハードウェアプロバイダに応じて使用できる各種のオプションの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">こちらのリンク</link>を確認してください。</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>ホストのネットワーク設定が、イメージのビルド時または<literal>BareMetalHost</literal>定義を通じて指定されていない場合、自動設定メカニズム(DHCP、DHCPv6、SLAAC)が使用されます。詳細または複雑な設定については、<xref
linkend="advanced-network-configuration"/>を参照してください。</para>
</listitem>
<listitem>
<para>シングルスタックIPv6クラスタは技術プレビュー状態であり、まだ公式にはサポートされていません。</para>
</listitem>
<listitem>
<para>アーキテクチャは、登録するベアメタルホストのアーキテクチャに応じて、<literal>x86_64</literal>または<literal>aarch64</literal>のいずれかである必要があります。</para>
</listitem>
<listitem>
<para>すべての最新サーバにはデュアルスタック対応のBMCが搭載されていますが、デュアルスタック環境で運用する前に、IPv6のサポート(および仮想メディア機能にホスト名を使用するオプション)を確認する必要があります。</para>
</listitem>
</itemizedlist>
</note>
<para>ファイルを作成したら、管理クラスタで次のコマンドを実行して、管理クラスタへのベアメタルホストの登録を開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>新しいベアメタルホストオブジェクトが登録され、その状態が「Registering (登録中)」から「Inspecting
(検査中)」に変わり、「Available (使用可能)」になります。この変更は次のコマンドを使用して確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para><literal>BaremetalHost</literal>オブジェクトは、<literal>BMC</literal>の資格情報が検証されるまでは「<literal>Registering
(登録中)</literal>」の状態です。資格情報の検証が完了すると、<literal>BaremetalHost</literal>オブジェクトの状態が「<literal>Inspecting
(検査中)</literal>」に変わります。この手順はハードウェアによっては多少時間がかかる可能性があります(最大で20分)。「Inspecting
(検査中)」のフェーズ中に、ハードウェア情報が取得されてKubernetesオブジェクトが更新されます。<literal>kubectl get bmh
-o yaml</literal>コマンドを使用して情報を確認してください。</para>
</note>
<para><emphasis role="strong">プロビジョニング手順</emphasis></para>
<para>3つのベアメタルホストが登録されて使用可能になったら、次の手順は、ベアメタルホストをプロビジョニングしてオペレーティングシステムとKubernetesクラスタをインストールして設定し、ロードバランサを作成して3つのベアメタルホストを管理することです。そのためには、次の情報を使用して管理クラスタにファイル<literal>capi-provisioning-example.yaml</literal>を作成する必要があります(capi-provisioning-example.yamlは、次のブロックを結合して生成できます)。</para>
<note>
<itemizedlist>
<listitem>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</listitem>
<listitem>
<para><literal>VIP</literal>アドレスは、どのノードにも割り当てられていない予約済みのIPアドレスであり、ロードバランサを設定するために使用されます。デュアルスタッククラスタでは、IPv4とIPv6の両方を指定できますが、以下の例ではIPv4アドレスが優先されます。</para>
</listitem>
</itemizedlist>
</note>
<para>以下はクラスタ定義です。ここで、<literal>pods</literal>ブロックと<literal>services</literal>ブロックを使用してクラスタのネットワークを設定できます。また、ここにはコントロールプレーンと、使用するインフラストラクチャ(<literal>Metal<superscript>3</superscript></literal>プロバイダを使用)のオブジェクトへの参照も含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:1234:4321::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:5678:8765:4321::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<note>
<para>シングルスタックとデュアルスタックの両方のデプロイメントが可能です。IPv4のみのクラスタの場合は、IPv6 CIDRおよびIPv6
VIPアドレス(後述するセクション)を削除してください</para>
</note>
<para><literal>Metal3Cluster</literal>オブジェクトには、予約済みの<literal>VIP</literal>アドレス(
<literal>${EDGE_VIP_ADDRESS_IPV4}</literal>を置き換え)を使用して設定するコントロールプレーンのエンドポイントと、<literal>noCloudProvider</literal>
(3つのベアメタルノードを使用しているため)を指定しています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>オブジェクトには、使用するコントロールプレーンの設定を指定し、<literal>Metal3MachineTemplate</literal>オブジェクトには、使用するコントロールプレーンのイメージを指定します。</para>
<itemizedlist>
<listitem>
<para>使用するレプリカの数(ここでは3)。</para>
</listitem>
<listitem>
<para>ロードバランサで使用するアドバタイズモード(<literal>address</literal>ではL2実装を使用)、および使用するアドレス(<literal>${EDGE_VIP_ADDRESS}</literal>を<literal>VIP</literal>アドレスに置き換え)。</para>
</listitem>
<listitem>
<para>使用する<literal>CNI</literal>プラグイン(ここでは<literal>Cilium</literal>)と、<literal>tlsSan</literal>の下にリストされる追加の
<literal>VIP</literal>アドレスと名前が含まれる<literal>serverConfig</literal>。</para>
</listitem>
<listitem>
<para>agentConfigブロックには、使用する<literal>Ignition</literal>の形式と、<literal>RKE2</literal>ノードに次のような情報を設定するために使用する<literal>additionalUserData</literal>が含まれています。</para>
<itemizedlist>
<listitem>
<para>プロビジョニングプロセス中にIronicの情報を使用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換える、<literal>rke2-preinstall.service</literal>という名前のsystemdサービス。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>と<literal>endpoint-copier-operator</literal>のインストールに使用するHelmチャートが含まれている<literal>storage</literal>ブロック。</para>
</listitem>
<listitem>
<para>使用する<literal>IPaddressPool</literal>と<literal>L2Advertisement</literal>が含まれている<literal>metalLB</literal>カスタムリソースファイル(<literal>${EDGE_VIP_ADDRESS_IPV4}</literal>を<literal>VIP</literal>アドレスに置き換え)。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>が<literal>VIP</literal>アドレスを管理するために使用する<literal>kubernetes-vip</literal>サービスの設定に使用する<literal>endpoint-svc.yaml</literal>ファイル。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>最後の情報ブロックには、使用するKubernetesバージョンが含まれています。<literal>${RKE2_VERSION}</literal>は使用する<literal>RKE2</literal>のバージョンで、この値は置き換えます(例:
<literal>v1.33.3+rke2r1</literal>)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS_IPV4}
      - ${EDGE_VIP_ADDRESS_IPV6}
      - https://${EDGE_VIP_ADDRESS_IPV4}.sslip.io
      - https://${EDGE_VIP_ADDRESS_IPV6}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS_IPV4}/32
                      - ${EDGE_VIP_ADDRESS_IPV6}/128
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ipFamilyPolicy: PreferDualStack
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "Node-multinode-cluster"</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには次の情報を指定します。</para>
<itemizedlist>
<listitem>
<para>テンプレートへの参照として使用する<literal>dataTemplate</literal>。</para>
</listitem>
<listitem>
<para>登録プロセス中に作成されたラベルとの一致に使用する<literal>hostSelector</literal>。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で<literal>EIB</literal>を使用して生成されたイメージへの参照として使用する<literal>image</literal>、およびそのイメージを検証するために使用する<literal>checksum</literal>と<literal>checksumType</literal>。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>以下のyamlファイルは、ワーカーノードの設定例です。</para>
<para><literal>MachineDeployment</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: multinode-cluster
    nodepool: nodepool-0
  name: multinode-cluster-workers
  namespace: default
spec:
  clusterName: multinode-cluster
  replicas: 3
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: multinode-cluster
      nodepool: nodepool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: multinode-cluster
        nodepool: nodepool-0
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: RKE2ConfigTemplate
          name: multinode-cluster-workers
      clusterName: multinode-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: multinode-cluster-workers
      nodeDrainTimeout: 0s
      version: ${RKE2_VERSION}</screen>
<para>RKE2ConfigTemplateオブジェクトには、マルチノードクラスタのワーカーノードに使用する設定テンプレートを指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: RKE2ConfigTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "Node-multinode-cluster-worker"
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには、ワーカーノードの<literal>dataTemplate</literal>、<literal>hostSelector</literal>、<literal>image</literal>への参照が含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ワーカーノードのダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-workers-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>前のブロックを結合してファイルが作成されたら、管理クラスタで次のコマンドを実行し、新しい3つのベアメタルホストのプロビジョニングを開始します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>高度なネットワーク設定</title>
<para>ダイレクトネットワークプロビジョニングワークフローを使用すると、静的IP、ボンディング、VLAN、IPv6など、ダウンストリームクラスタの特定のネットワーク設定が可能になります。</para>
<para>次の各セクションでは、高度なネットワーク設定を使用してダウンストリームクラスタをプロビジョニングできるようにするために必要な追加手順について説明します。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>このセクション(<xref
linkend="add-network-eib"/>)に従って、<literal>EIB</literal>を使用して生成したイメージにネットワークフォルダとスクリプトを含める必要があります。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">設定</emphasis></para>
<para>続行する前に、ホストの登録およびプロビジョニングに必要な手順に関するガイダンスについて、次のいずれかのセクションを参照してください。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード) (<xref
linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード) (<xref
linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>高度なネットワーク設定は、登録時に<literal>BareMetalHost</literal>ホスト定義および<literal>nmstate</literal>形式の<literal>networkData</literal>ブロックを含む関連するシークレットを通じて適用する必要があります。次のサンプルファイルは、ダウンストリームクラスタホストに静的<literal>IP</literal>と<literal>VLAN</literal>を要求する、必須の<literal>networkData</literal>を含むシークレットを定義しています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      identifier: mac-address
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>ご覧のとおり、この例は、静的IPを使用してインタフェースを有効にするための設定と、ベースインタフェースを使用してVLANを有効にするための設定を示しています。インフラストラクチャに応じて、以下の変数を実際の値に置き換えてください。</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE1_INTERFACE}</literal> —
エッジクラスタに使用するコントロールプレーンインタフェース(例:
<literal>eth0</literal>)。<literal>identifier:
mac-address</literal>を含めると、名前がMACアドレスに基づいて自動的に検査されるため、任意のインタフェース名を使用できます。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP}</literal> —
エッジクラスタのエンドポイントとして使用するIPアドレス(kubeapiサーバのエンドポイントと一致する必要があります)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX}</literal> — エッジクラスタに使用するCIDR (例:
<literal>/24</literal>または<literal>255.255.255.0</literal>を使用する場合には<literal>24</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY}</literal> — エッジクラスタに使用するゲートウェイ(例:
<literal>192.168.100.1</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_MAC}</literal> — コントロールプレーンインタフェースに使用するMACアドレス(例:
<literal>00:0c:29:3e:3e:3e</literal>)。</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal> — エッジクラスタに使用するDNS (例:
<literal>192.168.100.2</literal>)。</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal> — エッジクラスタに使用するVLAN ID (例:
<literal>100</literal>)。</para>
</listitem>
</itemizedlist>
<para>他の<literal>nmstate</literal>準拠の定義を使用して、ダウンストリームクラスタのネットワークを特定の要件に適合するように設定できます。たとえば、静的デュアルスタック設定を指定できます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: ${CONTROLPLANE_MAC}
      ipv4:
        enabled: true
        dhcp: false
        address:
        - ip: ${CONTROLPLANE_IP_V4}
          prefix-length: ${CONTROLPLANE_PREFIX_V4}
      ipv6:
        enabled: true
        dhcp: false
        autoconf: false
        address:
        - ip: ${CONTROLPLANE_IP_V6}
          prefix-length: ${CONTROLPLANE_PREFIX_V6}
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V4}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
      - destination: ::/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V6}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
    dns-resolver:
      config:
        server:
        - ${DNS_SERVER_V4}
        - ${DNS_SERVER_V6}</screen>
<para>前の例と同様に、インフラストラクチャに応じて、以下の変数を実際の値に置き換えてください。</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_IP_V4}</literal> - ホストに割り当てるIPv4アドレス</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V4}</literal> - ホストIPが属するネットワークのIPv4プレフィックス</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP_V6}</literal> - ホストに割り当てるIPv6アドレス</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V6}</literal> - ホストIPが属するネットワークのIPv6プレフィックス</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V4}</literal> -
デフォルトルートに一致するトラフィック用のゲートウェイのIPv4アドレス</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V6}</literal> -
デフォルトルートに一致するトラフィック用のゲートウェイのIPv6アドレス</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal> -
IPv4とIPv6の両方で、デフォルトルートに一致する出力トラフィックにアドレスを割り当て、使用するインタフェースの名前</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER_V4}</literal>および/または<literal>${DNS_SERVER_V6}</literal>
- 使用するDNSサーバのIPアドレス。単一または複数のエントリとして指定できます。IPv4および/またはIPv6アドレスがサポートされています。</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>IPv6専用設定およびデュアルスタック設定を含む、より複雑な例については、<link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/edge-clusters">SUSE
Telco Cloud例のリポジトリ</link>を参照してください。</para>
</listitem>
<listitem>
<para>シングルスタックのIPv6デプロイメントは技術プレビュー状態であり、公式にはサポートされていません。</para>
</listitem>
</itemizedlist>
</note>
<para>最後に、ネットワーク設定の詳細に関わらず、ホストを管理クラスタに正常に登録するために、<literal>BaremetalHost</literal>オブジェクトに<literal>preprovisioningNetworkDataName</literal>を追加してシークレットが参照されるようにします。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<itemizedlist>
<listitem>
<para>マルチノードクラスタをデプロイする必要がある場合は、同じプロセスを各ノードに対して実行する必要があります。</para>
</listitem>
<listitem>
<para><literal>Metal3DataTemplate</literal>、<literal>networkData</literal>、および<literal>Metal3
IPAM</literal>は現在サポートされていません。静的シークレットを介した設定のみが完全にサポートされています。</para>
</listitem>
<listitem>
<para>アーキテクチャは、登録するベアメタルホストのアーキテクチャに応じて、<literal>x86_64</literal>または<literal>aarch64</literal>のいずれかである必要があります。</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="add-telco">
<title>通信機能(DPDK、SR-IOV、CPUの分離、Huge Page、NUMAなど)</title>
<para>ダイレクトネットワークプロビジョニングのワークフローでは、ダウンストリームクラスタで使用する通信機能を自動化して、そのサーバ上で通信ワークロードを実行できます。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して生成したイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上の管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>こちらのセクション(<xref
linkend="add-telco-feature-eib"/>)に従って、<literal>EIB</literal>を使用して生成したイメージに特定の通信機能を含める必要がある。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">設定</emphasis></para>
<para>次の2つのセクションをベースとして使用し、ホストを登録してプロビジョニングします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード) (<xref
linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード) (<xref
linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>このセクションで説明する通信機能を次に示します。</para>
<itemizedlist>
<listitem>
<para>DPDKとVFの作成</para>
</listitem>
<listitem>
<para>ワークロードで使用されるSR-IOVとVFの割り当て</para>
</listitem>
<listitem>
<para>CPUの分離とパフォーマンスの調整</para>
</listitem>
<listitem>
<para>Huge Pageの設定</para>
</listitem>
<listitem>
<para>カーネルパラメータの調整</para>
</listitem>
</itemizedlist>
<note>
<para>通信機能の詳細については、<xref linkend="atip-features"/>を参照してください。</para>
</note>
<para>上記の通信機能を有効にするために必要な変更はすべて、プロビジョニングファイル<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロック内にあります。ファイル<literal>capi-provisioning-example.yaml</literal>内の残りの情報は、プロビジョニングに関するセクション(<xref
linkend="single-node-provision"/>)で指定した情報と同じです。</para>
<para>このプロセスを明確にするために、通信機能を有効にするためにそのブロック(<literal>RKE2ControlPlane</literal>)で必要な変更を次に示します。</para>
<itemizedlist>
<listitem>
<para><literal>RKE2</literal>インストールプロセスの前にコマンドを実行するために使用する<literal>preRKE2Commands</literal>。ここでは、<literal>modprobe</literal>コマンドを使用して、<literal>vfio-pci</literal>と<literal>SR-IOV</literal>のカーネルモジュールを有効にします。</para>
</listitem>
<listitem>
<para>作成してワークロードに公開するインタフェース、ドライバ、および<literal>VF</literal>の数を定義するために使用するIgnitionファイル<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>。</para>
<itemizedlist>
<listitem>
<para>実際の値に置き換える値は、設定マップ<literal>sriov-custom-auto-config</literal>内の値のみです。</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal> —
最初の<literal>PF</literal>インタフェースに使用するリソース名(例:
<literal>sriov-resource-du1</literal>)。このリソース名はプレフィックス<literal>rancher.io</literal>に追加されてラベルとして使用され、ワークロードで使用されます(例:
<literal>rancher.io/sriov-resource-du1</literal>)。</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal> —
使用する最初の<literal>PF</literal>インタフェースの名前(例: <literal>eth0</literal>)。</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal> —
使用する最初の物理機能<literal>PF</literal>の名前。これを使用して、より複雑なフィルタを生成します(例:
<literal>eth0#2-5</literal>)。</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal> —
最初の<literal>VF</literal>インタフェースに使用するドライバ名(例: <literal>vfio-pci</literal>)。</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal> —
最初の<literal>PF</literal>インタフェース用に作成する<literal>VF</literal>の数(例:
<literal>8</literal>)。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>/var/sriov-auto-filler.sh</literal>は、高レベルの設定マップ<literal>sriov-custom-auto-config</literal>と、低レベルのハードウェア情報を含む<literal>sriovnetworknodepolicy</literal>との間で情報を変換するために使用されます。このスクリプトは、ユーザがハードウェア情報を事前に把握する手間をなくすために作成されています。このファイルを変更する必要はありませんが、<literal>sr-iov</literal>を有効にして<literal>VF</literal>を作成する必要がある場合は、このファイルが存在する必要があります。</para>
</listitem>
<listitem>
<para>次の機能を有効にするために使用するカーネル引数:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>パラメータ</para></entry>
<entry align="left" valign="top"><para>値</para></entry>
<entry align="left" valign="top"><para>説明</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain、nohz、&#x200B;managed_irq、1-30、33-62</para></entry>
<entry align="left" valign="top"><para>コア1-30および33-62を分離します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>分離されたCPU間でカーネルがタイマー割り込みをずらすことができるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでタイマーティックを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>カーネルブートパラメータは、完全なdynticksとCPU分離の設定を行うための現在の主要インタフェースです。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでRCUコールバックを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUで割り込みを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>アイドル状態から抜け出すまでのレイテンシを最小化します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>vfioをdpdkインタフェースに使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>vfioをVFに使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageのサイズを1Gに設定できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>前に定義したHuge Pageの数。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageを有効にする場合のデフォルト値。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>ウォッチドッグを無効にします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>NMIウォッチドッグを無効にします。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>次のsystemdサービスは、以下のサービスを有効にするために使用します。</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> -
プロビジョニングプロセス中にIronicの情報を利用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換えます。</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> -
<literal>CPU</literal>の分離コアを有効にします(例: <literal>1-30,33-62</literal>)。</para>
</listitem>
<listitem>
<para><literal>performance-settings.service</literal> - CPUのパフォーマンス調整を有効にします。</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> - <literal>sriov</literal>
Helmチャートをインストールし、カスタムリソースが作成されるまで待機し、<literal>/var/sriov-auto-filler.sh</literal>を実行して設定マップ<literal>sriov-custom-auto-config</literal>の値を置き換えてワークロードが使用する<literal>sriovnetworknodepolicy</literal>を作成します。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>${RKE2_VERSION}</literal>は使用する<literal>RKE2</literal>のバージョンであり、この値は置き換えます(例:
<literal>v1.33.3+rke2r1</literal>)。</para>
</listitem>
</itemizedlist>
<para>これらの変更をすべて行うと、<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロックは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-crd
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-network-operator
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>前の各ブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行し、通信機能を使用する新しいダウンストリームクラスタのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>プライベートレジストリ</title>
<para>ワークロードで使用するイメージのミラーとしてプライベートレジストリを設定できます。</para>
<para>そのために、ダウンストリームクラスタで使用するプライベートレジストリに関する情報を含むシークレットを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para><literal>tls.crt</literal>、<literal>tls.key</literal>、および<literal>ca.crt</literal>は、プライベートレジストリを認証するために使用する証明書です。<literal>username</literal>および<literal>password</literal>は、プライベートレジストリを認証するために使用する資格情報です。</para>
<note>
<para><literal>tls.crt</literal>、<literal>tls.key</literal>、<literal>ca.crt</literal>、<literal>username</literal>、および<literal>password</literal>は、シークレットで使用する前にbase64形式でエンコードする必要があります。</para>
</note>
<para>これらの変更をすべて行うと、<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロックは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>ここで、<literal>registry.example.com</literal>は、ダウンストリームクラスタで使用するプライベートレジストリの名前の例です。これは実際の値に置き換える必要があります。</para>
</section>
<section xml:id="airgap-deployment">
<title>エアギャップシナリオでのダウンストリームクラスタのプロビジョニング</title>
<para>ダイレクトネットワークプロビジョニングワークフローでは、エアギャップシナリオでのダウンストリームクラスタのプロビジョニングを自動化できます。</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>エアギャップシナリオの要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>EIB</literal>を使用して生成された<literal>生</literal>のイメージには、エアギャップシナリオでダウンストリームクラスタを実行するために必要な特定のコンテナイメージ(HelmチャートOCIイメージとコンテナイメージ)を含める必要があります。詳細については、こちらのセクション(<xref
linkend="eib-edge-image-airgap"/>)を参照してください。</para>
</listitem>
<listitem>
<para>SR-IOVまたはその他のカスタムワークロードを使用する場合、プライベートレジストリへのプリロードに関するセクション(<xref
linkend="preload-private-registry"/>)に従って、ワークロードを実行するために必要なイメージをプライベートレジストリにプリロードする必要があります。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>エアギャップシナリオでのベアメタルホストの登録</title>
<para>管理クラスタにベアメタルホストを登録するプロセスは、前のセクション(<xref
linkend="enroll-bare-metal-host"/>)で説明したプロセスと同じです。</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>エアギャップシナリオでのダウンストリームクラスタのプロビジョニング</title>
<para>エアギャップシナリオでダウンストリームクラスタをプロビジョニングするために必要となる重要な変更がいくつかあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>capi-provisioning-example.yaml</literal>ファイルの<literal>RKE2ControlPlane</literal>ブロックに<literal>spec.agentConfig.airGapped:
true</literal>ディレクティブを含める必要があります。</para>
</listitem>
<listitem>
<para>プライベートレジストリに関するセクション(<xref
linkend="atip-private-registry"/>)に従って、プライベートレジストリの設定を<literal>capi-provisioning-airgap-example.yaml</literal>ファイルの<literal>RKE2ControlPlane</literal>ブロックに含める必要があります。</para>
</listitem>
<listitem>
<para>SR-IOV、またはHelmチャートをインストールする必要があるその他の<literal>AdditionalUserData</literal>設定(combustionスクリプト)を使用している場合、内容を変更して、パブリックレジストリを使用するのではなくプライベートレジストリを参照する必要があります。</para>
</listitem>
</orderedlist>
<para>次の例は、プライベートレジストリを参照するために必要な変更を行った、<literal>capi-provisioning-airgap-example.yaml</literal>ファイルの<literal>AdditionalUserData</literal>ブロックのSR-IOVの設定を示しています。</para>
<itemizedlist>
<listitem>
<para>プライベートレジストリのシークレットの参照</para>
</listitem>
<listitem>
<para>パブリックOCIイメージではなくプライベートレジストリを使用するHelmチャートの定義</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=1800
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --timeout=30m --all ; do sleep 10 ; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>ライフサイクルのアクション</title>
<para>このセクションでは、SUSE Telco Cloudを介してデプロイされたクラスタのライフサイクル管理アクションについて説明します。</para>
<section xml:id="id-management-cluster-upgrades">
<title>管理クラスタのアップグレード</title>
<para>管理クラスタのアップグレードには複数のコンポーネントが関係します。アップグレードする必要がある一般的なコンポーネントのリストについては、<literal>Day
2</literal>管理クラスタ(<xref linkend="day2-mgmt-cluster"/>)のドキュメントを参照してください。</para>
<para>このセットアップに固有のコンポーネントのアップグレード手順は以下のとおりです。</para>
<para><emphasis role="strong">Metal<superscript>3</superscript>のアップグレード</emphasis></para>
<para><literal>Metal<superscript>3</superscript></literal>をアップグレードするには、次のコマンドを使用してHelmリポジトリキャッシュを更新し、最新のチャートをフェッチしてHelmチャートリポジトリから<literal>Metal<superscript>3</superscript></literal>をインストールします。</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>その後、現在の設定をファイルにエクスポートしてから、その前のファイルを使用して<literal>Metal<superscript>3</superscript></literal>のバージョンをアップグレードすると、簡単にアップグレードできます。新しいバージョンで何らかの変更が必要な場合、アップグレードの前にそのファイルを編集できます。</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=304.0.16+up0.12.6</screen>
</section>
<section xml:id="atip-lifecycle-downstream">
<title>ダウンストリームクラスタのアップグレード</title>
<para>ダウンストリームクラスタをアップグレードするには、複数のコンポーネントを更新する必要があります。次の各セクションでは、各コンポーネントのアップグレードプロセスについて説明します。</para>
<para><emphasis role="strong">オペレーティングシステムのアップグレード</emphasis></para>
<para>このプロセスでは、次の参照資料(<xref
linkend="eib-edge-image-connected"/>)を確認して、新しいオペレーティングシステムバージョンで新しいイメージを構築します。<literal>EIB</literal>で生成されたこの新しいイメージにより、次のプロビジョニングフェーズでは、指定した新しいオペレーティングシステムバージョンが使用されます。次の手順では、この新しいイメージを使用してノードをアップグレードします。</para>
<para><emphasis role="strong">RKE2クラスタのアップグレード</emphasis></para>
<para>自動化されたワークフローを使用して<literal>RKE2</literal>クラスタをアップグレードするために必要な変更は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>次のセクション(<xref
linkend="single-node-provision"/>)に示す<literal>capi-provisioning-example.yaml</literal>のブロック<literal>RKE2ControlPlane</literal>を次のように変更します。</para>
<itemizedlist>
<listitem>
<para>目的の<literal>rolloutStrategy</literal>を指定します。</para>
</listitem>
<listitem>
<para><literal>${RKE2_NEW_VERSION}</literal>を置き換えて、<literal>RKE2</literal>クラスタのバージョンを新しいバージョンに変更します。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  version: ${RKE2_NEW_VERSION}
  replicas: 1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>次のセクション(<xref
linkend="single-node-provision"/>)に示す<literal>capi-provisioning-example.yaml</literal>のブロック<literal>Metal3MachineTemplate</literal>を次のように変更します。</para>
<itemizedlist>
<listitem>
<para>イメージ名およびチェックサムを、前の手順で生成した新しいバージョンに変更します。</para>
</listitem>
<listitem>
<para>ディレクティブ<literal>nodeReuse</literal>を追加して<literal>true</literal>に設定し、新しいノードが作成されないようにします。</para>
</listitem>
<listitem>
<para>ディレクティブ<literal>automatedCleaningMode</literal>を追加して<literal>metadata</literal>に設定し、ノードの自動クリーニングを有効にします。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>これらの変更を行った後、次にコマンドを使用して<literal>capi-provisioning-example.yaml</literal>ファイルをクラスタに適用できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-troubleshooting-3">
<title>トラブルシューティング</title>
<partintro>
<para>このセクションでは、SUSE
Edgeのデプロイメントと操作に関する一般的な問題を診断し、解決するためのガイダンスを提供します。さまざまなトピックを網羅していて、コンポーネント固有のトラブルシューティング手順、主要ツール、関連するログの場所を説明しています。</para>
</partintro>
<chapter xml:id="general-troubleshooting-principles">
<title>一般的なトラブルシューティングの原則</title>
<para>コンポーネント固有の問題に取りかかる前に、以下の一般的な原則を考慮してください。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ログの確認</emphasis>:
ログは主要な情報源です。ほとんどの場合、エラーは説明不要で、何が失敗したかに関するヒントが含まれています。</para>
</listitem>
<listitem>
<para><emphasis role="strong">クロックの確認</emphasis>:
システム間にクロックの相違があると、あらゆる種類の異なるエラーが発生する可能性があります。クロックが同期していることを確認します。EIBにブート時にクロック同期を強制するように指示することができます。「OSの時刻の設定」(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ブートの問題</emphasis>:
システムがブート中に停止した場合、最後に表示されたメッセージをメモします。コンソール(物理的またはBMC経由)にアクセスして、ブートメッセージを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ネットワークの問題</emphasis>: ネットワークインタフェースの設定(<literal>ip
a</literal>)、ルーティングテーブル(<literal>ip
route</literal>)、他のノードと外部サービスとの間のテスト接続(<literal>ping</literal>、
<literal>nc</literal>)を確認します。ファイアウォールルールがネットワークポートをブロックしていないことを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コンポーネントのステータスの確認</emphasis>: Kubernetesリソースの場合は、
<literal>kubectl get</literal>と<literal>kubectl
describe</literal>を使用します。特定のKubernetesネームスペースのイベントを確認するには、<literal>kubectl
get events --sort-by='.lastTimestamp' -n &lt;namespace&gt;</literal>を使用します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">サービスステータスの確認</emphasis>:
systemdサービスの場合は、<literal>systemctl status &lt;service&gt;</literal>を使用します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構文の確認</emphasis>:
ソフトウェアは設定ファイルに特定の構造と構文を想定しています。たとえばYAMLファイルの場合、<literal>yamllint</literal>や類似のツールを使用して、正しい構文を確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">問題の特定</emphasis>:
問題を特定のコンポーネントまたはレイヤ(たとえば、ネットワーク、ストレージ、OS、Kubernetes、Metal<superscript>3</superscript>、Ironicなど​)に絞り込みます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ドキュメント</emphasis>: 詳細については、常に公式の<link
xl:href="https://documentation.suse.com/suse-edge/">SUSE
Edgeドキュメント</link>およびアップストリームドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">バージョン</emphasis>: SUSE
Edgeは、さまざまなSUSEコンポーネントを独自に徹底的にテストしたバージョンです。SUSE
Edgeリリースごとの各コンポーネントのバージョンは、<link
xl:href="https://documentation.suse.com/suse-edge/support-matrix/html/support-matrix/index.html">SUSE
Edgeサポートマトリックス</link>で確認できます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">既知の問題</emphasis>: SUSE
Edgeの各リリースには、リリースノートに「既知の問題」セクションがあり、今後のリリースで修正する予定だが、現在のリリースに影響する可能性のある問題に関する情報が記載されています。</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="troubleshooting-kiwi">
<title>Kiwiのトラブルシューティング</title>
<para>Kiwiを使用して、Edge Image Builderで使用される、更新されたSUSE Linux Microイメージを生成します。</para>
<itemizedlist>
<title>一般的な問題</title>
<listitem>
<para><emphasis role="strong">SL Microバージョンの不一致</emphasis>:
構築ホストのオペレーティングシステムのバージョンは、構築されるオペレーティングシステムのバージョンと一致する必要があります(SL Micro
6.0ホスト → SL Micro 6.0イメージ)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">SELinuxをEnforcing状態にする</emphasis>:
特定の制限のため、Kiwiを使用してイメージを構築できるように、現在はSELinuxを一時的に無効にする必要があります。<literal>getenforce</literal>でSElinuxステータスを確認し、<literal>setenforce
0</literal>を使用して構築プロセスを実行する前にSElinuxを無効にしてください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構築ホストが登録されていない</emphasis>: 構築プロセスでは、SUSE
SCCからパッケージをプルできるように、構築ホストサブスクリプションを使用します。ホストが登録されていない場合は失敗します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ループデバイステスト失敗</emphasis>: 初めてKiwi構築プロセスが実行されると、
開始直後に失敗し、「ERROR: Early loop device test failed, please retry the container
run. (エラー:
ループデバイスの早期テストが失敗しました。もう一度コンテナの実行を試してください)」というエラーメッセージが表示されます。これは、基になるホストシステム上に作成されるループデバイスがすぐにはコンテナイメージ内に表示されないことを示す症状です。Kiwi構築プロセスを再実行すれば、問題なく続行されるはずです。</para>
</listitem>
<listitem>
<para><emphasis role="strong">許可がない</emphasis>:
構築プロセスはルートユーザとして(またはsudoを介して)実行されることが想定されています。</para>
</listitem>
<listitem>
<para><emphasis role="strong">間違った権限</emphasis>:
構築プロセスは、コンテナを実行する際には<literal>--privileged</literal>フラグが指定されていることが想定されています。このフラグが指定されていることを再度確認してください。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>ログ</title>
<listitem>
<para><emphasis role="strong">構築コンテナのログ</emphasis>:
構築コンテナのログを確認します。ログはアーティファクトの保存に使用されたディレクトリに生成されます。必要な情報については、dockerログまたはpodmanログも確認してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">一時構築ディレクトリ</emphasis>:
Kiwiは、構築プロセス中に一時ディレクトリを作成します。メイン出力が不十分な場合は、これらのディレクトリに中間ログやアーティファクトがないか確認します。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>トラブルシューティング手順</title>
<listitem>
<para><emphasis role="strong"><literal>build-image</literal>出力の確認</emphasis>:
コンソール出力のエラーメッセージは通常、非常にわかりやすいものです。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構築環境の確認</emphasis>: Kiwiを実行しているマシンでKiwi自体のすべての前提条件(
docker/podman、SElinux, 十分なディスク容量など)が満たされていることを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構築コンテナログの検査</emphasis>:
失敗したコンテナのログで詳細なエラーについて確認します(上記参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">定義ファイルの確認</emphasis>:
カスタムKiwiイメージ定義ファイルを使用している場合、タイポや構文についてファイルを再確認します。</para>
</listitem>
</orderedlist>
<note>
<para><link
xl:href="https://documentation.suse.com/appliance/kiwi-9/html/kiwi/troubleshooting.html">Kiwiトラブルシューティングガイド</link>を確認してください。</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-image-builder">
<title>Edge Image Builder (EIB)のトラブルシューティング</title>
<para>EIBは、カスタムSUSE Edgeイメージを作成するために使用されます。</para>
<itemizedlist>
<title>一般的な問題</title>
<listitem>
<para><emphasis role="strong">間違ったSCCコード</emphasis>: EIB定義ファイルに使用されているSCCコードがSL
Microのバージョンとアーキテクチャに一致していることを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">依存関係がない</emphasis>: 構築環境内に不足しているパッケージやツールがないか確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">不正なイメージサイズ</emphasis>:
RAWイメージの場合、<literal>diskSize</literal>パラメータが必要であり、イメージ、RPM、およびイメージに含まれている他のアーティファクトに大きく依存します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">許可</emphasis>:
custom/filesディレクトリにスクリプトを保存する場合、スクリプトに実行許可があることを確認します。これらのファイルはCombustion時にのみ利用可能であり、EIBによって変更は実行されないためです。</para>
</listitem>
<listitem>
<para><emphasis role="strong">オペレーティングシステムグループの依存関係</emphasis>:
カスタムユーザとグループを使用してイメージを作成する場合、「<literal>primaryGroup</literal>」として設定されるグループを明示的に作成する必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">オペレーティングシステムユーザのsshkeysにはホームフォルダが必要</emphasis>:
sshkeysを持つユーザでイメージを作成する場合、
<literal>createHomeDir=true</literal>を指定してホームフォルダも作成する必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Combustionの問題</emphasis>:
EIBは、OSのカスタマイズおよびその他のすべてのSUSE
Edgeコンポーネントのデプロイメントに対してCombustionに依存しています。これには、custom/scriptsフォルダに配置されるカスタムスクリプトも含まれます。Combustionプロセスは<literal>initrd</literal>時に実行されるため、スクリプト実行時にシステムが完全に起動していないことに注意してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Podmanマシンサイズ</emphasis>: EIBの「ヒントとコツ」セクション(<xref
linkend="tips-and-tricks"/>)で説明されているように、Linux以外のオペレーティングシステムでEIBコンテナを実行するための十分なCPU/メモリがPodmanマシンにあることを確認します。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>ログ</title>
<listitem>
<para><emphasis role="strong">EIB出力</emphasis>: <literal>eib
build</literal>コマンドのコンソール出力は重要です。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構築コンテナのログ</emphasis>:
構築コンテナのログを確認します。ログは、アーティファクトを保存するために使用されたディレクトリに生成されます。必要な情報については<literal>dockerログ</literal>または<literal>podmanログ</literal>も確認してください。</para>
<note>
<para>詳細については、「 <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Debugging
(デバッグ)</link>」を参照してください。</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">一時的な構築ディレクトリ</emphasis>:
EIBは構築プロセス中に一時ディレクトリを作成します。メイン出力が不十分な場合は、これらのディレクトリで中間ログやアーティファクトを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Combustionログ</emphasis>:
EIBで構築されているイメージが何らかの理由で起動しない場合、
ルートシェルを利用できます。ホストコンソールに(物理的に、またはBMC経由などで)接続し、<literal>journalctl -u
burning</literal>でCombustionログを確認し、通常は<literal>journalctl</literal>ですべてのオペレーティングシステムログを確認して、失敗の根本原因を特定します。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>トラブルシューティング手順</title>
<listitem>
<para><emphasis role="strong"><literal>eib-build</literal>出力の確認</emphasis>:
コンソール出力のエラーメッセージは通常、非常にわかりやすいです。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構築環境の確認</emphasis>:
EIBを実行しているマシンで、EIB自体のすべての前提条件(docker/podman、十分なディスク容量など)が満たされていることを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">構築コンテナログの検査</emphasis>:
失敗したコンテナのログで詳細なエラーについて確認します(上記参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong"><literal>eib</literal>設定の確認</emphasis>:
<literal>eib</literal>設定ファイルにタイポがないか、ソースファイルや構築スクリプトへのパスが間違っていないか、再確認します。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">コンポーネントの個別テスト</emphasis>:
EIBビルドにカスタムスクリプトやステージが含まれている場合は、個別に実行して失敗を特定します。</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para><link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Edge
Image Builderの「Debugging (デバッグ)」</link>を確認してください。</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-networking">
<title>Edgeネットワーキング(NMC)のトラブルシューティング</title>
<para>NMCは、SL Micro
EIBイメージに挿入され、起動時に、Combustionを介してEdgeホストのネットワークを設定します。また、Metal3ワークフローでは検査プロセスの一環として実行されています。ホストの初回起動時、またはMetal3検査プロセス中に問題が発生する可能性があります。</para>
<itemizedlist>
<title>一般的な問題</title>
<listitem>
<para><emphasis role="strong">ホストが初回起動時に正常に起動できない</emphasis>:
不正な形式のネットワーク定義ファイルにより、combustionフェーズが失敗し、ホストがルートシェルをドロップする可能性があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ファイルが適切に生成されない</emphasis>: ネットワークファイルが<link
xl:href="https://nmstate.io/examples.html">NMState</link>形式と一致していることを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ネットワークインタフェースが正常に設定されない</emphasis>:
MACアドレスがホストで使用されているインタフェースと一致していることを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">インタフェース名の不一致</emphasis>:
<literal>net.ifnames=1</literal>カーネル引数により、 <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">ネットワークインタフェースの予測可能な命名規則</link>が有効になるため、<literal>eth0</literal>はなくなり、<literal>enp2s0</literal>などの他の命名規則が使用されます。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>ログ</title>
<listitem>
<para><emphasis role="strong">Combustionログ</emphasis>:
Combustion時にnmcが使用されるため、プロビジョニング中のホスト上で、<literal>journalctl -u
combustion</literal>を使用してCombustionログを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">NetworkManagerログ</emphasis>:
Metal<superscript>3</superscript>デプロイメントワークフローでは、nmcはIPA実行の一部であり、systemdのExecStartPre機能を使用してNetworkManagerサービスの依存関係として実行されます。IPAホスト上で<literal>journalctl
-u
NetworkManager</literal>としてNetworkManagerログを確認します(IPAで起動したときにホストにアクセスする方法については、「ダイレクトネットワークプロビジョニングのトラブルシューティング」(<xref
linkend="troubleshooting-directed-network-provisioning"/>)セクションを参照してください)。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>トラブルシューティング手順</title>
<listitem>
<para><emphasis role="strong">yaml構文の確認</emphasis>:
nmc設定ファイルはyamlファイルです。適切な構文かどうかを<literal>yamllint</literal>または同様のツールで確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">nmcを手動で実行する</emphasis>:
nmcはEIBコンテナの一部であるため、問題をデバッグするには、ローカルpodmanコマンドを使用できます。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>nmcファイルを保存するための一時フォルダを作成します。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p ${HOME}/tmp/foo</screen>
</listitem>
<listitem>
<para>その場所にnmcファイルを保存します。</para>
<screen language="shell" linenumbering="unnumbered">❯ tree --noreport ${HOME}/tmp/foo
/Users/johndoe/tmp/foo
├── host1.example.com.yaml
└── host2.example.com.yaml</screen>
</listitem>
<listitem>
<para>エントリポイントとしてnmcを使用してEIBコンテナを実行し、generateコマンドを使用して、Combustion時にnmcが実行するのと同じタスクを実行します。</para>
<screen language="shell" linenumbering="unnumbered">podman run -it --rm -v ${HOME}/tmp/foo:/tmp/foo:Z --entrypoint=/usr/bin/nmc registry.suse.com/edge/3.3/edge-image-builder:1.2.0 generate --config-dir /tmp/foo --output-dir /tmp/foo/

[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host2.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host1.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc] Successfully generated and stored network config</screen>
</listitem>
<listitem>
<para>一時フォルダに生成されるログとファイルを確認します。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-phone-home-scenarios">
<title>Phone-Homeシナリオのトラブルシューティング</title>
<para>Phone-Homeシナリオには、Elementalを使用して管理クラスタに接続し直し、EIBを使用してElemental登録情報を含むOSイメージを作成することが含まれます。ホストの初回起動時、EIB構築プロセス中、または管理クラスタに登録しようとするときに問題が発生する可能性があります。</para>
<itemizedlist>
<title>一般的な問題</title>
<listitem>
<para><emphasis role="strong">システムを登録できない</emphasis>:
UIでノードが登録されていません。ホストが適切に起動していること、Rancherと通信できること、クロックが同期していること、Elementalサービスが正常であることを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">システムをプロビジョニングできない</emphasis>:
ノードは登録されているが、プロビジョニングに失敗します。ホストがRancerと通信できること、クロックが同期していること、Elementalサービスに問題がないことを確認します。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>ログ</title>
<listitem>
<para><emphasis role="strong">システムログ</emphasis>: <literal>journalctl</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Elemental-system-agentログ</emphasis>:
<literal>journalctl -u elemental-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s/RKE2ログ</emphasis>: <literal>journalctl -u
k3sまたはjournalctl -u rke2-server</literal> (または<literal>rke2-agent</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">ElementalオペレータPod</emphasis>: <literal>kubectl logs
-n cattle-elemental-system -l app=elemental-operator</literal></para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>トラブルシューティング手順</title>
<listitem>
<para><emphasis role="strong">ログの確認</emphasis>:
ElementalオペレータPodのログで、問題がないかどうかを確認します。ノードが起動している場合は、ホストログを確認してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">MachineRegistrationとTPMの確認</emphasis>:
デフォルトで、TPMは<link
xl:href="https://elemental.docs.rancher.com/authentication/">認証</link>に使用されますが、TPMのないホストには、代替が存在します。</para>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-directed-network-provisioning">
<title>ダイレクトネットワークプロビジョニングのトラブルシューティング</title>
<para>ダイレクトネットワークプロビジョニングシナリオには、Metal<superscript>3</superscript>およびCAPI要素を使用したダウンストリームクラスタのプロビジョニングが含まれます。EIBを使用したOSイメージの作成も含まれます。ホストの初回起動時、または検査プロセスまたはプロビジョニングプロセス中に問題が発生する可能性があります。</para>
<itemizedlist>
<title>一般的な問題</title>
<listitem>
<para><emphasis role="strong">古いファームウェア</emphasis>:
使用されている物理ホスト上のさまざまなファームウェアがすべて最新であることを確認します。これには、BMCファームウェアが含まれます。Metal<superscript>3</superscript>
<link
xl:href="https://book.metal3.io/bmo/supported_hardware#redfish-and-its-variants">では特定のファームウェアや更新されたファームウェアが必要になる</link>場合があるためです。</para>
</listitem>
<listitem>
<para><emphasis role="strong">プロビジョニングがSSLエラーで失敗した</emphasis>:
イメージを提供するWebサーバがhttpsを使用する場合、Metal<superscript>3</superscript>はIPAイメージに証明書を挿入して信頼するように設定する必要があります。<literal>ca-additional.crt</literal>ファイルをMetal<superscript>3</superscript>チャートに含める方法については、Kubernetesフォルダ(<xref
linkend="mgmt-cluster-kubernetes-folder"/>)を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">IPAを使用したホストの起動時に証明書の問題が発生する</emphasis>:
一部のサーバベンダは仮想メディアISOイメージをBMCにアタッチする際にSSL接続を確認します。このため、Metal3デプロイメント用に生成された証明書が自己署名されているために、問題が発生する可能性があります。ホストが起動中であるにもかかわらずUEFIシェルに切り替わってしまう可能性があります。この問題の解決方法については、「仮想メディアISOをアタッチするためのTLSの無効化」(<xref
linkend="disabling-tls-for-virtualmedia-iso-attachment"/>)を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">間違った名前またはラベル参照</emphasis>:
クラスタが間違った名前またはラベルでノードを参照する場合、クラスタはデプロイ済みと表示されますが、BMHは「使用可能」のままとなります。BMHに関連するオブジェクトの参照を再確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">BMC通信の問題</emphasis>:
管理クラスタ上で実行されているMetal<superscript>3</superscript>
Podがプロビジョニング中のホストのBMCにアクセスできることを確認します(通常BMCネットワークは非常に制限されています)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">不正なベアメタルホストの状態</emphasis>: BMHオブジェクトは、
その存続期間中にさまざまな状態(検査中、準備中、プロビジョニング済みなど)に移行します(<link
xl:href="https://book.metal3.io/bmo/state_machine">存続期間中のマシンの状態</link>)。不正な状態が検出された場合は、BMHオブジェクトの<literal>status</literal>フィールドを確認してください。このフィールドには<literal>kubectl
get bmh &lt;name&gt; -o jsonpath=’{.status}’| jq</literal>のような詳細情報が含まれています。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ホストがプロビジョニング解除されない</emphasis>:
プロビジョニング解除しようとしていたホストが失敗した場合、BMHオブジェクトに次のように「detached」アノテーションを追加すると削除を試行できます:
<literal>kubectl annotate bmh/&lt;BMH&gt;
baremetalhost.metal3.io/detached=””</literal>。</para>
</listitem>
<listitem>
<para><emphasis role="strong">イメージエラー</emphasis>:
ダウンストリームクラスタに対してEIBを使用して構築されるイメージが使用可能であり、適切なチェックサムがあり、解凍するには大きすぎない、またはディスクに対して大きすぎないことを確認します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ディスクサイズの不一致</emphasis>:
デフォルトでは、全ディスクを満たすようにディスクは拡張されません。「Growfsスクリプト」(<xref
linkend="growfs-script"/>)セクションで説明しているように、ダウンストリームクラスタホスト用にEIBで構築されるイメージにはgrowfsスクリプトを含める必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">クリーニングプロセスの停止</emphasis>:
クリーニングプロセスは数回再試行されます。ホストの問題によりクリーニングができなくなった場合は、まず、BMHオブジェクトの<literal>automatedCleanMode</literal>フィールドを<literal>disabled</literal>に設定してクリーニングを無効にします。</para>
<warning>
<para>クリーニングプロセスに必要以上に時間がかかっている、または失敗している場合は、ファイナライザを手動で削除することは推奨されません。手動で削除すると、Kubernetesからホストレコードが削除されますが、Ironicには残ります。現在実行中のアクションはバックグラウンドで続行されるため、ホストを再度追加しようとすると競合により失敗する可能性があります。</para>
</warning>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3/Rancher Turtles/CAPI Podの問題</emphasis>:
すべての必要なコンポーネントのデプロイメントフローは次のとおりです。</para>
<itemizedlist>
<listitem>
<para>Rancher TurtlesコントローラはCAPIオペレータコントローラをデプロイします。</para>
</listitem>
<listitem>
<para>次に、CAPIオペレータコントローラはプロバイダコントローラ(CAPIコア、CAPM3、およびRKE2コントロールプレーン/ブートストラップ)をデプロイします。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>すべてのPodが正常に実行されていることを確認し、そうでない場合はログを確認します。</para>
<itemizedlist>
<title>ログ</title>
<listitem>
<para><emphasis role="strong">Metal<superscript>3</superscript>ログ</emphasis>:
さまざまなPodのログを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n metal3-system -l app.kubernetes.io/component=baremetal-operator
kubectl logs -n metal3-system -l app.kubernetes.io/component=ironic</screen>
<note>
<para>metal3-ironic Podには、少なくとも4つの異なるコンテナ(<literal>ironic-httpd</literal>、「
ironic-log-watch」、 <literal>ironic</literal>
、<literal>ironic-ipa-downloader</literal>
(init))が同じPod上に含まれています。<literal>kubectlログ</literal>を使用する場合は<literal>-c</literal>フラグを指定して、各コンテナのログを確認します。</para>
</note>
<note>
<para><literal>ironic-log-watch</literal>コンテナは、ネットワーク接続により、管理クラスタにコンソールログを返送できる場合は、検査/プロビジョニング後にホストからこのログを公開します。これはプロビジョニングエラーが発生したが、BMCコンソールログに直接アクセスできない場合に役立つ可能性があります。</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Rancher Turtlesログ</emphasis>: さまざまなPodのログを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n rancher-turtles-system -l control-plane=controller-manager
kubectl logs -n rancher-turtles-system -l app.kubernetes.io/name=cluster-api-operator
kubectl logs -n rke2-bootstrap-system -l cluster.x-k8s.io/provider=bootstrap-rke2
kubectl logs -n rke2-control-plane-system -l cluster.x-k8s.io/provider=control-plane-rke2
kubectl logs -n capi-system -l cluster.x-k8s.io/provider=cluster-api
kubectl logs -n capm3-system -l cluster.x-k8s.io/provider=infrastructure-metal3</screen>
</listitem>
<listitem>
<para><emphasis role="strong">BMCログ</emphasis>:
通常BMCには、ほとんどの操作を実行できるUIがあります。また、通常は潜在的な問題(イメージにアクセスできない、ハードウェアエラーなど)がないか確認できる「ログ」セクションもあります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コンソールログ</emphasis>: BMCコンソールに接続して(BMC Web
UI、シリアルなどを経由)、書き込まれているログにエラーがないか確認します。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>トラブルシューティング手順</title>
<listitem>
<para><emphasis role="strong"><literal>BareMetalHost</literal>ステータスの確認</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>kubectl get bmh
-A</literal>を実行すると、現在の状態が表示されます。<literal>provisioning</literal>、<literal>ready</literal>、<literal>error</literal>、<literal>registering</literal>を探してください。</para>
</listitem>
<listitem>
<para><literal>kubectl describe bmh -n &lt;namespace&gt;
&lt;bmh_name&gt;</literal>を実行すると、BMHが停止する可能性がある理由を説明する、詳細なイベントと状態が表示されます。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">RedFishの接続性のテスト</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Metal<superscript>3</superscript>コントロールプレーンから<literal>curl</literal>を使用して、RedFishを介してBMCへの接続性をテストします。</para>
</listitem>
<listitem>
<para><literal>BareMetalHost-Secret</literal>定義に正しいBMC資格情報が提供されていることを確認します。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">turtles/CAPI/metal3 Podのステータスの確認</emphasis>:
管理クラスタ上のコンテナが稼働中であることを確認します: <literal>kubectl get pods -n
metal3-system</literal>および<literal>kubectl get pods -n
rancher-turtles-system</literal>
(<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>、および<literal>rke2-control-plane-system</literal>も参照)。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">Ironicエンドポイントにプロビジョニング中のホストからアクセスできることを確認する</emphasis>:
プロビジョニング中のホストは、レポートをMetal<superscript>3</superscript>に返すため、Ironicエンドポイントにアクセスできる必要があります。<literal>kubectl
get svc -n metal3-system
metal3-metal3-ironic</literal>を使用してIPを確認し、<literal>curl/nc</literal>経由でアクセスを試みてください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">IPAイメージがBMCからアクセスできることを確認する</emphasis>:
IPAはIronicエンドポイントによって提供されており、仮想CDとして使用されているため、BMCからアクセスできる必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">OSイメージがプロビジョニング中のホストからアクセスできることを確認する</emphasis>:
ホストのプロビジョニングに使用されるイメージは、一時的にダウンロードされ、ディスクに書き込まれるため、ホスト自体からアクセスできる必要があります(IPA実行時)。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">Metal<superscript>3</superscript>コンポーネントログの調査</emphasis>:
上記を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">BMH検査の再トリガ</emphasis>:
検査に失敗するか、使用可能なホストのハードウェアが変更された場合、BMHオブジェクトに<literal>inspect.metal3.io:
""</literal>というアノテーションを付けることで、新しい検査プロセスをトリガできます。詳細については、<link
xl:href="https://book.metal3.io/bmo/inspect_annotation">Metal<superscript>3</superscript>の「Controlling
inspection (検査の制御)」</link>ガイドを参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ベアメタルIPAコンソール</emphasis>:
IPAの問題をトラブルシューティングするには、いくつかの代替手段が存在します。</para>
<itemizedlist>
<listitem>
<para>「自動ログイン」を有効にします。これにより、IPAコンソールへの接続時に、ルートユーザが自動的にログインできるようになります。</para>
<warning>
<para>これはホストへの完全なアクセス権を付与するため、デバッグ目的のみに使用されます。</para>
</warning>
<para>自動ログインを有効にするには、Metal3
helmの<literal>global.ironicKernelParams</literal>値が<literal>console=ttyS0
suse.autologin=ttyS0</literal>のようになる必要があります(コンソールによっては、<literal>ttyS0</literal>を変更できます)。次にMetal<superscript>3</superscript>チャートの再デプロイメントを実行する必要があります(<literal>ttyS0</literal>は一例であり、実際の端末と一致する必要があることに注意してください。たとえば、ベアメタルでは多くの場合<literal>tty1</literal>となります。これは、起動時にIPA
RAMディスクのコンソール出力で<literal>/etc/issue</literal>にコンソール名が表示されていることを見ることで確認できます)。</para>
<para>別の方法は、<literal>metal3-system</literal>ネームスペースの<literal>ironic-bmo</literal>
configmapの<literal>IRONIC_KERNEL_PARAMS</literal>パラメータを変更することです。この方法は、<literal>kubectl</literal>
editで実行できるため簡単ですが、チャートを更新すると上書きされます。その後、Metal<superscript>3</superscript>
Podを<literal>kubectl delete pod -n metal3-system -l
app.kubernetes.io/component=ironic</literal>で再起動する必要があります。</para>
</listitem>
<listitem>
<para>IPAにルートユーザのsshキーを挿入します。</para>
<warning>
<para>これはホストへの完全なアクセス権を付与するため、デバッグ目的のみに使用されます。</para>
</warning>
<para>ルートユーザのsshキーを挿入するには、Metal<superscript>3</superscript> helm
<literal>debug.ironicRamdiskSshKey</literal>値を使用する必要があります。その後、Metal<superscript>3</superscript>
チャートの再デプロイメントを実行する必要があります。</para>
<para>別の方法は、<literal>metal3-system</literal>ネームスペースの<literal>ironic-bmo
configmap</literal>の<literal>IRONIC_RAMDISK_SSH_KEY</literal>パラメータを変更することです。この方法は、<literal>kubectl</literal>
editで実行できるため簡単ですが、チャートを更新すると上書きされます。その後、Metal<superscript>3</superscript>
Podを<literal>kubectl delete pod -n metal3-system -l
app.kubernetes.io/component=ironic</literal>で再起動する必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para><link
xl:href="https://cluster-api.sigs.k8s.io/user/troubleshooting">CAPIトラブルシューティング</link>ガイドと<link
xl:href="https://book.metal3.io/troubleshooting">Metal<superscript>3</superscript>トラブルシューティング</link>ガイドを確認してください。</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-other-components">
<title>その他のコンポーネントのトラブルシューティング</title>
<para>その他のSUSE Edgeコンポーネントのトラブルシューティングガイドについては、それぞれの公式ドキュメントを参照してください。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://documentation.suse.com/smart/micro-clouds/html/SLE-Micro-5.5-admin/index.html#id-1.10">SUSE
Linux Microのトラブルシューティング</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.rke2.io/known_issues">RKE2の既知の問題</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.k3s.io/known-issues">K3sの既知の問題</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/troubleshooting/general-troubleshooting">Rancherの一般的なトラブルシューティング</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://documentation.suse.com/multi-linux-manager/5.1/en/docs/administration/troubleshooting/tshoot-intro.html">SUSE
Multi-Linux Managerのトラブルシューティング</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://elemental.docs.rancher.com/troubleshooting-support/">Elementalサポート</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://turtles.docs.rancher.com/turtles/stable/en/troubleshooting/troubleshooting.html">Rancher
Turtlesのトラブルシューティング</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://longhorn.io/docs/1.9.1/troubleshoot/troubleshooting/">Longhornのトラブルシューティング</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://open-docs.neuvector.com/next/troubleshooting/troubleshooting/">Neuvectorのトラブルシューティング</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://fleet.rancher.io/troubleshooting">Fleetのトラブルシューティング</link></para>
</listitem>
</itemizedlist>
<para>また、<link
xl:href="https://www.suse.com/support/kb/">SUSEナレッジベース</link>も参照してください。</para>
</chapter>
<chapter xml:id="collecting-diagnostics-for-support">
<title>サポートのための診断情報の収集</title>
<para>SUSEサポートに連絡する場合は、包括的な診断情報を提供することが重要です。</para>
<itemizedlist>
<title>収集すべき重要な情報</title>
<listitem>
<para><emphasis role="strong">問題の詳細な説明</emphasis>:
何が起こったか、いつ起こったか、その際に何をしていたのか、どのような動作が期待されるか、実際の動作はどのようなものか?</para>
</listitem>
<listitem>
<para><emphasis role="strong">再現する手順</emphasis>: 問題を忠実に再現できますか?
再現できる場合は、正確な手順をリストしてください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コンポーネントのバージョン</emphasis>: SUSE
Edgeバージョン、コンポーネントのバージョン(RKE2/K3、EIB、Metal<superscript>3</superscript>、Elemental、..)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">関連するログ</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>journalctl</literal>の出力(可能な場合はサービス別にフィルタリング、または完全な起動ログ)。</para>
</listitem>
<listitem>
<para>Kubernetes Podのログ(kubectlログ)。</para>
</listitem>
<listitem>
<para>Metal³/Elementalコンポーネントのログ。</para>
</listitem>
<listitem>
<para>EIB構築ログおよびその他のログ。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">システム情報</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>uname -a</literal></para>
</listitem>
<listitem>
<para><literal>df -h</literal></para>
</listitem>
<listitem>
<para><literal>ip a</literal></para>
</listitem>
<listitem>
<para><literal>/etc/os-release</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">設定ファイル</emphasis>:
Elemental、Metal<superscript>3</superscript>、EIBに関連する設定ファイル(Helmチャート値、configmapなど)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Kubernetes情報</emphasis>: ノード、サービス、デプロイメントなど。</para>
</listitem>
<listitem>
<para><emphasis role="strong">影響を受けるKubernetesオブジェクト</emphasis>:
BMH、MachineRegistrationなど。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>収集方法</title>
<listitem>
<para><emphasis role="strong">ログの場合</emphasis>: コマンド出力をファイルにリダイレクトします(例:
<literal>journalctl -u k3s &gt; k3s_logs.txt</literal>)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Kubernetesリソースの場合</emphasis>: <literal>kubectl get
&lt;resource&gt; -o yaml &gt;
&lt;resource_name&gt;.yaml</literal>を使用して、詳細なYAML定義を取得します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">システム情報の場合</emphasis>: 上記のコマンドの出力を収集します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">SL Microの場合</emphasis>:
<literal>supportconfig</literal>を使用してサポートのためのシステム情報を収集する方法については、<link
xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-adm-support-slemicro.html">SUSE
Linux Microトラブルシューティングガイド</link>のドキュメントを確認してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2/Rancher の場合</emphasis>: Rancher v2.x
Linuxログコレクタスクリプトを実行するには、<link
xl:href="https://www.suse.com/support/kb/doc/?id=000020191">Rancher v2.x
Linuxログコレクタスクリプト</link>の記事を確認してください。</para>
</listitem>
</itemizedlist>
<formalpara>
<title>サポートに問い合わせる</title>
<para>SUSEサポートへの問い合わせ方法の詳細については、「<link
xl:href="https://www.suse.com/support/kb/doc/?id=000019452">How-to
effectively work with SUSE Technical Support
(SUSEテクニカルサポートを効果的に活用する方法)</link>」にある記事と、「<link
xl:href="https://www.suse.com/support/handbook/">SUSEテクニカルサポートのハンドブック</link>」にあるサポートハンドブックを参照してください。</para>
</formalpara>
</chapter>
</part>
<part xml:id="id-appendix">
<title>付録</title>
<chapter xml:id="id-release-notes">
<title>リリースノート</title>
<section xml:id="release-notes">
<title>要約</title>
<para>SUSE Edge
3.4は、インフラストラクチャとクラウドネイティブなアプリケーションをエッジにデプロイするという独自の課題に対処することに特化した、緊密に統合されて包括的に検証されたエンドツーエンドのソリューションです。重点を置いているのは、独創的でありながら高い柔軟性とスケーラビリティを持つセキュアなプラットフォームを提供し、初期デプロイメントイメージの構築からノードのプロビジョニングとオンボーディング、アプリケーションのデプロイメント、可観測性、ライフサイクル管理にまで対応することです。</para>
<para>このソリューションは、顧客の要件や期待はさまざまであるため「万能」なエッジプラットフォームは存在しないという考え方に基づいて設計されています。エッジデプロイメントにより、実に困難な問題を解決し、継続的に進化させることが要求されます。たとえば、大規模なスケーラビリティ、ネットワークの可用性の制限、物理的なスペースの制約、新たなセキュリティの脅威と攻撃ベクトル、ハードウェアアーキテクチャとシステムリソースのバリエーション、レガシインフラストラクチャやレガシアプリケーションのデプロイとインタフェースの要件、耐用年数を延長している顧客ソリューションといった課題があります。</para>
<para>SUSE
Edgeは、最良のオープンソースソフトウェアに基づいてゼロから構築されており、SUSEが持つ、30年にわたってセキュアで安定した定評あるSUSE
Linuxプラットフォームを提供してきた歴史と、Rancherポートフォリオによって拡張性に優れ機能豊富なKubernetes管理を提供してきた経験の両方に合致するものです。SUSE
Edgeは、これらの機能の上に構築されており、小売、医療、輸送、物流、通信、スマート製造、産業用IoTなど、さまざまな市場セグメントに対応できる機能を提供します。</para>
<para>SUSE Edgeの製品サポートライフサイクルの更新に関する詳細については、<link
xl:href="https://www.suse.com/lifecycle/#suse-edge-33">製品サポートライフサイクル</link>を参照してください。</para>
<note>
<para>SUSE Telco Cloud (旧称SUSE Edge for Telco)はSUSE
Edgeの派生製品にあたり、このプラットフォームを通信事業者の要件に対処可能にするための最適化とコンポーネントが追加されています。明記されていない限り、すべてのリリースノートはSUSE
Edge 3.4とSUSE Telco Cloud 3.4の両方に適用されます。</para>
</note>
</section>
<section xml:id="id-about">
<title>概要</title>
<para>これらのリリースノートは、明示的に指定および説明されていない限り、すべてのアーキテクチャで同一です。また、最新バージョンは、その他すべてのSUSE製品のリリースノートとともに、常に<link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>でオンラインで参照できます。</para>
<para>エントリが記載されるのは1回だけですが、そのエントリが重要で複数のセクションに属している場合は複数の場所で参照できます。リリースノートには通常、連続する2つのリリース間の変更のみが記載されます。特定の重要なエントリは、以前の製品バージョンのリリースノートから繰り返し記載される場合があります。このようなエントリを特定しやすくするために、該当するエントリにはその旨を示すメモが含まれています。</para>
<para>ただし、繰り返し記載されているエントリは厚意としてのみ提供されています。したがって、リリースを1つ以上スキップする場合は、スキップしたリリースのリリースノートも確認してください。現行リリースのリリースノートしか確認しないと、システムの動作に影響する可能性がある重要な変更を見逃す可能性があります。SUSE
Edgeのバージョンはx.y.zで定義され、「x」はメジャーバージョン、「y」はマイナーバージョン、「z」は「z
ストリーム」とも呼ばれるパッチバージョンを表します。SUSE
Edgeの製品ライフサイクルは、「3.4」のようなマイナーリリースを中心に定義されますが、「3.4.1」のように、ライフサイクルを通じて後続のパッチアップデートが適用されます。</para>
<note>
<para>SUSE Edge
zストリームリリースは、バージョン管理されたスタックとして緊密に統合されていて、綿密にテストされています。個々のコンポーネントを上記のバージョンとは異なるバージョンにアップグレードすると、システムのダウンタイムが発生する可能性が高くなります。テストされていない設定でEdgeクラスタを実行することは可能ですが、推奨されません。また、サポートチャンネルを通じて解決策を提供するのに時間がかかる場合があります。</para>
</note>
</section>
<section xml:id="release-notes-3-4-0">
<title>リリース3.4.0</title>
<para>公開日: 2025年9月24日</para>
<para>完全サポート終了日: 2026年3月20日</para>
<para>保守サポート終了日: 2027年9月20日</para>
<para>EOL: 2027年9月21日</para>
<para>概要: SUSE Edge 3.4.0はSUSE Edge 3.4リリースストリームの最初のリリースです。</para>
<section xml:id="id-new-features">
<title>新機能</title>
<itemizedlist>
<listitem>
<para>Kubernetes 1.33およびRancher Prime 2.12に更新</para>
</listitem>
<listitem>
<para>Rancher Turtles、Cluster API、Metal3/Ironicのバージョンを更新</para>
</listitem>
<listitem>
<para>SUSE Storage (Longhorn) 1.9.1に更新(<link
xl:href="https://longhorn.io/docs/1.9.1/">リリースノート</link>)</para>
</listitem>
<listitem>
<para>AArch64ダウンストリームクラスタのより柔軟なデプロイメントが、ダイレクトネットワークプロビジョニングフローを通じて可能になりました。詳細については、<xref
linkend="atip-automated-provisioning"/>を参照してください。</para>
</listitem>
<listitem>
<para>デュアルスタッククラスタのデプロイメントが完全にサポートされるようになりました(シングルスタックIPv6は<xref
linkend="tech-previews"/>のままです)。</para>
</listitem>
<listitem>
<para>MetalLBのBGPモードが技術プレビューとして利用可能になりました。詳細については、<xref
linkend="tech-previews"/>および<xref
linkend="guides-metallb-k3s-l3"/>を参照してください。</para>
</listitem>
<listitem>
<para>Edge Image Builderが1.3.0に更新されました。<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/RELEASE_NOTES.md">アップストリームリリースノート</link>を参照してください。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>バグおよびセキュリティの修正</title>
<itemizedlist>
<listitem>
<para>Rancher Prime 2.12にはいくつかのバグ修正が含まれています(<link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">アップストリームRancherリリースノート</link>)</para>
</listitem>
<listitem>
<para>Rancher Prime
2.12には、拡張機能のアップグレードの利用可能性を判断する際のAppVersionに関連する問題の修正が含まれています。この問題はEdgeチャートでの動作に影響を与えていました(<link
xl:href="https://github.com/rancher/dashboard/issues/14204">アップストリームの問題</link>)</para>
</listitem>
<listitem>
<para>SUSE Storage (Longhorn) 1.9.1には複数のバグ修正が含まれています(<link
xl:href="https://github.com/longhorn/longhorn/releases/tag/v1.9.1">アップストリームのLonghornバグ修正</link>)</para>
</listitem>
<listitem>
<para>更新されたMetal<superscript>3</superscript>チャートは、検査中にボンディングされたインタフェースに対して間違ったMACが収集される可能性のある問題を修正しました(<link
xl:href="https://bugs.launchpad.net/ironic-python-agent/+bug/2103450">アップストリームのIPAの問題</link>)</para>
</listitem>
<listitem>
<para>更新されたMetal<superscript>3</superscript>チャートは、ConfigMapの更新時にデプロイメントが正しく再起動されない可能性のある問題を修正しました(<link
xl:href="https://github.com/suse-edge/charts/issues/219">アップストリームの問題</link>)</para>
</listitem>
<listitem>
<para>Rancher Turtlesの更新には、MachineTemplate ownerReferencesがRKE2
CAPIプロバイダによって適用されなかった問題を解決する修正が含まれています(<link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/500">アップストリームの問題</link>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-known-issues-8">
<title>既知の問題</title>
<warning>
<para>新しいクラスタをデプロイする場合は、<xref
linkend="guides-kiwi-builder-images"/>に従って、まず新規イメージを構築してください。これはAMD64/Intel
64とAArch64アーキテクチャ両方のクラスタ、および管理クラスタとダウンストリームクラスタを作成するために必要な最初の手順です。</para>
</warning>
<itemizedlist>
<listitem>
<para>Edge Image
Builder経由でデプロイする場合、<literal>HelmChartConfigs</literal>マニフェストを<literal>kubernetes/manifests</literal>設定ディレクトリに配置すると失敗する可能性があります。代わりに、EIB
os-filesインタフェースを使用して、<literal>HelmChartConfigs</literal>を<literal>/var/lib/rancher/{rke2/k3s}/server/manifests/</literal>に配置することをお勧めします。例については、<xref
linkend="mgmt-cluster-directory-structure"/>を参照してください。これを実行しないと、<link
xl:href="https://github.com/rancher/rke2/issues/8357">#8357
RKE2の問題</link>で説明されているように、ノードが初期起動時に<literal>NotReady</literal>状態のままになる可能性があります。</para>
</listitem>
<listitem>
<para>RKE2/K3s
1.31、1.32、1.33バージョンでは、CNI設定の保存に使用されるディレクトリ<literal>/etc/cni</literal>で、<literal>overlayfs</literal>に関連する特定の条件により、そこで書き込まれるファイルの通知を
<literal>containerd</literal>にトリガされない場合があります(<link
xl:href="https://github.com/rancher/rke2/issues/8356">#8356
RKE2の問題</link>)。この結果、RKE2/K3sのデプロイメントがCNIが起動するのを待機した状態で停止し、RKE2/K3sノードが<literal>NotReady</literal>状態のままになります。これは、<literal>kubectl
describe node &lt;affected_node&gt;</literal>を使用してノードレベルで確認できます。</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Conditions:
  Type   Status  LastHeartbeatTime                LastTransitionTime               Reason           Message
  ----   ------  -----------------                ------------------               ------           -------
  Ready  False   Thu, 05 Jun 2025 17:41:28 +0000  Thu, 05 Jun 2025 14:38:16 +0000  KubeletNotReady  container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized</screen>
<para>回避策として、RKE2が起動する前に、tmpfsボリュームを<literal>/etc/cni</literal>ディレクトリにマウントできます。これにより、overlayfsの使用が回避され、
containerdの通知が見つからない問題や、ノードが再起動されてPodのinitcontainerが再実行されるたびに設定が書き換えられる問題を回避できます。EIBを使用する場合、これは<literal>custom/scripts</literal>ディレクトリ内の<literal>04-tmpfs-cni.sh</literal>スクリプトになり(こちら[<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom">https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom</link>]で説明)、次のようになります。</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash
mkdir -p /etc/cni
mount -t tmpfs -o mode=0700,size=5M tmpfs /etc/cni
echo "tmpfs /etc/cni tmpfs defaults,size=5M,mode=0700 0 0" &gt;&gt; /etc/fstab</screen>
<itemizedlist>
<listitem>
<para>Elementalを使用してリモートホストをオンボーディングする際に、<literal>dbus.service</literal>と<literal>elemental-system-agent.service</literal>の間に競合状態が発生し、リモートホスト上の<literal>rancher-system-agent.service</literal>が以下のようなエラーで起動に失敗する可能性があります(詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/784">#784
Edge Image Builderの問題</link>を参照してください)。</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Sep 19 19:38:07 elementalvm elemental-system-agent[3671]: time="2025-09-19T19:38:07Z" level=info msg="[6b20fe64c854da2639804884b34129bb8f718eb59578111da58d9de1509c24db_1:stderr]: Failed to restart rancher-system-agent.service: Message recipient disconnected from message bus without replying"</screen>
<para>回避策として、systemdのオーバーライドファイルを以下のように作成できます。</para>
<screen language="bash" linenumbering="unnumbered">[Unit]
Wants=dbus.service network-online.target
After=dbus.service network-online.target time-sync.target

[Service]
ExecStartPre=/bin/bash -c 'echo "Waiting for dbus to become active..." | systemd-cat -p info -t elemental-system-agent; sleep 15; timeout 300 bash -c "while ! systemctl is-active --quiet dbus.service; do sleep 15; done"'</screen>
<para>また、<literal>30a-copy-elemental-system-agent-override.sh</literal>というカスタムスクリプトを使用して、EIBの<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/pkg/combustion/templates/31-elemental-register.sh.tpl">31-elemental-register.sh</link>スクリプトが燃焼フェーズ中に実行される前に、オーバーライドを<literal>/etc/systemd/system/elemental-system-agent.service.d</literal>に配置できます。</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash

/bin/mkdir -p /etc/systemd/system/elemental-system-agent.service.d
/bin/cp -f elemental-system-agent-override.conf /etc/systemd/system/elemental-system-agent.service.d/override.conf</screen>
</section>
<section xml:id="id-component-versions">
<title>コンポーネントバージョン</title>
<para>以下の表は、3.4.0リリースを構成する個々のコンポーネントを示します。これには、バージョン、Helmチャートバージョン(該当する場合)、およびリリースされたアーティファクトをバイナリ形式でプルできる場所が含まれています。使用法とデプロイメントの例について、関連するドキュメントを参照してください。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>名前</para></entry>
<entry align="left" valign="top"><para>バージョン</para></entry>
<entry align="left" valign="top"><para>Helmチャートバージョン</para></entry>
<entry align="left" valign="top"><para>アーティファクトの場所(URL/イメージ)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Linux Micro</para></entry>
<entry align="left" valign="top"><para>6.1 (最新)</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SUSE Linux
Microダウンロードページ</link><?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso (sha256
70b9be28f2d92bc3b228412e4fc2b1d5026e691874b728e530b8063522158854)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-SelfInstall-GM.install.iso (sha256
9ce83e4545d4b36c7c6a44f7841dc3d9c6926fe32dbff694832e0fbd7c496e9d)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-GM.raw.xz (sha256
36e3efa55822113840dd76fdf6914e933a7b7e88a1dce5cb20c424ccf2fb4430)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-GM.raw.xz (sha256
2ee66735da3e1da107b4878e73ae68f5fb7309f5ec02b5dfdb94e254fda8415e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Multi-Linux Manager</para></entry>
<entry align="left" valign="top"><para>5.0.5</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE Multi-Linux
Managerダウンロードページ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.33.3%2Bk3s1">アップストリームK3sリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.33.3%2Brke2r1">アップストリームRKE2リリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://charts.rancher.com/server-charts/prime/index.yaml">Rancher
Prime Helmリポジトリ</link><?asciidoc-br?> <link
xl:href="https://github.com/rancher/rancher/releases/download/v2.12.1/rancher-images.txt">Rancher
2.12.1コンテナイメージ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Storage (Longhorn)</para></entry>
<entry align="left" valign="top"><para>1.9.1</para></entry>
<entry align="left" valign="top"><para>107.0.0+up1.9.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher Charts
Helmリポジトリ</link><?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.61<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Security</para></entry>
<entry align="left" valign="top"><para>5.4.5</para></entry>
<entry align="left" valign="top"><para>107.0.0+up2.8.7</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher Charts
Helmリポジトリ</link><?asciidoc-br?>
registry.suse.com/rancher/neuvector-controller:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-enforcer:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-manager:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-compliance-config:1.0.6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-registry-adapter:0.1.8<?asciidoc-br?>
registry.suse.com/rancher/neuvector-scanner:6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-updater:0.0.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles (CAPI)</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles:304.0.6_up0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-controller:v1.10.2<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-ipam-controller:v1.10.2<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-controller:v1.10.5<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtlesエアギャップリソース</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles-airgap-resources:304.0.6_up0.24.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>0.11.5</para></entry>
<entry align="left" valign="top"><para>304.0.16+up0.12.6</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metal3:304.0.16_up0.12.6<?asciidoc-br?>
registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic:29.0.4.3<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic-ipa-downloader:3.0.9<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metallb:304.0.0_up0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-controller:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-speaker:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.4/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.7.3</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elementalダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/4.0.0/charts/elemental/3.0.1">Elemental拡張機能Helmチャート</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/edge-image-builder:1.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.3</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.3">NMConfiguratorアップストリームリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.5.2</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-operator:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-api:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-controller:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-handler:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirtダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>1.3.2</para></entry>
<entry align="left" valign="top"><para>304.0.3+up1.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt-dashboard-extension:304.0.3_up1.3.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.62.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/cdi:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-operator:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.3.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.3.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/endpoint-copier-operator:304.0.1_up0.3.0<?asciidoc-br?>
registry.suse.com/edge/3.4/endpoint-copier-operator:0.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (廃止)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/akri:304.0.0_up0.12.20<?asciidoc-br?>
registry.suse.com/edge/charts/akri-dashboard-extension:304.0.0_up1.3.1<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SR-IOV Network Operator</para></entry>
<entry align="left" valign="top"><para>1.5.0</para></entry>
<entry align="left" valign="top"><para>304.0.2+up1.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/sriov-network-operator:304.0.2_up1.5.0<?asciidoc-br?>
registry.suse.com/edge/charts/sriov-crd:304.0.2_up1.5.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.16.0</para></entry>
<entry align="left" valign="top"><para>107.0.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher Charts
Helmリポジトリ</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.16.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.1.1</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.1.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/upgrade-controller:304.0.1_up0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/upgrade-controller:0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/kubectl:1.33.4<?asciidoc-br?>
registry.suse.com/edge/3.4/release-manifest:3.4.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Kiwi Builder</para></entry>
<entry align="left" valign="top"><para>10.2.12.0</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-deprecated-features">
<title>廃止された機能</title>
<para>特に記載のない限り、これらは3.4.0リリースおよびそれ以降のすべてのzストリームバージョンに適用されます。</para>
<itemizedlist>
<listitem>
<para>Akriは、以前のEdgeリリースでは技術プレビュー製品でしたが、現在は廃止されています。今後のリリースで削除される予定です。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="tech-previews">
<title>技術プレビュー</title>
<para>特に記載のない限り、これらは3.4.0リリースおよびそれ以降のすべてのzストリームバージョンに適用されます。</para>
<itemizedlist>
<listitem>
<para>シングルスタックIPv6デプロイメントは、技術プレビュー製品であり、標準的なサポート範囲の対象外です。</para>
</listitem>
<listitem>
<para>ダウンストリームデプロイメントにおけるPrecision Time Protocol
(PTP)は、技術プレビュー製品であり、標準的なサポート範囲の対象外です。</para>
</listitem>
<listitem>
<para>MetalLBのBGPモードは、技術プレビュー製品であり、標準的なサポート範囲の対象外です。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-component-verification">
<title>コンポーネントの検証</title>
<para>上記のコンポーネントはSoftware Bill Of Materials
(SBOM)のデータを使用して検証できます。たとえば、以下に説明するように<literal>cosign</literal>を使用します。</para>
<para><link
xl:href="https://www.suse.com/support/security/keys/">SUSE署名キーのソース</link>からSUSE
Edge Containerの公開鍵をダウンロードします。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>コンテナイメージのハッシュを検証します。たとえば、<literal>crane</literal>を使用します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1 --platform linux/amd64
sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374</screen>
<note>
<para>マルチアーキテクチャイメージの場合、ダイジェストを取得する際にはプラットフォームを指定する必要があります。例: <literal>--platform
linux/amd64</literal>または<literal>--platform
linux/arm64</literal>。指定しないと、次の手順でエラーが発生します(<literal>Error: no matching
attestations (エラー: 一致する認証がありません)</literal>)。</para>
</note>
<para><literal>cosign</literal>を使用して検証します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 &gt; /dev/null
#
Verification for registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The signatures were verified against the specified public key</screen>
<para><link xl:href="https://www.suse.com/support/security/sbom/">SUSE
SBOMドキュメント</link>の説明に従ってSBOMデータを抽出します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>アップグレード手順</title>
<para>新しいリリースにアップグレードする方法の詳細については、<xref linkend="day-2-operations"/>を参照してください。</para>
</section>
<section xml:id="id-product-support-lifecycle">
<title>製品サポートライフサイクル</title>
<para>SUSE
Edgeは、SUSEが提供する定評あるサポートに支えられています。SUSEは、エンタープライズ品質のサポートサービスの提供において確固たる実績を誇るテクノロジリーダーです。詳細については、<link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>、およびサポートポリシーのページ(<link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>)を参照してください。サポートケースの作成、SUSEが重大度レベルを分類する方法、またはサポートの範囲について質問がある場合は、テクニカルサポートハンドブック(<link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>)を参照してください。</para>
<para>SUSE
Edge「3.4」は24か月間の運用サポートでサポートされ、最初の6か月間は「完全サポート」、その後の18か月間は「保守サポート」が提供されます。これらのサポートフェーズ後、製品は「サポート終了済み」(EOL)となり、サポートは終了となります。ライフサイクルフェーズの詳細については、以下の表を参照してください。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">完全サポート(6か月)</emphasis></para></entry>
<entry align="left" valign="top"><para>緊急かつ選択された優先度の高いバグ修正は、完全サポート期間中にリリースされます。その他のすべてのパッチ(緊急でないもの、機能強化、新機能)は、通常のリリーススケジュールに従ってリリースされます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">保守サポート(18 か月)</emphasis></para></entry>
<entry align="left" valign="top"><para>この期間中は、重要な修正のみがパッチとしてリリースされます。その他のバグ修正はSUSEの裁量によりリリースされる可能性がありますが、必ずしも提供されるとは限りません。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">サポート終了済み(EOL)</emphasis></para></entry>
<entry align="left" valign="top"><para>製品リリースがサポート終了済み日を迎えた場合、顧客は製品ライセンス契約の条件に従い、製品を引き続き使用することができます。SUSEのサポートプランは、EOL日を過ぎた製品リリースには適用されません。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>明記されていない限り、記載されているコンポーネントはすべて一般提供(GA)とみなされ、SUSEの標準のサポート範囲の対象となります。一部のコンポーネントは「技術プレビュー」として記載されている場合があります。この場合、SUSEは評価のためにGA前の機能への早期アクセスをお客様に提供しますが、これらの機能には標準のサポートポリシーが適用されず、運用ユースケースには推奨されません。SUSEでは、技術プレビューのコンポーネントに関するフィードバックや、当該コンポーネントの改良についてのご提案を心からお待ちしております。しかし、機能がお客様のニーズを満たさない場合やSUSEが求める成熟度に達しない場合、一般提供になる前に技術プレビューの機能を廃止する権利を留保します。</para>
<para>SUSEは場合により、機能の廃止やAPIの仕様変更を行わなければならないことがあることに注意してください。機能の廃止やAPIの変更の理由としては、機能が新しい実装によって更新または置き換えられた、新しい機能セットが導入された、アップストリームの技術が利用できなくなった、アップストリームコミュニティによって互換性のない変更が導入された、などが考えられます。これは特定のマイナーリリース(x.z)内で発生することは意図されていないため、すべてのzストリームリリースではAPIの互換性と機能が維持されます。SUSEは、廃止に関する警告をリリースノート内で十分に余裕をもって提供し、併せて回避策、推奨事項、サービスの中断を最小限に抑える軽減策も提供するよう努めます。</para>
<para>SUSE Edgeチームはコミュニティからのフィードバックも歓迎しており、<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>の各コードリポジトリ内で問題を報告できます。</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>ソースコードの取得</title>
<para>このSUSE製品には、GNU General Public License
(GPL)やその他のさまざまなオープンソースライセンスの下でSUSEにライセンスされた素材が含まれます。SUSEはGPLに従ってGPLでライセンスされた素材に対応するソースコードを提供する必要があるほか、その他すべてのオープンソースライセンスの要件にも準拠します。よって、SUSEはすべてのソースコードを利用可能にしており、一般的にSUSE
Edge GitHubリポジトリ(<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>)にあります。また、依存コンポーネントについてはSUSE
Rancher GitHubリポジトリ(<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)にあり、特にSUSE
Linux Microについては<link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>の「Medium
2」でソースコードをダウンロードできます。</para>
</section>
<section xml:id="id-legal-notices">
<title>法的通知</title>
<para>SUSEは、この文書の内容や使用に関していかなる表明や保証も行いません。特に、商品性または特定目的への適合性に関する明示的または暗黙的な保証は一切行いません。さらに、SUSEは本書を改訂し、その内容に随時変更を加える権利を留保しますが、いかなる個人または団体に対しても当該の改訂または変更を通知する義務を負いません。</para>
<para>SUSEは、いかなるソフトウェアに関しても、いかなる表明や保証も行いません。特に、商品性または特定目的への適合性に関する明示的または暗黙的な保証は一切行いません。さらに、SUSEはSUSEソフトウェアのあらゆる部分に随時変更を加える権利を留保しますが、いかなる個人または団体に対しても当該の変更を通知する義務を負いません。</para>
<para>本契約の下で提供されるいかなる製品または技術情報も、米国の輸出管理法規および他国の貿易法の対象となる場合があります。お客様はすべての輸出管理規制を遵守し、成果物の輸出、再輸出、または輸入のために必要なライセンスまたは分類を取得することに同意します。お客様は、現行の米国輸出禁止リストに記載されている団体や米国輸出法に規定された禁輸国やテロ支援国への輸出や再輸出を行わないことに同意します。また、成果物を禁止されている核、ミサイル、または化学/生物兵器の最終用途に使用しないことにも同意します。SUSEソフトウェアの輸出に関する詳細情報については、<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>を参照してください。SUSEは、必要な輸出許可の取得を怠ったことに対する責任を一切負いません。</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>このリリースノート文書は、Creative Commons Attribution-NoDerivatives 4.0 International
License
(CC-BY-ND-4.0)の下でライセンスされています。お客様は、この文書と併せてライセンスのコピーを受け取っている必要があります。受け取っていない場合は、<link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>を参照してください。</para>
<para>SUSEは、本書で説明されている製品に組み込まれた技術に関連する知的財産権を有しています。これらの知的財産権には、特に、<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>に記載されている1つまたは複数の米国特許、ならびに米国およびその他の国における1つまたは複数のその他の特許または出願中の特許申請が含まれていることがありますが、これらに限定されません。</para>
<para>SUSEの商標については、SUSEの商標とサービスマークのリスト(<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>)を参照してください。第三者のすべての商標は各所有者の財産です。SUSEのブランド情報と使用要件については、<link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link>で公開されているガイドラインを参照してください。</para>
</section>
</chapter>
</part>
</book>
