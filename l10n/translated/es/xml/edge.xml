<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xml:lang="es-es">
<info>
<title>Documentación de SUSE Edge</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>07/09/2025</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="id-suse-edge-3-3-1-documentation">
<title>Documentación de SUSE Edge 3.3.1</title>
<para>Le damos la bienvenida a la documentación de SUSE Edge. Aquí, encontrará una
descripción general de la arquitectura, guías de inicio rápido, diseños
validados, orientación sobre el uso de los componentes, integraciones de
terceros y prácticas recomendadas para gestionar su infraestructura y sus
cargas de trabajo de edge computing.</para>
<section xml:id="id-what-is-suse-edge">
<title>¿Qué es SUSE Edge?</title>
<para>SUSE Edge es una solución integral, diseñada específicamente, estrechamente
integrada y validada de forma exhaustiva para abordar los retos únicos que
plantea el despliegue de infraestructura y aplicaciones nativas de la nube
en el perímetro. Su objetivo principal es proporcionar una plataforma
propia, pero altamente flexible, escalable y segura, que abarque desde la
creación de imágenes de distribución inicial, el aprovisionamiento y la
incorporación de nodos, el despliegue de aplicaciones, la observabilidad y
las operaciones completas del ciclo de vida. La plataforma se basa por
completo en el mejor software de código abierto, en consonancia con nuestros
más de 30 años de historia proporcionando plataformas SUSE Linux seguras,
estables y certificadas, y en nuestra experiencia en la gestión de
Kubernetes altamente escalable y rica en funciones con nuestra cartera de
productos Rancher. SUSE Edge se basa en estas capacidades para ofrecer
funcionalidades que pueden aplicarse a numerosos segmentos de mercado,
incluyendo el comercio minorista, la medicina, el transporte, la logística,
las telecomunicaciones, la fabricación inteligente y el IoT industrial.</para>
</section>
<section xml:id="id-design-philosophy">
<title>Filosofía de diseño</title>
<para>La solución se ha diseñado partiendo de la idea de que no existe una
plataforma periférica única "válida para todo", debido a la gran variedad de
requisitos y expectativas de los clientes. Los despliegues periféricos nos
obligan a evolucionar para resolver continuamente problemas difíciles, como
la escalabilidad masiva, la disponibilidad limitada de la red, las
restricciones de espacio físico, las nuevas amenazas de seguridad y vectores
de ataque, las variaciones en la arquitectura del hardware y los recursos
del sistema, la necesidad de implantar e interactuar con infraestructuras y
aplicaciones heredadas, y las soluciones de los clientes que tienen una vida
útil prolongada. Dado que para muchos de estos retos es preciso pensar más
allá de las fórmulas habituales, por ejemplo, en el despliegue de
infraestructura y aplicaciones dentro de centros de datos o en la nube
pública, tenemos que examinar el diseño con mucho más detalle y
replantearnos muchas ideas que no hay que dar por hechas.</para>
<para>Por ejemplo, valoramos el minimalismo, la modularidad y la facilidad de
uso. El minimalismo es importante para los entornos periféricos, ya que
cuanto más complejo es un sistema, más probable es que tenga
problemas. Cuando se analizan cientos de ubicaciones, y hasta cientos de
miles, los sistemas complejos fallan de formas complejas. La modularidad de
nuestra solución permite una mayor elección por parte del usuario, al tiempo
que elimina la complejidad innecesaria en la plataforma. También debemos
aportar facilidad de uso. Los seres humanos pueden cometer errores al
repetir un proceso miles de veces, por lo que la plataforma debe garantizar
que cualquier error potencial sea recuperable, eliminando la necesidad de
visitas de técnicos in situ, pero también procurando que se consiga
coherencia y estandarización.</para>
</section>
<section xml:id="id-high-level-architecture">
<title>Arquitectura general</title>
<para>La arquitectura general del sistema SUSE Edge se divide en dos categorías
principales: clústeres de "gestión" y clústeres "descendentes". El clúster
de gestión se encarga de la gestión remota de uno o varios clústeres
descendentes, aunque se entiende que, en determinadas circunstancias, los
clústeres descendentes deben funcionar sin gestión remota; por ejemplo, en
situaciones en las que un sitio periférico no tenga conectividad externa y
deba funcionar de forma aislada. En SUSE Edge, los componentes técnicos que
se utilizan para el funcionamiento de los clústeres de gestión y
descendentes son en gran medida comunes, aunque se pueden diferenciar tanto
en las especificaciones del sistema como en las aplicaciones que residen en
ellos; es decir, el clúster de gestión ejecutaría aplicaciones que permitan
la gestión de sistemas y las operaciones del ciclo de vida, mientras que los
clústeres descendentes cumplirían los requisitos para dar servicio a las
aplicaciones de los usuarios.</para>
<section xml:id="id-components-used-in-suse-edge">
<title>Componentes usados en SUSE Edge</title>
<para>SUSE Edge está formado por componentes de SUSE y de Rancher, junto con
funciones y componentes adicionales creados por el equipo de Edge para
permitirnos abordar las limitaciones y complejidades que requiere la edge
computing. En el diagrama simplificado siguiente se explican los componentes
usados tanto en los clústeres de gestión como en los clústeres
descendentes. Sin ser una lista exhaustiva, muestra la arquitectura general:</para>
<section xml:id="id-management-cluster">
<title>Clúster de gestión</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-management-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>clúster de gestión de suse edge</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Gestión</emphasis>: esta es la parte centralizada de
SUSE Edge que se usa para gestionar el aprovisionamiento y el ciclo de vida
de los clústeres descendentes. El clúster de gestión suele incluir los
componentes siguientes:</para>
<itemizedlist>
<listitem>
<para>Gestión de varios clústeres con Rancher Prime (<xref
linkend="components-rancher"/>), que incluye un panel de control común para
incorporar clústeres descendentes y para la gestión continua del ciclo de
vida de la infraestructura y las aplicaciones, además de proporcionar un
aislamiento completo de los inquilinos e integraciones de
<literal>IDP</literal> (proveedores de identidad), un amplio mercado de
integraciones y extensiones de terceros, y una API independiente del
proveedor.</para>
</listitem>
<listitem>
<para>Gestión de sistemas Linux con SUSE Multi-Linux Manager, que permite la
gestión automatizada de parches y configuraciones de Linux del sistema
operativo Linux subyacente (*SUSE Linux Micro, <xref
linkend="components-slmicro"/>) que se ejecuta en los clústeres
descendentes. Tenga en cuenta que aunque este componente está en
contenedores, actualmente debe ejecutarse en un sistema independiente del
resto de los componentes de gestión, por lo que se etiqueta como "Gestión de
Linux" en el diagrama anterior.</para>
</listitem>
<listitem>
<para>Un controlador dedicado de gestión del ciclo de vida (<xref
linkend="components-upgrade-controller"/>) que se encarga de la
actualización de los componentes del clúster de gestión a una versión
determinada de SUSE Edge.</para>
</listitem>
<listitem>
<para>Incorporación remota del sistema a Rancher Prime con Elemental (<xref
linkend="components-elemental"/>), lo que permite la vinculación posterior
de los nodos periféricos conectados a los clústeres de Kubernetes deseados y
el despliegue de aplicaciones, por ejemplo, a través de GitOps.</para>
</listitem>
<listitem>
<para>Compatibilidad opcional completa del ciclo de vida y la gestión física (bare
metal) con los proveedores de infraestructura Metal3 (<xref
linkend="components-metal3"/>), MetalLB (<xref
linkend="components-metallb"/>) y <literal>CAPI</literal> (Cluster API), lo
que permite el aprovisionamiento completo de extremo a extremo de sistemas
físicos con capacidades de gestión remota.</para>
</listitem>
<listitem>
<para>Un motor GitOps opcional llamado Fleet (<xref linkend="components-fleet"/>)
para gestionar el aprovisionamiento y el ciclo de vida de los clústeres
descendentes y las aplicaciones que residen en ellos.</para>
</listitem>
<listitem>
<para>El clúster de gestión tiene SUSE Linux Micro (<xref
linkend="components-slmicro"/>) como sistema operativo base y RKE2 (<xref
linkend="components-rke2"/>) como distribución de Kubernetes que aporta
compatibilidad con las aplicaciones del clúster de gestión.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-downstream-clusters">
<title>Clústeres descendentes</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-downstream-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>clúster descendente de suse edge</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Descendentes</emphasis>: esta es la parte
distribuida de SUSE Edge que se utiliza para ejecutar las cargas de trabajo
del usuario en el perímetro; es decir, el software que se ejecuta en la
propia ubicación perimetral y que, normalmente, se compone de los siguientes
componentes:</para>
<itemizedlist>
<listitem>
<para>Una selección de distribuciones de Kubernetes, con distribuciones seguras y
ligeras como K3s (<xref linkend="components-k3s"/>) y RKE2 (<xref
linkend="components-rke2"/>) (<literal>RKE2</literal> está reforzada,
certificada y optimizada para su uso en el sector público y en industrias
reguladas).</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>) para habilitar
funciones de seguridad como el análisis de vulnerabilidades de imágenes, la
inspección profunda de paquetes y la protección en tiempo real contra
amenazas y vulnerabilidades.</para>
</listitem>
<listitem>
<para>Software de almacenamiento en bloques con SUSE Storage (<xref
linkend="components-suse-storage"/>) que permite un sistema de
almacenamiento en bloques ligero, persistente, resistente y escalable.</para>
</listitem>
<listitem>
<para>Un sistema operativo Linux ligero, optimizado para contenedores y reforzado
con SUSE Linux Micro (<xref linkend="components-slmicro"/>), que proporciona
un sistema operativo inmutable y altamente resiliente para ejecutar
contenedores y máquinas virtuales en el perímetro. SUSE Linux Micro está
disponible para las arquitecturas AArch64 y AMD64/Intel 64, y también admite
<literal>kernel en tiempo real</literal> para aplicaciones sensibles a la
latencia (por ejemplo, para usos en telecomunicaciones).</para>
</listitem>
<listitem>
<para>Para los clústeres conectados (los que tienen conectividad con el clúster de
gestión), se despliegan dos agentes: Rancher System Agent, para gestionar la
conectividad con Rancher Prime, y venv-salt-minion, para recibir
instrucciones de SUSE Multi-Linux Manager y aplicar las actualizaciones de
software de Linux. Estos agentes no son necesarios para la gestión de
clústeres desconectados.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-connectivity">
<title>Conectividad</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-connected-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>arquitectura conectada de suse edge</phrase></textobject>
</mediaobject>
</informalfigure>
<para>La imagen anterior ofrece una descripción general de la arquitectura de los
clústeres descendentes <emphasis role="strong">conectados</emphasis> y sus
conexiones con el clúster de gestión. El clúster de gestión se puede
desplegar en una amplia variedad de plataformas de infraestructura
subyacentes, tanto en instalaciones locales como en la nube, dependiendo de
la disponibilidad de red entre los clústeres descendentes y el clúster de
gestión de destino. El único requisito para que funcione es que sea posible
acceder a las API y las URL de redirección a través de la red que conecta
los nodos del clúster descendente con la infraestructura de gestión.</para>
<para>Es importante entender que existen mecanismos distintos en los que se
establece esta conectividad en relación con el mecanismo de despliegue del
clúster descendente. Esto se explica con mucho más detalle en la siguiente
sección. Como descripción general: existen tres mecanismos principales para
que los clústeres descendentes conectados se establezcan como un clúster
"gestionado":</para>
<orderedlist numeration="arabic">
<listitem>
<para>Los clústeres descendentes se despliegan inicialmente en modo "desconectado"
(por ejemplo, mediante Edge Image Builder, <xref linkend="components-eib"/>)
y, a continuación, se importan al clúster de gestión si la conectividad lo
permite.</para>
</listitem>
<listitem>
<para>Los clústeres descendentes se configuran para utilizar el mecanismo de
incorporación integrado (por ejemplo, a través de Elemental (<xref
linkend="components-elemental"/>)) y se registran automáticamente en el
clúster de gestión en el primer arranque, lo que permite la vinculación
posterior de la configuración del clúster.</para>
</listitem>
<listitem>
<para>Los clústeres descendentes se han aprovisionado con capacidades de gestión
bare metal (CAPI + Metal<superscript>3</superscript>) y se importan
automáticamente en el clúster de gestión después de que este se haya
desplegado y configurado (a través del operador Rancher Turtles).</para>
</listitem>
</orderedlist>
<note>
<para>Se recomienda desplegar varios clústeres de gestión para adaptarse a la
escala de los despliegues de gran tamaño, optimizar el ancho de banda y la
latencia en entornos geográficamente dispersos, y minimizar las
interrupciones en caso de una caída del servicio o una actualización del
clúster de gestión. Encontrará los límites actuales de escalabilidad del
clúster de gestión y los requisitos del sistema <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/getting-started/installation-and-upgrade/installation-requirements">aquí</link>.</para>
</note>
</section>
</section>
<section xml:id="id-common-edge-deployment-patterns">
<title>Patrones de despliegue perimetrales comunes</title>
<para>Debido a la variedad de entornos operativos y requisitos de ciclo de vida
existentes, hemos implantado compatibilidad para una serie de patrones de
despliegue que se ajustan de manera general a los sectores del mercado y los
casos de uso en los que funciona SUSE Edge. Hemos creado una guía de inicio
rápido para cada uno de estos patrones a fin de que pueda familiarizarse con
la plataforma SUSE Edge en función de sus necesidades. A continuación, se
describen los tres patrones de despliegue que admitimos actualmente, con un
enlace a la página de inicio rápido correspondiente.</para>
<section xml:id="id-directed-network-provisioning">
<title>Aprovisionamiento de red dirigida</title>
<para>En el aprovisionamiento de red dirigida se conocen los detalles del hardware
en el que se desea realizar el despliegue y se tiene acceso directo a la
interfaz de gestión fuera de banda para orquestar y automatizar todo el
proceso de aprovisionamiento. En este caso, nuestros clientes esperan una
solución que permita aprovisionar los sitios periféricos de forma totalmente
automatizada desde una ubicación centralizada, lo que va mucho más allá de
la simple creación de una imagen de arranque al minimizar las operaciones
manuales en la ubicación periférica; basta con montar, alimentar y conectar
las redes necesarias al hardware físico y el proceso de automatización
enciende el equipo mediante la gestión fuera de banda (por ejemplo, a través
de la API Redfish) y se encarga del aprovisionamiento, la incorporación y la
implantación de la infraestructura sin intervención del usuario. La clave
para que este modelo funcione es que los administradores conozcan los
sistemas y que sepan qué hardware hay en cada ubicación y que el despliegue
se gestionará de forma centralizada.</para>
<para>Esta solución es la más robusta, ya que interactúa directamente con la
interfaz de gestión del hardware, trabaja con hardware conocido y tiene
menos restricciones en cuanto a la disponibilidad de la red. En cuanto a la
funcionalidad, esta solución utiliza ampliamente Cluster API y
Metal<superscript>3</superscript> para el aprovisionamiento automatizado,
desde bare metal, pasando por el sistema operativo y Kubernetes hasta las
aplicaciones en capas; y ofrece la posibilidad de vincularse al resto de las
capacidades comunes de gestión del ciclo de vida de SUSE Edge tras el
despliegue. La guía de inicio rápido para esta solución se puede encontrar
en el <xref linkend="quickstart-metal3"/>.</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>Aprovisionamiento de red "phone home"</title>
<para>A veces, se trabaja en un entorno en el que el clúster de gestión central no
puede gestionar el hardware directamente (por ejemplo, si la red remota está
protegida por un cortafuegos o no hay una interfaz de gestión fuera de
banda, algo habitual en el hardware de tipo "PC" que suele encontrarse en el
perímetro). En este caso, proporcionamos herramientas para aprovisionar de
forma remota los clústeres y sus cargas de trabajo sin necesidad de saber
dónde se envía el hardware cuando se arranca. Este sistema es en el que la
mayoría de la gente piensa al hablar de la edge computing: son los miles de
sistemas algo desconocidos que se arrancan en ubicaciones periféricas y se
comunican de forma segura con la central, validando quiénes son y recibiendo
instrucciones sobre lo que deben hacer. Aquí se espera un aprovisionamiento
y una gestión del ciclo de vida con muy poca intervención del usuario,
aparte de la creación previa de imágenes del equipo en fábrica o de
simplemente conectar una imagen de arranque, por ejemplo, a través de USB, y
encender el sistema. Los principales retos en este modelo son cómo abordar
la escala, la coherencia, la seguridad y el ciclo de vida de estos
dispositivos en el mundo real.</para>
<para>Esta solución ofrece una gran flexibilidad y coherencia en la forma en que
se aprovisionan e incorporan los sistemas, independientemente de su
ubicación, tipo o especificaciones, o de cuándo se encienden por primera
vez. SUSE Edge permite una flexibilidad y personalización totales del
sistema a través de Edge Image Builder y aprovecha las capacidades de
registro que ofrece Elemental para la incorporación de nodos y el
aprovisionamiento de Kubernetes, junto con SUSE Multi-Linux Manager para la
aplicación de parches al sistema operativo. La guía de inicio rápido de esta
solución se puede encontrar en el <xref linkend="quickstart-elemental"/>.</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>Aprovisionamiento basado en imágenes</title>
<para>Para los clientes que necesitan operar en entornos independientes, aislados
o con limitaciones de red, SUSE Edge ofrece una solución que permite generar
medios de instalación totalmente personalizados con todos los artefactos de
despliegue necesarios para habilitar clústeres de Kubernetes de alta
disponibilidad en el perímetro, tanto de un solo nodo como de múltiples
nodos, incluyendo cualquier carga de trabajo o los componentes adicionales
en capas que se requieran. Todo ello sin necesidad de conectividad de red
con el mundo exterior y sin la intervención de una plataforma de gestión
centralizada. La experiencia del usuario se asemeja mucho a la solución
"phone home", en la que se proporcionan medios de instalación a los sistemas
de destino, pero la solución se "arranca in situ". En este escenario, es
posible conectar los clústeres resultantes a Rancher para su gestión
continua (es decir, pasar de un modo de funcionamiento "desconectado" a
"conectado" sin necesidad de realizar una reconfiguración o un redespliegue
importantes), o bien seguir funcionando de forma aislada. Tenga en cuenta
que, en ambos casos, se puede aplicar el mismo mecanismo unificado para
automatizar las operaciones del ciclo de vida.</para>
<para>Además, esta solución se puede utilizar para crear rápidamente clústeres de
gestión que puedan alojar la infraestructura centralizada que admite tanto
el modelo de "aprovisionamiento de red dirigida" como el de
"aprovisionamiento de red 'phone home'", ya que puede ser la forma más
rápida y sencilla de aprovisionar todo tipo de infraestructura
periférica. Esta solución utiliza en gran medida las capacidades de SUSE
Edge Image Builder para crear medios de instalación totalmente
personalizados y desatendidos. La guía de inicio rápido se puede encontrar
en el <xref linkend="quickstart-eib"/>.</para>
</section>
</section>
<section xml:id="id-suse-edge-stack-validation">
<title>Validación de la pila SUSE Edge</title>
<para>Todas las versiones de SUSE Edge tienen componentes estrechamente integrados
y minuciosamente validados que se lanzan unidos. Como parte de los esfuerzos
de integración continua y validación de la pila, que no solo prueban la
integración entre componentes, sino que también garantizan que el sistema
funcione según lo esperado en escenarios de fallo forzado, el equipo de SUSE
Edge publica todas las pruebas realizadas y sus resultados. Estos, junto con
todos los parámetros de entrada, se pueden encontrar en <link
xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link>.</para>
</section>
<section xml:id="id-full-component-list">
<title>Lista completa de componentes</title>
<para>Esta es la lista completa de componentes, junto con un enlace a una
descripción general de cada uno de ellos y cómo se utilizan en SUSE Edge:</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Extensiones de panel de control de Rancher (<xref
linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>Rancher Turtles (<xref linkend="components-rancher-turtles"/>)</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux Manager</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>Akri (<xref linkend="components-akri"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>SUSE Storage (<xref linkend="components-suse-storage"/>)</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>)</para>
</listitem>
<listitem>
<para>Upgrade Controller (<xref linkend="components-upgrade-controller"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>Guías de inicio rápido</title>
<partintro>
<para>Aquí están las guías de inicio rápido</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>Despliegues automatizados de BMC con Metal<superscript>3</superscript></title>
<para>Metal<superscript>3</superscript> es un <link
xl:href="https://metal3.io/">projecto de la CNCF</link> que proporciona
capacidades de gestión de infraestructura bare metal para Kubernetes.</para>
<para>Metal<superscript>3</superscript> ofrece recursos nativos de Kubernetes para
gestionar el ciclo de vida de los servidores bare metal que admiten la
gestión a través de protocolos fuera de banda, como <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>También cuenta con un soporte maduro para <link
xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link> que
permite la gestión de recursos de infraestructura a través de múltiples
proveedores de infraestructura mediante API ampliamente adoptadas y
neutrales con respecto a los proveedores.</para>
<section xml:id="id-why-use-this-method">
<title>Por qué usar este método</title>
<para>Este método es útil para situaciones en las que el hardware de destino
admite la gestión fuera de banda y se desea un flujo de gestión de la
infraestructura totalmente automatizado.</para>
<para>Se configura un clúster de gestión para proporcionar API declarativas que
permitan la gestión del inventario y el estado de los servidores bare metal
del clúster descendente, incluida la inspección, limpieza y
aprovisionamiento/desaprovisionamiento automatizados.</para>
</section>
<section xml:id="id-high-level-architecture-2">
<title>Arquitectura general</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>inicio rápido de la arquitectura de metal3</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>Requisitos previos</title>
<para>Existen algunas restricciones específicas relacionadas con el hardware y la
red del servidor del clúster descendente:</para>
<itemizedlist>
<listitem>
<para>Clúster de gestión</para>
<itemizedlist>
<listitem>
<para>Debe tener conectividad de red con la API de gestión/BMC del servidor de
destino</para>
</listitem>
<listitem>
<para>Debe tener conectividad de red con la red del plano de control del servidor
de destino</para>
</listitem>
<listitem>
<para>Para clústeres de gestión de varios nodos, se requiere una dirección IP
reservada adicional</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Hosts que se van a controlar</para>
<itemizedlist>
<listitem>
<para>Deben admitir la gestión fuera de banda mediante interfaces Redfish, iDRAC o
iLO</para>
</listitem>
<listitem>
<para>Deben admitir el despliegue mediante medios virtuales (actualmente no se
admite PXE)</para>
</listitem>
<listitem>
<para>Deben tener conectividad de red con el clúster de gestión para acceder a las
API de aprovisionamiento de Metal<superscript>3</superscript></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Se requieren algunas herramientas, que se pueden instalar en el clúster de
gestión o en un host que pueda acceder a él.</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>,
<link xl:href="https://helm.sh">Helm</link> y <link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para>Un entorno de ejecución de contenedores como <link
xl:href="https://podman.io">Podman</link> o <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link></para>
</listitem>
</itemizedlist>
<para>El archivo de imagen de sistema operativo
<literal>SL-Micro.x86_64-6.1-Base-GM.raw</literal> se debe descargar del
<link xl:href="https://scc.suse.com/">Centro de servicios al cliente de
SUSE</link> o de la <link
xl:href="https://www.suse.com/download/sle-micro/">página de descargas de
SUSE</link>.</para>
<section xml:id="id-setup-management-cluster">
<title>Configuración del clúster de gestión</title>
<para>Los pasos básicos para instalar un clúster de gestión y usar
Metal<superscript>3</superscript> son:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Instalar un clúster de gestión RKE2</para>
</listitem>
<listitem>
<para>Instalar Rancher</para>
</listitem>
<listitem>
<para>Instalar un proveedor de almacenamiento (opcional)</para>
</listitem>
<listitem>
<para>Instalar las dependencias de Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>Instalar las dependencias de CAPI mediante Rancher Turtles</para>
</listitem>
<listitem>
<para>Crear una imagen del sistema operativo SLEMicro para los hosts del clúster
descendente</para>
</listitem>
<listitem>
<para>Registrar los CR de BareMetalHost para definir el inventario de bare metal</para>
</listitem>
<listitem>
<para>Crear un clúster descendente definiendo los recursos de CAPI</para>
</listitem>
</orderedlist>
<para>En esta guía se entiende que ya existe un clúster RKE2 y que se ha instalado
Rancher (incluido cert-manager), por ejemplo, utilizando Edge Image Builder
(<xref linkend="components-eib"/>).</para>
<tip>
<para>Estos pasos también se pueden automatizar por completo, como se describe en
la documentación sobre el clúster de gestión (<xref
linkend="atip-management-cluster"/>).</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Instalación de las dependencias de Metal<superscript>3</superscript></title>
<para>Si aún no se ha hecho como parte de la instalación de Rancher, se debe
instalar y ejecutar cert-manager.</para>
<para>Se debe instalar un proveedor de almacenamiento persistente. Se recomienda
SUSE Storage, pero también se puede utilizar
<literal>local-pathprovisioner</literal> para entornos de
desarrollo/PoC. Las instrucciones siguientes dan por sentado que se ha <link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">marcado
una StorageClass como predeterminada</link>; de lo contrario, se requiere
una configuración adicional para el chart de
Metal<superscript>3</superscript>.</para>
<para>Se requiere una IP adicional, gestionada por <link
xl:href="https://metallb.universe.tf/">MetalLB</link> para proporcionar un
punto final coherente para los servicios de gestión de
Metal<superscript>3</superscript>. Esta IP debe formar parte de la subred
del plano de control y estar reservada para la configuración estática (no
debe formar parte de ningún grupo DHCP).</para>
<tip>
<para>Si el clúster de gestión es un solo nodo, no es necesario contar con una IP
fleetnte adicional gestionada mediante MetalLB. Consulte la <xref
linkend="id-single-node-configuration"/>.</para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>Primero, se instala MetalLB:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>Luego, se definen <literal>IPAddressPool</literal> y
<literal>L2Advertisement</literal> con la IP reservada, definida como
<literal>STATIC_IRONIC_IP</literal> a continuación:</para>
<screen language="bash" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>Ya se puede instalar Metal<superscript>3</superscript>:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/charts/metal3 \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="$STATIC_IRONIC_IP"</screen>
</listitem>
<listitem>
<para>El contenedor de inicio puede tardar unos dos minutos en ejecutarse en este
despliegue, así que asegúrese de que todos los pods estén funcionando antes
de continuar:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para>No continúe con los siguientes pasos hasta que todos los pods del espacio de
nombres <literal>metal3-system</literal> estén en ejecución.</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Instalación de las dependencias de Cluster API</title>
<para>Las dependencias de Cluster API se gestionan mediante el chart de Helm de
Rancher Turtles:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/charts/rancher-turtles \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</screen>
<para>Después de un tiempo, los pods del controlador deberían estar ejecutándose
en los espacios de nombres <literal>capi-system</literal>,
<literal>capm3-system</literal>, <literal>rke2-bootstrap-system</literal> y
<literal>rke2-control-plane-system</literal>.</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>Preparación de la imagen del clúster descendente</title>
<para>Kiwi (<xref linkend="guides-kiwi-builder-images"/>) y Edge Image Builder
(<xref linkend="components-eib"/>) se usan para preparar una imagen base de
SLEMicro modificada que se aprovisiona en los hosts de clústeres
descendentes.</para>
<para>En esta guía, se trata la configuración mínima necesaria para desplegar el
clúster descendente.</para>
<section xml:id="id-image-configuration">
<title>Configuración de la imagen</title>
<note>
<para>Como primer paso necesario para crear clústeres, cree una imagen nueva
siguiendo las instrucciones del <xref
linkend="guides-kiwi-builder-images"/>.</para>
</note>
<para>Al ejecutar Edge Image Builder, se monta un directorio desde el host, por lo
que es necesario crear una estructura de directorios para almacenar los
archivos de configuración usados para definir la imagen de destino.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> es el archivo de
definición de la imagen. Consulte el <xref linkend="quickstart-eib"/> para
obtener más detalles.</para>
</listitem>
<listitem>
<para>La imagen base, cuando se descarga, está comprimida con
<literal>xz</literal>, por lo que debe descomprimirse con
<literal>unxz</literal> y copiarse/moverse a la carpeta
<literal>base-images</literal>.</para>
</listitem>
<listitem>
<para>La carpeta <literal>network</literal> es opcional. Para obtener más
información, consulte la <xref linkend="metal3-add-network-eib"/>.</para>
</listitem>
<listitem>
<para>El directorio custom/scripts contiene guiones que se ejecutan en el primer
arranque; actualmente, se requiere un guion
<literal>01-fix-growfs.sh</literal> para cambiar el tamaño de la partición
raíz del sistema operativo en el despliegue.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>Archivo de definición de la imagen del clúster descendente</title>
<para>El archivo <literal>downstream-cluster-config.yaml</literal> es el principal
archivo de configuración para la imagen del clúster descendente. A
continuación, se muestra un ejemplo mínimo de despliegue mediante
Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
  sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Donde <literal>$SCC_REGISTRATION_CODE</literal> es el código de registro
copiado del <link xl:href="https://scc.suse.com/">Centro de servicios al
cliente de SUSE</link> y la lista de paquetes contiene
<literal>jq</literal>, que es obligatorio.</para>
<para><literal>$ROOT_PASSWORD</literal> es la contraseña cifrada del usuario root,
que puede ser útil para pruebas y depuración. Se puede generar con el
comando <literal>openssl passwd -6 PASSWORD</literal>.</para>
<para>En entornos de producción, se recomienda usar las claves SSH que se pueden
añadir al bloque de usuarios sustituyendo <literal>$USERKEY1</literal> por
las claves SSH reales.</para>
<note>
<para><literal>net.ifnames=1</literal> permite los <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">nombres
predecibles para las interfaces de red</link></para>
<para>Esto coincide con la configuración predeterminada para el chart de
Metal<superscript>3</superscript>, pero el ajuste debe coincidir con el
valor de <literal>predictableNicNames</literal> configurado en el chart.</para>
<para>Tenga en cuenta también que
<literal>ignition.platform.id=openstack</literal> es obligatorio. Sin este
argumento, la configuración de SUSE Linux Micro a través de Ignition fallará
en el flujo automatizado de Metal<superscript>3</superscript>.</para>
<para>La sección <literal>time</literal> es opcional, pero se recomienda
encarecidamente configurarla para evitar posibles problemas con los
certificados y la desviación del reloj. Los valores proporcionados en este
ejemplo son solo ilustrativos. Ajústelos según sus requisitos específicos.</para>
</note>
</section>
<section xml:id="growfs-script">
<title>Guion Growfs</title>
<para>Actualmente, se requiere un guion personalizado
(<literal>custom/scripts/01-fix-growfs.sh</literal>) para ampliar el sistema
de archivos y que coincida con el tamaño del disco en el primer arranque
después del aprovisionamiento. El guion <literal>01-fix-growfs.sh</literal>
contiene la siguiente información:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>Use el mismo método para añadir sus propios guiones personalizados que se
ejecuten durante el proceso de aprovisionamiento. Para obtener más
información, consulte el <xref linkend="quickstart-eib"/>.</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>Creación de la imagen</title>
<para>Cuando se haya preparado la estructura de directorios siguiendo las
secciones anteriores, ejecute el siguiente comando para crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>Esto crea el archivo de imagen de salida denominado
<literal>SLE-Micro-eib-output.raw</literal>, basado en la definición
descrita anteriormente.</para>
<para>La imagen resultante debe estar disponible a través de un servidor web, ya
sea el contenedor del servidor multimedia habilitado mediante el chart de
Metal3 (<xref linkend="metal3-media-server"/>) o algún otro servidor al que
se pueda acceder localmente. En los ejemplos siguientes, nos referimos a
este servidor como <literal>imagecache.local:8080</literal>.</para>
<note>
<para>Al desplegar imágenes de EIB en clústeres descendentes, también es necesario
incluir la suma sha256 de la imagen en el objeto
<literal>Metal3MachineTemplate</literal>. Se puede generar de la siguiente
manera:</para>
<screen language="shell" linenumbering="unnumbered">sha256sum &lt;image_file&gt; &gt; &lt;image_file&gt;.sha256
# On this example:
sha256sum SLE-Micro-eib-output.raw &gt; SLE-Micro-eib-output.raw.sha256</screen>
</note>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>Adición del inventario de BareMetalHost</title>
<para>El registro de servidores bare metal para el despliegue automatizado
requiere la creación de dos recursos: un secreto que almacena las
credenciales de acceso a BMC y un recurso BareMetalHost de
Metal<superscript>3</superscript> que define la conexión de BMC y otros
detalles:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>Tenga en cuenta lo siguiente:</para>
<itemizedlist>
<listitem>
<para>El nombre de usuario y la contraseña del secreto deben estar cifrados en
base64. No deben incluir saltos de línea al final (por ejemplo, use
<literal>echo -n</literal>, no solo <literal>echo</literal>)</para>
</listitem>
<listitem>
<para>La etiqueta <literal>cluster-role</literal> se puede establecer ahora o más
adelante, durante la creación del clúster. En el ejemplo siguiente, se
espera <literal>control-plane</literal> o <literal>worker</literal></para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal> debe ser una dirección MAC válida que
coincida con la NIC de plano de control del host</para>
</listitem>
<listitem>
<para>La dirección <literal>bmc</literal> es la conexión a la API de gestión de
BMC. Se admiten las siguientes:</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;DIRECCIÓN
IP&gt;/redfish/v1/Systems/&lt;ID DEL SISTEMA&gt;</literal>: medio virtual
Redfish, por ejemplo, SuperMicro</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;DIRECCIÓN
IP&gt;/redfish/v1/Systems/System.Embedded.1</literal>: iDRAC de Dell</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Consulte la <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">documentación
original de la API</link> para obtener más información sobre la API de
BareMetalHost</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>Configuración de IP estáticas</title>
<para>El ejemplo anterior de BareMetalHost presupone que DHCP proporciona la
configuración de red del plano de control, pero para situaciones en las que
se necesite una configuración manual, como en el caso de las IP estáticas,
es posible proporcionar una configuración adicional, como se describe a
continuación.</para>
<section xml:id="metal3-add-network-eib">
<title>Guion adicional para la configuración de la red estática</title>
<para>Al crear la imagen base con Edge Image Builder, en la carpeta de
<literal>network</literal>, cree el archivo
<literal>configure-network.sh</literal> siguiente.</para>
<para>Esto consume datos de la unidad de configuración en el primer arranque y
configura la red del host usando la <link
xl:href="https://github.com/suse-edge/nm-configurator">herramienta NM
Configurator</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>Secreto adicional con configuración de red de host</title>
<para>Se puede definir un secreto adicional que contenga datos en el formato <link
xl:href="https://nmstate.io/">nmstate</link> compatible con NM Configurator
(<xref linkend="components-nmc"/>) para cada host.</para>
<para>A continuación, se hace referencia al secreto en el recurso
<literal>BareMetalHost</literal> mediante el campo de especificación
<literal>preprovisioningNetworkDataName</literal>.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>En algunas circunstancias, la dirección MAC puede omitirse. Consulte la
<xref linkend="networking-unified"/> para obtener más detalles.</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>Preparación de BareMetalHost</title>
<para>Después de crear el recurso BareMetalHost y los secretos asociados como se
ha descrito, se activa un flujo de trabajo de preparación del host:</para>
<itemizedlist>
<listitem>
<para>Se arranca una imagen ramdisk mediante la conexión de un dispositivo virtual
al BMC del host de destino</para>
</listitem>
<listitem>
<para>El ramdisk inspecciona los detalles del hardware y prepara el host para el
aprovisionamiento (por ejemplo, limpiando los discos de datos anteriores)</para>
</listitem>
<listitem>
<para>Una vez completado este proceso, se actualizan los detalles del hardware en
el campo <literal>status.hardware</literal> de BareMetalHost y se pueden
verificar</para>
</listitem>
</itemizedlist>
<para>Este proceso puede tardar varios minutos, pero una vez completado, debería
ver que el estado de BareMetalHost pasa a ser <literal>available</literal>
(disponible):</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>Creación de clústeres descendentes</title>
<para>Ahora creamos recursos de Cluster API que definen el clúster descendente y
recursos del equipo que provocarán que se aprovisionen los recursos de
BareMetalHost y, a continuación, se arranquen para formar un clúster RKE2.</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>Despliegue del plano de control</title>
<para>Para desplegar el plano de control, se define un manifiesto YAML similar al
que se muestra a continuación, que contiene los siguientes recursos:</para>
<itemizedlist>
<listitem>
<para>El recurso Cluster define el nombre del clúster, las redes y el tipo de
proveedor de plano de control/infraestructura (en este caso, RKE2/Metal3)</para>
</listitem>
<listitem>
<para>Metal3Cluster define el punto final del plano de control (IP del host para
un solo nodo, punto final LoadBalancer para varios nodos; en este ejemplo se
supone que hay un solo nodo)</para>
</listitem>
<listitem>
<para>RKE2ControlPlane define la versión de RKE2 y cualquier configuración
adicional necesaria durante el arranque del clúster</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate define la imagen del sistema operativo que se aplicará
a los recursos BareMetalHost, y hostSelector define qué recursos
BareMetalHost se consumirán</para>
</listitem>
<listitem>
<para>Metal3DataTemplate define los metadatos adicionales que se pasarán a
BareMetalHost (tenga en cuenta que networkData no es compatible actualmente
con Edge)</para>
</listitem>
</itemizedlist>
<note>
<para>Para simplificar, en este ejemplo se entiende que hay un plano de control de
un solo nodo en el que BareMetalHost se ha configurado con la IP
<literal>192.168.125.200</literal>. Para ejemplos más avanzados de varios
nodos, consulte el <xref linkend="atip-automated-provisioning"/>.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  version: v1.32.4+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Una vez adaptado a su entorno, puede aplicar el ejemplo mediante
<literal>kubectl</literal> y, a continuación, supervisar el estado del
clúster mediante <literal>clusterctl</literal>.</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>Despliegue de trabajadores/computación</title>
<para>De forma similar al despliegue del plano de control, se define un manifiesto
YAML que contiene los siguientes recursos:</para>
<itemizedlist>
<listitem>
<para>MachineDeployment define el número de réplicas (hosts) y el proveedor de
arranque/infraestructura (en este caso, RKE2/Metal3)</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplate describe la versión de RKE2 y la configuración de
arranque inicial para el arranque del host del agente</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate define la imagen del sistema operativo que se aplicará
a los recursos BareMetalHost, y el selector de host define qué recursos
BareMetalHost se consumirán</para>
</listitem>
<listitem>
<para>Metal3DataTemplate define los metadatos adicionales que se pasarán a
BareMetalHost (tenga en cuenta que <literal>networkData</literal> no es
compatible actualmente)</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.32.4+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.32.4+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Una vez copiado y adaptado el ejemplo anterior a su entorno, se puede
aplicar mediante <literal>kubectl</literal> y, a continuación, se puede
supervisar el estado del clúster con <literal>clusterctl</literal>.</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait for the worker nodes to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>Desaprovisionamiento del clúster</title>
<para>El clúster descendente se puede desaprovisionar eliminando los recursos
aplicados en los pasos anteriores:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>Esto activa el desaprovisionamiento de los recursos BareMetalHost, lo que
puede tardar varios minutos, tras lo cual deberían volver a estar
disponibles:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>Problemas conocidos</title>
<itemizedlist>
<listitem>
<para>El <link
xl:href="https://github.com/metal3-io/ip-address-manager">controlador de
gestión de direcciones IP</link> actualmente no se admite, ya que aún no es
compatible con nuestras herramientas de configuración de red y la cadena de
herramientas de primer arranque en SLEMicro.</para>
</listitem>
<listitem>
<para>Del mismo modo, los recursos IPAM y los campos networkData de
Metal3DataTemplate no son compatibles actualmente.</para>
</listitem>
<listitem>
<para>Actualmente, solo se admite el despliegue mediante redfish-virtualmedia.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-planned-changes">
<title>Cambios previstos</title>
<itemizedlist>
<listitem>
<para>Habilitar la compatibilidad con los recursos y la configuración de IPAM a
través de los campos networkData.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>Recursos adicionales</title>
<para>La documentación de SUSE Edge for Telco (<xref linkend="atip"/>) tiene
ejemplos de uso más avanzados de Metal<superscript>3</superscript> para
casos de uso relacionados con las telecomunicaciones.</para>
<section xml:id="id-single-node-configuration">
<title>Configuración de un solo nodo</title>
<para>En entornos de prueba/PoC en los que el clúster de gestión es de un solo
nodo, no se necesita una IP fleetnte adicional gestionada a través de
MetalLB.</para>
<para>En este modo, el punto final para las API del clúster de gestión es la IP
del clúster de gestión, por lo que debe reservarse cuando se utiliza DHCP o
debe configurarse de forma estática para garantizar que la IP del clúster de
gestión no cambie, lo que se denomina
<literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal> abajo.</para>
<para>Para habilitar este escenario, los valores necesarios del chart de
Metal<superscript>3</superscript> son los siguientes:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="disabling-tls-for-virtualmedia-iso-attachment">
<title>Inhabilitación de TLS para la conexión ISO de medios virtuales</title>
<para>Algunos proveedores de servidores verifican la conexión SSL al conectar
imágenes ISO de medios virtuales al BMC, lo que puede causar un problema
porque los certificados generados para el despliegue de
Metal<superscript>3</superscript> son autofirmados. Para solucionar este
problema, es posible desactivar TLS solo para la conexión del disco de
medios virtuales con los valores del chart de
Metal<superscript>3</superscript> de la siguiente manera:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>Una solución alternativa es configurar los BMC con el certificado de la CA;
en este caso, puede leer los certificados del clúster utilizando
<literal>kubectl</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>A continuación, el certificado se puede configurar en el panel de control
del BMC del servidor, aunque ese proceso es específico de cada proveedor (y
no es posible para todos los proveedores, en cuyo caso puede ser necesario
el indicador <literal>enable_vmedia_tls</literal>).</para>
</section>
<section xml:id="id-storage-configuration">
<title>Configuración de almacenamiento</title>
<para>Para entornos de prueba/PoC en los que el clúster de gestión es de un solo
nodo, no se requiere almacenamiento persistente, pero para casos de uso en
producción se recomienda instalar SUSE Storage (Longhorn) en el clúster de
gestión para que las imágenes relacionadas con
Metal<superscript>3</superscript> puedan conservarse durante el
reinicio/reprogramación de un pod.</para>
<para>Para habilitar este almacenamiento persistente, los valores necesarios del
chart de Metal<superscript>3</superscript> son los siguientes:</para>
<screen language="yaml" linenumbering="unnumbered">metal3-ironic:
  persistence:
    ironic:
      size: "5Gi"</screen>
<para>La documentación sobre el clúster de gestión en SUSE Edge for Telco (<xref
linkend="atip-management-cluster"/>) contiene más detalles sobre cómo
configurar un clúster de gestión con almacenamiento persistente.</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Incorporación de hosts remotos con Elemental</title>
<para>En esta sección se explica la solución de aprovisionamiento de red "phone
home" (el comando que se usa, traducido como "llamar a casa") como parte de
SUSE Edge. En ella se utiliza Elemental para ayudar con la incorporación de
nodos. Elemental es una pila de software que permite el registro remoto de
hosts y la gestión centralizada y totalmente nativa en la nube del sistema
operativo con Kubernetes. En la pila de SUSE Edge se utiliza la función de
registro de Elemental para permitir la incorporación remota de hosts en
Rancher, de modo que los hosts puedan integrarse en una plataforma de
gestión centralizada y, desde allí, desplegar y gestionar clústeres de
Kubernetes junto con componentes en capas, aplicaciones y su ciclo de vida,
todo desde un lugar común.</para>
<para>Este enfoque puede ser útil en situaciones en las que los dispositivos que
se desean controlar no se encuentren en la misma red que el clúster de
gestión o no dispongan de un controlador de gestión fuera de banda integrado
que permita un control más directo, y en los que se están arrancando muchos
sistemas "desconocidos" diferentes en el perímetro y es necesario
incorporarlos y gestionarlos de forma segura a gran escala. Este es un
escenario común para usos en el comercio minorista, el IoT industrial u
otros espacios en los que se tiene poco control sobre la red en la que se
instalan los dispositivos.</para>
<section xml:id="id-high-level-architecture-3">
<title>Arquitectura general</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>guía de inicio rápido de la arquitectura de elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>Recursos necesarios</title>
<para>A continuación se describen los requisitos mínimos del sistema y del entorno
para la ejecución:</para>
<itemizedlist>
<listitem>
<para>Un host para el clúster de gestión centralizada (el que aloja Rancher y
Elemental):</para>
<itemizedlist>
<listitem>
<para>Mínimo de 8 GB de RAM y 20 GB de espacio en disco para desarrollo o pruebas
(consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/getting-started/installation-and-upgrade/installation-requirements#hardware-requirements">este
documento</link> para el uso en producción)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Un nodo de destino que se va a aprovisionar, es decir, el dispositivo
periférico (se puede utilizar una máquina virtual para fines de demostración
o pruebas)</para>
<itemizedlist>
<listitem>
<para>Mínimo de 4 GB de RAM, CPU de 2 núcleos y 20 GB de disco</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Un nombre de host resoluble para el clúster de gestión o una dirección IP
estática para utilizar con un servicio como sslip.io</para>
</listitem>
<listitem>
<para>Un host para crear el medio de instalación a través de Edge Image Builder</para>
<itemizedlist>
<listitem>
<para>Ejecutar SLES 15 SP6, openSUSE Leap 15.6 u otro sistema operativo compatible
que admita Podman</para>
</listitem>
<listitem>
<para>Con <link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>,
<link xl:href="https://podman.io">Podman</link> y <link
xl:href="https://helm.sh">Helm</link> instalados</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Una unidad flash USB desde la que arrancar (si se utiliza hardware físico)</para>
</listitem>
<listitem>
<para>Una copia descargada de la última imagen ISO de autoinstalación de SUSE
Linux Micro 6.1, que se encuentra <link
xl:href="https://www.suse.com/download/sle-micro/">aquí</link></para>
</listitem>
</itemizedlist>
<note>
<para>Los datos existentes en los equipos de destino se sobrescribirán como parte
del proceso, por lo que debe asegurarse de realizar una copia de seguridad
de todos los datos almacenados en dispositivos USB y discos conectados a los
nodos de despliegue de destino.</para>
</note>
<para>En esta guía se utiliza un droplet de Digital Ocean para alojar el clúster
ascendente y un NUC Intel como dispositivo descendente. Para crear el medio
de instalación, se utiliza SUSE Linux Enterprise Server.</para>
</section>
<section xml:id="build-bootstrap-cluster">
<title>Creación del clúster de arranque</title>
<para>Para empezar, cree un clúster capaz de alojar Rancher y Elemental. Este
clúster debe ser enrutable desde la red a la que están conectados los nodos
descendentes.</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Creación del clúster de Kubernetes</title>
<para>Si utiliza un hiperescalador (como Azure, AWS o Google Cloud), la forma más
sencilla de configurar un clúster es utilizando sus herramientas
integradas. Para que esta guía sea más concisa, no se detalla el proceso de
cada una de estas opciones.</para>
<para>Si va a realizar la instalación en un servidor físico u otro servicio de
alojamiento en el que también deba proporcionar la distribución de
Kubernetes, le recomendamos que utilice <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>.</para>
</section>
<section xml:id="id-set-up-dns">
<title>Configuración de DNS</title>
<para>Antes de continuar, debe configurar el acceso a su clúster. Como ocurre con
la configuración del propio clúster, la forma de configurar el servidor DNS
variará en función del lugar donde esté alojado.</para>
<tip>
<para>Si no desea ocuparse de la configuración de los registros DNS (por ejemplo,
si se trata solo de un servidor de prueba efímero), puede utilizar un
servicio como <link xl:href="https://sslip.io">sslip.io</link> en su
lugar. Con este servicio, puede resolver cualquier dirección IP con
<literal>&lt;dirección&gt;.sslip.io</literal>.</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Instalar Rancher</title>
<para>Para instalar Rancher, necesitas acceder a la API de Kubernetes del clúster
que acabas de crear. Esto varía en función de la distribución de Kubernetes
que se utilice.</para>
<para>Para RKE2, el archivo kubeconfig se habrá escrito en
<literal>/etc/rancher/rke2/rke2.yaml</literal>. Guarde este archivo como
<literal>~/.kube/config</literal> en su sistema local. Es posible que tenga
que editar el archivo para incluir la dirección IP o el nombre de host
externos correctos.</para>
<para>Instale Rancher fácilmente con los comandos que encontrará en la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">documentación
de Rancher</link>:</para>
<para>Instale <link xl:href="https://cert-manager.io">cert-manager</link>:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true</screen>
<para>Después, instale Rancher:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt; \
  --version 2.11.2</screen>
<note>
<para>Si se trata de un sistema de producción, use cert-manager para configurar un
certificado real (como uno de Let’s Encrypt).</para>
</note>
<para>Busque el nombre de host que ha configurado e inicie sesión en Rancher con
la contraseña que ha utilizado en <literal>bootstrapPassword</literal>. Se
le guiará a través de un breve proceso de configuración.</para>
</section>
<section xml:id="install-elemental">
<title>Instalación de Elemental</title>
<para>Una vez instalado Rancher, ya puede instalar el operador de Elemental y las
CRD necesarias. El chart de Helm para Elemental se publica como un artefacto
OCI, por lo que la instalación es un poco más sencilla que la de otros
charts. Se puede instalar desde la misma shell que se utilizó para instalar
Rancher o en el navegador desde la shell de Rancher.</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.6.8

helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.6.8</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>(Opcional) Instale la extensión de interfaz de usuario de Elemental</title>
<orderedlist numeration="arabic">
<listitem>
<para>Para usar la interfaz de usuario de Elemental, inicie sesión en su instancia
de Rancher y haga clic en el menú de tres líneas de la parte superior
izquierda:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width="85%"/> </imageobject>
<textobject><phrase>Instalación de la extensión 1 de Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>En la pestaña "Available" (Disponible) de esta página, haga clic en
"Install" (Instalar) en la tarjeta de Elemental:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width="85%"/> </imageobject>
<textobject><phrase>Instalación de la extensión 2 de Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Confirme que desea instalar la extensión:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width="100%"/> </imageobject>
<textobject><phrase>Instalación de la extensión 3 de Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Cuando se instale, se le pedirá que vuelva a cargar la página.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width="100%"/> </imageobject>
<textobject><phrase>Instalación de la extensión 4 de Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Tras el reinicio, podrá acceder a la extensión de Elemental a través de la
aplicación global "OS Management" (Gestión del sistema operativo).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width="100%"/> </imageobject>
<textobject><phrase>Acceso a la extensión de Elemental</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configure-elemental">
<title>Configuración de Elemental</title>
<para>Por sencillez, se recomienda configurar la variable <literal>$ELEM</literal>
con la vía completa donde desea que se encuentre el directorio de
configuración:</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>Para permitir que las máquinas se registren en Elemental, se necesita crear
un objeto <literal>MachineRegistration</literal> en el espacio de nombres
<literal>fleet-default</literal>.</para>
<para>Vamos a crear una versión básica de este objeto:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>El comando <literal>cat</literal> escapa cada <literal>$</literal> con una
barra invertida (<literal>\</literal>) para que Bash no las utilice como
plantilla. Elimine las barras invertidas si va a realizar una copia
manualmente.</para>
</note>
<para>Una vez creado el objeto, busque y anote el punto final que se le asigna:</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>Alternativamente, puede hacerlo desde la interfaz de usuario.</para>
<variablelist>
<varlistentry>
<term>Extensión de interfaz de usuario</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>Desde la extensión de gestión del sistema operativo, haga clic en "Create
Registration Endpoint" (Crear punto final de registro):</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png" width="100%"/>
</imageobject>
<textobject><phrase>Haga clic en "Create Registration" (Crear registro).</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Asigne un nombre a esta configuración.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png" width="100%"/>
</imageobject>
<textobject><phrase>Adición del nombre</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Puede ignorar el campo "Cloud Configuration" (Configuración de la nube)
porque estos datos se sobrescriben con los del paso siguiente con Edge Image
Builder.</para>
</note>
</listitem>
<listitem>
<para>A continuación, desplácese hacia abajo y haga clic en "Add Label" (Añadir
etiqueta) para cada etiqueta que desee que aparezca en el recurso que se
crea cuando se registra un equipo. Esto resulta útil para distinguir cada
equipo.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png" width="100%"/>
</imageobject>
<textobject><phrase>Adición de etiquetas</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Haga clic en "Create" (Crear) para guardar la configuración.</para>
</listitem>
<listitem>
<para>Una vez creado el registro, debería ver la URL de registro en la lista y
puede hacer clic en "Copy" (Copiar) para copiar la dirección:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width="100%"/>
</imageobject>
<textobject><phrase>Copia de la URL</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>Si ha salido de esa pantalla, puede hacer clic en "Registration Endpoints"
(Puntos finales de registro) en el menú de la izquierda y, a continuación,
hacer clic en el nombre del punto final que acaba de crear.</para>
</tip>
<para>Esta URL se usa en el paso siguiente.</para>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="build-installation-media">
<title>Creación de la imagen</title>
<para>Aunque la versión actual de Elemental permite crear sus propios medios de
instalación, en SUSE Edge 3.3.1 se hace con Kiwi y Edge Image Builder, por
lo que el sistema resultante se crea con <link
xl:href="https://www.suse.com/products/micro/">SUSE Linux Micro</link> como
sistema operativo base.</para>
<tip>
<para>Para obtener más información sobre Kiwi, siga el proceso de creación de
imágenes de Kiwi (<xref linkend="guides-kiwi-builder-images"/>) para crear
imágenes nuevas en primer lugar. En el caso de Edge Image Builder, consulte
la guía de introducción a Edge Image Builder (<xref
linkend="quickstart-eib"/>) y la documentación de los componentes (<xref
linkend="components-eib"/>).</para>
</tip>
<para>Desde un sistema Linux con Podman instalado, cree los directorios y coloque
la imagen base que está creando Kiwi:</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/{micro-base-image-iso} $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.2
image:
    imageType: iso
    arch: x86_64
    baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: XXX
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para>La sección <literal>time</literal> es opcional, pero se recomienda
encarecidamente configurarla para evitar posibles problemas con los
certificados y la desviación del reloj. Los valores proporcionados en este
ejemplo son solo ilustrativos. Ajústelos según sus requisitos específicos.</para>
</listitem>
<listitem>
<para>La contraseña sin cifrar es <literal>eib</literal>.</para>
</listitem>
<listitem>
<para>Se necesita el código <literal>sccRegistrationCode</literal> para descargar
e instalar los paquetes RPM necesarios de las fuentes oficiales
(alternativamente, los RPM <literal>elemental-register</literal> y
<literal>elemental-system-agent</literal> se pueden cargar de forma manual y
local).</para>
</listitem>
<listitem>
<para>El comando <literal>cat</literal> escapa cada <literal>$</literal> con una
barra invertida (<literal>\</literal>) para que Bash no las utilice como
plantilla. Elimine las barras invertidas si va a realizar una copia
manualmente.</para>
</listitem>
<listitem>
<para>El dispositivo de instalación se borrará durante la instalación.</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
 build --definition-file eib-config.yaml</screen>
<para>Si está arrancando un dispositivo físico, es necesario grabar la imagen en
una unidad flash USB. Puede hacerlo con:</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>Arranque de los nodos descendentes</title>
<para>Ahora que se ha creado el medio de instalación, se pueden arrancar los nodos
descendentes con él.</para>
<para>Para cada sistema que desee controlar con Elemental, añada el medio de
instalación y arranque el dispositivo. Tras la instalación, se reiniciará y
se registrará automáticamente.</para>
<para>Si utiliza la extensión de la interfaz de usuario, debería ver su nodo
aparecer en "Inventory of Machines" (Inventario de equipos).</para>
<note>
<para>No retire el medio de instalación hasta que haya visto el mensaje de inicio
de sesión; durante el primer arranque, se sigue accediendo a los archivos
desde la memoria USB.</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>Creación de clústeres descendentes</title>
<para>Para aprovisionar un clúster nuevo con Elemental, debe crear dos objetos.</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>El primero es <literal>MachineInventorySelectorTemplate</literal>. Este
objeto permite especificar una asignación entre los clústeres y los equipos
del inventario.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Cree un selector que coincida con cualquier equipo del inventario con una
etiqueta:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>Aplique el recurso al clúster:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>Obtenga el nombre del equipo máquina y añada la etiqueta correspondiente:</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>Cree un recurso de clúster K3s simple de un solo nodo y aplíquelo al
clúster:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.32.4+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Extensión de interfaz de usuario</term>
<listitem>
<para>La extensión de la interfaz de usuario permite utilizar algunos
atajos. Tenga en cuenta que la gestión de varias ubicaciones puede implicar
mucho trabajo manual.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Como antes, abra el menú de tres líneas de la izquierda y seleccione "OS
Management" (Gestión del sistema operativo). Esto le llevará de vuelta a la
pantalla principal para gestionar sus sistemas Elemental.</para>
</listitem>
<listitem>
<para>En la barra lateral izquierda, haga clic en "Inventory of Machines"
(Inventario de máquinas). Se abrirá el inventario de equipos que se han
registrado.</para>
</listitem>
<listitem>
<para>Para crear un clúster a partir de estos equipos, seleccione los sistemas que
desee, haga clic en la lista desplegable "Actions" (Acciones) y, a
continuación, en "Create Elemental Cluster" (Crear clúster de Elemental). Se
abrirá el cuadro de diálogo "Cluster Creation" (Creación de clúster) y, al
mismo tiempo, se creará una plantilla MachineSelectorTemplate para
utilizarla en segundo plano.</para>
</listitem>
<listitem>
<para>En esta pantalla, configure el clúster que quiera crear. En esta guía, se
selecciona K3s v1.30.5+k3s1, y el resto de las opciones se dejan tal cual.</para>
<tip>
<para>Tendrá que desplazarse por la página para ver más opciones.</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>Después de crear estos objetos, debería ver cómo se inicia un nuevo clúster
de Kubernetes que usa el nuevo nodo que acaba de instalar.</para>
</section>
<section xml:id="id-node-reset-optional">
<title>Restablecimiento del nodo (opcional)</title>
<para>SUSE Rancher Elemental permite realizar un "restablecimiento de nodo", que
se puede activar opcionalmente cuando se elimina todo un clúster de Rancher
o un solo nodo de un clúster, o cuando se elimina manualmente un nodo del
inventario de equipos. Esto resulta útil cuando se desean restablecer y
limpiar los recursos huérfanos y se quiere volver a incorporar
automáticamente el nodo limpiado al inventario de equipos para que pueda
reutilizarse. Esta función no está habilitada de forma predeterminada, por
lo que no se limpiará cualquier sistema que se elimine (es decir, los datos
no se eliminarán y los recursos del clúster de Kubernetes seguirán
funcionando en los clústeres descendentes) y será necesaria una intervención
manual para borrar los datos y volver a registrar el equipo en Rancher a
través de Elemental.</para>
<para>Si desea que esta funcionalidad esté habilitada de forma predeterminada,
debe asegurarse de que esté explícitamente habilitada en
<literal>MachineRegistration</literal> añadiendo
<literal>config.elemental.reset.enabled: true</literal>, por ejemplo:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>A continuación, todos los sistemas registrados con este registro
<literal>MachineRegistration</literal> recibirán automáticamente la
anotación <literal>elemental.cattle.io/resettable: “true”</literal> en su
configuración. Si desea hacerlo manualmente en nodos individuales, por
ejemplo, porque tiene un <literal>MachineInventory</literal> que no tiene
esa anotación o porque ya ha desplegado nodos, puede modificar
<literal>MachineInventory</literal> y añadir la configuración
<literal>resettable</literal>. Por ejemplo:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>En SUSE Edge 3.1, el operador de Elemental coloca un marcador en el sistema
operativo que activará automáticamente el proceso de limpieza, detendrá
todos los servicios de Kubernetes, eliminará todos los datos persistentes,
desinstalará todos los servicios de Kubernetes, limpiará cualquier
directorio restante de Kubernetes/Rancher y forzará un nuevo registro en
Rancher a través de la configuración original de
<literal>MachineRegistration</literal> de Elemental. Esto ocurre
automáticamente, sin necesidad de intervención manual. El guion que se
ejecuta se encuentra en
<literal>/opt/edge/elemental_node_cleanup.sh</literal> y se activa a través
de <literal>systemd.path</literal> al colocar el marcador, por lo que su
ejecución es inmediata.</para>
<warning>
<para>El uso de la función <literal>resettable</literal> presupone que el
comportamiento deseado al eliminar un nodo/clúster de Rancher es borrar los
datos y forzar un nuevo registro. En esta situación, la pérdida de datos
está garantizada, por lo que solo debe utilizarla si está seguro de que
desea que se realice un restablecimiento automático.</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>Pasos siguientes</title>
<para>Se recomienda estudiar los siguientes recursos después de utilizar esta
guía:</para>
<itemizedlist>
<listitem>
<para>Automatización de extremo a extremo en el <xref linkend="components-fleet"/></para>
</listitem>
<listitem>
<para>Opciones de configuración de red adicionales en el <xref
linkend="components-nmc"/></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Clústeres independientes con Edge Image Builder</title>
<para>Edge Image Builder (EIB) es una herramienta que agiliza el proceso de
generación de imágenes de disco personalizadas y listas para arrancar (CRB)
para equipos de arranque, incluso en entornos totalmente aislados. EIB se
utiliza para crear imágenes de despliegue para su uso en las tres huellas de
despliegue de SUSE Edge, ya que es lo suficientemente flexible como para
ofrecer las personalizaciones más pequeñas, como añadir un usuario o
configurar la zona horaria, a través de una imagen configurada de forma
integral que establece, por ejemplo, configuraciones de red complejas,
despliega clústeres de Kubernetes de varios nodos, despliega cargas de
trabajo de clientes y se registra en la plataforma de gestión centralizada a
través de Rancher/Elemental y SUSE Multi-Linux Manager. EIB se ejecuta como
una imagen de contenedor, lo que lo hace increíblemente portátil entre
plataformas y garantiza que todas las dependencias necesarias sean
autónomas, con un impacto mínimo en los paquetes instalados del sistema que
se utiliza para operar la herramienta.</para>
<note>
<para>En escenarios de varios nodos, EIB despliega automáticamente MetalLB y
Endpoint Copier Operator para que los hosts aprovisionados con la misma
imagen compilada se unan automáticamente a un clúster de Kubernetes.</para>
</note>
<para>Para obtener más información, lea la introducción de Edge Image Builder
(<xref linkend="components-eib"/>).</para>
<warning>
<para>Edge Image Builder 1.2.1 admite la personalización de imágenes de SUSE Linux
Micro 6.1. En versiones anteriores, como SUSE Linux Enterprise Micro 5.5 o
6.0, no se admite.</para>
</warning>
<section xml:id="id-prerequisites-2">
<title>Requisitos previos</title>
<itemizedlist>
<listitem>
<para>Un equipo host AMD64/Intel 64 (físico o virtual) en el que se ejecute SLES
15 SP6.</para>
</listitem>
<listitem>
<para>El motor de contenedores Podman</para>
</listitem>
<listitem>
<para>Una imagen ISO de autoinstalación de SUSE Linux Micro 6.1 creada mediante el
procedimiento Kiwi Builder (<xref linkend="guides-kiwi-builder-images"/>)</para>
</listitem>
</itemizedlist>
<note>
<para>Para fines no relacionados con la producción, se puede utilizar openSUSE
Leap 15.6 u openSUSE Tumbleweed como equipo host de creación. Otros sistemas
operativos pueden funcionar, siempre que se disponga de un entorno de
ejecución de contenedores compatible.</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>Obtención de la imagen de EIB</title>
<para>La imagen del contenedor de EIB está disponible públicamente y se puede
descargar desde el registro de SUSE Edge ejecutando el siguiente comando en
el host de creación de imágenes:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.3/edge-image-builder:1.2.1</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>Creación del directorio de configuración de imágenes</title>
<para>Dado que EIB se ejecuta dentro de un contenedor, es necesario montar un
directorio de configuración desde el host, lo que le permite especificar la
configuración deseada. Así, durante el proceso de creación, EIB tiene acceso
a cualquier archivo de entrada y artefactos de apoyo necesarios. Este
directorio debe tener una estructura específica. Vamos a crearlo suponiendo
que este directorio existirá en su directorio de inicio y se llamará "eib":</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>En el paso anterior creamos el directorio "base-images" donde se alojará la
imagen de entrada de SUSE Linux Micro 6.1. Ahora, nos aseguraremos de que la
imagen se copie en el directorio de configuración:</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>Durante la ejecución de EIB, la imagen base original <emphasis
role="strong">no</emphasis> se modifica; se crea una nueva versión
personalizada con la configuración deseada en la raíz del directorio de
configuración de EIB.</para>
</note>
<para>El directorio de configuración en este momento debería tener el siguiente
aspecto:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>Creación del archivo de definición de imagen</title>
<para>El archivo de definición describe la mayoría de las opciones configurables
que admite Edge Image Builder. Encontrará un ejemplo completo de las
opciones <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/pkg/image/testdata/full-valid-example.yaml">aquí</link>.
Le recomendamos que consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md">guía
original sobre creación de imágenes</link> para ver ejemplos más completos
que los que se muestran a continuación. Comencemos con un archivo de
definición muy básico para nuestra imagen del sistema operativo:</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>Esta definición especifica que estamos generando una imagen de salida para
un sistema basado en AMD64/Intel 64. Para modificaciones posteriores, se
utilizará como base una imagen <literal>iso</literal> denominada
<literal>slemicro.iso</literal>, que se espera que se encuentre en
<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>. También indica que,
una vez que EIB haya terminado de modificarla, la imagen de salida se
llamará <literal>eib-image.iso</literal> y, por defecto, se ubicará en
<literal>$CONFIG_DIR</literal>.</para>
<para>Ahora, nuestra estructura de directorio debe tener este aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>En las siguientes secciones veremos algunos ejemplos de operaciones comunes:</para>
<section xml:id="id-configuring-os-users">
<title>Configuración de usuarios del sistema operativo</title>
<para>EIB permite preconfigurar la información de inicio de sesión de los
usuarios, como las contraseñas o las claves SSH, incluyendo la configuración
de una contraseña raíz fija. Como parte de este ejemplo, vamos a fijar la
contraseña raíz, y el primer paso es utilizar <literal>OpenSSL</literal>
para crear una contraseña cifrada unidireccional:</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>Se obtendrá un resultado similar a:</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>A continuación, podemos añadir una sección en el archivo de definición
llamada <literal>operatingSystem</literal> con una matriz
<literal>users</literal> en su interior. El archivo resultante debería tener
el siguiente aspecto:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>También es posible añadir usuarios adicionales, crear los directorios de
inicio, establecer identificadores de usuario, añadir autenticación mediante
claves SSH y modificar la información de los grupos. Consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md">guía
original sobre creación de imágenes</link> para ver más ejemplos.</para>
</note>
</section>
<section xml:id="configuring-os-time">
<title>Configuración de la hora del sistema operativo</title>
<para>La sección <literal>time</literal> es opcional, pero se recomienda
encarecidamente configurarla para evitar posibles problemas con los
certificados y la desviación del reloj. EIB configurará chronyd y
<literal>/etc/localtime</literal> en función de los parámetros aquí
indicados.</para>
<screen language="console" linenumbering="unnumbered">operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2</screen>
<itemizedlist>
<listitem>
<para><literal>timezone</literal>: especifica la zona horaria en el formato
"Región/Localidad" (por ejemplo, "Europa/Madrid"). Para ver la lista
completa, ejecute <literal>timedatectl list-timezones</literal> en un
sistema Linux.</para>
</listitem>
<listitem>
<para>ntp: define los atributos relacionados con la configuración de NTP (usando
chronyd).</para>
</listitem>
<listitem>
<para>forceWait: solicita que chronyd intente sincronizar las fuentes de tiempo
antes de iniciar otros servicios, con un tiempo de espera de 180 segundos.</para>
</listitem>
<listitem>
<para>pools: especifica una lista de grupos que chronyd usará como fuentes de
datos (utilizando <literal>iburst</literal> para mejorar el tiempo que tarda
la sincronización inicial).</para>
</listitem>
<listitem>
<para>servers: especifica una lista de servidores que chronyd usará como fuentes
de datos (utilizando <literal>iburst</literal> para mejorar el tiempo que
tarda la sincronización inicial).</para>
</listitem>
</itemizedlist>
<note>
<para>Los valores proporcionados en este ejemplo son solo ilustrativos. Ajústelos
para que se adapten a sus necesidades específicas.</para>
</note>
</section>
<section xml:id="adding-certificates">
<title>Adición de certificados</title>
<para>Los archivos de certificado con la extensión .pem o .crt almacenados en el
directorio <literal>certificates</literal> se instalarán en el almacén de
certificados del sistema del nodo:</para>
<screen language="console" linenumbering="unnumbered">.
├── definition.yaml
└── certificates
    ├── my-ca.pem
    └── my-ca.crt</screen>
<para>Consulte la guía <link
xl:href="https://documentation.suse.com/smart/security/html/tls-certificates/index.html#tls-adding-new-certificates">Securing
Communication with TLS Certificate</link> (Protección de las comunicaciones
con un certificado TLS) para obtener más información.</para>
</section>
<section xml:id="eib-configuring-rpm-packages">
<title>Configuración de paquetes RPM</title>
<para>Una de las principales características de EIB es que proporciona un
mecanismo para añadir paquetes de software adicionales a la imagen, de modo
que, una vez completada la instalación, el sistema puede aprovechar los
paquetes instalados de inmediato. EIB permite a los usuarios especificar lo
siguiente:</para>
<itemizedlist>
<listitem>
<para>Paquetes por su nombre dentro de una lista en la definición de imagen</para>
</listitem>
<listitem>
<para>Repositorios de red para buscar estos paquetes</para>
</listitem>
<listitem>
<para>Credenciales del Centro de servicios al cliente de SUSE (SCC) para buscar
los paquetes mostrados en los repositorios oficiales de SUSE</para>
</listitem>
<listitem>
<para>Mediante un directorio <literal>$CONFIG_DIR/rpms</literal>, paquetes RPM
personalizados y locales que no existen en los repositorios de red</para>
</listitem>
<listitem>
<para>Mediante el mismo directorio (<literal>$CONFIG_DIR/rpms/gpg-keys</literal>),
claves GPG para habilitar la validación de paquetes de terceros</para>
</listitem>
</itemizedlist>
<para>EIB ejecuta un proceso de resolución de paquetes en el momento de la
creación de la imagen, tomando la imagen base como entrada, e intentará
extraer e instalar todos los paquetes suministrados, ya sea especificados a
través de la lista o proporcionados localmente. EIB descarga todos los
paquetes, incluidas las dependencias, en un repositorio que existe dentro de
la imagen de salida e indica al sistema que los instale durante el primer
proceso de arranque. Realizar este proceso durante la creación de la imagen
garantiza que los paquetes se instalarán correctamente durante el primer
arranque en la plataforma deseada, por ejemplo, el nodo del perímetro. Esto
también es útil en entornos en los que se desea integrar los paquetes
adicionales en la imagen en lugar de descargarlos a través de la red durante
el funcionamiento, por ejemplo, en entornos de red restringidos o aislados.</para>
<para>Como ejemplo de demostración simple, vamos a instalar el paquete RPM
<literal>nvidia-container-toolkit</literal> que se encuentra en el
repositorio NVIDIA de terceros:</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>El archivo de definición resultante tiene el siguiente aspecto:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>Este es un ejemplo sencillo. Si necesita uno más detallad, descargue la
clave de firma del paquete NVIDIA antes de ejecutar la generación de la
imagen:</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; $CONFIG_DIR/rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>La adición de paquetes RPM adicionales mediante este método permite añadir
componentes de terceros compatibles o paquetes proporcionados (y mantenidos)
por el usuario. Este mecanismo no debe utilizarse para añadir paquetes que
normalmente no serían compatibles con SUSE Linux Micro. Si se utiliza este
mecanismo para añadir componentes de los repositorios de openSUSE (que no
son compatibles), incluidos los de versiones más recientes o paquetes de
servicio, es posible que se obtenga una configuración no compatible,
especialmente si al resolver las dependencias se sustituyen partes
fundamentales del sistema operativo, aunque el sistema resultante parezca
funcionar según lo esperado. Si no está seguro, póngase en contacto con su
representante de SUSE para que le ayude a determinar la compatibilidad de la
configuración que desea.</para>
</warning>
<note>
<para>Encontrará una guía más completa con ejemplos adicionales en la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/installing-packages.md">guía
original sobre instalación de paquetes</link>.</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Configuración del clúster de Kubernetes y las cargas de trabajo de los
usuarios</title>
<para>Otra característica de EIB es la posibilidad de utilizarlo para automatizar
el despliegue de clústeres de Kubernetes de alta disponibilidad, tanto de un
solo nodo como de varios nodos, que se "arrancan en el lugar" (es decir, que
no requieren infraestructura de gestión centralizada para coordinarse). Este
enfoque está pensado principalmente para despliegues en entornos aislados o
con restricciones de red, pero también sirve como una forma de arrancar
rápidamente clústeres independientes, incluso si se dispone de acceso
completo y sin restricciones a la red.</para>
<para>Este método no solo permite el despliegue del sistema operativo
personalizado, sino también la posibilidad de especificar la configuración
de Kubernetes, cualquier componente adicional en capas a través de charts de
Helm y cualquier carga de trabajo del usuario a través de los manifiestos de
Kubernetes proporcionados. Sin embargo, el principio de diseño detrás del
uso de este método es que asumimos por defecto que el usuario desea usar un
entorno aislado y, por lo tanto, cualquier elemento especificado en la
definición de la imagen se incorporará a la imagen y a las cargas de trabajo
proporcionadas por el usuario. EIB se asegurará de que cualquier imagen
descubierta que sea requerida por las definiciones proporcionadas se copie
localmente y que el registro de imágenes integrado las proporcione al
sistema desplegado resultante.</para>
<para>En el siguiente ejemplo, vamos a utilizar nuestra definición de imagen
existente y especificaremos una configuración de Kubernetes (no se muestra
una lista de los sistemas y sus funciones, por lo que entenderemos por
defecto que se trata de un solo nodo). Se indicará a EIB que aprovisione un
clúster RKE2 Kubernetes de un solo nodo. Para mostrar la automatización
tanto del despliegue de las cargas de trabajo proporcionadas por el usuario
(a través del manifiesto) como de los componentes en capas (a través de
Helm), vamos a instalar KubeVirt a través del chart de Helm de SUSE Edge,
así como NGINX a través de un manifiesto de Kubernetes. La configuración
adicional que debemos añadir a la definición de imagen existente es la
siguiente:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.32.4+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 303.0.0+up0.5.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<para>El archivo de definición completo resultante debería tener ahora este
aspecto:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.32.4+k3s1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 303.0.0+up0.5.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<note>
<para>Encontrará más ejemplos como despliegues de varios nodos, redes
personalizadas y opciones/valores de chart de Helm en la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md">documentación
original</link>.</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>Configuración de la red</title>
<para>En el último ejemplo de esta guía rápida, vamos a configurar la red que se
activará cuando se aprovisione un sistema con la imagen generada por EIB. Es
importante comprender que, a menos que se proporcione una configuración de
red, el modelo predeterminado es que se utilizará DHCP en todas las
interfaces detectadas en el momento del arranque. Sin embargo, no siempre se
trata de la configuración deseable, especialmente si DHCP no está disponible
y es necesario proporcionar configuraciones estáticas, o si es necesario
configurar estructuras de red más complejas (por ejemplo, enlaces, LACP y
VLAN) o si hay que anular ciertos parámetros (por ejemplo, nombres de host,
servidores DNS y rutas).</para>
<para>EIB ofrece la posibilidad de proporcionar configuraciones por nodo (donde el
sistema en cuestión se identifica de forma única por su dirección MAC) o de
sustituir la existente y proporcionar una configuración idéntica a cada
equipo, lo que resulta más útil cuando se desconocen las direcciones MAC del
sistema. EIB utiliza una herramienta adicional llamada Network Manager
Configurator, o <literal>nmc</literal> para abreviar, que es una herramienta
creada por el equipo de SUSE Edge para permitir la aplicación de
configuraciones de red personalizadas basadas en el esquema de red
declarativo <link xl:href="https://nmstate.io/">nmstate.io</link>, y que en
el momento del arranque identificará el nodo en el que se está iniciando y
aplicará la configuración de red deseada antes de que se inicien los
servicios.</para>
<para>Ahora aplicaremos una configuración de red estática para un sistema con una
única interfaz describiendo el estado de red deseado en un archivo
específico del nodo (basado en el nombre de host deseado) en el directorio
<literal>network</literal> requerido:</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>El ejemplo anterior está configurado para la subred predeterminada
<literal>192.168.122.0/24</literal>, suponiendo que la prueba se ejecuta en
una máquina virtual. Adáptelo a su entorno, sin olvidar la dirección
MAC. Dado que se puede utilizar la misma imagen para aprovisionar varios
nodos, la configuración de red realizada por EIB (a través de
<literal>nmc</literal>) depende de que pueda identificar de forma única el
nodo por su dirección MAC y, por lo tanto, durante el arranque,
<literal>nmc</literal> aplicará la configuración de red correcta a cada
equipo. Esto significa que deberá conocer las direcciones MAC de los
sistemas en los que desea realizar la instalación. Alternativamente, el
comportamiento predeterminado es confiar en DHCP, pero puede utilizar el
gancho <literal>configure-network.sh</literal> para aplicar una
configuración común a todos los nodos. Consulte la guía sobre conexión de
redes (<xref linkend="components-nmc"/>) para obtener más detalles.</para>
</warning>
<para>La estructura de archivo resultante tendrá este aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>La configuración de red que acabamos de crear se analizará y los archivos de
conexión de NetworkManager necesarios se generarán automáticamente y se
insertarán en la nueva imagen de instalación que creará EIB. Estos archivos
se aplicarán durante el aprovisionamiento del host, lo que dará como
resultado una configuración de red completa.</para>
<note>
<para>Consulte la sección sobre el componente de red de Edge (<xref
linkend="components-nmc"/>) para obtener una explicación más completa de la
configuración anterior y ejemplos de esta función.</para>
</note>
</section>
</section>
<section xml:id="eib-how-to-build-image">
<title>Creación de la imagen</title>
<para>Ahora que tenemos una imagen base y una definición de imagen para que EIB la
utilice, vamos a crear la imagen. Para ello, solo tenemos que usar
<literal>Podman</literal> para llamar al contenedor EIB con el comando
"build", especificando el archivo de definición:</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file iso-definition.yaml</screen>
<para>El resultado del comando debe tener este aspecto:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Downloading file: dl-manifest-1.yaml 100% (498/498 B, 9.5 MB/s)
Pulling selected Helm charts... 100% (1/1, 43 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (3/3, 10 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (657/657 MB, 48 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (368/368 MB, 48 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (35/35 MB, 50 MB/s)
Downloading file: sha256sum-amd64.txt 100% (4.3/4.3 kB, 6.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>La imagen ISO creada se guarda en
<literal>$CONFIG_DIR/eib-image.iso</literal>:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>Para cada imagen se crea una carpeta con marca de tiempo en
<literal>$CONFIG_DIR/_build/</literal> que incluye los registros del proceso
de creación, los artefactos utilizados durante la creación y los directorios
<literal>combustion</literal> y <literal>artefacts</literal>, que contienen
todos los guiones y artefactos que se añaden a la imagen lista para
arrancar.</para>
<para>El contenido de este directorio tendrá este aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:&lt;version&gt;-registry.tar.zst
│   │   │   ├── rancher_kubectl:&lt;version&gt;-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.6_virt-operator:&lt;version&gt;-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   ├── nvidia-container-toolkit-&lt;version&gt;.rpm
│   │   │       │   ├── nvidia-container-toolkit-base-&lt;version&gt;.rpm
│   │   │       │   ├── libnvidia-container1-&lt;version&gt;.rpm
│   │   │       │   └── libnvidia-container-tools-&lt;version&gt;.rpm
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt
│   │       └── kubevirt-0.4.0.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>Si la compilación falla, <literal>eib-build.log</literal> es el primer
registro que contiene información. Desde allí, se le dirigirá al componente
que falló para su depuración.</para>
<para>En este punto, debe tener una imagen lista para usar que:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Desplegará SUSE Linux Micro 6.1</para>
</listitem>
<listitem>
<para>Configurará la contraseña raíz</para>
</listitem>
<listitem>
<para>Instalará el paquete <literal>nvidia-container-toolkit</literal></para>
</listitem>
<listitem>
<para>Configurará un registro de contenedor integrado para proporcionar contenido
localmente</para>
</listitem>
<listitem>
<para>Instalará RKE2 de un solo nodo</para>
</listitem>
<listitem>
<para>Configurará la red estática</para>
</listitem>
<listitem>
<para>Instalará KubeVirt</para>
</listitem>
<listitem>
<para>Desplegará un manifiesto proporcionado por el usuario</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>Depuración del proceso de creación de imágenes</title>
<para>Si el proceso de creación de imágenes falla, consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/debugging.md">guía
original sobre depuración</link>.</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>Prueba de la imagen recién creada</title>
<para>Para obtener instrucciones sobre cómo probar la imagen lista para arrancar
recién creada, consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/testing-guide.md">guía
original sobre pruebas de imágenes</link>.</para>
</section>
</chapter>
<chapter xml:id="quickstart-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Multi-Linux Manager está incluido en SUSE Edge y proporciona
automatización y control con el fin de mantener el sistema operativo
subyacente SUSE Linux Micro constantemente actualizado en todos los nodos de
su despliegue periférico.</para>
<para>Esta guía de inicio rápido tiene como objetivo familiarizarle con SUSE
Multi-Linux Manager lo antes posible, con el fin de proporcionar
actualizaciones del sistema operativo a sus nodos periféricos. No aborda
temas como el dimensionamiento del almacenamiento, la creación y gestión de
canales de software adicionales con fines de preparación, ni la gestión de
usuarios, grupos de sistemas y organizaciones para despliegues de mayor
envergadura. Para su uso en producción, le recomendamos encarecidamente que
se familiarice con la completa <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">documentación
de SUSE Multi-Linux Manager</link>.</para>
<para>Para preparar SUSE Edge a fin de que SUSE Multi-Linux Manager se use de
forma eficaz, es necesario seguir los siguientes pasos:</para>
<itemizedlist>
<listitem>
<para>Desplegar y configurar SUSE Multi-Linux Manager Server</para>
</listitem>
<listitem>
<para>Sincronizar los repositorios de paquetes de SUSE Linux Micro</para>
</listitem>
<listitem>
<para>Crear grupos de sistemas</para>
</listitem>
<listitem>
<para>Crear claves de activación</para>
</listitem>
<listitem>
<para>Usar Edge Image Builder para preparar el medio de instalación para el
registro de SUSE Multi-Linux Manager</para>
</listitem>
</itemizedlist>
<section xml:id="id-deploy-suse-multi-linux-manager-server">
<title>Despliegue de SUSE Multi-Linux Manager Server</title>
<para>Si ya tiene una instancia de la última versión de SUSE Multi-Linux Manager
5.0 en ejecución, puede omitir este paso.</para>
<para>Puede ejecutar SUSE Multi-Linux Manager Server en un servidor físico
dedicado, como máquina virtual en su propio hardware, o en la nube. Se
proporcionan imágenes de máquinas virtuales preconfiguradas para SUSE
Multi-Linux Server para las nubes públicas compatibles.</para>
<para>En esta guía de inicio rápido se utiliza la imagen "qcow2"
<literal>SUSE-Manager-Server.x86_64-5.0.2-Qcow-2024.12.qcow2</literal> para
AMD64/Intel 64. La puede encontrar en <link
xl:href="https://www.suse.com/download/suse-manager/">https://www.suse.com/download/suse-manager/</link>
o en el Centro de servicios al cliente de SUSE. Esta imagen funciona como
una máquina virtual en hipervisores como KVM. Compruebe siempre que dispone
de la versión más reciente de la imagen y utilícela para las nuevas
instalaciones.</para>
<para>También puede instalar SUSE Multi-Linux Manager Server en cualquier otra
arquitectura de hardware compatible. En tal caso, elija la imagen que
coincida con su arquitectura de hardware.</para>
<para>Una vez que haya descargado la imagen, cree una máquina virtual que cumpla,
como mínimo, con las siguientes especificaciones de hardware:</para>
<itemizedlist>
<listitem>
<para>16 GB de RAM</para>
</listitem>
<listitem>
<para>4 núcleos físicos o virtuales</para>
</listitem>
<listitem>
<para>Un dispositivo de bloque adicional con una capacidad mínima de 100 GB</para>
</listitem>
</itemizedlist>
<para>Con la imagen qcow2, no es necesario instalar el sistema operativo. Puede
conectar directamente la imagen como su partición raíz.</para>
<para>Debe configurar la red para que sus nodos periféricos puedan acceder
posteriormente a SUSE Multi-Linux Manager Server con un nombre de host que
contenga el nombre de dominio completo.</para>
<para>Cuando inicie SUSE Multi-Linux Manager por primera vez, deberá realizar
algunas configuraciones iniciales:</para>
<itemizedlist>
<listitem>
<para>Seleccionar la disposición del teclado</para>
</listitem>
<listitem>
<para>Aceptar el acuerdo de licencia</para>
</listitem>
<listitem>
<para>Seleccionar la zona horaria</para>
</listitem>
<listitem>
<para>Introducir la contraseña raíz del sistema operativo</para>
</listitem>
</itemizedlist>
<para>Los pasos siguientes se deben realizar con permisos de usuario root:</para>
<para>Para el siguiente paso, necesitará dos códigos de registro, que encontrará
en el Centro de servicios al cliente de SUSE:</para>
<itemizedlist>
<listitem>
<para>Su código de registro para SLE Micro 5.5</para>
</listitem>
<listitem>
<para>Su código de registro para la extensión de SUSE Multi-Linux Manager</para>
</listitem>
</itemizedlist>
<para>Registre SUSE Linux Micro:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -r &lt;REGCODE&gt; -e &lt;your_email&gt;</screen>
<para>Registre SUSE Multi-Linux Manager:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -p SUSE-Manager-Server/5.0/x86_64 -r &lt;REGCODE&gt;</screen>
<para>La cadena del producto depende de la arquitectura de su hardware. Por
ejemplo, si utiliza SUSE Multi-Linux Manager en un sistema ARM de 64 bits,
la cadena es "SUSE-Manager-Server/5.0/aarch64".</para>
<para>Rearranque.</para>
<para>Actualice el sistema:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update</screen>
<para>A menos que no haya habido cambios, reinicie el sistema para aplicar las
actualizaciones.</para>
<para>SUSE Multi-Linux Manager se proporciona a través de un contenedor gestionado
por Podman. El comando <literal>mgradm</literal> se encarga de la
instalación y la configuración.</para>
<warning>
<para>Es muy importante que el nombre de host configurado en su instancia de SUSE
Multi-Linux Manager Server tenga un nombre de dominio completo que los nodos
periféricos que desea administrar puedan resolver correctamente en su red.</para>
</warning>
<para>Antes de instalar y configurar el contenedor de SUSE Multi-Linux Manager
Server, debe preparar el dispositivo de bloque adicional que ha añadido
previamente. Para ello, debe conocer el nombre que la máquina virtual ha
asignado al dispositivo. Por ejemplo, si el dispositivo de bloque es
<literal>/dev/vdb</literal>, puede configurarlo para que se utilice con SUSE
Multi-Linux Manager mediante el siguiente comando:</para>
<screen language="shell" linenumbering="unnumbered">mgr-storage-server /dev/vdb</screen>
<para>Despliegue SUSE Multi-Linux Manager:</para>
<screen language="shell" linenumbering="unnumbered">mgradm install podman &lt;FQDN&gt;</screen>
<para>Proporcione la contraseña para el certificado de CA. Esta contraseña debe
ser diferente de sus contraseñas de inicio de sesión. Por lo general, no es
necesario introducirla más adelante, pero debe anotarla.</para>
<para>Proporcione la contraseña para el usuario "admin". Se trata del usuario
inicial para iniciar sesión en SUSE Multi-Linux Manager. Más adelante podrá
crear usuarios adicionales con derechos completos o restringidos.</para>
</section>
<section xml:id="id-configure-suse-multi-linux-manager">
<title>Configuración de SUSE Multi-Linux Manager</title>
<para>Una vez finalizado el despliegue, puede iniciar sesión en la interfaz de
usuario web de SUSE Multi-Linux Manager utilizando el nombre de host que
proporcionó anteriormente. El usuario inicial es "admin". Use la contraseña
que proporcionó en el paso anterior.</para>
<para>Para el siguiente paso, necesitará las credenciales de su organización, que
encontrará en la segunda subpestaña de la pestaña "Usuarios" de su
organización en el Centro de servicios al cliente de SUSE. Con esas
credenciales, SUSE Multi-Linux Manager puede sincronizar todos los productos
a los que está suscrito.</para>
<para>Seleccione <menuchoice><guimenu>Admin</guimenu> <guimenu>Setup
Wizard</guimenu></menuchoice> (Admin > Asistente de configuración).</para>
<para>En la pestaña <literal>Organization Credentials</literal> (Credenciales de
la organización) cree nuevas credenciales con sus datos de
<literal>Username</literal> (Nombre de usuario) y
<literal>Password</literal> (Contraseña) que encontró en el Centro de
servicios al cliente de SUSE.</para>
<para>Diríjase a la pestaña siguiente, <literal>SUSE Products</literal> (Productos
SUSE). Debe esperar hasta que finalice la primera sincronización de datos
con el Centro de servicios al cliente de SUSE.</para>
<para>Cuando la lista se haya rellenado, utilice el filtro para mostrar solo
"Micro 6". Marque la casilla de SUSE Linux Micro 6.1 para la arquitectura de
hardware en la que se ejecutarán sus nodos periféricos
(<literal>x86_64</literal> o <literal>aarch64</literal>).</para>
<para>Haga clic en <literal>Add Products</literal> (Añadir productos). Esto
añadirá el repositorio de paquetes principal ("canal") para SUSE Linux Micro
y añadirá automáticamente el canal para las herramientas cliente de SUSE
Manager como un subcanal.</para>
<para>Dependiendo de su conexión a Internet, la primera sincronización tardará un
poco. Ya puede empezar con los siguientes pasos:</para>
<para>En <literal>Systems &gt; System Groups</literal> (Sistemas > Grupos de
sistemas), cree al menos un grupo al que sus sistemas se unirán
automáticamente cuando se incorporen. Los grupos son una forma importante de
categorizar los sistemas, ya que permiten aplicar configuraciones o acciones
a un conjunto completo de sistemas a la vez. Conceptualmente, son similares
a las etiquetas de Kubernetes.</para>
<para>Haga clic en <literal>+ Create Group</literal> (Crear grupo).</para>
<para>Proporcione un nombre corto, por ejemplo, "Nodos periféricos", y una
descripción detallada.</para>
<para>En <literal>Systems &gt; Activation Keys</literal> (Sistemas > Claves de
activación), cree al menos una clave de activación. Las claves de activación
pueden considerarse como un perfil de configuración que se aplica
automáticamente a los sistemas cuando se incorporan a SUSE Multi-Linux
Manager. Si desea que determinados nodos periféricos se añadan a diferentes
grupos o usen una configuración diferente, puede crear claves de activación
independientes para ellos y utilizarlas posteriormente en Edge Image Builder
para crear medios de instalación personalizados.</para>
<para>Un caso de uso avanzado típico de las claves de activación sería asignar los
clústeres de prueba a los canales de software con las últimas
actualizaciones y los clústeres de producción a los canales de software que
solo reciben esas últimas actualizaciones una vez que se han probado en el
clúster de prueba.</para>
<para>Haga clic en <literal>+ Create Key</literal> (Crear clave).</para>
<para>Elija una descripción corta, por ejemplo, "Nodos periféricos". Proporcione
un nombre único que identifique la clave, por ejemplo, "edge-x86_64" para
sus nodos periféricos con arquitectura de hardware AMD64/Intel 64. Se añade
automáticamente un prefijo numérico a la clave. Para la organización
predeterminada, el número es siempre "1". Si crea organizaciones adicionales
en SUSE Multi-Linux Manager y crea claves para ellas, ese número puede ser
diferente.</para>
<para>Si no ha creado ningún canal de software duplicado, puede conservar la
configuración del canal base como "SUSE Manager Default" (Predeterminada de
SUSE Manager). De este modo, se asignará automáticamente el repositorio de
actualizaciones de SUSE adecuado para sus nodos periféricos.</para>
<para>Como "Child Channel" (Canal secundario), seleccione el control deslizante
"Include recommended" (Incluir recomendados) para la arquitectura de
hardware para la que se utiliza su clave de activación. Esto añadirá el
canal "SUSE-Manager-Tools-For-SL-Micro-6.1".</para>
<para>En la pestaña "Groups" (Grupos), añada el grupo que ha creado
anteriormente. Todos los nodos que se incorporen utilizando esta clave de
activación se añadirán automáticamente a ese grupo.</para>
</section>
<section xml:id="id-create-a-customized-installation-image-with-edge-image-builder">
<title>Creación de una imagen de instalación personalizada con Edge Image Builder</title>
<para>Para usar Edge Image Builder, solo necesita un entorno en el que pueda
iniciar un contenedor basado en Linux con Podman.</para>
<para>Para una configuración mínima del laboratorio, es posible usar la misma
máquina virtual en la que se ejecuta SUSE Multi-Linux Manager
Server. Asegúrese de que dispone de suficiente espacio en disco en la
máquina virtual. Esta configuración no es recomendable en entornos de
producción. Consulte la <xref linkend="id-prerequisites-2"/> para conocer
los sistemas operativos host con los que hemos probado Edge Image Builder.</para>
<para>Inicie sesión en su host de SUSE Multi-Linux Manager Server como usuario
root.</para>
<para>Extraiga el contenedor de Edge Image Builder:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.3/edge-image-builder:1.2.1</screen>
<para>Cree el directorio <literal>/opt/eib</literal> y un subdirectorio
<literal>base-images</literal>:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p /opt/eib/base-images</screen>
<para>En esta guía de inicio rápido se usa la versión "autoinstalable" de la
imagen de SUSE Linux Micro. Esa imagen se puede grabar posteriormente en una
unidad USB física y utilizarse para la instalación en servidores físicos. Si
su servidor tiene la opción de conexión remota de imágenes ISO de
instalación a través de un BMC (Baseboard Management Controller), también
puede utilizar ese método. Por último, esa imagen también se puede utilizar
con la mayoría de las herramientas de virtualización.</para>
<para>Si desea precargar la imagen directamente en un nodo físico o iniciarla
directamente desde una máquina virtual, también puede utilizar el tipo de
imagen "raw".</para>
<para>Encontrará esas imágenes en el Centro de servicios al cliente de SUSE o en
<link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro/</link>.</para>
<para>Descargue o copie la imagen
<literal>SL-Micro.x86_64-6.1-Default-SelfInstall-GM.install.iso</literal> en
el directorio <literal>base-images</literal> y asígnele el nombre
"slemicro.iso".</para>
<para>La creación de imágenes AArch64 en un host de creación basado en ARM es una
tecnología en fase preliminar en SUSE Edge 3.3.1. Es muy probable que
funcione, pero aún no es compatible. Si desea probarlo, debe ejecutar Podman
en un equipo ARM de 64 bits y sustituir "x86_64" por "aarch64" en todos los
ejemplos y fragmentos de código.</para>
<para>En <literal>/opt/eib</literal>, cree un archivo llamado
<literal>iso-definition.yaml</literal>. Esta es tu definición de creación
para Edge Image Builder.</para>
<para>En este sencillo ejemplo se instala SL Micro 6.1, se establece una
contraseña raíz y el mapa de teclas, se inicia la interfaz gráfica Cockpit y
se registra el nodo en SUSE Multi-Linux Manager:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
  - username: root
    createHomeDir: true
    encryptedPassword: $6$aaBTHyqDRUMY1HAp$pmBY7.qLtoVlCGj32XR/Ogei4cngc3f4OX7fwBD/gw7HWyuNBOKYbBWnJ4pvrYwH2WUtJLKMbinVtBhMDHQIY0
  keymap: de
  systemd:
    enable:
      - cockpit.socket
  packages:
    noGPGCheck: true
  suma:
    host: ${fully qualified hostname of your SUSE Multi-Linux Manager Server}
    activationKey: 1-edge-x86_64</screen>
<para>Edge Image Builder también puede configurar la red, instalar automáticamente
Kubernetes en el nodo e, incluso, desplegar aplicaciones a través de charts
de Helm. Consulte el <xref linkend="quickstart-eib"/> para obtener ejemplos
más completos.</para>
<para>Para <literal>baseImage</literal>, especifique el nombre real de la ISO en
el directorio <literal>base-images</literal> que desea utilizar.</para>
<para>En este ejemplo, la contraseña raíz sería "root". Consulte la <xref
linkend="id-configuring-os-users"/> para crear hashes para la contraseña
segura que desea utilizar.</para>
<para>Defina el mapa de teclas de la disposición de teclado actual que desee que
tenga el sistema después de la instalación.</para>
<note>
<para>Utilizamos la opción <literal>noGPGCheck: true</literal> porque no vamos a
proporcionar una clave GPG para comprobar los paquetes RPM. En la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/installing-packages.md">guía
original sobre instalación de paquetes</link> encontrará instrucciones
completas para realizar una configuración más segura, que se recomienda en
entornos de producción.</para>
</note>
<para>Como se ha mencionado en varias ocasiones, el host de SUSE Multi-Linux
Manager requiere un nombre de host completo que pueda resolverse en la red
en la que se iniciarán los nodos periféricos.</para>
<para>El valor de <literal>activationKey</literal> debe coincidir con la clave que
ha creado en SUSE Multi-Linux Manager.</para>
<para>Para crear una imagen de instalación que registre automáticamente sus nodos
periféricos en SUSE Multi-Linux Manager después de la instalación, también
debe preparar dos artefactos:</para>
<itemizedlist>
<listitem>
<para>El paquete minion de Salt que instala el agente de gestión para SUSE
Multi-Linux Manager</para>
</listitem>
<listitem>
<para>El certificado de CA de su instancia de SUSE Multi-Linux Manager Server</para>
</listitem>
</itemizedlist>
<section xml:id="id-download-the-venv-salt-minion-package">
<title>Descarga del paquete venv-salt-minion</title>
<para>En <literal>/opt/eib</literal>, cree un subdirectorio
<literal>rpms</literal>.</para>
<para>Descargue el paquete <literal>venv-salt-minion</literal> desde su instancia
de SUSE Multi-Linux Manager Server en ese directorio. Puede obtenerlo a
través de la interfaz de usuario web buscando el paquete en
<literal>Software &gt; Channel List</literal> (Software > Lista de canales)
y descargándolo desde el canal SUSE-Manager-Tools​, o bien descargarlo desde
el repositorio de arranque de SUSE Multi-Linux Manager con una herramienta
como curl:</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/repositories/slmicro/6/1/bootstrap/x86_64/venv-salt-minion-3006.0-3.1.x86_64.rpm</screen>
<para>El nombre real del paquete puede variar si ya se ha lanzado una versión más
reciente. Si hay varios paquetes entre los que elegir, elija siempre el más
reciente.</para>
</section>
</section>
<section xml:id="id-download-the-suse-multi-linux-manager-ca-certificate">
<title>Descarga del certificado de CA de SUSE Multi-Linux Manager</title>
<para>En <literal>/opt/eib</literal>, cree un subdirectorio
<literal>certificates</literal>.</para>
<para>Descargue el certificado de CA de SUSE Multi-Linux Manager en ese
directorio:</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/RHN-ORG-TRUSTED-SSL-CERT</screen>
<warning>
<para>Debe cambiar el nombre del certificado a
<literal>RHN-ORG-TRUSTED-SSL-CERT.crt</literal>. Edge Image Builder se
asegurará de que el certificado se instale y se active en el nodo periférico
durante la instalación.</para>
</warning>
<para>Ahora puede ejecutar Edge Image Builder:</para>
<screen language="bash" linenumbering="unnumbered">cd /opt/eib
podman run --rm -it --privileged -v /opt/eib:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file iso-definition.yaml</screen>
<para>Si ha usado un nombre diferente para su archivo de definición YAML o desea
utilizar una versión diferente de Edge Image Builder, deberá adaptar el
comando en consecuencia.</para>
<para>Una vez finalizada la creación, encontrará la imagen ISO de instalación en
el directorio <literal>/opt/eib</literal> con el nombre
<literal>eib-image.iso</literal>.</para>
</section>
</chapter>
</part>
<part xml:id="id-components">
<title>Componentes</title>
<partintro>
<para>Lista de componentes de Edge</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para>Consulte la documentación de Rancher en <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11">https://ranchermanager.docs.rancher.com/v2.11</link>.</para>
<blockquote>
<para>Rancher es una potente plataforma de gestión de Kubernetes de código abierto
que optimiza el despliegue, las operaciones y la supervisión de clústeres de
Kubernetes en múltiples entornos. Tanto si gestiona clústeres en sus
instalaciones, en la nube o en el perímetro, Rancher le ofrece una
plataforma unificada y centralizada para todas sus necesidades de
Kubernetes.</para>
</blockquote>
<section xml:id="id-key-features-of-rancher">
<title>Funciones clave de Rancher</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Gestión de varios clústeres:</emphasis> la interfaz
intuitiva de Rancher permite gestionar clústeres de Kubernetes desde
cualquier lugar: nubes públicas, centros de datos privados y ubicaciones
periféricas.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Seguridad y conformidad:</emphasis> Rancher aplica
políticas de seguridad, control de acceso basado en roles (RBAC) y
estándares de cumplimiento en todo su entorno de Kubernetes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Operaciones simplificadas del clúster:</emphasis>
Rancher automatiza el aprovisionamiento, las actualizaciones y la resolución
de problemas de los clústeres, lo que simplifica las operaciones de
Kubernetes para equipos de todos los tamaños.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Catálogo de aplicaciones centralizado:</emphasis> el
catálogo de aplicaciones de Rancher ofrece una amplia gama de charts de Helm
y operadores de Kubernetes, lo que facilita el despliegue y gestión de
aplicaciones en contenedores.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Entrega continua:</emphasis> Rancher es compatible
con GitOps y los procesos de CI/CD, lo que permite automatizar y optimizar
los procesos de entrega de aplicaciones.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>Uso de Rancher en SUSE Edge</title>
<para>Rancher proporciona varias funcionalidades básicas a la pila de SUSE Edge:</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Gestión centralizada de Kubernetes</title>
<para>En despliegues periféricos típicos con numerosos clústeres distribuidos,
Rancher actúa como un plano de control central para gestionar estos
clústeres de Kubernetes. Ofrece una interfaz unificada para el
aprovisionamiento, la actualización, la supervisión y la resolución de
problemas, lo que simplifica las operaciones y garantiza la coherencia.</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>Despliegue simplificado de clústeres</title>
<para>Rancher optimiza la creación de clústeres de Kubernetes en el sistema
operativo ligero SUSE Linux Micro, lo que facilita el despliegue de
infraestructura periférica con sólidas capacidades de Kubernetes.</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>Despliegue y gestión de aplicaciones</title>
<para>El catálogo integrado de aplicaciones de Rancher puede simplificar el
despliegue y la gestión de aplicaciones en contenedores en clústeres de SUSE
Edge, lo que permite un despliegue fluido de las cargas de trabajo en el
perímetro.</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>Seguridad y aplicación de directivas</title>
<para>Rancher proporciona herramientas de gobernanza basadas en directivas,
control de acceso basado en roles (RBAC) e integración con proveedores de
autenticación externos. Esto ayuda a mantener la seguridad y el cumplimiento
normativo en los despliegues de SUSE Edge, aspectos fundamentales en
entornos distribuidos.</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>Prácticas recomendadas</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>Rancher incluye Fleet como un componente integrado para permitir la gestión
de configuraciones de clústeres y despliegues de aplicaciones con código
almacenado en git.</para>
</section>
<section xml:id="id-observability">
<title>Observabilidad</title>
<para>Rancher incluye herramientas integradas de supervisión y registro, como
Prometheus y Grafana, que proporcionan información detallada sobre el estado
y el rendimiento del clúster.</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Instalación con Edge Image Builder</title>
<para>SUSE Edge usa <xref linkend="components-eib"/> para personalizar las
imágenes del sistema operativo base SUSE Linux Micro. Siga las instrucciones
de la <xref linkend="rancher-install"/> para realizar una instalación en un
entorno aislado de Rancher sobre clústeres de Kubernetes aprovisionados por
EIB.</para>
</section>
<section xml:id="id-additional-resources-2">
<title>Recursos adicionales</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Documentación de Rancher</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Comunidad de Rancher</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Charts de Helm</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Operadores de Kubernetes</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Extensiones de panel de control de Rancher</title>
<para>Las extensiones permiten a los usuarios, desarrolladores, partners y
clientes ampliar y mejorar la interfaz de usuario de Rancher. SUSE Edge
proporciona extensiones para los paneles de control KubeVirt y Akri.</para>
<para>Consulte la <literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/rancher-extensions">documentación
de Rancher</link></literal> para obtener información general sobre las
extensiones de panel de control de Rancher.</para>
<section xml:id="id-installation">
<title>Instalación</title>
<para>Todos los componentes de SUSE Edge 3.3.1, incluidas las extensiones de panel
de control, se despliegan como artefactos OCI. Para instalar extensiones de
SUSE Edge, puede usar la interfaz del usuario del panel de control de
Rancher, Helm o Fleet:</para>
<section xml:id="id-installing-with-rancher-dashboard-ui">
<title>Instalación con la interfaz de usuario del panel de control de Rancher</title>
<orderedlist numeration="arabic">
<listitem>
<para>Haga clic en <emphasis role="strong">Extensions</emphasis> (Extensiones) en
la sección <emphasis role="strong">Configuration</emphasis> (Configuración)
de la barra lateral de navegación.</para>
</listitem>
<listitem>
<para>En la página Extensions (Extensiones), haga clic en el menú de tres puntos
de la parte superior derecha y seleccione <emphasis role="strong">Manage
Repositories</emphasis> (Gestionar repositorios).</para>
<para>Cada extensión se despliega mediante su propio artefacto OCI. Están
disponibles en el repositorio de charts de Helm de SUSE Edge.</para>
</listitem>
<listitem>
<para>En la página <emphasis role="strong">Repositories</emphasis> (Repositorios),
haga clic en <literal>Create</literal> (Crear).</para>
</listitem>
<listitem>
<para>En el formulario, especifique el nombre y la URL del repositorio y haga clic
en <literal>Create</literal> (Crear).</para>
<para>URL del repositorio de charts de Helm de SUSE Edge:
<literal>oci://registry.suse.com/edge/charts</literal></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-create-oci-repository.png" width="100%"/>
</imageobject>
<textobject><phrase>repositorio de oci para crear extensiones de panel de control</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Puede comprobar que el repositorio de extensiones se ha añadido a la lista y
se encuentra en estado <literal>Active</literal> (Activo).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-repositories-list.png" width="100%"/>
</imageobject>
<textobject><phrase>lista de repositorios de extensiones de panel de control</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Vuelva a <emphasis role="strong">Extensions</emphasis> (Extensiones) en la
sección <emphasis role="strong">Configuration</emphasis> (Configuración) de
la barra lateral de navegación.</para>
<para>En la pestaña <emphasis role="strong">Available</emphasis> (Disponibles),
consulte las extensiones disponibles para instalar.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-available-extensions.png" width="100%"/>
</imageobject>
<textobject><phrase>extensiones de panel de control disponibles</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>En la tarjeta de la extensión, haga clic en <literal>Install</literal> y
confirme el proceso.</para>
<para>Una vez instalada la extensión, la interfaz de usuario de Rancher solicita
que se vuelva a cargar la página, tal y como se describe en la página
<literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/rancher-extensions#installing-extensions">Installing
Extensions Rancher</link></literal> (Instalación de extensiones de Rancher)
de la documentación.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-helm">
<title>Instalación con Helm</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension --version 303.0.2+up1.3.2 --namespace cattle-ui-plugin-system

# Akri extension
helm install akri-dashboard-extension oci://registry.suse.com/edge/charts/akri-dashboard-extension --version 303.0.2+up1.3.1 --namespace cattle-ui-plugin-system</screen>
<note>
<para>Las extensiones se deben instalar en el espacio de nombres
<literal>cattle-ui-plugin-system</literal>.</para>
</note>
<note>
<para>Después de instalar una extensión, es necesario volver a cargar la interfaz
de usuario del panel de control de Rancher.</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Instalación con Fleet</title>
<para>Para instalar extensiones de panel de control con Fleet, es necesario
definir un recurso <literal>gitRepo</literal> que apunte a un repositorio
Git con archivos de configuración <literal>fleet.yaml</literal> de bundle
personalizados.</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension
  version: "303.0.2+up1.3.2"</screen>
<screen language="yaml" linenumbering="unnumbered"># Akri extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: akri-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/akri-dashboard-extension
  version: "303.0.2+up1.3.1"</screen>
<note>
<para>La propiedad <literal>releaseName</literal> es obligatoria y debe coincidir
con el nombre de la extensión para que esta se instale correctamente.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>Para obtener más información, consulte el <xref linkend="components-fleet"/>
y el repositorio <literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>.</para>
<para>Después de que se hayan instalado las extensiones, aparecen en la sección
<emphasis role="strong">Extensions</emphasis> (Extensiones) de la pestaña
<emphasis role="strong">Installled</emphasis> (Instaladas). Dado que no se
instalan a través de Apps/Marketplace, se marcan con la etiqueta
<literal>Third-Party</literal> (De terceros).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width="100%"/> </imageobject>
<textobject><phrase>extensiones de panel de control instaladas</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>Extensión de panel de control KubeVirt</title>
<para>La extensión KubeVirt proporciona funciones básicas de gestión de máquinas
virtuales para la interfaz de usuario del panel de control de Rancher. Sus
capacidades se describen en la <xref
linkend="kubevirt-dashboard-extension-usage"/>.</para>
</section>
<section xml:id="id-akri-dashboard-extension">
<title>Extensión de panel de control Akri</title>
<para>Akri es una interfaz de recursos de Kubernetes que permite exponer
fácilmente dispositivos periféricos heterogéneos (como cámaras IP y
dispositivos USB) como recursos en un clúster de Kubernetes, al tiempo que
admite la exposición de recursos de hardware integrados, como GPU y
FPGA. Akri detecta continuamente los nodos que tienen acceso a estos
dispositivos y programa las cargas de trabajo en función de ellos.</para>
<para>La extensión de panel de control Akri permite utilizar la interfaz de
usuario del panel de control de Rancher para gestionar y supervisar
dispositivos periféricos y ejecutar cargas de trabajo una vez que se han
detectado dichos dispositivos.</para>
<para>Las capacidades de la extensión se describen en más detalle en la <xref
linkend="akri-dashboard-extension-usage"/>.</para>
</section>
</chapter>
<chapter xml:id="components-rancher-turtles">
<title>Rancher Turtles</title>
<para>Consulte la documentación de Rancher Turtles en <link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/">https://documentation.suse.com/cloudnative/cluster-api/</link></para>
<blockquote>
<para>Rancher Turtles es un operador de Kubernetes que proporciona integración
entre Rancher Manager y Cluster API (CAPI) con el objetivo de ofrecer
compatibilidad total con CAPI a Rancher.</para>
</blockquote>
<section xml:id="id-key-features-of-rancher-turtles">
<title>Funciones principales de Rancher Turtles</title>
<itemizedlist>
<listitem>
<para>Importar automáticamente clústeres CAPI a Rancher instalando el agente
Rancher Cluster Agent en clústeres aprovisionados por CAPI.</para>
</listitem>
<listitem>
<para>Instalar y configurar las dependencias del controlador CAPI a través de
<link xl:href="https://cluster-api-operator.sigs.k8s.io/">CAPI
Operator</link>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-rancher-turtles-use-in-suse-edge">
<title>Uso de Rancher Turtles en SUSE Edge</title>
<para>La pila de SUSE Edge proporciona un chart wrapper de Helm que instala
Rancher Turtles con una configuración específica que habilita lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Los componentes de controlador esenciales de CAPI</para>
</listitem>
<listitem>
<para>Los componentes de proveedor de plano de control y arranque de RKE2</para>
</listitem>
<listitem>
<para>Los componentes de proveedor de infraestructura de Metal3 (<xref
linkend="components-metal3"/>)</para>
</listitem>
</itemizedlist>
<para>Solo se admiten los proveedores predeterminados instalados a través del
chart wrapper. Los proveedores alternativos de plano de control, arranque e
infraestructura no son compatibles actualmente como parte de la pila de SUSE
Edge.</para>
</section>
<section xml:id="id-installing-rancher-turtles">
<title>Instalación de Rancher Turtles</title>
<para>Rancher Turtles se puede instalar siguiendo las instrucciones de la guía de
inicio rápido de Metal3 (<xref linkend="quickstart-metal3"/>) o la
documentación del clúster de gestión (<xref
linkend="atip-management-cluster"/>).</para>
</section>
<section xml:id="id-additional-resources-3">
<title>Recursos adicionales</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Documentación de Rancher</link></para>
</listitem>
<listitem>
<para><link xl:href="https://cluster-api.sigs.k8s.io/">Manual de Cluster
API</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link xl:href="https://fleet.rancher.io">Fleet</link> es un motor de gestión
y despliegue de contenedores diseñado para ofrecer a los usuarios un mayor
control sobre el clúster local y una supervisión constante a través de
GitOps. Fleet no solo se centra en la escalabilidad, sino que también
proporciona a los usuarios un alto grado de control y visibilidad para
supervisar exactamente lo que hay instalado en el clúster.</para>
<para>Fleet puede gestionar despliegues desde Git de YAML sin procesar de
Kubernetes, charts de Helm, Kustomize o cualquier combinación de los tres
sistemas. Independientemente de la fuente, todos los recursos se convierten
dinámicamente en charts de Helm, y Helm se utiliza como motor para desplegar
todos los recursos en el clúster. Como resultado, los usuarios disfrutan de
un alto grado de control, coherencia y auditabilidad de sus clústeres.</para>
<para>Para obtener información sobre cómo funciona Fleet, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/architecture">Fleet
Architecture</link> (Arquitectura de Fleet).</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Instalación de Fleet con Helm</title>
<para>Fleet viene integrado en Rancher, pero también se puede <link
xl:href="https://fleet.rancher.io/installation">instalar</link> como una
aplicación independiente en cualquier clúster de Kubernetes utilizando Helm.</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>Uso de Fleet con Rancher</title>
<para>Rancher usa Fleet para desplegar aplicaciones en clústeres gestionados. La
entrega continua con Fleet introduce GitOps a gran escala, un sistema
diseñado para gestionar aplicaciones que se ejecutan en un gran número de
clústeres.</para>
<para>Fleet funciona de forma óptima como parte integrante de Rancher. En los
clústeres gestionados con Rancher se despliega automáticamente el agente de
Fleet como parte del proceso de instalación/importación, y el clúster queda
inmediatamente disponible para ser gestionado por Fleet.</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Acceso a Fleet en la interfaz de usuario de Rancher</title>
<para>Fleet viene preinstalado en Rancher y se gestiona con la opción <emphasis
role="strong">Continuous Delivery</emphasis> (Entrega continua) en la
interfaz de usuario de Rancher.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width="100%"/>
</imageobject>
<textobject><phrase>panel de control de fleet</phrase></textobject>
</mediaobject>
</informalfigure>
<para>La sección Continuous Delivery (Entrega continua) incluye estos elementos:</para>
<section xml:id="id-dashboard">
<title>Dashboard (Panel de control)</title>
<para>Se trata de una vista general de todos los repositorios GitOps en todos los
espacios de trabajo. Solo se muestran los espacios de trabajo con
repositorios.</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos (Repositorios Git)</title>
<para>Muestra una lista de repositorios GitOps del espacio de trabajo
seleccionado. Seleccione el espacio de trabajo activo en la lista
desplegable de la parte superior de la página.</para>
</section>
<section xml:id="id-clusters">
<title>Clusters (Clústeres)</title>
<para>Muestra una lista de clústeres gestionados. De forma predeterminada, todos
los clústeres gestionados por Rancher se añaden al espacio de trabajo
<literal>fleet-default</literal>. El espacio de trabajo
<literal>fleet-local</literal> incluye el clúster local (de gestión). Desde
aquí, es posible usar las acciones <literal>Pause</literal> (Pausar) o
<literal>Force update</literal> (Forzar la actualización) en los clústeres o
mover el clúster a otro espacio de trabajo. Editar el clúster permite
actualizar las etiquetas y anotaciones utilizadas para agrupar los
clústeres.</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups (Grupos de clústeres)</title>
<para>Esta sección permite agrupar de forma personalizada los clústeres dentro del
espacio de trabajo mediante selectores.</para>
</section>
<section xml:id="id-advanced">
<title>Advanced (Avanzado)</title>
<para>Esta sección permite gestionar los espacios de trabajo y otros recursos
relacionados con Fleet.</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Ejemplo de instalación de KubeVirt con Rancher y Fleet usando el panel de
control de Rancher</title>
<orderedlist numeration="arabic">
<listitem>
<para>Cree un repositorio Git que contenga el archivo
<literal>fleet.yaml</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/charts/kubevirt"
  version: "303.0.0+up0.5.0"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>En el panel de control de Rancher, diríjase a <emphasis role="strong">☰ &gt;
Continuous Delivery &gt; Git Repos</emphasis> (☰ > Entrega continua >
Repositorios Git) y haga clic en <literal>Add Repository</literal> (Añadir
repositorio).</para>
</listitem>
<listitem>
<para>El asistente para la creación de repositorios le guía a través del proceso
de creación del repositorio Git. Proporcione el nombre (<emphasis
role="strong">Name</emphasis>) y la URL del repositorio (<emphasis
role="strong">Repository URL</emphasis>) que haga referencia al repositorio
Git creado en el paso anterior, y seleccione la rama o revisión adecuada. En
el caso de un repositorio más complejo, especifique las rutas (<emphasis
role="strong">Paths</emphasis>) para utilizar varios directorios en un solo
repositorio.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width="100%"/>
</imageobject>
<textobject><phrase>creación de repositorio 1 con fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Haga clic en <literal>Next</literal> (Siguiente).</para>
</listitem>
<listitem>
<para>En el paso siguiente, puede definir dónde se desplegarán las cargas de
trabajo. Existen varias opciones básicas para seleccionar clústeres: puede
no seleccionar ningún clúster, seleccionarlos todos o elegir directamente un
clúster gestionado específico o un grupo de clústeres (si se ha
definido). La opción Advanced (Avanzado) permite editar directamente los
selectores a través del archivo YAML.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width="100%"/>
</imageobject>
<textobject><phrase>creación de repositorio 2 con fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Haga clic en <literal>Create</literal> (Crear). Se crea el repositorio. A
partir de ahora, las cargas de trabajo se instalan y se mantienen
sincronizadas en los clústeres que coinciden con la definición del
repositorio.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>Depuración y solución de problemas</title>
<para>La sección "Advanced" (Avanzado) ofrece una visión general de los recursos
de Fleet de nivel inferior. Un <link
xl:href="https://fleet.rancher.io/ref-bundle-stages">bundle</link> es un
recurso interno que se utiliza para la coordinación de recursos de
Git. Cuando se analiza un repositorio Git, se generan uno o varios Bundles.</para>
<para>Para encontrar Bundles relevantes para un repositorio específico, vaya a la
página de detalles del repositorio Git y haga clic en la pestaña
<literal>Bundles</literal>.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width="100%"/>
</imageobject>
<textobject><phrase>Bundles en repositorios fleet</phrase></textobject>
</mediaobject>
</informalfigure>
<para>El bundle se aplica a un recurso BundleDeployment que se crea para cada
clúster. Para ver los detalles de BundleDeployment, haga clic en el botón
<literal>Graph</literal> (Gráfico) situado en la parte superior derecha de
la página de detalles del repositorio Git. Se carga un gráfico de <emphasis
role="strong">Repo > Bundles > BundleDeployments</emphasis> (Repositorio >
Bundles > BundleDeployments). Haga clic en BundleDeployment en el gráfico
para ver sus detalles y haga clic en <literal>Id</literal> para ver el YAML
de BundleDeployment.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width="100%"/>
</imageobject>
<textobject><phrase>gráfico de repositorios de fleet</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Para consejos sobre la resolución de problemas de Fleet, consulte <link
xl:href="https://fleet.rancher.io/troubleshooting">este documento</link>.</para>
</section>
<section xml:id="id-fleet-examples">
<title>Ejemplos de Fleet</title>
<para>El equipo de Edge mantiene un <link
xl:href="https://github.com/suse-edge/fleet-examples">repositorio</link> con
ejemplos de instalación de proyectos de Edge con Fleet.</para>
<para>El proyecto de Fleet incluye un repositorio <link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>
que cubre todos los posibles casos de uso para la <link
xl:href="https://fleet.rancher.io/gitrepo-content">estructura de
repositorios de Git</link>.</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SUSE Linux Micro</title>
<para>Consulte la <link
xl:href="https://documentation.suse.com/sle-micro/6.1/">documentación
oficial de SUSE Linux Micro</link></para>
<blockquote>
<para>SUSE Linux Micro es un sistema operativo ligero y seguro para el
perímetro. Combina los componentes reforzados para empresas de SUSE Linux
Enterprise con las características que los desarrolladores desean en un
sistema operativo moderno e inmutable. Como resultado, se obtiene una
plataforma de infraestructura fiable con el mejor cumplimiento normativo de
su clase y que, además, es fácil de usar.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-suse-linux-micro">
<title>¿Cómo se usa SUSE Linux Micro en SUSE Edge?</title>
<para>Utilizamos SUSE Linux Micro como sistema operativo base para nuestra
plataforma. Proporciona una base segura, estable y mínima sobre la que
construir.</para>
<para>El uso de instantáneas del sistema de archivos (Btrfs) para permitir
reversiones sencillas en caso de que algo salga mal con una actualización es
único en SUSE Linux Micro. Esto permite actualizaciones remotas seguras para
toda la plataforma, incluso sin acceso físico en caso de que surjan
problemas.</para>
</section>
<section xml:id="id-best-practices-2">
<title>Prácticas recomendadas</title>
<section xml:id="id-installation-media">
<title>Medios de instalación</title>
<para>SUSE Edge usa Edge Image Builder (<xref linkend="components-eib"/>) para
preconfigurar la imagen de autoinstalación de SUSE Linux Micro.</para>
</section>
<section xml:id="id-local-administration">
<title>Administración local</title>
<para>SUSE Linux Micro incluye Cockpit para la gestión local del host a través de
una aplicación web.</para>
<para>Este servicio está desactivado de forma predeterminada, pero se puede
iniciar habilitando el servicio <literal>cockpit.socket</literal> de
systemd.</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>Problemas conocidos</title>
<itemizedlist>
<listitem>
<para>Por el momento, SUSE Linux Micro no dispone de ningún entorno de escritorio,
pero se está desarrollando una solución en contenedores.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>
es un proyecto de la CNCF que proporciona capacidades de gestión de
infraestructura bare metal para Kubernetes.</para>
<para>Metal<superscript>3</superscript> ofrece recursos nativos de Kubernetes para
gestionar el ciclo de vida de los servidores bare metal que admiten la
gestión a través de protocolos fuera de banda, como <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>También cuenta con un soporte maduro para <link
xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link> que
permite la gestión de recursos de infraestructura a través de múltiples
proveedores de infraestructura mediante API ampliamente adoptadas y
neutrales con respecto a los proveedores.</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>¿Cómo se usa Metal<superscript>3</superscript> en SUSE Edge?</title>
<para>Este método es útil para situaciones en las que el hardware de destino
admite la gestión fuera de banda y se desea un flujo de gestión de la
infraestructura totalmente automatizado.</para>
<para>Este método proporciona API declarativas que permiten la gestión del
inventario y el estado de los servidores bare metal, incluyendo la
inspección, la limpieza y el aprovisionamiento/desaprovisionamiento
automatizados.</para>
</section>
<section xml:id="id-known-issues-3">
<title>Problemas conocidos</title>
<itemizedlist>
<listitem>
<para>Actualmente, no se admite el <link
xl:href="https://github.com/metal3-io/ip-address-manager">controlador de
gestión de direcciones IP </link> original, ya que aún no es compatible con
las herramientas de configuración de red que hemos elegido.</para>
</listitem>
<listitem>
<para>Del mismo modo, tampoco se admiten los recursos IPAM ni los campos
networkData Metal3DataTemplate.</para>
</listitem>
<listitem>
<para>Actualmente, solo se admite el despliegue mediante redfish-virtualmedia.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para>Consulte el <link
xl:href="https://github.com/suse-edge/edge-image-builder">repositorio
oficial</link>.</para>
<para>Edge Image Builder (EIB) es una herramienta que optimiza la generación de
imágenes de disco personalizadas y listas para arrancar (CRB) para equipos
de arranque. Estas imágenes permiten el despliegue integral de toda la pila
de software de SUSE con una sola imagen.</para>
<para>Si bien EIB puede crear imágenes listas para arrancar para cualquier
escenario de aprovisionamiento, resulta especialmente útil en despliegues en
entornos aislados (air-gapped) con redes limitadas o completamente aisladas.</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>¿Cómo se usa Edge Image Builder en SUSE Edge?</title>
<para>SUSE Edge utiliza EIB para la configuración simplificada y rápida de
imágenes de SUSE Linux Micro personalizadas en distintas situaciones. Por
ejemplo, para el arranque de máquinas virtuales y bare metal con lo
siguiente:</para>
<itemizedlist>
<listitem>
<para>Despliegues de K3s/RKE2 de Kubernetes (de un solo nodo y de varios nodos) en
entornos totalmente aislados</para>
</listitem>
<listitem>
<para>Despliegues de chart de Helm y de manifiestos de Kubernetes en entornos
totalmente aislados</para>
</listitem>
<listitem>
<para>Registro en Rancher mediante la API de Elemental</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>Configuración de red personalizada (por ejemplo, IP estática, nombre de
host, VLAN, vinculación, etc.)</para>
</listitem>
<listitem>
<para>Configuraciones personalizadas del sistema operativo (por ejemplo, usuarios,
grupos, contraseñas, claves SSH, proxies, NTP, certificados SSL
personalizados, etc.)</para>
</listitem>
<listitem>
<para>Instalación en entornos aislados de paquetes RPM a nivel de host y locales
(incluida la resolución de dependencias)</para>
</listitem>
<listitem>
<para>Registro en SUSE Multi-Linux Manager para la gestión del sistema operativo</para>
</listitem>
<listitem>
<para>Imágenes de contenedor integradas</para>
</listitem>
<listitem>
<para>Argumentos de línea de comando del kernel</para>
</listitem>
<listitem>
<para>Unidades de systemd que se deben habilitar/inhabilitar en el arranque</para>
</listitem>
<listitem>
<para>Guiones y archivos personalizados para cualquier tarea manual</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>Inicio</title>
<para>Encontrará la documentación completa sobre el uso y las pruebas de Edge
Image Builder <link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.2/docs">aquí</link>.</para>
<para>Asimismo, en el <xref linkend="quickstart-eib"/> se explica un escenario
básico de despliegue.</para>
<para>Cuando se haya familiarizado con esta herramienta, encontrará más
información útil en la página de <link xl:href="../tips/eib.adoc">consejos y
trucos</link>.</para>
</section>
<section xml:id="id-known-issues-4">
<title>Problemas conocidos</title>
<itemizedlist>
<listitem>
<para>EIB crea plantillas de charts de Helm y analiza todas las imágenes de la
plantilla para aislar los charts de Helm. Si un chart de Helm no tiene todas
sus imágenes dentro de la plantilla y, en su lugar, carga las imágenes de
forma local, EIB no podrá aislar esas imágenes automáticamente. La solución
es añadir manualmente cualquier imagen no detectada a la sección
<literal>embeddedArtifactRegistry</literal> del archivo de definición.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Conexiones de red de Edge</title>
<para>En esta sección se describe el enfoque de la configuración de redes en la
solución SUSE Edge. Se explica cómo configurar NetworkManager en SUSE Linux
Micro de forma declarativa y cómo se integran las herramientas relacionadas.</para>
<section xml:id="id-overview-of-networkmanager">
<title>Descripción general de NetworkManager</title>
<para>NetworkManager es una herramienta que gestiona la conexión de red principal
y otras interfaces de conexión.</para>
<para>NetworkManager almacena las configuraciones de redes como archivos de
conexión que contienen el estado deseado en el directorio
<literal>/etc/NetworkManager/system-connections/</literal>.</para>
<para>Encontrará más información sobre NetworkManager en la <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-network-configuration/index.html">documentación
de SUSE Linux Micro</link>.</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>Descripción general de nmstate</title>
<para>nmstate es una biblioteca de amplia adopción (que incluye una herramienta de
interfaz de línea de comandos) que ofrece una API declarativa para
configuraciones de red a través de un esquema predefinido.</para>
<para>Encontrará información sobre nmstate en la <link
xl:href="https://nmstate.io/">documentación original</link>.</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>NetworkManager Configurator (nmc)</title>
<para>Para acceder a las opciones de personalización de redes disponibles en SUSE
Edge se usa una herramienta de interfaz de línea de comandos llamada
NetworkManager Configurator, o <emphasis>nmc</emphasis> para
abreviar. Aprovecha la funcionalidad que ofrece la biblioteca nmstate y es
capaz de configurar por si sola direcciones IP estáticas, servidores DNS,
VLAN, vinculaciones, puentes, etc. Esta herramienta permite generar
configuraciones de redes a partir de estados predefinidos deseados y
aplicarlas de forma automatizada en muchos nodos diferentes.</para>
<para>Encontrará información sobre NetworkManager Configurator (nmc) en el <link
xl:href="https://github.com/suse-edge/nm-configurator">repositorio
original</link>.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>¿Cómo se usa NetworkManager Configurator en SUSE Edge?</title>
<para>SUSE Edge usa <emphasis>nmc</emphasis> para personalizar las redes en los
distintos modelos de aprovisionamiento:</para>
<itemizedlist>
<listitem>
<para>Configuraciones de red personalizadas en casos de aprovisionamiento de red
dirigida (<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>Configuraciones estáticas declarativas en casos de aprovisionamiento basado
en imágenes (<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Configuración con Edge Image Builder</title>
<para>Edge Image Builder (EIB) es una herramienta que permite configurar varios
hosts con una sola imagen del sistema operativo. En esta sección, se muestra
cómo se puede utilizar un enfoque declarativo para describir los estados de
red deseados, cómo se convierten en las respectivas conexiones de
NetworkManager y cómo se aplican durante el proceso de aprovisionamiento.</para>
<section xml:id="id-prerequisites-3">
<title>Requisitos previos</title>
<para>Si está siguiendo esta guía, se entiende que ya dispone de lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Un host físico (o máquina virtual) AMD64/Intel 64 que ejecute SLES 15 SP6 u
openSUSE Leap 15.6</para>
</listitem>
<listitem>
<para>Un entorno de ejecución de contenedores disponible (por ejemplo, Podman)</para>
</listitem>
<listitem>
<para>Una copia de la imagen RAW de SUSE Linux Micro 6.1 que se encuentra <link
xl:href="https://www.suse.com/download/sle-micro/">aquí</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Obtención de la imagen del contenedor de Edge Image Builder</title>
<para>La imagen del contenedor de EIB está disponible públicamente y se puede
descargar desde el registro de SUSE Edge ejecutando:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.3/edge-image-builder:1.2.1</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>Creación del directorio de configuración de imágenes</title>
<para>Para empezar, se crea el directorio de configuración:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>Ahora, hay que asegurarse de que la copia de la imagen base descargada se
traslade al directorio de configuración:</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SL-Micro.x86_64-6.1-Base-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIB nunca modificará la imagen base introducida. Creará una nueva imagen con
sus modificaciones.</para>
</note>
</blockquote>
<para>El directorio de configuración en este momento debería tener el siguiente
aspecto:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>Creación del archivo de definición de imagen</title>
<para>El archivo de definición describe la mayoría de las opciones configurables
que admite Edge Image Builder.</para>
<para>Comencemos con un archivo de definición muy básico para nuestra imagen del
sistema operativo:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.2
image:
  arch: x86_64
  imageType: raw
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para>La sección <literal>image</literal> es obligatoria y especifica la imagen de
entrada, su arquitectura y su tipo, así como el nombre que se le dará a la
imagen de salida. La sección <literal>operatingSystem</literal> es opcional
y contiene la configuración para habilitar el inicio de sesión en los
sistemas aprovisionados con el nombre de usuario/contraseña
<literal>root/eib</literal>.</para>
<blockquote>
<note>
<para>Puede usar libremente su propia contraseña cifrada ejecutando
<literal>openssl passwd -6 &lt;contraseña&gt;</literal>.</para>
</note>
</blockquote>
<para>El directorio de configuración en este momento debería tener el siguiente
aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>Definición de las configuraciones de redes</title>
<para>Las configuraciones de redes deseadas no forman parte del archivo de
definición de imagen que se acaba de crear. Se introducen ahora en el
directorio <literal>network/</literal> especial. Vamos a crearlo:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>Como se mencionó anteriormente, la herramienta NetworkManager Configurator
(<emphasis>nmc</emphasis>) espera una entrada en forma de esquema
predefinido. Encontrará cómo configurar una amplia variedad de opciones de
red diferentes en la <link
xl:href="https://nmstate.io/examples.html">documentación de ejemplos de
NMState original</link>.</para>
<para>En esta guía se explica cómo configurar la red en tres nodos diferentes:</para>
<itemizedlist>
<listitem>
<para>Un nodo que usa dos interfaces Ethernet</para>
</listitem>
<listitem>
<para>Un nodo que usa vinculación de interfaces de red</para>
</listitem>
<listitem>
<para>Un nodo que usa un puente de redes</para>
</listitem>
</itemizedlist>
<warning>
<para>No se recomienda utilizar configuraciones de redes completamente diferentes
en entornos de producción, especialmente si se configuran clústeres de
Kubernetes. Las configuraciones de redes deben ser, por lo general,
homogéneas entre los nodos o, al menos, entre las funciones dentro de un
clúster determinado. Esta guía incluye varias opciones diferentes solo a
modo de referencia.</para>
</warning>
<blockquote>
<note>
<para>En el ejemplo siguiente se entiende que se usa una red
<literal>libvirt</literal> predeterminada con un rango de direcciones IP
<literal>192.168.122.1/24</literal>. Ajústelo según las necesidades
concretas de su entorno.</para>
</note>
</blockquote>
<para>Vamos a crear los estados deseados para el primer nodo, al que llamaremos
<literal>node1.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>En este ejemplo, definimos el estado deseado de dos interfaces Ethernet
(eth0 y eth3), sus direcciones IP solicitadas, el enrutamiento y la
resolución DNS.</para>
<warning>
<para>Debe asegurarse de que las direcciones MAC de todas las interfaces Ethernet
estén incluidas en la lista. Estas direcciones se utilizan durante el
proceso de aprovisionamiento como identificadores de los nodos y sirven para
determinar qué configuraciones deben aplicarse. De esta manera, podemos
configurar varios nodos utilizando una sola imagen ISO o RAW.</para>
</warning>
<para>El siguiente es el segundo nodo, al que llamaremos
<literal>node2.suse.com</literal> y que utilizará vinculación de interfaces
de red:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>En este ejemplo definimos un estado deseado de dos interfaces Ethernet (eth0
y eth1) que no permiten el direccionamiento IP, así como una vinculación con
una directiva round-robin y su dirección respectiva que se utilizará para
reenviar el tráfico de red.</para>
<para>Por último, crearemos el tercer y último archivo de estado deseado, que
usará un puente de red y al que llamaremos
<literal>node3.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>El directorio de configuración en este momento debería tener el siguiente
aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<blockquote>
<note>
<para>Los nombres de los archivos del directorio <literal>network/</literal> son
intencionados. Corresponden a los nombres de host que se establecerán
durante el proceso de aprovisionamiento.</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>Creación de la imagen del sistema operativo</title>
<para>Ahora que todas las configuraciones necesarias están listas, podemos crear
la imagen ejecutando:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.3/edge-image-builder:1.2.1 build --definition-file definition.yaml</screen>
<para>El resultado debe ser parecido a esto:</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>El fragmento anterior nos indica que el componente
<literal>Network</literal> se ha configurado correctamente y que podemos
continuar con el aprovisionamiento de nuestros nodos periféricos.</para>
<blockquote>
<note>
<para>Es posible inspeccionar un archivo de registro
(<literal>network-config.log</literal>) y los respectivos archivos de
conexión de NetworkManager en el directorio <literal>_build</literal>
resultante, dentro de un directorio con marca de hora de la ejecución de la
imagen.</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>Aprovisionamiento de los nodos periféricos</title>
<para>Vamos a copiar la imagen RAW resultante:</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>Notará que hemos copiado la imagen creada cuatro veces, pero solo
especificamos las configuraciones de redes para tres nodos. Esto se debe a
que también queremos mostrar lo que sucederá si aprovisionamos un nodo que
no coincide con ninguna de las configuraciones deseadas.</para>
<blockquote>
<note>
<para>En esta guía se usará virtualización para los ejemplos de aprovisionamiento
de nodos. Asegúrese de que estén habilitadas las extensiones necesarias en
el BIOS (consulte <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">este
documento</link> para obtener más detalles).</para>
</note>
</blockquote>
<para>Utilizaremos <literal>virt-install</literal> para crear máquinas virtuales
utilizando los discos sin procesar copiados. Cada máquina virtual utilizará
10 GB de RAM y 6 CPU virtuales.</para>
<section xml:id="id-provisioning-the-first-node">
<title>Aprovisionamiento del primer nodo</title>
<para>Vamos a crear la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>Es importante que creemos las interfaces de red con las mismas direcciones
MAC que las del estado deseado que hemos descrito anteriormente.</para>
</note>
</blockquote>
<para>Una vez completada la operación, veremos algo similar a lo siguiente:</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Micro 6.0 (x86_64) - Kernel 6.4.0-18-default (tty1).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>Ahora podemos iniciar sesión con el par de credenciales
<literal>root:eib</literal>. Si lo preferimos, también podemos conectarnos
por SSH al host, en lugar de a la <literal>consola virsh</literal> que se
presenta aquí.</para>
<para>Cuando haya iniciado sesión, confirmaremos que todos los ajustes estén
correctos.</para>
<para>Verifique que el nombre de host esté configurado correctamente:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>Verifique que la configuración de enrutamiento sea correcta:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>Verifique que la conexión a Internet esté disponible:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>Verifique que haya exactamente dos interfaces Ethernet configuradas y que
solo una de ellas esté activa:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>Notará que la segunda interfaz es <literal>eth1</literal> en lugar de la
predefinida en nuestro estado de red deseado, <literal>eth3</literal>. Esto
se debe a que NetworkManager Configurator (<emphasis>nmc</emphasis>) es
capaz de detectar que el sistema operativo ha asignado un nombre diferente a
la tarjeta de red con la dirección MAC <literal>34:8a:b1:4b:16:e2</literal>
y ajusta su configuración en consecuencia.</para>
<para>Verifique que, efectivamente, esto sea así inspeccionando la fase de
Combustion del aprovisionamiento:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>Ahora, aprovisionaremos el resto de los nodos, pero solo mostraremos las
diferencias en la configuración final. No dude en aplicar cualquiera de las
comprobaciones anteriores, o todas ellas, a todos los nodos que vaya a
aprovisionar.</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>Aprovisionamiento del segundo nodo</title>
<para>Vamos a crear la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>Cuando la máquina virtual esté activa y en funcionamiento, se puede
confirmar que este nodo está utilizando interfaces vinculadas:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>Confirme que el enrutamiento utiliza la vinculación de interfaces:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>Asegúrese de que los archivos de conexión estática se utilicen
correctamente:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>Aprovisionamiento del tercer nodo</title>
<para>Vamos a crear la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>Cuando la máquina virtual está activa y en funcionamiento, se puede
confirmar que este nodo está utilizando un puente de red:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>Confirme que el enrutamiento utiliza el puente:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>Asegúrese de que los archivos de conexión estática se utilicen
correctamente:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>Aprovisionamiento del cuarto nodo</title>
<para>Por último, se aprovisionará un nodo que no coincida con ninguna de las
configuraciones predefinidas por una dirección MAC. En estos casos,
utilizaremos DHCP de forma predeterminada para configurar las interfaces de
red.</para>
<para>Vamos a crear la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>Cuando la máquina virtual esté activa y en funcionamiento, podemos confirmar
que este nodo utiliza una dirección IP aleatoria para su interfaz de red:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>Verifique que nmc no ha podido aplicar configuraciones estáticas para este
nodo:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>Verifique que la interfaz Ethernet se haya configurado mediante DHCP:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="networking-unified">
<title>Configuraciones unificadas de nodos</title>
<para>Hay ocasiones en las que no es posible basarse en direcciones MAC
conocidas. En esos casos, podemos optar por la denominada
<emphasis>configuración unificada</emphasis>, que permite especificar los
ajustes en un archivo <literal>_all.yaml</literal> que luego se aplicará a
todos los nodos aprovisionados.</para>
<para>Crearemos y aprovisionaremos un nodo periférico con una estructura de
configuración diferente. Siga todos los pasos desde la <xref
linkend="image-config-dir-creation"/> hasta la <xref
linkend="default-network-definition"/>.</para>
<para>En este ejemplo, definimos un estado deseado de dos interfaces Ethernet
(eth0 y eth1): una utiliza DHCP y a la otra se le asigna una dirección IP
estática.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.3/edge-image-builder:1.2.1 build --definition-file definition.yaml</screen>
<para>Cuando la imagen se haya creado correctamente, crearemos una máquina virtual
con dicha imagen:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>El proceso de aprovisionamiento puede tardar unos minutos. Una vez
finalizado, inicie sesión en el sistema con las credenciales proporcionadas.</para>
<para>Verifique que la configuración de enrutamiento sea correcta:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>Verifique que la conexión a Internet esté disponible:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Verifique que las interfaces Ethernet estén configuradas y activas:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>Configuraciones de red personalizadas</title>
<para>Ya hemos tratado la configuración de red predeterminada para Edge Image
Builder, que se basa en NetworkManager Configurator. Sin embargo, también
existe la opción de modificarla mediante un guion personalizado. Aunque esta
opción es muy flexible y tampoco depende de la dirección MAC, su limitación
radica en el hecho de que su uso resulta mucho menos práctico cuando se
arrancan varios nodos con una sola imagen.</para>
<blockquote>
<note>
<para>Se recomienda utilizar la configuración de red predeterminada mediante
archivos que describen los estados de red deseados en el directorio
<literal>/network</literal>. Utilice guiones personalizados únicamente
cuando ese comportamiento no sea aplicable a su caso de uso.</para>
</note>
</blockquote>
<para>Crearemos y aprovisionaremos un nodo periférico con una estructura de
configuración diferente. Siga todos los pasos desde la <xref
linkend="image-config-dir-creation"/> hasta la <xref
linkend="default-network-definition"/>.</para>
<para>En este ejemplo, crearemos un guion personalizado que aplica una
configuración estática para la interfaz <literal>eth0</literal> en todos los
nodos aprovisionados, además de eliminar e inhabilitar las conexiones con
cable que haya creado automáticamente NetworkManager. Esto resulta útil en
situaciones en las que se desea garantizar que todos los nodos del clúster
tengan una configuración de red idéntica, por lo que no es necesario
preocuparse por la dirección MAC de cada nodo antes de crear la imagen.</para>
<para>Para empezar, se almacena el archivo de conexión en el directorio
<literal>/custom/files</literal>:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>Ahora que ya hemos creado la configuración estática, también crearemos
nuestro guion de red personalizado:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>El binario de nmc seguirá estando incluido por defecto, por lo que también
se puede utilizar en el guion <literal>configure-network.sh</literal> si es
necesario.</para>
</note>
</blockquote>
<warning>
<para>El guion personalizado siempre debe proporcionarse en
<literal>/network/configure-network.sh</literal> en el directorio de
configuración. Si está presente, se ignorarán todos los demás archivos. NO
es posible configurar una red utilizando simultáneamente configuraciones
estáticas en formato YAML y un guion personalizado.</para>
</warning>
<para>El directorio de configuración en este momento debería tener el siguiente
aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.3/edge-image-builder:1.2.1 build --definition-file definition.yaml</screen>
<para>Cuando la imagen se haya creado correctamente, crearemos una máquina virtual
con dicha imagen:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>El proceso de aprovisionamiento puede tardar unos minutos. Una vez
finalizado, inicie sesión en el sistema con las credenciales proporcionadas.</para>
<para>Verifique que la configuración de enrutamiento sea correcta:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>Verifique que la conexión a Internet esté disponible:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>Verifique que se ha configurado de forma estática una interfaz Ethernet
utilizando nuestro archivo de conexión y que esta interfaz esté activa:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elemental es una pila de software que permite la gestión centralizada y
completa del sistema operativo nativo en la nube con Kubernetes. La pila
Elemental consta de varios componentes que residen en Rancher o en los nodos
periféricos. Los componentes principales son:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis>: el operador central
que reside en Rancher y gestiona las solicitudes de registro de los
clientes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis>: el cliente que se
ejecuta en los nodos periféricos y permite el registro a través de
<literal>elemental-operator</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis>: un agente que
reside en los nodos periféricos. Toma su configuración de
<literal>elemental-register</literal> y recibe un <literal>plan</literal>
para configurar el agente <literal>rancher-system-agent</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis>: cuando el nodo
periférico se ha registrado por completo, toma el control de
<literal>elemental-system-agent</literal> y espera nuevos
<literal>planes</literal> de Rancher Manager (por ejemplo, para la
instalación de Kubernetes).</para>
</listitem>
</itemizedlist>
<para>Consulte la <link
xl:href="https://elemental.docs.rancher.com/">documentación original de
Elemental</link> para obtener información completa sobre Elemental y su
relación con Rancher.</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>¿Cómo se usa Elemental en SUSE Edge?</title>
<para>Partes de Elemental se usan para gestionar dispositivos remotos en caso de
que no sea posible hacerlo con Metal<superscript>3</superscript> (por
ejemplo, si no hay BMC o si el dispositivo está tras una puerta de enlace
NAT). Esto permite a los operadores arrancar sus dispositivos en un
laboratorio antes de saber cuándo o dónde se enviarán. En concreto, se
aprovechan los componentes <literal>elemental-register</literal> y
<literal>elemental-system-agent</literal> para permitir la incorporación de
hosts de SUSE Linux Micro en Rancher para casos de uso de aprovisionamiento
de red "phone home". Si se usa Edge Image Builder (EIB) para crear imágenes
de despliegue, es posible realizar el registro automático a través de
Rancher vía Elemental especificando la configuración de registro en el
directorio de configuración de EIB.</para>
<note>
<para>En SUSE Edge 3.3.1 <emphasis role="strong">no</emphasis> se aprovechan las
funciones de gestión del sistema operativo de Elemental, por lo que no es
posible gestionar los parches del sistema operativo a través de Rancher. En
lugar de utilizar las herramientas de Elemental para crear imágenes de
despliegue, SUSE Edge usa Edge Image Builder, que hace uso de la
configuración de registro.</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>Prácticas recomendadas</title>
<section xml:id="id-installation-media-2">
<title>Medios de instalación</title>
<para>La forma recomendada por SUSE Edge para crear imágenes de despliegue que
puedan aprovechar Elemental para el registro en Rancher en el método de
aprovisionamiento de red "phone home" es seguir las instrucciones descritas
en la guía de inicio rápido de incorporación de hosts remotos con Elemental
(<xref linkend="quickstart-elemental"/>).</para>
</section>
<section xml:id="id-labels">
<title>Etiquetas</title>
<para>Elemental realiza un seguimiento de su inventario con la CRD
<literal>MachineInventory</literal> y ofrece una forma de seleccionar el
inventario (por ejemplo, para seleccionar equipos en los que desplegar
clústeres de Kubernetes) basada en etiquetas. Esto permite a los usuarios
predefinir la mayor parte (si no la totalidad) de sus necesidades de
infraestructura antes incluso de adquirir el hardware. Además, dado que los
nodos pueden añadir o eliminar etiquetas en sus objetos de inventario
respectivos (volviendo a ejecutar <literal>elemental-register</literal> con
el indicador adicional <literal>--label "FOO=BAR"</literal>), es posible
escribir guiones que detecten y comuniquen a Rancher dónde se ha iniciado un
nodo.</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>Problemas conocidos</title>
<itemizedlist>
<listitem>
<para>La interfaz de usuario de Elemental no puede actualmente crear medios de
instalación ni actualizar sistemas operativos que no sean "Elemental
Teal". Esto debería solucionarse en futuras versiones.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-akri">
<title>Akri</title>
<para>Akri es un proyecto experimental de la CNCF cuyo objetivo es detectar
dispositivos periféricos para presentarlos como recursos nativos de
Kubernetes. También permite programar un pod o un trabajo para cada
dispositivo detectado. Los dispositivos pueden ser locales o estar
conectados en red, y pueden utilizar una amplia variedad de protocolos.</para>
<para>La documentación original de Akri está en: <link
xl:href="https://docs.akri.sh">https://docs.akri.sh</link></para>
<section xml:id="id-how-does-suse-edge-use-akri">
<title>¿Cómo se usa Akri en SUSE Edge?</title>
<warning>
<para>Akri es actualmente una tecnología en fase preliminar en la pila de SUSE
Edge.</para>
</warning>
<para>Akri está disponible como parte de la pila de Edge siempre que sea necesario
detectar y programar cargas de trabajo en dispositivos periféricos.</para>
</section>
<section xml:id="id-installing-akri">
<title>Instalación de Akri</title>
<para>Akri está disponible como un chart de Helm dentro del repositorio de Helm de
Edge. La forma recomendada de configurar Akri es utilizando el chart de Helm
proporcionado para desplegar los diferentes componentes (agente,
controlador, gestores de descubrimiento) y, a continuación, utilizar su
mecanismo de despliegue preferido para desplegar las CRD de configuración de
Akri.</para>
</section>
<section xml:id="id-configuring-akri">
<title>Configuración de Akri</title>
<para>Akri se configura mediante un objeto
<literal>akri.sh/Configuration</literal>. En él, se recopila toda la
información sobre cómo detectar los dispositivos, así como indicaciones de
qué hacer cuando se detecta uno compatible.</para>
<para>A continuación se muestra un ejemplo de la configuración con todos los
campos explicados:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: akri.sh/v0
kind: Configuration
metadata:
  name: sample-configuration
spec:</screen>
<para>Esta parte describe la configuración del controlador de detección. Debe
especificar su nombre (los controladores disponibles como parte del chart de
Akri son <literal>udev</literal>, <literal>opcua</literal> y
<literal>onvif</literal>). <literal>discoveryDetails</literal> es específico
de cada controlador; consulte la documentación del controlador para saber
cómo configurarlo.</para>
<screen language="yaml" linenumbering="unnumbered">  discoveryHandler:
    name: debugEcho
    discoveryDetails: |+
      descriptions:
        - "foo"
        - "bar"</screen>
<para>Esta sección define la carga de trabajo que se desplegará para cada
dispositivo detectado. El ejemplo muestra una versión mínima de una
configuración de <literal>pod</literal> en
<literal>brokerPodSpec</literal>. Aquí se pueden utilizar todos los campos
habituales de la especificación de un pod. También muestra la sintaxis
específica de Akri para solicitar el dispositivo en la sección
<literal>resources</literal>.</para>
<para>Como alternativa, puede utilizar un trabajo en lugar de un pod, utilizando
la clave <literal>brokerJobSpec</literal> y proporcionándole la parte
específica de un trabajo.</para>
<screen language="yaml" linenumbering="unnumbered">  brokerSpec:
    brokerPodSpec:
      containers:
      - name: broker-container
        image: rancher/hello-world
        resources:
          requests:
            "{{PLACEHOLDER}}" : "1"
          limits:
            "{{PLACEHOLDER}}" : "1"</screen>
<para>Estas dos secciones muestran cómo configurar Akri para desplegar un servicio
por agente (<literal>instanceService</literal>) o apuntando a todos los
agentes (<literal>configurationService</literal>). Contienen todos los
elementos relacionados con un servicio habitual.</para>
<screen language="yaml" linenumbering="unnumbered">  instanceServiceSpec:
    type: ClusterIp
    ports:
    - name: http
      port: 80
      protocol: tcp
      targetPort: 80
  configurationServiceSpec:
    type: ClusterIp
    ports:
    - name: https
      port: 443
      protocol: tcp
      targetPort: 443</screen>
<para>El campo <literal>brokerProperties</literal> es un almacén de claves/valores
que se expondrán como variables de entorno adicionales a cualquier pod que
solicite un dispositivo detectado.</para>
<para>La capacidad es el número permitido de usuarios simultáneos de un
dispositivo detectado.</para>
<screen language="yaml" linenumbering="unnumbered">  brokerProperties:
    key: value
  capacity: 1</screen>
</section>
<section xml:id="id-writing-and-deploying-additional-discovery-handlers">
<title>Escritura y despliegue de controladores de detección adicionales</title>
<para>En caso de que el protocolo utilizado por su dispositivo no esté cubierto
por un controlador de detección existente, puede escribir su propio
protocolo utilizando la <link
xl:href="https://docs.akri.sh/development/handler-development">guía de
desarrollo de controladores</link>.</para>
</section>
<section xml:id="akri-dashboard-extension-usage">
<title>Extensión de panel de control Akri de Rancher</title>
<para>La extensión de panel de control Akri permite utilizar la interfaz de
usuario del panel de control de Rancher para gestionar y supervisar
dispositivos periféricos y ejecutar cargas de trabajo una vez que se han
detectado dichos dispositivos.</para>
<para>Consulte el <xref linkend="components-rancher-dashboard-extensions"/> para
obtener instrucciones sobre la instalación.</para>
<para>Una vez instalada la extensión, puede dirigirse a cualquier clúster
gestionado compatible con Akri utilizando el explorador de clústeres. En el
grupo de navegación <emphasis role="strong">Akri</emphasis>, puede ver las
secciones <emphasis role="strong">Configurations</emphasis>
(Configuraciones) e <emphasis role="strong">Instances</emphasis>
(Instancias).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configurations.png"
width="100%"/> </imageobject>
<textobject><phrase>configuraciones de extensión de akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>La lista de configuraciones proporciona información sobre
<literal>Configuration Discovery Handler</literal> (gestor de detección de
configuraciones) y el número de instancias. Al hacer clic en el nombre, se
abre una página con los detalles de la configuración.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-detail.png"
width="100%"/> </imageobject>
<textobject><phrase>detalle de configuración de la extensión de akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>También puede editar o crear una nueva <emphasis
role="strong">configuración</emphasis>. La extensión le permite seleccionar
el controlador de detección, configurar el pod o el trabajo del agente,
personalizar las configuraciones y los servicios de instancia, y establecer
la capacidad de configuración.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-edit.png"
width="100%"/> </imageobject>
<textobject><phrase>edición de configuración de extensión de akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Los dispositivos descubiertos se muestran en la lista <emphasis
role="strong">Instances</emphasis> (Instancias).</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instances-list.png"
width="100%"/> </imageobject>
<textobject><phrase>lista de instancias de extensión de akri</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Al hacer clic en un nombre de <emphasis role="strong">Instance</emphasis>
(Instancia), se abre una página de detalles donde se muestran las cargas de
trabajo y el servicio de la instancia.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instance-detail.png"
width="100%"/> </imageobject>
<textobject><phrase>detalle de instancia de extensión de akri</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link xl:href="https://k3s.io/">K3s</link> es una distribución de Kubernetes
certificada y de alta disponibilidad, diseñada para cargas de trabajo de
producción en ubicaciones remotas sin supervisión y con recursos limitados,
o dentro de dispositivos IoT.</para>
<para>Se presenta como un único archivo binario de pequeño tamaño, por lo que su
instalación y actualización es rápida y sencilla.</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>¿Cómo se usa K3s en SUSE Edge?</title>
<para>K3s se puede utilizar como la distribución de Kubernetes que respalda la
pila de SUSE Edge. Está diseñado para instalarse en un sistema operativo
SUSE Linux Micro.</para>
<para>El uso de K3s como distribución de Kubernetes de la pila de SUSE Edge solo
se recomienda cuando resulta imposible usar etcd como backend. Si es posible
usar etcd como backend, es mejor utilizar RKE2 (<xref
linkend="components-rke2"/>).</para>
</section>
<section xml:id="id-best-practices-4">
<title>Prácticas recomendadas</title>
<section xml:id="id-installation-2">
<title>Instalación</title>
<para>La forma recomendada de instalar K3s como parte de la pila de SUSE Edge es
utilizando Edge Image Builder (EIB). Consulte la documentación (<xref
linkend="components-eib"/>) para obtener más detalles sobre cómo configurar
EIB para desplegar K3s.</para>
<para>Admite automáticamente tanto la configuración de HA (alta disponibilidad)
como la configuración de Elemental.</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>Fleet para flujo de trabajo de GitOps</title>
<para>La pila de SUSE Edge utiliza Fleet como su herramienta GitOps
preferida. Para obtener más información sobre la instalación y el uso,
consulte la sección Fleet (<xref linkend="components-fleet"/>) de esta
documentación.</para>
</section>
<section xml:id="id-storage-management">
<title>Gestión del almacenamiento</title>
<para>K3s incluye almacenamiento de ruta local preconfigurado, lo cual resulta
útil para los clústeres de un solo nodo. En el caso de los clústeres de
varios nodos, se recomienda utilizar SUSE Storage (<xref
linkend="components-suse-storage"/>).</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>Equilibrio de carga y HA</title>
<para>Si ha instalado K3s utilizando EIB, esta parte ya se trata en la
documentación de EIB, en la sección sobre HA.</para>
<para>De lo contrario, deberá instalar y configurar MetalLB siguiendo las
instrucciones correspondientes de la documentación (<xref
linkend="guides-metallb-k3s"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para>Consulte la <link xl:href="https://docs.rke2.io/">documentación oficial de
RKE2</link>.</para>
<para>RKE2 es una distribución de Kubernetes totalmente compatible que se centra
en la seguridad y el cumplimiento normativo:</para>
<itemizedlist>
<listitem>
<para>Proporcionando valores predeterminados y opciones de configuración que
permiten a los clústeres superar la prueba CIS Kubernetes Benchmark v1.6 o
v1.23 con una intervención mínima por parte del operador</para>
</listitem>
<listitem>
<para>Habilitando la conformidad con FIPS 140-2</para>
</listitem>
<listitem>
<para>Escaneando regularmente los componentes en busca de CVE mediante <link
xl:href="https://trivy.dev">trivy</link> en el proceso de creación de RKE2</para>
</listitem>
</itemizedlist>
<para>RKE2 lanza los componentes del plano de control como pods estáticos,
gestionados por kubelet. El entorno de ejecución del contenedor integrado es
containerd.</para>
<para>Nota: RKE2 también se conoce como RKE Government cuando se emplea en otro
caso de uso y en el sector al que se dirige.</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2 frente a K3s</title>
<para>K3s es una distribución de Kubernetes totalmente compatible y ligera,
centrada en Edge, IoT y ARM, optimizada para que su uso sea sencillo y para
entornos con recursos limitados.</para>
<para>RKE2 combina lo mejor de la versión 1.x de RKE (en adelante, RKE1) y de K3s.</para>
<para>De K3s, hereda la facilidad de uso, la simplicidad de funcionamiento y el
modelo de despliegue.</para>
<para>De RKE1, hereda una estrecha alineación con la versión original de
Kubernetes. En algunos aspectos, K3s se ha desviado de la versión original
de Kubernetes con el fin de optimizar los despliegues periféricos; pero RKE1
y RKE2 pueden mantenerse estrechamente alineados con dicha versión original.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>¿Cómo se usa RK2 en SUSE Edge?</title>
<para>RKE2 es una pieza fundamental de la pila de SUSE Edge. Se sitúa sobre SUSE
Linux Micro (<xref linkend="components-slmicro"/>) y proporciona una
interfaz de Kubernetes estándar necesaria para desplegar cargas de trabajo
de Edge.</para>
</section>
<section xml:id="id-best-practices-5">
<title>Prácticas recomendadas</title>
<section xml:id="id-installation-3">
<title>Instalación</title>
<para>La forma recomendada de instalar RKE2 como parte de la pila de SUSE Edge es
utilizando Edge Image Builder (EIB). Consulte la documentación de EIB (<xref
linkend="components-eib"/>) para obtener más detalles sobre cómo hacerlo.</para>
<para>EIB es lo suficientemente flexible como para admitir cualquier parámetro
requerido por RKE2, como especificar la versión de RKE2, la configuración de
<link
xl:href="https://docs.rke2.io/reference/server_config">servidores</link> o
la configuración de los <link
xl:href="https://docs.rke2.io/reference/linux_agent_config">agentes</link>,
cubriendo todos los casos de uso de Edge.</para>
<para>También se usa e instala para otros casos de uso relacionados con
Metal<superscript>3</superscript>. En ellos, la versión de <link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">RK2
de proveedor de Cluster API</link> despliega automáticamente RKE2 en los
clústeres que se aprovisionan con Metal<superscript>3</superscript>
utilizando la pila de Edge.</para>
<para>En esos casos, la configuración de RKE2 debe aplicarse en las diferentes CRD
involucradas. Un ejemplo de cómo proporcionar una CNI diferente utilizando
la CRD <literal>RKE2ControlPlane</literal> sería el siguiente:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>Para obtener más información sobre los casos de uso de
Metal<superscript>3</superscript>, consulte el <xref
linkend="components-metal3"/>.</para>
</section>
<section xml:id="id-high-availability">
<title>High Availability</title>
<para>Para los despliegues de HA (High Availability, alta disponibilidad), EIB
despliega y configura automáticamente MetalLB (<xref
linkend="components-metallb"/>) y el operador Endpoint Copier Operator
(<xref linkend="components-eco"/>) para exponer externamente el punto final
de la API de RKE2.</para>
</section>
<section xml:id="id-networking">
<title>Redes</title>
<para>La pila de SUSE Edge admite <link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link> y <link
xl:href="https://docs.tigera.io/calico/latest/about/">Calico</link>, siendo
Cilium su CNI por defecto. El metacomplemento <link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>
también se puede usar si los pods requieren varias interfaces de red. RKE2
por sí mismo admite <link
xl:href="https://docs.rke2.io/install/network_options">una gama más amplia
de opciones de CNI</link>.</para>
</section>
<section xml:id="id-storage">
<title>Almacenamiento</title>
<para>RKE2 no proporciona ninguna clase ni operador de almacenamiento
persistente. Para clústeres que abarquen varios nodos, se recomienda
utilizar SUSE Storage (<xref linkend="components-suse-storage"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-suse-storage">
<title><link xl:href="https://www.suse.com/products/rancher/storage/">SUSE
Storage</link></title>
<para>SUSE Storage es un sistema de almacenamiento en bloques distribuido ligero,
fiable y fácil de usar diseñado para Kubernetes. Se trata de un producto
basado en Longhorn, un proyecto de código abierto desarrollado inicialmente
por Rancher Labs y actualmente incubado bajo la CNCF.</para>
<section xml:id="id-prerequisites-4">
<title>Requisitos previos</title>
<para>Si está siguiendo esta guía, se da por hecho que ya dispone de lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Al menos un host con SUSE Linux Micro 6.1 instalado; puede ser físico o
virtual</para>
</listitem>
<listitem>
<para>Un clúster de Kubernetes instalado; ya sea K3s o RKE2</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-suse-storage">
<title>Instalación manual de SUSE Storage</title>
<section xml:id="id-installing-open-iscsi">
<title>Instalación de Open-iSCSI</title>
<para>Un requisito fundamental para desplegar y utilizar SUSE Storage es instalar
el paquete <literal>open-iscsi</literal> y que el daemon
<literal>iscsid</literal> se ejecute en todos los nodos de Kubernetes. Esto
es obligatorio, ya que Longhorn depende de que <literal>iscsiadm</literal>
esté en el host para proporcionar volúmenes persistentes a Kubernetes.</para>
<para>Vamos a instalarlo:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>Es importante tener en cuenta que, una vez completada la operación, el
paquete solo se instala en una nueva instantánea, ya que SUSE Linux Micro es
un sistema operativo inmutable. Para cargarlo y que el daemon
<literal>iscsid</literal> comience a ejecutarse, hay que volver a arrancar
en esa nueva instantánea que acabamos de crear. Ejecute el comando reboot
cuando esté listo:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>Para obtener ayuda adicional sobre cómo instalar open-iscsi, consulte la
<link
xl:href="https://longhorn.io/docs/1.8.1/deploy/install/#installing-open-iscsi">documentación
oficial de Longhorn</link>.</para>
</tip>
</section>
<section xml:id="id-installing-suse-storage">
<title>Instalación de SUSE Storage</title>
<para>Hay varias formas de instalar SUSE Storage en los clústeres de
Kubernetes. Esta guía explica la instalación con Helm, pero si desea usar
otro método, puede seguir la <link
xl:href="https://longhorn.io/docs/1.8.1/deploy/install/">documentación
oficial</link>.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Añada el repositorio de charts de Helm de Rancher:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>Obtenga los charts más recientes del repositorio:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para>Instale SUSE Storage en el espacio de nombres
<literal>longhorn-system</literal>:</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn-crd rancher-charts/longhorn-crd --namespace longhorn-system --create-namespace --version 106.2.0+up1.8.1
helm install longhorn rancher-charts/longhorn --namespace longhorn-system --version 106.2.0+up1.8.1</screen>
</listitem>
<listitem>
<para>Confirme que el despliegue se ha realizado correctamente:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-suse-storage-volumes">
<title>Creación de volúmenes de SUSE Storage</title>
<para>SUSE Storage utiliza recursos de Kubernetes denominados
<literal>StorageClass</literal> para aprovisionar automáticamente objetos
<literal>PersistentVolume</literal> para los pods. Piense en
<literal>StorageClass</literal> como una forma que tienen los
administradores de describir las <emphasis>clases</emphasis> o los
<emphasis>perfiles</emphasis> de almacenamiento que ofrecen.</para>
<para>Vamos a crear un recurso <literal>StorageClass</literal> con algunas
opciones predeterminadas:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para>Ahora que ya tenemos nuestro <literal>StorageClass</literal>, necesitamos
una solicitud <literal>PersistentVolumeClaim</literal> que haga referencia a
él. Una <literal>PersistentVolumeClaim</literal> (PVC) es una solicitud de
almacenamiento por parte de un usuario. Las PVC consumen recursos
<literal>PersistentVolume</literal>. Las solicitudes pueden pedir tamaños y
modos de acceso específicos (por ejemplo, se pueden montar una vez en modo
lectura/escritura o varias veces en modo solo lectura).</para>
<para>Vamos a crear una <literal>PersistentVolumeClaim</literal>:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>Tras crear la <literal>PersistentVolumeClaim</literal>, podemos adjuntarla a
un <literal>pod</literal>. Cuando se despliega el <literal>pod</literal>,
Kubernetes crea el volumen de Longhorn y lo vincula al
<literal>ood</literal> si hay almacenamiento disponible.</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>El concepto de almacenamiento en Kubernetes es un tema complejo, pero
importante. Hemos mencionado brevemente algunos de los recursos más comunes
de Kubernetes, sin embargo, le sugerimos que se familiarice con la <link
xl:href="https://longhorn.io/docs/1.8.1/terminology/">terminología
específica</link> en la documentación de Longhorn.</para>
</tip>
<para>En este ejemplo, el resultado debería ser similar a esto:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>Acceso a la interfaz del usuario</title>
<para>Si ha instalado Longhorn con kubectl o Helm, debe configurar un controlador
Ingress para permitir el tráfico externo hacia el clúster. La autenticación
no está habilitada de forma predeterminada. Si se ha usado la aplicación de
catálogo de Rancher, Rancher habrá creado automáticamente un controlador
Ingress con control de acceso (el proxy de rancher).</para>
<orderedlist numeration="arabic">
<listitem>
<para>Obtenga la dirección IP del servicio externo de Longhorn:</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para>Cuando haya recuperado la dirección IP de
<literal>longhorn-frontend</literal>, puede acceder a la interfaz de usuario
en su navegador y empezar a usarla.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Instalación con Edge Image Builder</title>
<para>SUSE Edge utiliza <xref linkend="components-eib"/> para personalizar las
imágenes del sistema operativo SUSE Linux Micro base. Vamos a mostrar cómo
hacerlo para aprovisionar un clúster RKE2 con Longhorn sobre él.</para>
<para>Vamos a crear el archivo de definición:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.2
image:
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 106.2.0+up1.8.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 106.2.0+up1.8.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>Es posible personalizar cualquiera de los valores del chart de Helm mediante
un archivo independiente proporcionado en
<literal>helm.charts[].valuesFile</literal>. Consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#kubernetes">documentación
original</link> para obtener más detalles.</para>
</note>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.3/edge-image-builder:1.2.1 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>Una vez creada la imagen, puede usarla para instalar su sistema operativo en
un host físico o virtual. Tras completar el aprovisionamiento, podrá iniciar
sesión en el sistema utilizando el par de credenciales
<literal>root:eib</literal>.</para>
<para>Asegúrese de que Longhorn se haya desplegado correctamente:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>Esta instalación no funciona en entornos completamente aislados. Para esos
casos, consulte la <xref linkend="suse-storage-install"/>.</para>
</note>
</section>
</chapter>
<chapter xml:id="components-suse-security">
<title><link xl:href="https://www.suse.com/products/rancher/security/">SUSE
Security</link></title>
<para>SUSE Security es una solución de seguridad para Kubernetes que proporciona
seguridad de red L7, seguridad en el entorno de ejecución, seguridad en la
cadena de suministro y comprobaciones de cumplimiento en un paquete
cohesionado.</para>
<para>SUSE Security es un producto que se despliega como una plataforma de
múltiples contenedores, que se comunican entre sí a través de varios puertos
e interfaces. En segundo plano, utiliza NeuVector como componente de
seguridad de contenedores subyacente. La plataforma SUSE Security está
formada por siguientes contenedores:</para>
<itemizedlist>
<listitem>
<para>Manager (Administrador). Un contenedor sin estado que presenta el panel de
control basado en web. Normalmente, solo se necesita uno y puede ejecutarse
en cualquier lugar. Si el administrador falla, ninguna de las operaciones
del controlador o del ejecutor se ven afectadas. Sin embargo, el
administrador almacena en la memoria caché ciertas notificaciones (eventos)
y datos de conexiones recientes, por lo que su visualización sí se vería
afectada.</para>
</listitem>
<listitem>
<para>Controller (Controlador). El "plano de control" de SUSE Security debe
desplegarse en una configuración HA (de alta disponibilidad), de modo que la
configuración no se pierda en caso de fallo de un nodo. Los controladores se
pueden ejecutar en cualquier lugar, aunque se suele optar por colocarlos en
nodos de "gestión", maestros o de infraestructura debido a su importancia
crítica.</para>
</listitem>
<listitem>
<para>Enforcer (Aplicador). Este contenedor se despliega como conjunto de daemons,
por lo que hay un aplicador en cada nodo que se vaya a
proteger. Normalmente, se despliega en todos los nodos de trabajador, pero
es posible programar que también se despliegue en los nodos maestros y de
infraestructura. Nota: si el aplicador no está en un nodo del clúster y las
conexiones provienen de un pod en ese nodo, SUSE Security las etiqueta como
cargas de trabajo "no gestionadas".</para>
</listitem>
<listitem>
<para>Scanner (Escáner). Realiza el análisis de vulnerabilidades utilizando la
base de datos de CVE integrada, según las instrucciones del controlador. Se
pueden desplegar varios escáneres para aumentar la capacidad de
análisis. Los escáneres pueden ejecutarse en cualquier lugar, pero es
habitual que sea en los nodos donde se ejecutan los controladores. Tenga en
cuenta las consideraciones sobre el tamaño de los nodos del escáner
expuestas a continuación. También es posible invocar un escáner de forma
independiente para el análisis de la fase de creación, por ejemplo, dentro
de un canal que activa un análisis, recupera los resultados y detiene el
escáner. El escáner contiene la base de datos de CVE más reciente, por lo
que debe actualizarse a diario.</para>
</listitem>
<listitem>
<para>Updater (Actualizador). El actualizador activa una actualización del escáner
mediante una tarea programada de Kubernetes cuando se desea actualizar la
base de datos de CVE. Asegúrese de adaptarlo a su entorno.</para>
</listitem>
</itemizedlist>
<para>Encontrará documentación más detallada sobre la incorporación de SUSE
Security y las prácticas recomendadas <link
xl:href="https://open-docs.neuvector.com/">aquí</link>.</para>
<section xml:id="id-how-does-suse-edge-use-suse-security">
<title>¿Cómo se usa SUSE Security en SUSE Edge?</title>
<para>SUSE Edge proporciona una configuración más ágil de SUSE Security como punto
de partida para despliegues periféricos.</para>
</section>
<section xml:id="id-important-notes">
<title>Notas importantes</title>
<itemizedlist>
<listitem>
<para>El contenedor <literal>Scanner</literal> debe tener suficiente memoria para
cargar la imagen que se va a escanear en la memoria y ampliarla. Para
escanear imágenes de más de 1 GB, aumente la memoria del escáner hasta un
poco más del tamaño máximo previsto de la imagen.</para>
</listitem>
<listitem>
<para>En el modo de protección, se espera un gran número de conexiones de red. El
contenedor <literal>Enforcer</literal> requiere CPU y memoria cuando se
encuentra en ese modo (bloqueo de firewall en línea) para conservar e
inspeccionar las conexiones y la posible carga útil (DLP). Aumentar la
memoria y dedicar un núcleo de CPU a <literal>Enforcer</literal> puede
garantizar una capacidad de filtrado de paquetes adecuada.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Instalación con Edge Image Builder</title>
<para>SUSE Edge utiliza <xref linkend="components-eib"/> para personalizar las
imágenes del sistema operativo SUSE Linux Micro base. Siga las instrucciones
de la <xref linkend="suse-security-install"/> para realizar una instalación
en entornos aislados de SUSE Security sobre clústeres de Kubernetes
aprovisionados por EIB.</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para>Consulte la <link xl:href="https://metallb.universe.tf/">documentación
oficial de MetalLB</link>.</para>
<blockquote>
<para>MetalLB es una implementación de equilibrador de carga para clústeres de
Kubernetes en bare metal que utiliza protocolos de enrutamiento estándar.</para>
<para>En entornos bare metal, configurar equilibradores de carga de red es
notablemente más complejo que en entornos en la nube. A diferencia de las
sencillas llamadas a la API de las configuraciones en la nube, en bare metal
se requieren dispositivos de red dedicados o una combinación de
equilibradores de carga y configuraciones de IP virtual (VIP) para gestionar
la alta disponibilidad (HA) o abordar el posible punto único de fallo (SPOF)
inherente a que haya un equilibrador de carga de un solo nodo. Estas
configuraciones no se automatizan fácilmente, lo que plantea retos en los
despliegues de Kubernetes, donde los componentes se escalan dinámicamente.</para>
<para>MetalLB aborda estos retos aprovechando el modelo de Kubernetes para crear
servicios de tipo LoadBalancer como si estuvieran operando en un entorno de
nube, incluso en configuraciones bare metal.</para>
<para>Hay dos enfoques distintos, mediante el <link
xl:href="https://metallb.universe.tf/concepts/layer2/">modo L2</link> (que
usa <emphasis>tricks</emphasis> ARP) o mediante <link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>. A rasgos
generales, el modo L2 no necesita ningún equipo de red especial, pero BGP
suele ser mejor. El uso de un enfoque u otro dependerá de los casos de uso.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>¿Cómo se usa MetalLB en SUSE Edge?</title>
<para>SUSE Edge usa MetalLB principalmente de dos formas:</para>
<itemizedlist>
<listitem>
<para>Como solución de equilibrador de carga: MetalLB sirve como solución de
equilibrio de carga para equipos bare metal.</para>
</listitem>
<listitem>
<para>En configuraciones K3s/RKE2 de HA: MetalLB permite equilibrar la carga de la
API de Kubernetes utilizando una dirección IP virtual.</para>
</listitem>
</itemizedlist>
<note>
<para>Para poder exponer la API, se utiliza el operador Endpoint Copier Operator
(<xref linkend="components-eco"/>) para mantener sincronizados los puntos
finales de la API de K8s desde el servicio <literal>kubernetes</literal> con
un servicio LoadBalancer <literal>kubernetes-vip</literal>.</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>Prácticas recomendadas</title>
<para>La instalación de MetalLB en el modo L2 se describe en el <xref
linkend="guides-metallb-k3s"/>.</para>
<para>Hay disponible una guía sobre cómo instalar MetalLB delante del
<literal>kube-api-server</literal> para lograr una topología de alta
disponibilidad en el <xref linkend="guides-metallb-kubernetes"/>.</para>
</section>
<section xml:id="id-known-issues-6">
<title>Problemas conocidos</title>
<itemizedlist>
<listitem>
<para>K3s incluye su propia solución de equilibrio de carga llamada
<literal>Klipper</literal>. Para utilizar MetalLB, es necesario inhabilitar
<literal>Klipper</literal>. Para hacerlo, inicie el servidor K3s con la
opción <literal>--disable servicelb</literal>, tal y como se describe en la
<link xl:href="https://docs.k3s.io/networking">documentación de K3s</link>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eco">
<title>Endpoint Copier Operator</title>
<para><link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link> es un operador de Kubernetes cuyo objetivo es crear
una copia de un servicio y un punto final de Kubernetes y mantenerlos
sincronizados.</para>
<section xml:id="id-how-does-suse-edge-use-endpoint-copier-operator">
<title>¿Cómo se usa Endpoint Copier Operator en SUSE Edge?</title>
<para>En SUSE Edge, Endpoint Copier Operator desempeña un papel crucial para
conseguir una configuración de alta disponibilidad (HA) para clústeres
K3s/RKE2. Esto se logra creando un servicio
<literal>kubernetes-vip</literal> de tipo <literal>LoadBalancer</literal>,
lo que garantiza que su punto final se sincronice constantemente con el
punto final de Kubernetes. MetalLB (<xref linkend="components-metallb"/>) se
utiliza para gestionar el servicio <literal>kubernetes-vip</literal>, ya que
la dirección IP expuesta se utiliza desde otros nodos para unirse al
clúster.</para>
</section>
<section xml:id="id-best-practices-7">
<title>Prácticas recomendadas</title>
<para>La documentación completa para utilizar Endpoint Copier Operator se
encuentra <link
xl:href="https://github.com/suse-edge/endpoint-copier-operator/blob/main/README.md">aquí</link>.</para>
<para>También puede consultar nuestra guía (<xref linkend="guides-metallb-k3s"/>)
sobre cómo lograr una configuración HA de K3s/RKE2 utilizando Endpoint
Copier Operator y MetalLB.</para>
</section>
<section xml:id="id-known-issues-7">
<title>Problemas conocidos</title>
<para>Actualmente, Endpoint Copier Operator solo puede trabajar con un
servicio/punto final. Se prevé realizar mejoras en el futuro para admitir
múltiples servicios/puntos finales.</para>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>En esta sección se describe cómo se puede utilizar Edge Virtualization para
ejecutar máquinas virtuales en los nodos periféricos. Edge Virtualization se
hadiseñado para casos de uso de virtualización ligeros, en los que se espera
que se utilice un flujo de trabajo común para el despliegue y la gestión
tanto de aplicaciones virtualizadas como en contenedores.</para>
<para>SUSE Edge Virtualization admite dos métodos para ejecutar máquinas
virtuales:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Despliegue manual de las máquinas virtuales mediante libvirt+qemu-kvm a
nivel del host (sin intervención de Kubernetes)</para>
</listitem>
<listitem>
<para>Despliegue del operador KubeVirt para la gestión basada en Kubernetes de
máquinas virtuales</para>
</listitem>
</orderedlist>
<para>Ambas opciones son válidas, pero solo la segunda se trata a continuación. Si
desea utilizar los mecanismos de virtualización estándares listos para usar
que proporciona SUSE Linux Micro, encontrará una guía completa en <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/chap-virtualization-introduction.html">este
documento</link>. Aunque está escrita principalmente para SUSE Linux
Enterprise Server, los conceptos son prácticamente idénticos.</para>
<para>Esta guía explica inicialmente cómo desplegar los componentes de
virtualización adicionales en un sistema que ya ha sido predesplegado, pero
incluye una sección que describe cómo integrar esta configuración en el
despliegue inicial a través de Edge Image Builder. Si no desea repasar los
conceptos básicos y configurarlo todo manualmente, pase directamente a esa
sección.</para>
<section xml:id="id-kubevirt-overview">
<title>Descripción general de KubeVirt</title>
<para>KubeVirt permite gestionar máquinas virtuales con Kubernetes junto con el
resto de sus cargas de trabajo en contenedores. Para ello, ejecuta la parte
del espacio de usuario de la pila de virtualización de Linux en un
contenedor. Esto minimiza los requisitos del sistema host, lo que facilita
la configuración y la gestión.</para>
<informalexample>
<para>Encontrará los detalles sobre la arquitectura de KubeVirt en la <link
xl:href="https://kubevirt.io/user-guide/architecture/">documentación
original</link>.</para>
</informalexample>
</section>
<section xml:id="id-prerequisites-5">
<title>Requisitos previos</title>
<para>Si está siguiendo esta guía, se da por hecho que ya dispone de lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Al menos un host físico con SUSE Linux Micro 6.1 instalado y con las
extensiones de virtualización habilitadas en el BIOS (consulte <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">este
documento</link> para obtener más detalles).</para>
</listitem>
<listitem>
<para>Un clúster Kubernetes de K3s/RKE2 ya desplegado en todos sus nodos y con una
<literal>kubeconfig</literal> adecuada que permita el acceso de superusuario
al clúster.</para>
</listitem>
<listitem>
<para>Acceso al usuario root: estas instrucciones dan por sentado que usted es el
usuario root y que <emphasis>no</emphasis> está derivando sus privilegios
mediante <literal>sudo</literal>.</para>
</listitem>
<listitem>
<para>Debe tener <link xl:href="https://helm.sh/docs/intro/install/">Helm</link>
disponible localmente con una conexión de red adecuada para poder enviar
configuraciones a su clúster de Kubernetes y descargar las imágenes
necesarias.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Instalación manual de Edge Virtualization</title>
<para>Esta guía no explica el despliegue de Kubernetes, pero da por sentado que ha
instalado la versión de <link xl:href="https://k3s.io/">K3s</link> o <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> adecuada para
SUSE Edge y que tiene su kubeconfig configurada adecuadamente para que los
comandos estándar <literal>kubectl</literal> se puedan ejecutar como
superusuario. También se presupone que su nodo forma un clúster de un solo
nodo, aunque no se esperan diferencias significativas para los despliegues
de varios nodos.</para>
<para>SUSE Edge Virtualization se despliega mediante tres charts de Helm
distintos, en concreto:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>: los componentes básicos de
virtualización, es decir, las CRD de Kubernetes, los operadores y otros
componentes necesarios para que Kubernetes pueda desplegar y gestionar
máquinas virtuales.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Extensión de panel de control KubeVirt</emphasis>:
una extensión opcional de la interfaz de usuario de Rancher que permite la
gestión básica de máquinas virtuales; por ejemplo, iniciar/detener máquinas
virtuales, así como acceder al panel de control.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>: un
componente adicional que permite la integración del almacenamiento
persistente para KubeVirt, proporcionando capacidades para que las máquinas
virtuales puedan usar sistemas secundarios de almacenamiento existentes de
Kubernetes para los datos, pero también permitiendo a los usuarios importar
o clonar volúmenes de datos para máquinas virtuales.</para>
</listitem>
</itemizedlist>
<para>Cada uno de estos charts de Helm tiene versiones distintas según la versión
de SUSE Edge que esté utilizando actualmente. Para usarlos en entornos de
producción o con asistencia, utilice los artefactos que se pueden encontrar
en el registro de SUSE.</para>
<para>En primer lugar, asegúrese de que puede acceder correctamente a
<literal>kubectl</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>Esto debería mostrar algo similar a lo siguiente:</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.30.5+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1</screen>
<para>Ahora, puede instalar los charts de Helm de <emphasis
role="strong">KubeVirt</emphasis> y <emphasis role="strong">Containerized
Data Importer (CDI)</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/charts/kubevirt --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/charts/cdi --namespace cdi-system --create-namespace</screen>
<para>En unos minutos, debería tener todos los componentes de KubeVirt y CDI
desplegados. Puede comprobar que todos los recursos se han desplegado en los
espacios de nombres <literal>kubevirt-system</literal> y
<literal>cdi-system</literal>.</para>
<para>Verifique los recursos de KubeVirt:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>Esto debería mostrar algo similar a lo siguiente:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>Verifique los recursos de CDI:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>Esto debería mostrar algo similar a lo siguiente:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para>Para verificar que las definiciones de recursos personalizados (CRD) de
<literal>VirtualMachine</literal> están desplegadas, puede usar:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para>Esto debería dar como resultado la definición del objeto
<literal>VirtualMachine</literal>, que debería tener este aspecto:</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>Despliegue de máquinas virtuales</title>
<para>Ahora que KubeVirt y CDI se han desplegado, vamos a definir una máquina
virtual sencilla basada en <link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE
Tumbleweed</link>. La configuración de esta máquina virtual es muy sencilla:
usa una "conexión de red de pod" idéntica a la de cualquier otro
pod. También emplea almacenamiento no persistente, lo que garantiza que el
almacenamiento sea efímero, al igual que en cualquier contenedor que no
tenga un <link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>El resultado debe ser que se ha creado una
<literal>VirtualMachine</literal>:</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>Esta definición de <literal>VirtualMachine</literal> es mínima y especifica
muy poco sobre la configuración. Solo indica que se trata de una máquina de
tipo <link xl:href="https://wiki.qemu.org/Features/Q35">q35</link> con 2 GB
de memoria que usa una imagen de disco basada en un <literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>
efímero (es decir, una imagen de disco que se almacena en una imagen de
contenedor desde un repositorio de imágenes remoto), y especifica un disco
cloudInit con cifrado base64, que solo utilizamos para la creación de
usuarios y la obligación de usar contraseña en el momento del arranque
(utilice <literal>base64 -d</literal> para descifrarlo).</para>
<blockquote>
<note>
<para>Esta imagen de máquina virtual sirve solo para realizar pruebas. La imagen
no cuenta con asistencia oficial y solo se ofrece como ejemplo para fines de
documentación.</para>
</note>
</blockquote>
<para>Esta máquina tarda unos minutos en arrancar, ya que necesita descargar la
imagen de disco de openSUSE Tumbleweed, pero una vez que lo haya hecho,
podrá ver más detalles sobre ella:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>Esto debería dar como resultado el nodo en el que se inició la máquina
virtual y la dirección IP de la máquina virtual. Recuerde que, dado que
utiliza conexión de red de pod, la dirección IP indicada será igual que la
de cualquier otro pod y, por lo tanto, enrutable:</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>Al ejecutar estos comandos en los propios nodos del clúster de Kubernetes,
con una CNI que enruta el tráfico directamente a los pods (por ejemplo,
Cilium), debería poder conectarse directamente por <literal>ssh</literal> a
la máquina. Sustituya la siguiente dirección IP por la que se le haya
asignado a su máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>Cuando se encuentre en esta máquina virtual, puede practicar un poco, pero
recuerde que tiene recursos limitados y solo cuenta con 1 GB de espacio en
disco. Cuando termine, pulse <literal>Ctrl+D</literal> o
<literal>exit</literal> para desconectarse de la sesión SSH.</para>
<para>El proceso de la máquina virtual sigue estando envuelto en un pod estándar
de Kubernetes. La CRD <literal>VirtualMachine</literal> es una
representación de la máquina virtual deseada, pero el proceso en el que se
inicia realmente la máquina virtual es a través del pod <literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>,
un pod estándar de Kubernetes, igual que cualquier otra aplicación. Por cada
máquina virtual iniciada, se puede ver que hay un pod
<literal>virt-launcher</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>Esto debería mostrar el único pod <literal>virt-launcher</literal> para la
máquina Tumbleweed que hemos definido:</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>Si echamos un vistazo a este pod <literal>virt-launcher</literal>, veremos
que está ejecutando los procesos <literal>libvirt</literal> y
<literal>qemu-kvm</literal>. Podemos entrar en el propio pod y echar un
vistazo, pero tenga en cuenta que es necesario adaptar el siguiente comando
al nombre de su pod:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Una vez en el pod, pruebe a ejecutar comandos <literal>virsh</literal> y
observe los procesos. Comprobará que se está ejecutando el binario
<literal>qemu-system-x86_64</literal>, junto con ciertos procesos para
supervisar la máquina virtual. También verá la ubicación de la imagen de
disco y cómo está conectada la red (como un dispositivo tap):</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>Por último, eliminemos esta máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>Uso de virtctl</title>
<para>Además de la estándar de Kubernetes (<literal>kubectl</literal>), KubeVirt
incluye una interfaz de línea de comandos adicional que permite interactuar
con el clúster de una forma que obvia ciertas diferencias entre los entornos
de virtualización y los usos para los que se diseñó Kubernetes. Por ejemplo,
la herramienta <literal>virtctl</literal> permite gestionar el ciclo de vida
de las máquinas virtuales (iniciar, detener, reiniciar, etc.), proporciona
acceso a los paneles de control virtuales, permite cargar imágenes de
máquinas virtuales e interactuar con construcciones de Kubernetes como
servicios, sin utilizar directamente la API o las CRD.</para>
<para>Vamos a descargar la última versión estable de la herramienta
<literal>virtctl</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.4.0
$ wget https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64</screen>
<para>Si utiliza una arquitectura diferente o una máquina que no sea Linux,
encontrará otras versiones <link
xl:href="https://github.com/kubevirt/kubevirt/releases">aquí</link>. Debe
hacer que este archivo sea ejecutable antes de continuar, y puede ser útil
moverlo a una ubicación dentro de <literal>$PATH</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-$VERSION-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>A continuación, puede utilizar la herramienta de línea de comandos
<literal>virtctl</literal> para crear máquinas virtuales. Vamos a replicar
nuestra máquina virtual anterior, teniendo en cuenta que estamos canalizando
la salida directamente a <literal>kubectl apply</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>Esto debería mostrar la máquina virtual en ejecución (esta vez debería
iniciarse mucho más rápido, ya que la imagen del contenedor estará
almacenada en caché):</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>Ahora, podemos usar <literal>virtctl</literal> para conectarnos directamente
a la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para>Se pueden utilizar muchos otros comandos con <literal>virtctl</literal>. Por
ejemplo, <literal>virtctl console</literal> permite acceder al panel de
control serie si la red no funciona, y puede utilizar <literal>virtctl
guestosinfo</literal> para obtener información completa sobre el sistema
operativo, siempre que el invitado tenga instalado y en ejecución
<literal>qemu-guest-agent</literal>.</para>
<para>Por último, hagamos una pausa y reiniciemos la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para>Verá que el objeto <literal>VirtualMachine</literal> aparece como <emphasis
role="strong">Paused</emphasis> (En pausa) y el objeto
<literal>VirtualMachineInstance</literal> aparece como <emphasis
role="strong">Running</emphasis> (En ejecución), pero como <emphasis
role="strong">READY=False</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>También comprobará que ya no puede conectarse a la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>Reiniciemos la máquina virtual y volvamos a intentarlo:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>Ahora deberíamos poder restablecer la conexión:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>Por último, eliminemos la máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>Conexión de redes sencilla con Ingress</title>
<para>En esta sección, se explica cómo exponer máquinas virtuales como servicios
estándar de Kubernetes y cómo hacerlas disponibles mediante el servicio
Ingress de Kubernetes; por ejemplo, <link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINX
con RKE2</link> o <link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">Traefik
con K3s</link>. En este documento se entiende que estos componentes ya están
configurados adecuadamente y que dispone de un puntero DNS adecuado (por
ejemplo, un comodín) que dirija a los nodos de su servidor de Kubernetes o a
su IP virtual de Ingress para que Ingress se resuelva adecuadamente.</para>
<blockquote>
<note>
<para>En SUSE Edge 3.1 y versiones posteriores, si utiliza K3s en una
configuración de nodos con varios servidores, es posible que haya tenido que
configurar una IP virtual basada en MetalLB para Ingress. Esto no es
necesario para RKE2.</para>
</note>
</blockquote>
<para>En el entorno de ejemplo, se despliega otra máquina virtual Tumbleweed de
openSUSE, se usa cloud-init para instalar NGINX como servidor Web sencillo
en el momento del arranque y se configura un mensaje sencillo, que se envía
para verificar que funciona según lo esperado cuando se realiza una
llamada. Para ver cómo se hace, use el comando <literal>base64 -d</literal>
en la sección cloud-init del siguiente ejemplo.</para>
<para>Vamos a crear esta máquina virtual:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>Cuando esta máquina virtual se haya iniciado correctamente, se podrá usar el
comando <literal>virtctl</literal> para exponer la instancia
<literal>VirtualMachineInstance</literal> con el puerto externo
<literal>8080</literal> y el puerto de destino <literal>80</literal> (en el
que NGINX escucha por defecto). Aquí, utilizamos el comando
<literal>virtctl</literal>, ya que entiende la asignación entre el objeto de
máquina virtual y el pod. Esto crea un nuevo servicio:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>A continuación, se crea automáticamente el servicio adecuado:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>Ahora, si utiliza <literal>kubectl create ingress</literal>, puede crear un
objeto Ingress que apunte a este servicio. Adapte la URL (conocida como
"host" en el objeto <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link>)
aquí para que coincida con su configuración de DNS y asegúrese de que apunta
al puerto <literal>8080</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>Con el servidor DNS configurado correctamente, debería poder ejecutar el
comando curl en la URL de inmediato:</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>Para limpiar, eliminaremos esta máquina virtual y sus recursos de servicio y
de Ingress:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Uso de la extensión de interfaz de usuario de Rancher</title>
<para>SUSE Edge Virtualization proporciona una extensión de interfaz de usuario
para Rancher Manager, lo que permite la gestión básica de máquinas virtuales
mediante la interfaz de usuario del panel de control de Rancher.</para>
<section xml:id="id-installation-4">
<title>Instalación</title>
<para>Consulte la sección sobre extensiones de panel de control de Rancher (<xref
linkend="components-rancher-dashboard-extensions"/>) para obtener
instrucciones de instalación.</para>
</section>
<section xml:id="kubevirt-dashboard-extension-usage">
<title>Uso de la extensión de panel de control KubeVirt de Rancher</title>
<para>La extensión introduce una nueva sección <emphasis
role="strong">KubeVirt</emphasis> en Cluster Explorer. Esta sección se añade
a cualquier clúster gestionado que tenga instalado KubeVirt.</para>
<para>La extensión permite interactuar directamente con los recursos de la máquina
virtual de KubeVirt para gestionar el ciclo de vida de las máquinas
virtuales.</para>
<section xml:id="id-creating-a-virtual-machine">
<title>Creación de una máquina virtual</title>
<orderedlist numeration="arabic">
<listitem>
<para>En el panel de navegación izquierdo, diríjase a <emphasis
role="strong">Cluster Explorer</emphasis> (Explorador de clústeres) haciendo
clic en el clúster gestionado habilitado para KubeVirt.</para>
</listitem>
<listitem>
<para>Diríjase a la página <emphasis role="strong">KubeVirt &gt; Virtual
Machines</emphasis> (KubeVirt > Máquinas virtuales) y haga clic en
<literal>Create from YAML</literal> (Crear desde YAML) en la parte superior
derecha de la pantalla.</para>
</listitem>
<listitem>
<para>Rellene o pegue una definición de máquina virtual y pulse
<literal>Create</literal> (Crear). Use la definición de máquina virtual de
la sección Despliegue de máquinas virtuales como referencia.</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width="100%"/>
</imageobject>
<textobject><phrase>página de máquinas virtuales</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-virtual-machine-actions">
<title>Acciones de la máquina virtual</title>
<para>Puede utilizar el menú de acciones al que se accede desde la lista
desplegable <emphasis role="strong">⋮</emphasis> situada a la derecha de
cada máquina virtual para iniciar, detener, pausar o hacer un reinicio
suave. También puede usar las acciones de grupo de la parte superior de la
lista seleccionando las máquinas virtuales en las que desea realizar la
acción.</para>
<para>Estas acciones pueden afectar a la estrategia de ejecución de la máquina
virtual. Consulte la <link
xl:href="https://kubevirt.io/user-guide/compute/run_strategies/#virtctl">tabla
en la documentación de KubeVirt</link> para obtener más detalles.</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>Acceso al panel de control de la máquina virtual</title>
<para>La lista "Virtual Machines" (Máquinas virtuales) proporciona una lista
desplegable <literal>Console</literal> (Panel de control) que permite
conectarse a la máquina mediante <emphasis role="strong">VNC o un panel de
control serie</emphasis>. Esta acción solo está disponible para máquinas en
ejecución.</para>
<para>En algunos casos, para poder acceder al panel de control en una máquina
virtual recién iniciada se debe esperar un poco.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width="100%"/>
</imageobject>
<textobject><phrase>interfaz del panel de control de vnc</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Instalación con Edge Image Builder</title>
<para>SUSE Edge usa <xref linkend="components-eib"/> para personalizar las
imágenes del sistema operativo SUSE Linux Micro base. Siga las instrucciones
de la <xref linkend="kubevirt-install"/> para realizar una instalación en un
entorno aislado tanto de KubeVirt como de CDI sobre clústeres de Kubernetes
aprovisionados por EIB.</para>
</section>
</chapter>
<chapter xml:id="components-system-upgrade-controller">
<title>System Upgrade Controller</title>
<para>Consulte la <link
xl:href="https://github.com/rancher/system-upgrade-controller">documentación
de System Upgrade Controller</link>.</para>
<blockquote>
<para>System Upgrade Controller (SUC) proporciona un controlador de actualización
de uso general y nativo de Kubernetes (para nodos). Introduce una nueva CRD,
llamada "plan", para definir todas sus políticas y requisitos de
actualización. Un plan es una intención pendiente de mutar nodos en su
clúster.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-system-upgrade-controller">
<title>¿Cómo se usa System Upgrade Controller en SUSE Edge?</title>
<para>SUSE Edge usa <literal>SUC</literal> para facilitar diversas operaciones de
"día 2" relacionadas con las actualizaciones de la versión del sistema
operativo y Kubernetes en los clústeres de gestión y descendentes.</para>
<para>Las operaciones de "día 2" se definen mediante <literal>planes de
SUC</literal>. Según esos planes, <literal>SUC</literal> despliega las
cargas de trabajo en cada nodo para ejecutar la operación correspondiente de
"día 2".</para>
<para><literal>SUC</literal> también se utiliza en <xref
linkend="components-upgrade-controller"/>. Para obtener más información
sobre las diferencias clave entre SUC y Upgrade Controller, consulte la
<xref linkend="components-upgrade-controller-uc-vs-suc"/>.</para>
</section>
<section xml:id="components-system-upgrade-controller-install">
<title>Instalación de System Upgrade Controller</title>
<important>
<para>A partir de la versión <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.10.0">2.10.0</link>
de Rancher, <literal>System Upgrade Controller</literal> se instala
automáticamente.</para>
<para>Siga los pasos siguientes <emphasis role="strong">solo</emphasis> si su
entorno <emphasis role="strong">no</emphasis> está gestionado por Rancher, o
si su versión de Rancher es anterior a la <literal>2.10.0</literal>.</para>
</important>
<para>Se recomienda instalar SUC mediante Fleet (<xref
linkend="components-fleet"/>), situado en el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>.</para>
<note>
<para>Los recursos ofrecidos por el repositorio
<literal>suse-edge/fleet-examples</literal> <emphasis
role="strong">deben</emphasis> utilizarse siempre desde una versión válida
de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">fleet-examples</link>.
Para determinar qué versión debe utilizar, consulte las Notas de la versión
(<xref linkend="release-notes"/>).</para>
</note>
<para>Si no puede utilizar Fleet para la instalación de SUC, puede instalarlo
mediante el repositorio de charts de Helm de Rancher o incorporar el chart
de Helm de Rancher en su propio flujo de trabajo de GitOps de terceros.</para>
<para>En esta sección se trata lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Instalación con Fleet (<xref
linkend="components-system-upgrade-controller-fleet"/>)</para>
</listitem>
<listitem>
<para>Instalación con Helm (<xref
linkend="components-system-upgrade-controller-helm"/>)</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet">
<title>Instalación de System Upgrade Controller con Fleet</title>
<para>Hay dos posibles recursos que se pueden usar para desplegar SUC mediante
Fleet:</para>
<itemizedlist>
<listitem>
<para>Recurso <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>:
para casos de uso en los que se dispone de un servidor Git
externo/local. Para obtener instrucciones de instalación, consulte la
sección Instalación de System Upgrade Controller - GitRepo (<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>).</para>
</listitem>
<listitem>
<para>Recurso <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>:
para casos de uso en entornos aislados que no admiten la opción de servidor
Git local. Para obtener instrucciones de instalación, consulte la sección
Instalación de System Upgrade Controller - Bundle (<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet-gitrepo">
<title>Instalación de System Upgrade Controller - GitRepo</title>
<note>
<para>Este proceso también se puede realizar a través de la interfaz de usuario de
Rancher, si está disponible. Para obtener más información, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI</link> (Acceso a Fleet en la interfaz de Rancher).</para>
</note>
<para>En el clúster de <emphasis role="strong">gestión</emphasis>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Determine en qué clústeres desea desplegar SUC. Para ello, despliegue un
recurso <literal>GitRepo</literal> de SUC en el espacio de trabajo correcto
de Fleet de su clúster de <emphasis role="strong">gestión</emphasis>. De
forma predeterminada, Fleet tiene dos espacios de trabajo:</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal>: para recursos que deben desplegarse en el
clúster de <emphasis role="strong">gestión</emphasis>.</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal>: para recursos que deben desplegarse en
clústeres <emphasis role="strong">descendentes</emphasis>.</para>
<para>Para obtener más información sobre los espacios de trabajo de Fleet,
consulte la <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentación
original</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Despliegue el recurso <literal>GitRepo</literal>:</para>
<itemizedlist>
<listitem>
<para>Para desplegar SUC en el clúster de gestión:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-local -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.3.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF</screen>
</listitem>
<listitem>
<para>Para desplegar SUC en los clústeres descendentes:</para>
<note>
<para>Antes de desplegar el recurso que se indica a continuación, <emphasis
role="strong">debe</emphasis> proporcionar una configuración válida para
<literal>targets</literal>, de modo que Fleet sepa en qué clústeres
descendentes debe desplegar su recurso. Para obtener más información sobre
cómo realizar la asignación a clústeres descendentes, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Asignación a clústeres descendentes).</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.3.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Compruebe que el recurso <literal>GitRepo</literal> se ha desplegado:</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get gitrepo system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.3.0   1/1</screen>
</listitem>
<listitem>
<para>Compruebe que System Upgrade Controller se ha desplegado:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-fleet-bundle">
<title>Instalación de System Upgrade Controller - Bundle</title>
<para>Esta sección muestra cómo crear y desplegar un recurso
<literal>Bundle</literal> a partir de una configuración estándar de Fleet
utilizando <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<orderedlist numeration="arabic">
<listitem>
<para>En un equipo con acceso de red, descargue <literal>fleet-cli</literal>:</para>
<note>
<para>Asegúrese de que la versión de fleet-cli que descargue coincida con la
versión de Fleet que se ha desplegado en su clúster.</para>
</note>
<itemizedlist>
<listitem>
<para>Para usuarios de Mac, hay una versión propia de <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Para usuarios de Linux y Windows, los binarios son <emphasis
role="strong">recursos</emphasis> distintos para cada <link
xl:href="https://github.com/rancher/fleet/releases">versión</link> de Fleet.</para>
<itemizedlist>
<listitem>
<para>Linux AMD:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.12.2/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM:</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.12.2/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Permita que <literal>fleet-cli</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x fleet-cli</screen>
</listitem>
<listitem>
<para>Clone la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de <literal>suse-edge/fleet-examples</literal> que desee usar:</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.3.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Diríjase a la instancia de Fleet de SUC, situada en el repositorio
<literal>fleet-examples</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller</screen>
</listitem>
<listitem>
<para>Determine en qué clústeres desea desplegar SUC. Para ello, despliegue el
Bundle de SUC en el espacio de trabajo correcto de Fleet dentro de su
clúster de gestión. De forma predeterminada, Fleet tiene dos espacios de
trabajo:</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal>: para recursos que deben desplegarse en el
clúster de <emphasis role="strong">gestión</emphasis>.</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal>: para recursos que deben desplegarse en
clústeres <emphasis role="strong">descendentes</emphasis>.</para>
<para>Para obtener más información sobre los espacios de trabajo de Fleet,
consulte la <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentación
original</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Si tiene intención de desplegar SUC solo en clústeres descendentes, cree un
archivo <literal>targets.yaml</literal> que coincida con los clústeres
específicos:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>Para obtener información sobre cómo realizar la asignación a clústeres
descendentes, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
<listitem>
<para>Cree el Bundle:</para>
<note>
<para>Asegúrese de que <emphasis role="strong">no</emphasis> ha descargado
fleet-cli en el directorio
<literal>fleet-examples/fleets/day2/system-upgrade-controller</literal>, ya
que, si fuera así, se empaquetaría con el Bundle, lo cual no es
recomendable.</para>
</note>
<itemizedlist>
<listitem>
<para>Para desplegar SUC en su clúster de gestión, ejecute:</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>Para desplegar SUC en sus clústeres descendentes, ejecute:</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
<para>Para obtener más información sobre este proceso, consulte la sección <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Conversión de un chart de Helm en un
Bundle).</para>
<para>Para obtener más información sobre el comando <literal>fleet-cli
apply</literal>, consulte <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Transfiera el Bundle
<literal>system-upgrade-controller-bundle.yaml</literal> al equipo del
clúster de gestión:</para>
<screen language="bash" linenumbering="unnumbered">scp system-upgrade-controller-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para>En el clúster de gestión, despliegue el Bundle
<literal>system-upgrade-controller-bundle.yaml</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>En el clúster de gestión, compruebe que el Bundle está desplegado:</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get bundle system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1</screen>
</listitem>
<listitem>
<para>Según el espacio de trabajo de Fleet en el que haya desplegado su Bundle,
diríjase al clúster y compruebe que SUC se ha desplegado:</para>
<note>
<para>SUC siempre se despliega en el espacio de nombres <emphasis
role="strong">cattle-system</emphasis>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-helm">
<title>Instalación de System Upgrade Controller con Helm</title>
<orderedlist numeration="arabic">
<listitem>
<para>Añada el repositorio de charts de Rancher:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>Despliegue el chart de SUC:</para>
<screen language="bash" linenumbering="unnumbered">helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 106.0.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace</screen>
<para>Esto instalará la versión 0.15.2 de SUC, que es la que necesita la
plataforma Edge 3.3.1.</para>
</listitem>
<listitem>
<para>Compruebe que SUC se ha desplegado:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans">
<title>Supervisión de planes de System Upgrade Controller</title>
<para>Los planes de SUC se pueden consultar de las siguientes maneras:</para>
<itemizedlist>
<listitem>
<para>Mediante la interfaz de usuario de Rancher (<xref
linkend="components-system-upgrade-controller-monitor-plans-rancher"/>).</para>
</listitem>
<listitem>
<para>Manualmente dentro del clúster (<xref
linkend="components-system-upgrade-controller-monitor-plans-manual"/>).</para>
</listitem>
</itemizedlist>
<important>
<para>Los pods desplegados para los planes de SUC se mantienen activos <emphasis
role="strong">15</emphasis> minutos tras una ejecución correcta. Después,
son eliminados por el trabajo correspondiente que los creó. Para tener
acceso a los registros del pod después de este periodo de tiempo, debe
habilitar el registro para su clúster. Para obtener información sobre cómo
hacerlo en Rancher, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/logging">Rancher
Integration with Logging Services</link> (Integración de Rancher con
servicios de registro).</para>
</important>
<section xml:id="components-system-upgrade-controller-monitor-plans-rancher">
<title>Supervisión de planes de System Upgrade Controller - Interfaz de usuario de
Rancher</title>
<para>Para comprobar los registros del pod para el plan de SUC específico:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En la esquina superior izquierda, <emphasis role="strong">☰ →
&lt;nombre-de-su-clúster&gt;</emphasis>.</para>
</listitem>
<listitem>
<para>Seleccione Workloads → Pods (Cargas de trabajo > Pods).</para>
</listitem>
<listitem>
<para>Seleccione el menú desplegable <literal>Only User Namespaces</literal> (Solo
espacios de nombres de usuario) y añada el espacio de nombres
<literal>cattle-system</literal>.</para>
</listitem>
<listitem>
<para>En la barra de filtro Pod, escriba el nombre de su pod del plan de SUC. El
nombre tendrá el siguiente formato:
<literal>apply-&lt;nombre_plan&gt;-on-&lt;nombre_nodo&gt;</literal>.</para>
<note>
<para>Puede haber pods con los estados <literal>Completed</literal> (Completado) y
<literal>Unknown</literal> (Desconocido) para un plan de SUC
específico. Esto está previsto debido a la naturaleza de algunas de las
actualizaciones.</para>
</note>
</listitem>
<listitem>
<para>Seleccione el pod cuyos registros desea revisar y vaya a <emphasis
role="strong">⋮ → View Logs</emphasis> (⋮ > Ver registros).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans-manual">
<title>Supervisión de planes de System Upgrade Controller - Manual</title>
<note>
<para>Los pasos siguientes dan por supuesto que <literal>kubectl</literal> se ha
configurado para conectarse al clúster en el que se han desplegado los
<emphasis role="strong">planes de SUC</emphasis>.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Muestre los planes de <emphasis role="strong">SUC</emphasis> desplegados:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get plans -n cattle-system</screen>
</listitem>
<listitem>
<para>Obtenga el pod para el plan de <emphasis role="strong">SUC</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l upgrade.cattle.io/plan=&lt;plan_name&gt; -n cattle-system</screen>
<note>
<para>Puede haber pods con los estados <literal>Completed</literal> (Completado) y
<literal>Unknown</literal> (Desconocido) para un plan de SUC
específico. Esto está previsto debido a la naturaleza de algunas de las
actualizaciones.</para>
</note>
</listitem>
<listitem>
<para>Obtenga los registros del pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n cattle-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="components-upgrade-controller">
<title>Upgrade Controller</title>
<para>Upgrade Controller es un controlador de Kubernetes capaz de realizar
actualizaciones en los siguientes componentes de la plataforma SUSE Edge:</para>
<itemizedlist>
<listitem>
<para>Sistema operativo (SUSE Linux Micro)</para>
</listitem>
<listitem>
<para>Kubernetes (K3s y RKE2)</para>
</listitem>
<listitem>
<para>Componentes adicionales (Rancher, Elemental, SUSE Security, etc.)</para>
</listitem>
</itemizedlist>
<para><link xl:href="https://github.com/suse-edge/upgrade-controller">Upgrade
Controller</link> optimiza el proceso de actualización de esto componentes
encapsulando sus complejidades en un único recurso <literal>orientado al
usuario</literal> que sirve como <emphasis
role="strong">activador</emphasis> de la actualización. Los usuarios solo
tienen que configurar este recurso y <literal>Upgrade Controller</literal>
se encarga del resto.</para>
<note>
<para><literal>Upgrade Controller</literal> actualmente solo admite
actualizaciones de la plataforma SUSE Edge para clústeres <emphasis
role="strong">de gestión que no estén en entornos
aislados</emphasis>. Consulte la <xref
linkend="components-upgrade-controller-known-issues"/> para obtener más
información.</para>
</note>
<section xml:id="id-how-does-suse-edge-use-upgrade-controller">
<title>¿Cómo se usa Upgrade Controller en SUSE Edge?</title>
<para><emphasis role="strong">Upgrade Controller</emphasis> es esencial para
automatizar las operaciones de "día 2" (que antes eran manuales) necesarias
para actualizar los clústeres de gestión desde una versión de SUSE Edge a la
siguiente.</para>
<para>Para lograr esta automatización, Upgrade Controller usa herramientas como
System Upgrade Controller (<xref
linkend="components-system-upgrade-controller"/>) y <link
xl:href="https://github.com/k3s-io/helm-controller/">Helm Controller</link>.</para>
<para>Para obtener más detalles sobre cómo funciona Upgrade Controller, consulte
la <xref linkend="components-upgrade-controller-how"/>.</para>
<para>Para ver las limitaciones de Upgrade Controller, consulte la <xref
linkend="components-upgrade-controller-known-issues"/>.</para>
<para>Para ver las diferencias entre Upgrade Controller y System Upgrade
Controller, consulte la <xref
linkend="components-upgrade-controller-uc-vs-suc"/>.</para>
</section>
<section xml:id="components-upgrade-controller-uc-vs-suc">
<title>Diferencias entre Upgrade Controller y System Upgrade Controller</title>
<para>System Upgrade Controller (SUC) (<xref
linkend="components-system-upgrade-controller"/>) es una herramienta de uso
general que propaga instrucciones de actualización a nodos específicos de
Kubernetes.</para>
<para>Aunque admite algunas operaciones de "día 2" para la plataforma SUSE Edge,
<emphasis role="strong">no</emphasis> las cubre todas. Además, incluso en el
caso de las operaciones admitidas, los usuarios tienen que configurar,
mantener y desplegar manualmente múltiples <literal>planes de SUC</literal>,
un proceso propenso a errores que puede dar lugar a problemas inesperados.</para>
<para>Esto llevó a la necesidad de una herramienta que <emphasis
role="strong">automatizara</emphasis> y <emphasis
role="strong">resumiera</emphasis> la complejidad de gestionar diversas
operaciones de "día 2" para la plataforma SUSE Edge. Así, se desarrolló
<literal>Upgrade Controller</literal>, que simplifica el proceso de
actualización al introducir un único <literal>recurso orientado al
usuario</literal> que se encarga de la actualización. Los usuarios solo
tienen que gestionar este recurso y <literal>Upgrade Controller</literal> se
encarga del resto.</para>
</section>
<section xml:id="components-upgrade-controller-installation">
<title>Instalación de Upgrade Controller</title>
<section xml:id="id-prerequisites-6">
<title>Requisitos previos</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://helm.sh/docs/intro/install/">Helm</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://cert-manager.io/v1.15-docs/installation/helm/#installing-with-helm">cert-manager</link></para>
</listitem>
<listitem>
<para>System Upgrade Controller (<xref
linkend="components-system-upgrade-controller-install"/>)</para>
</listitem>
<listitem>
<para>Un clúster de Kubernetes; puede ser K3s o RKE2</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-steps">
<title>Pasos</title>
<orderedlist numeration="arabic">
<listitem>
<para>Instale el chart de Helm de Upgrade Controller en el clúster de gestión:</para>
<screen language="bash" linenumbering="unnumbered">helm install upgrade-controller oci://registry.suse.com/edge/charts/upgrade-controller --version 303.0.1+up0.1.1 --create-namespace --namespace upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Compruebe que Upgrade Controller se ha desplegado:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Compruebe el pod de Upgrade Controller:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>Compruebe los registros del pod de Upgrade Controller:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n upgrade-controller-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-how">
<title>¿Cómo funciona Upgrade Controller?</title>
<para>Para actualizar la versión de Edge, Upgrade Controller introduce dos nuevos
<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> de Kubernetes:</para>
<itemizedlist>
<listitem>
<para>UpgradePlan (<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>): creado
por el usuario; contiene configuraciones relacionadas con la actualización
de una versión de Edge.</para>
</listitem>
<listitem>
<para>ReleaseManifest (<xref
linkend="components-upgrade-controller-extensions-release-manifest"/>):
creado por Upgrade Controller; contiene las versiones de los componentes
específicas de una versión concreta de Edge. <emphasis role="strong">Este
archivo no debe ser editado por los usuarios.</emphasis></para>
</listitem>
</itemizedlist>
<para>Upgrade Controller crea un recurso <literal>ReleaseManifest</literal> que
contiene los datos de los componentes de la versión de Edge especificada por
el usuario en la propiedad <literal>releaseVersion</literal> del recurso
<literal>UpgradePlan</literal>.</para>
<para>Con los datos de los componentes de <literal>ReleaseManifest</literal>,
Upgrade Controller actualiza los componentes de la versión de Edge en el
orden siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Sistema operativo (SO) (<xref
linkend="components-upgrade-controller-how-os"/>)</para>
</listitem>
<listitem>
<para>Kubernetes (<xref linkend="components-upgrade-controller-how-k8s"/>)</para>
</listitem>
<listitem>
<para>Componentes adicionales (<xref
linkend="components-upgrade-controller-how-additional"/>)</para>
</listitem>
</orderedlist>
<note>
<para>Durante el proceso de actualización, Upgrade Controller envía continuamente
información sobre la actualización al recurso <literal>UpgradePlan</literal>
creado. Para obtener más información sobre cómo realizar un seguimiento del
proceso de actualización, consulte la sección correspondiente (<xref
linkend="components-upgrade-controller-how-track"/>).</para>
</note>
<section xml:id="components-upgrade-controller-how-os">
<title>Actualización del sistema operativo</title>
<para>Para actualizar el sistema operativo, Upgrade Controller crea planes de SUC
(<xref linkend="components-system-upgrade-controller"/>) que siguen este
formato de nombre:</para>
<itemizedlist>
<listitem>
<para>Para los planes de SUC relacionados con actualizaciones del SO del nodo de
plano de control:
<literal>control-plane-&lt;nombre-so&gt;-&lt;versión-so&gt;-&lt;sufijo&gt;</literal>.</para>
</listitem>
<listitem>
<para>Para los planes de SUC relacionados con actualizaciones del SO del nodo de
trabajador:
<literal>workers-&lt;nombre-so&gt;-&lt;versión-so&gt;-&lt;sufijo&gt;</literal>.</para>
</listitem>
</itemizedlist>
<para>Basándose en estos planes, SUC crea cargas de trabajo en cada nodo del
clúster que realizan la actualización real del sistema operativo.</para>
<para>Dependiendo del recurso <literal>ReleaseManifest</literal>, la actualización
del sistema operativo puede incluir:</para>
<itemizedlist>
<listitem>
<para>Solo actualizaciones de paquetes: para casos en los que la versión del
sistema operativo no cambie entre las versiones de Edge.</para>
</listitem>
<listitem>
<para>Migración completa del sistema operativo: para casos en los que la versión
del sistema operativo cambie entre versiones de Edge.</para>
</listitem>
</itemizedlist>
<para>La actualización se ejecuta <emphasis role="strong">nodo</emphasis> a nodo,
comenzando por los nodos de plano de control. Solo cuando finalice la
actualización del nodo de plano de control, se empezarán a actualizar los
nodos de trabajador.</para>
<note>
<para>Upgrade Controller configura los planes de SUC del sistema operativo para
realizar un <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drenaje</link>
de los nodos del clúster si este tiene más de <emphasis
role="strong">un</emphasis> nodo del tipo especificado.</para>
<para>En los clústeres en los que hay <emphasis role="strong">más de un</emphasis>
nodo de plano de control y hay <emphasis role="strong">un único</emphasis>
nodo de trabajador, el drenaje se realizará solo para los nodos de plano de
control, y viceversa.</para>
<para>Para obtener información sobre cómo inhabilitar el drenaje de nodos,
consulte la sección sobre UpgradePlan (<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>).</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-k8s">
<title>Actualización de Kubernetes</title>
<para>Para actualizar la distribución de Kubernetes de un clúster, Upgrade
Controller crea planes de SUC (<xref
linkend="components-system-upgrade-controller"/>) con el siguiente formato
de nombre:</para>
<itemizedlist>
<listitem>
<para>Para planes de SUC relacionados con actualizaciones de Kubernetes en nodos
de plano de control:
<literal>control-plane-&lt;versión-k8s&gt;-&lt;sufijo&gt;</literal>.</para>
</listitem>
<listitem>
<para>Para planes de SUC relacionados con actualizaciones de Kubernetes en nodos
de trabajador:
<literal>workers-&lt;versión-k8s&gt;-&lt;sufijo&gt;</literal>.</para>
</listitem>
</itemizedlist>
<para>Basándose en estos planes, SUC crea cargas de trabajo en cada nodo del
clúster que realizan la actualización real de Kubernetes.</para>
<para>La actualización de Kubernetes se realizará <emphasis role="strong">nodo a
nodo</emphasis>, comenzando por los nodos de plano de control. Solo cuando
finalice la actualización de los nodos de plano de control se empezarán a
actualizar los nodos de trabajador.</para>
<note>
<para>Upgrade Controller configura los planes de SUC de Kubernetes para realizar
un <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drenaje</link>
de los nodos del clúster si este tiene más de <emphasis role="strong">un
nodo</emphasis> del tipo especificado.</para>
<para>En los clústeres en los que hay <emphasis role="strong">más de un</emphasis>
nodo de plano de control y hay <emphasis role="strong">un único</emphasis>
nodo de trabajador, el drenaje se realizará solo para los nodos de plano de
control, y viceversa.</para>
<para>Para obtener información sobre cómo inhabilitar el drenaje de nodos,
consulte la <xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-additional">
<title>Actualización de componentes adicionales</title>
<para>Actualmente, todos los componentes adicionales se instalan mediante charts
de Helm. Para obtener una lista completa de los componentes de una versión
específica, consulte las Notas de la versión (<xref
linkend="release-notes"/>).</para>
<para>Para los charts de Helm desplegados a través de EIB (<xref
linkend="components-eib"/>), Upgrade Controller actualiza la <link
xl:href="https://docs.rke2.io/helm#using-the-helm-crd">CR de
HelmChart</link> existente de cada componente.</para>
<para>Para los charts de Helm desplegados fuera de EIB, Upgrade Controller crea un
recurso <literal>HelmChart</literal> para cada componente.</para>
<para>Tras la creación/actualización del recurso <literal>HelmChart</literal>,
Upgrade Controller se basa en <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>
para detectar este cambio y proceder con la actualización real del
componente.</para>
<para>Los charts se actualizan secuencialmente según su orden en
<literal>ReleaseManifest</literal>. También se pueden pasar valores
adicionales a través de <literal>UpgradePlan</literal>. Si la versión de un
chart permanece sin cambios en la nueva versión de SUSE Edge, no se
actualizará. Para obtener más información al respecto, consulte la <xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</section>
</section>
<section xml:id="components-upgrade-controller-extensions">
<title>Extensiones de la API de Kubernetes</title>
<para>Extensiones a la API de Kubernetes introducidas por Upgrade Controller.</para>
<section xml:id="components-upgrade-controller-extensions-upgrade-plan">
<title>UpgradePlan</title>
<para>Upgrade Controller introduce un nuevo <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recurso
personalizado</link> de Kubernetes llamado <literal>UpgradePlan</literal>.</para>
<para><literal>UpgradePlan</literal> sirve como mecanismo de instrucción para
Upgrade Controller y admite las siguientes configuraciones:</para>
<itemizedlist>
<listitem>
<para><literal>releaseVersion</literal>: versión de Edge a la que se va a
actualizar el clúster. La versión debe seguir el sistema <link
xl:href="https://semver.org">semántico</link> de versiones y debe obtenerse
de las Notas de la versión (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para><literal>disableDrain</literal>: (<emphasis
role="strong">opcional</emphasis>) indica a Upgrade Controller si debe
inhabilitar el <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drenaje</link>
de nodos. Resulta útil cuando se tienen cargas de trabajo con <link
xl:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">presupuestos
de interrupción</link>.</para>
<itemizedlist>
<listitem>
<para>Ejemplo de inhabilitación del drenaje del nodo de plano de control:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true</screen>
</listitem>
<listitem>
<para>Ejemplo de inhabilitación del drenaje del nodo de plano de control y del
nodo de trabajador:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true
    worker: true</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>helm</literal>: (<emphasis role="strong">opcional</emphasis>)
especifica valores adicionales para los componentes instalados a través de
Helm.</para>
<warning>
<para>Se recomienda utilizar este campo únicamente para valores que sean críticos
para las actualizaciones. Las actualizaciones estándares de valores de chart
deben realizarse después de que los charts correspondientes se hayan
actualizado a la siguiente versión.</para>
</warning>
<itemizedlist>
<listitem>
<para>Ejemplo:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  helm:
  - chart: foo
    values:
      bar: baz</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-upgrade-controller-extensions-release-manifest">
<title>ReleaseManifest</title>
<para>Upgrade Controller introduce un nuevo <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recurso
personalizado</link> de Kubernetes llamado
<literal>ReleaseManifest</literal>.</para>
<para>El recurso <literal>ReleaseManifest</literal> lo crea Upgrade Controller y
contiene datos de componentes para <emphasis role="strong">una</emphasis>
versión específica de Edge. Esto significa que cada actualización de la
versión de Edge estará representada por un recurso
<literal>ReleaseManifest</literal> diferente.</para>
<warning>
<para>El recurso ReleaseManifest siempre debe crearlo Upgrade Controller.</para>
<para>No es recomendable crear o editar manualmente los recursos
<literal>ReleaseManifest</literal>. Si un usuario decide hacerlo, será
<emphasis role="strong">bajo su propia responsabilidad</emphasis>.</para>
</warning>
<para>Estos son algunos de los componentes que incluye ReleaseManifest:</para>
<itemizedlist>
<listitem>
<para>Datos del sistema operativo: versión, arquitecturas compatibles, datos
adicionales sobre actualizaciones, etc.</para>
</listitem>
<listitem>
<para>Datos de distribución de Kubernetes: versiones admitidas de <link
xl:href="https://docs.rke2.io">RKE2</link>/<link
xl:href="https://k3s.io">K3s</link></para>
</listitem>
<listitem>
<para>Datos de componentes adicionales: datos del chart de Helm de SUSE
(ubicación, versión, nombre, etc.)</para>
</listitem>
</itemizedlist>
<para>Para ver un ejemplo de recurso ReleaseManifest, consulte la <link
xl:href="https://github.com/suse-edge/upgrade-controller/blob/main/config/samples/lifecycle_v1alpha1_releasemanifest.yaml">documentación
original</link>. <emphasis>Tenga en cuenta que se trata solo de un ejemplo,
no de un recurso <literal>ReleaseManifest</literal> válido.</emphasis></para>
</section>
</section>
<section xml:id="components-upgrade-controller-how-track">
<title>Seguimiento del proceso de actualización</title>
<para>Esta sección sirve como medio para realizar un seguimiento y depurar el
proceso de actualización que inicia Upgrade Controller después de que el
usuario haya creado un recurso <literal>UpgradePlan</literal>.</para>
<section xml:id="components-upgrade-controller-how-track-general">
<title>General</title>
<para>La información general sobre el estado del proceso de actualización se puede
consultar en las condiciones de estado del recurso UpgradePlan.</para>
<para>El estado del recurso UpgradePlan se puede consultar de la siguiente manera:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get upgradeplan &lt;upgradeplan_name&gt; -n upgrade-controller-system -o yaml</screen>
<formalpara>
<title>Ejemplo de UpgradePlan en ejecución:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.3.1
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Control plane nodes are being upgraded
    reason: InProgress
    status: "False"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Kubernetes upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Rancher upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Longhorn upgrade is not yet started
    reason: Pending
    status: Unknown
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: MetalLB upgrade is not yet started
    reason: Pending
    status: Unknown
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: CDI upgrade is not yet started
    reason: Pending
    status: Unknown
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: KubeVirt upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: NeuVector upgrade is not yet started
    reason: Pending
    status: Unknown
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: EndpointCopierOperator upgrade is not yet started
    reason: Pending
    status: Unknown
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Elemental upgrade is not yet started
    reason: Pending
    status: Unknown
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: SRIOV upgrade is not yet started
    reason: Pending
    status: Unknown
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Akri upgrade is not yet started
    reason: Pending
    status: Unknown
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Metal3 upgrade is not yet started
    reason: Pending
    status: Unknown
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: RancherTurtles upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherTurtlesUpgraded
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
<para>Aquí puede ver todos los componentes para los que Upgrade Controller
intentará programar una actualización. Cada condición sigue el siguiente
formato:</para>
<itemizedlist>
<listitem>
<para><literal>lastTransitionTime</literal>: la hora a la que esta condición del
componente pasó de un estado a otro por última vez.</para>
</listitem>
<listitem>
<para><literal>message</literal>: mensaje que indica el estado actual de
actualización de la condición específica del componente.</para>
</listitem>
<listitem>
<para><literal>reason</literal>: el estado actual de actualización de la condición
específica del componente. Los valores posibles para
<literal>reasons</literal> son:</para>
<itemizedlist>
<listitem>
<para><literal>Succeeded</literal>: la actualización del componente específico se
ha realizado correctamente.</para>
</listitem>
<listitem>
<para><literal>Failed</literal>: la actualización del componente específico ha
fallado.</para>
</listitem>
<listitem>
<para><literal>InProgress</literal>: la actualización del componente específico
está en curso.</para>
</listitem>
<listitem>
<para><literal>Pending</literal>: la actualización del componente específico aún
no se ha programado.</para>
</listitem>
<listitem>
<para><literal>Skipped</literal>: el componente específico no se encuentra en el
clúster, por lo que se omitirá su actualización.</para>
</listitem>
<listitem>
<para><literal>Error</literal>: se ha producido un error transitorio en un
componente específico.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>status</literal>: el estado de <literal>type</literal> (tipo) de la
condición actual. Puede ser <literal>True</literal> (Verdadero),
<literal>False</literal> (Falso) o <literal>Unknown</literal> (Desconocido).</para>
</listitem>
<listitem>
<para><literal>type</literal>: indicador del componente actualmente actualizado.</para>
</listitem>
</itemizedlist>
<para>Upgrade Controller crea planes de SUC para las condiciones de componentes de
tipo <literal>OSUpgraded</literal> y
<literal>KubernetesUpgraded</literal>. Para realizar un seguimiento más
detallado de los planes de SUC creados para estos componentes, consulte la
<xref linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<para>Es posible seguir realizando un seguimiento de todos los demás tipos de
condiciones de los componentes mediante la visualización de los recursos
creados para ellos por <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>.
Para obtener más información, consulte la <xref
linkend="components-upgrade-controller-how-track-helm"/>.</para>
<para>Un recurso UpgradePlan programado por Upgrade Controller se puede marcar
como <literal>successful</literal> (correcto) si cumple estas condiciones:</para>
<orderedlist numeration="arabic">
<listitem>
<para>No hay condiciones de componentes con el estado <literal>Pending</literal> o
<literal>InProgress</literal>.</para>
</listitem>
<listitem>
<para>La propiedad <literal>lastSuccessfulReleaseVersion</literal> apunta a la
versión de <literal>releaseVersion</literal> especificada en la
configuración de UpgradePlan. <emphasis>Upgrade Controller añade esta
propiedad al estado de UpgradePlan después de que el proceso de
actualización se ha completado correctamente.</emphasis></para>
</listitem>
</orderedlist>
<formalpara>
<title>Ejemplo de <literal>UpgradePlan</literal> correcto:</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.3.1
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:48Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:59Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart rancher upgrade succeeded
    reason: Succeeded
    status: "True"
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart longhorn is not installed
    reason: Skipped
    status: "False"
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Specified version of chart metallb is already installed
    reason: Skipped
    status: "False"
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart cdi is not installed
    reason: Skipped
    status: "False"
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart kubevirt is not installed
    reason: Skipped
    status: "False"
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart neuvector-crd is not installed
    reason: Skipped
    status: "False"
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Specified version of chart endpoint-copier-operator is already installed
    reason: Skipped
    status: "False"
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Chart elemental-operator upgrade succeeded
    reason: Succeeded
    status: "True"
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:27:15Z"
    message: Chart sriov-crd is not installed
    reason: Skipped
    status: "False"
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:27:16Z"
    message: Chart akri is not installed
    reason: Skipped
    status: "False"
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:27:19Z"
    message: Chart metal3 is not installed
    reason: Skipped
    status: "False"
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:27:27Z"
    message: Chart rancher-turtles is not installed
    reason: Skipped
    status: "False"
    type: RancherTurtlesUpgraded
  lastSuccessfulReleaseVersion: 3.3.1
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
</section>
<section xml:id="components-upgrade-controller-how-track-helm">
<title>Helm Controller</title>
<para>Esta sección explica cómo realizar un seguimiento de los recursos creados
por <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>.</para>
<note>
<para>En los pasos siguientes se da por supuesto que <literal>kubectl</literal> se
ha configurado para conectarse al clúster en el que se ha desplegado Upgrade
Controller.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Localice el recurso <literal>HelmChart</literal> para el componente
específico:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmcharts -n kube-system</screen>
</listitem>
<listitem>
<para>Use el nombre del recurso <literal>HelmChart</literal> para localizar el pod
de actualización que creó <literal>helm-controller</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n kube-system

# Example for Rancher
kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n kube-system
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-tv9wn   0/1     Completed   0          16m</screen>
</listitem>
<listitem>
<para>Consulte los registros del pod específico del componente:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n kube-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-known-issues">
<title>Limitaciones conocidas</title>
<itemizedlist>
<listitem>
<para>Las actualizaciones de clústeres descendentes aún no se gestionan mediante
Upgrade Controller. Para obtener información sobre cómo actualizar clústeres
descendentes, consulte el <xref linkend="day2-downstream-clusters"/>.</para>
</listitem>
<listitem>
<para>Upgrade Controller espera que cualquier chart de Helm de SUSE Edge adicional
que se despliegue a través de EIB (<xref linkend="components-eib"/>) tenga
su <link xl:href="https://docs.rke2.io/helm#using-the-helm-crd">CR de
HelmChart</link> desplegada en el espacio de nombres
<literal>kube-system</literal>. Para ello, configure la propiedad
<literal>installationNamespace</literal> en su archivo de definición de
EIB. Para obtener más información, consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">documentación
original</link>.</para>
</listitem>
<listitem>
<para>Actualmente, Upgrade Controller no tiene forma de determinar la versión
actual de Edge que se está ejecutando en el clúster de gestión. Asegúrese de
proporcionar una versión de Edge superior a la versión actual que se está
ejecutando en el clúster.</para>
</listitem>
<listitem>
<para>Actualmente, Upgrade Controller solo admite actualizaciones en entornos
<emphasis role="strong">no aislados</emphasis>. Aún no es posible realizar
actualizaciones en <emphasis role="strong">entornos aislados</emphasis>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Multi-Linux Manager está incluido en SUSE Edge y proporciona
automatización y control con el fin de mantener el sistema operativo
subyacente SUSE Linux Micro constantemente actualizado en todos los nodos de
su despliegue periférico.</para>
<para>Para obtener más información, consulte el <xref linkend="quickstart-suma"/>
y la <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">documentación
de SUSE Multi-Linux Manager</link>.</para>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>Guías prácticas</title>
<partintro>
<para>Guías y prácticas recomendadas</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>MetalLB en K3s (con el modo de capa 2)</title>
<para>MetalLB es una implementación de equilibrador de carga para clústeres de
Kubernetes en bare metal que utiliza protocolos de enrutamiento estándar.</para>
<para>En esta guía se explica cómo desplegar MetalLB en el modo de capa 2 (L2).</para>
<section xml:id="id-why-use-this-method-2">
<title>Por qué usar este método</title>
<para>MetalLB es una opción conveniente para el equilibrio de carga en clústeres
bare metal de Kubernetes por varias razones:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Integración nativa con Kubernetes: MetalLB se integra a la perfección con
Kubernetes, lo que facilita el despliegue y la gestión mediante las
herramientas y prácticas habituales de Kubernetes.</para>
</listitem>
<listitem>
<para>Compatibilidad bare metal: a diferencia de los equilibradores de carga
basados en la nube, MetalLB se ha diseñado específicamente para despliegues
locales en las que los sistemas de equilibrio de carga tradicionales podrían
no estar disponibles o no ser viables.</para>
</listitem>
<listitem>
<para>Compatibilidad con múltiples protocolos: MetalLB admite el modo de capa 2 y
BGP (Border Gateway Protocol, protocolo de gateway de frontera), lo que
proporciona flexibilidad para diferentes arquitecturas y requisitos de red.</para>
</listitem>
<listitem>
<para>Alta disponibilidad: dado que distribuye el equilibrio de carga entre varios
nodos, MetalLB garantiza una alta disponibilidad y fiabilidad para sus
servicios.</para>
</listitem>
<listitem>
<para>Escalabilidad: MetalLB puede manejar despliegues a gran escala, y se va
adaptando con su clúster de Kubernetes para satisfacer la demanda creciente.</para>
</listitem>
</orderedlist>
<para>En el modo de capa 2 (L2), un nodo asume la responsabilidad de anunciar un
servicio a la red local. Desde la perspectiva de la red, simplemente parece
que esa máquina tiene varias direcciones IP asignadas a su interfaz de red.</para>
<para>La mayor ventaja del modo de capa 2 es su universalidad: funciona en
cualquier red Ethernet sin necesidad de hardware especial, ni siquiera de
routers sofisticados.</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>MetalLB en K3s (con L2)</title>
<para>En este inicio rápido, se utilizará el modo L2. Esto significa que no se
necesita equipo de red especial, solo tres IP libres dentro del rango de
red.</para>
</section>
<section xml:id="id-prerequisites-7">
<title>Requisitos previos</title>
<itemizedlist>
<listitem>
<para>Un clúster K3s donde se vaya a desplegar MetalLB.</para>
</listitem>
</itemizedlist>
<warning>
<para>K3S incluye su propio equilibrador de carga de servicios llamado
Klipper. <link xl:href="https://metallb.universe.tf/configuration/k3s/">Debe
inhabilitarlo para ejecutar MetalLB</link>. Para inhabilitar Klipper, hay
que instalar K3s mediante el indicador
<literal>--disable=servicelb</literal>.</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>Tres direcciones IP libres dentro del rango de red. En este ejemplo
<literal>192.168.122.10-192.168.122.12</literal></para>
</listitem>
</itemizedlist>
<important>
<para>Debe asegurarse de que esas direcciones IP no están asignadas. En un entorno
DHCP esas direcciones no deben formar parte del pool DHCP para evitar dobles
asignaciones.</para>
</important>
</section>
<section xml:id="id-deployment">
<title>Despliegue</title>
<para>Utilizaremos el chart de Helm de MetalLB publicado como parte de la solución
SUSE Edge:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>Configuración</title>
<para>En este punto, la instalación ya está completa. Ahora hay que <link
xl:href="https://metallb.universe.tf/configuration/">configurarla</link> con
nuestros valores de ejemplo:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>Ahora, está lista para usarse. Es posible personalizar muchas cosas en el
modo L2, por ejemplo:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">La
IPv6 y los servicios de doble pila</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">El
control de la asignación automática de direcciones</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">La
reducción del alcance de la asignación de direcciones a espacios de nombres
y servicios específicos</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">La
limitación del conjunto de nodos desde los que se puede anunciar el
servicio</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">La
especificación de las interfaces de red desde las que se puede anunciar la
IP del equilibrador de carga</link></para>
</listitem>
</itemizedlist>
<para>Y mucho más para <link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>.</para>
<section xml:id="traefik-and-metallb">
<title>Traefik y MetalLB</title>
<para>Traefik se despliega por defecto con K3s (<link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">se puede
inhabilitar</link> con <literal>--disable=traefik</literal>) y se expone por
defecto como <literal>LoadBalancer</literal> (para usarse con Klipper). Sin
embargo, como hay que inhabilitar Klipper, el servicio Traefik para Ingress
sigue siendo de tipo <literal>LoadBalancer</literal>. Por lo tanto, en el
momento de desplegar MetalLB, la primera IP se asignará automáticamente para
Traefik Ingress.</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>Esto se aplicará más tarde en el proceso (<xref
linkend="ingress-with-metallb"/>).</para>
</section>
</section>
<section xml:id="id-usage">
<title>Uso</title>
<para>Vamos a crear un despliegue de ejemplo:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>Y, por último, el servicio:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>Veámoslo en acción:</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<section xml:id="ingress-with-metallb">
<title>Ingress con MetalLB</title>
<para>Como Traefik ya sirve como controlador de Ingress, podemos exponer todo el
tráfico HTTP/HTTPS a través de un objeto <literal>Ingress</literal> como:</para>
<screen language="bash" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>Y, a continuación:</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>Verifique que MetalLB funciona correctamente:</para>
<screen language="bash" linenumbering="unnumbered">% arping hellok3s.${IP}.sslip.io

ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>En el ejemplo anterior, el flujo de tráfico es el siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal> se resuelve en la IP real.</para>
</listitem>
<listitem>
<para>Después, el tráfico se gestiona con el pod
<literal>metallb-speaker</literal>.</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal> redirige el tráfico al controlador
<literal>traefik</literal>.</para>
</listitem>
<listitem>
<para>Por último, Traefik reenvía la petición al servicio
<literal>hello-kubernetes</literal>.</para>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>MetalLB delante del servidor de Kubernetes API</title>
<para>Esta guía explica cómo usar un servicio MetalLB para exponer la API de
RKE2/K3s externamente en un clúster de HA con tres nodos de plano de
control. Para lograrlo, se crearán manualmente un servicio de Kubernetes de
tipo <literal>LoadBalancer</literal> y puntos finales. Los puntos finales
mantienen las IP de todos los nodos de plano de control disponibles en el
clúster. Para que el punto final esté continuamente sincronizado con los
eventos que ocurren en el clúster (añadir/eliminar un nodo o que un nodo se
desconecte), se desplegará el Endpoint Copier Operator (<xref
linkend="components-eco"/>). Este operador supervisa los eventos que ocurren
en el punto final <literal>kubernetes</literal> por defecto y actualiza el
que se gestiona automáticamente para mantenerlos sincronizados. Dado que el
servicio gestionado es de tipo <literal>LoadBalancer</literal>, MetalLB le
asigna una IP externa (<literal>ExternalIP</literal> estática. Esta
<literal>ExternalIP</literal> se utilizará para comunicarse con el servidor
de API.</para>
<section xml:id="id-prerequisites-8">
<title>Requisitos previos</title>
<itemizedlist>
<listitem>
<para>Tres hosts para desplegar RKE2/K3s encima.</para>
<itemizedlist>
<listitem>
<para>Asegúrese de que los hosts tienen nombres distintos.</para>
</listitem>
<listitem>
<para>Para realizar pruebas, pueden ser máquinas virtuales.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Al menos 2 IP disponibles en la red (una para Traefik/Nginx y otra para el
servicio gestionado).</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-rke2k3s">
<title>Instalación de RKE2/K3s</title>
<note>
<para>Si no desea utilizar un clúster nuevo, omita este paso y continúe con el
siguiente.</para>
</note>
<para>En primer lugar, se debe reservar una IP libre en la red que se utilizará
posteriormente para <literal>ExternalIP</literal> en el servicio gestionado.</para>
<para>Use SSH para el primer host e instale la distribución deseada en modo de
clúster.</para>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC="server \
 --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" sh -

systemctl enable rke2-server.service
systemctl start rke2-server.service

# Fetch the cluster token:
RKE2_TOKEN=$(tr -d '\n' &lt; /var/lib/rancher/rke2/server/node-token)</screen>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>Asegúrese de que se proporciona el indicador
<literal>--disable=servicelb</literal> en el comando <literal>k3s
server</literal>.</para>
</note>
<important>
<para>A partir de ahora, los comandos deben ejecutarse en el equipo local.</para>
</important>
<para>Para acceder al servidor de API desde el exterior, se utilizará la IP de la
máquina virtual RKE2/K3s.</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
export KUBE_DISTRIBUTION=&lt;k3s/rke2&gt;

scp ${NODE_IP}:/etc/rancher/${KUBE_DISTRIBUTION}/${KUBE_DISTRIBUTION}.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-cluster">
<title>Configuración de un clúster existente</title>
<note>
<para>Este paso solo es válido si pretende utilizar un clúster RKE2/K3s existente.</para>
</note>
<para>Para usar un clúster existente, deben modificarse los indicadores
<literal>tls-san</literal>. Además, el equilibrador de carga
<literal>servicelb</literal> debe inhabilitarse para K3s.</para>
<para>Para cambiar los indicadores de los servidores RKE2 o K3s, es necesario
modificar el archivo <literal>/etc/systemd/system/rke2.service</literal> o
el archivo <literal>/etc/systemd/system/k3s.service</literal> en todas las
máquinas virtuales del clúster, en función de la distribución.</para>
<para>Los indicadores deben insertarse en <literal>ExecStart</literal>. Por
ejemplo:</para>
<para>Para RKE2:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/rke2 \
    server \
        '--write-kubeconfig-mode=644' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>Para K3s:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>A continuación, se deben ejecutar los siguientes comandos para cargar las
nuevas configuraciones:</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart ${KUBE_DISTRIBUTION}</screen>
</section>
<section xml:id="id-installing-metallb">
<title>Instalación de MetalLB</title>
<para>Para desplegar <literal>MetalLB</literal>, se puede usar la guía de MetalLB
en K3s (<xref linkend="guides-metallb-k3s"/>).</para>
<para><emphasis role="strong">NOTA:</emphasis> asegúrese de que las direcciones IP
del IPAddressPool <literal>ip-pool</literal> no se solapan con las
direcciones IP previamente seleccionadas para el servicio
<literal>LoadBalancer</literal>.</para>
<para>Cree un <literal>IpAddressPool</literal> independiente que se utilizará solo
para el servicio gestionado.</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Instalación de Endpoint Copier Operator</title>
<screen language="bash" linenumbering="unnumbered">helm install \
endpoint-copier-operator oci://registry.suse.com/edge/charts/endpoint-copier-operator \
--namespace endpoint-copier-operator \
--create-namespace</screen>
<para>El comando de arriba desplegará el operador
<literal>endpoint-copier-operator</literal> con dos réplicas. Una será la
líder y la otra asumirá el papel de líder si fuera necesario.</para>
<para>Ahora se debe desplegar el servicio <literal>kubernetes-vip</literal>, que
el operador reconciliará, y se creará un punto final con los puertos y la IP
configurados.</para>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  type: LoadBalancer
EOF</screen>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer
EOF</screen>
<para>Verifique que el servicio <literal>kubernetes-vip</literal> tenga la
dirección IP correcta:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para>Asegúrese de que los recursos de punto final
<literal>kubernetes-vip</literal> y <literal>kubernetes</literal> del
espacio de nombres <literal>default</literal> dirijan a las mismas IP.</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpoints kubernetes kubernetes-vip</screen>
<para>Si todo está correcto, lo último que queda por hacer es usar
<literal>VIP_SERVICE_IP</literal> en nuestro <literal>Kubeconfig</literal>.</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>A partir de ahora, todo <literal>kubectl</literal> pasará por el servicio
<literal>kubernetes-vip</literal>.</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>Adición de nodos de plano de control</title>
<para>Para supervisar todo el proceso, se pueden abrir dos pestañas de terminal
más.</para>
<para>Primer terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>Segundo terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpoints</screen>
<para>Ahora, ejecute los comandos siguientes en el segundo y el tercer nodo.</para>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
systemctl enable rke2-server.service


mkdir -p /etc/rancher/rke2/
cat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml
server: https://${VIP_SERVICE_IP}:9345
token: ${RKE2_TOKEN}
EOF

systemctl start rke2-server.service</screen>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Despliegues en entornos aislados con Edge Image Builder</title>
<section xml:id="id-intro">
<title>Introducción</title>
<para>Esta guía explica cómo desplegar varios componentes de SUSE Edge en entornos
completamente aislados en SUSE Linux Micro 6.1 utilizando Edge Image
Builder(EIB) (<xref linkend="components-eib"/>). De este modo, podrá
arrancar en una imagen personalizada lista para arrancar (CRB) creada por
EIB y desplegar los componentes especificados en un clúster RKE2 o K3s sin
conexión a Internet ni pasos manuales. Esta configuración es muy conveniente
para los clientes que desean preparar de antemano todos los artefactos
necesarios para el despliegue en su imagen del sistema operativo para que
estén disponible de inmediato en el arranque.</para>
<para>Muestra una instalación en un entorno aislado de:</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-security"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-storage"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIB analizará y descargará previamente todas las imágenes a las que se haga
referencia en los charts de Helm y los manifiestos de Kubernetes
proporcionados. Sin embargo, es posible que algunos de ellos intenten
extraer imágenes de contenedores y crear recursos de Kubernetes basados en
estas imágenes en el entorno de ejecución. Si se dieran estos casos, para
configurar un entorno completamente aislado, habrá que especificar
manualmente las imágenes necesarias en el archivo de definición.</para>
</warning>
</section>
<section xml:id="id-prerequisites-9">
<title>Requisitos previos</title>
<para>Si sigue esta guía, se entiende que ya está familiarizado con EIB (<xref
linkend="components-eib"/>). Si no es así, siga la guía de inicio rápido
(<xref linkend="quickstart-eib"/>) para comprender mejor los conceptos que
se muestran en la práctica de abajo.</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Configuración de red de Libvirt</title>
<note>
<para>Como ejemplo de un despliegue en entorno aislado, en esta guía se usa una
red <literal>libvirt</literal> aislada simulada, por lo que la configuración
que se muestra se adapta en consecuencia. Para sus despliegues, puede que
tenga que modificar la configuración de <literal>host1.local.yaml</literal>
que se introducirá en el siguiente paso.</para>
</note>
<para>Si desea utilizar la misma configuración de red de
<literal>libvirt</literal>, siga adelante. Si no, salte a la <xref
linkend="config-dir-creation"/>.</para>
<para>Vamos a crear una configuración de red aislada con un rango de direcciones
IP <literal>192.168.100.2/24</literal> para DHCP:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>Ahora, solo queda crear la red y ponerla en marcha:</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>Configuración del directorio base</title>
<para>La configuración del directorio base es la misma para todos los componentes,
así que la configuraremos aquí.</para>
<para>Primero, creamos los subdirectorios necesarios:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>Asegúrese de añadir la imagen base que tenga previsto utilizar en el
directorio <literal>base-images</literal>. Esta guía se centra en la imagen
ISO de autoinstalación que se encuentra <link
xl:href="https://www.suse.com/download/sle-micro/">aquí</link>.</para>
<para>Copiemos la imagen descargada:</para>
<screen language="shell" linenumbering="unnumbered">cp SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIB nunca modifica la entrada de la imagen base.</para>
</note>
<para>Vamos a crear un archivo que contenga la configuración de red deseada:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>Esta configuración garantiza la presencia de los siguientes elementos en los
sistemas aprovisionados (utilizando la dirección MAC especificada):</para>
<itemizedlist>
<listitem>
<para>una interfaz Ethernet con una dirección IP estática</para>
</listitem>
<listitem>
<para>enrutamiento</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>nombre de host (<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>La estructura de archivos resultante debería tener el siguiente aspecto:</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>Archivo de definición base</title>
<para>Edge Image Builder usa <emphasis>archivos de definición</emphasis> para
modificar las imágenes de SUSE Linux Micro. Estos archivos contienen la
mayoría de las opciones configurables. Muchas de estas opciones se repetirán
en las diferentes secciones de los componentes, por lo que las mostraremos y
explicaremos aquí.</para>
<tip>
<para>La lista completa de opciones de personalización del archivo de definición
se encuentra en la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#image-definition-file">documentación
original</link>.</para>
</tip>
<para>Veamos los campos que estarán presentes en todos los archivos de definición:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.32.4+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para>La sección <literal>image</literal> es obligatoria y especifica la imagen de
entrada, su arquitectura y su tipo, y cómo se llamará la imagen de salida.</para>
<para>La sección <literal>operatingSystem</literal> es opcional y contiene la
configuración para permitir el inicio de sesión en los sistemas
aprovisionados con el nombre de usuario/contraseña
<literal>root/eib</literal>.</para>
<para>La sección <literal>kubernetes</literal> es opcional y define el tipo y la
versión de Kubernetes. Vamos a usar la distribución RKE2. Utilice
<literal>kubernetes.version: v1.32.4+k3s1</literal> si desea K3s en su
lugar. A menos que se configure explícitamente a través del campo
<literal>kubernetes.nodes</literal>, todos los clústeres que arranquemos en
esta guía serán de un solo nodo.</para>
<para>La sección <literal>embeddedArtifactRegistry</literal> incluirá todas las
imágenes a las que solo se hace referencia y solo se extraen en el tiempo de
ejecución del componente específico.</para>
</section>
<section xml:id="rancher-install">
<title>Instalación de Rancher</title>
<note>
<para>El despliegue de Rancher (<xref linkend="components-rancher"/>) de este
ejemplo es muy reducido para que sea más fácil de explicar. En sus
despliegues reales, pueden ser necesarios artefactos adicionales,
dependiendo de su configuración.</para>
</note>
<para>Los recursos de la versión <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.11.2">Rancher
2.11.2</link> contienen un archivo <literal>rancher-images.txt</literal> que
muestra todas las imágenes necesarias para una instalación en entornos
aislados.</para>
<para>Hay más de 600 imágenes de contenedor en total, lo que significa que la
imagen lista para arrancar resultante tendría aproximadamente 30 GB. Para
nuestra instalación de Rancher, reduciremos la lista a la configuración de
trabajo más pequeña posible. A partir de ahí, es posible añadir cualquier
imagen que necesite en sus despliegues.</para>
<para>Crearemos el archivo de definición e incluiremos la lista de imágenes
desglosada:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.32.4+rke2r1
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.11.2
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.15.3
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v7.0.1
    - name: registry.rancher.com/rancher/calico-cni:v3.29.0-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.4.0
    - name: registry.rancher.com/rancher/flannel-cni:v1.4.1-rancher1
    - name: registry.rancher.com/rancher/fleet-agent:v0.12.2
    - name: registry.rancher.com/rancher/fleet:v0.12.2
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.22-build20250110
    - name: registry.rancher.com/rancher/hardened-calico:v3.29.2-build20250306
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.9.0-build20241126
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.6.2-build20250306
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.0-build20241126
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.24.0-build20241211
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.19-k3s1-build20250306
    - name: registry.rancher.com/rancher/hardened-flannel:v0.26.5-build20250306
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.2-build20250110
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.32.3-rke2r1-build20250312
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.1.4-build20250108
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.8.0-build20250131
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.32.3-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.4-build20250113
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.13
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.4
    - name: registry.rancher.com/rancher/kubectl:v1.32.2
    - name: registry.rancher.com/rancher/kuberlr-kubectl:v4.0.2
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.31
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher125
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.9.5
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.1-hardened1
    - name: registry.rancher.com/rancher/prom-prometheus:v2.55.1
    - name: registry.rancher.com/rancher/prometheus-federator:v3.0.1
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.4-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.4-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.11.1
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v6.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.7.1
    - name: registry.rancher.com/rancher/rancher:v2.11.1
    - name: registry.rancher.com/rancher/remotedialer-proxy:v0.4.4
    - name: registry.rancher.com/rancher/rke-tools:v0.1.111
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.32.0-rc3.0.20241220224140-68fbd1a6b543-build20250101
    - name: registry.rancher.com/rancher/rke2-runtime:v1.32.3-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.32.3-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.6.0
    - name: registry.rancher.com/rancher/shell:v0.4.0
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.32.3-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.32.3-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.12-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.15.2
    - name: registry.rancher.com/rancher/ui-plugin-catalog:4.0.1
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.2</screen>
<para>En comparación con la lista completa de más de 600 imágenes, esta versión
reducida solo tiene unas 60, por lo que la nueva imagen lista para arrancar
solo ocupa unos 7 GB.</para>
<para>También necesitamos crear un archivo de valores de Helm para Rancher:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para>Establece en <literal>systemDefaultRegistry</literal> el valor
<literal>registry.rancher.com</literal> permite a Rancher buscar
automáticamente imágenes en el registro de artefactos integrados iniciado
dentro de la imagen lista para arrancar en el arranque. Si se omite este
campo, quizás no se encuentren las imágenes de contenedor en el nodo.</para>
</warning>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file eib-iso-definition.yaml</screen>
<para>El resultado debe ser parecido a esto:</para>
<screen language="console" linenumbering="unnumbered">Downloading file: dl-manifest-1.yaml 100% |██████████████████████████████████████████████████████████████████████████████| (583/583 kB, 12 MB/s)
Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████| (56/56, 8 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% |███████████████████████████████████████████████████████████| (644/644 MB, 29 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% |█████████████████████████████████████████████████████████| (400/400 MB, 29 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% |███████████████████████████████████████████████████████████████████████████| (36/36 MB, 30 MB/s)
Downloading file: sha256sum-amd64.txt 100% |█████████████████████████████████████████████████████████████████████████████| (4.3/4.3 kB, 29 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Una vez que se aprovisiona un nodo que usa la imagen creada, es posible
verificar la instalación de Rancher:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cattle-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>El resultado debería ser similar a lo siguiente, donde se muestra que todo
se ha desplegado correctamente:</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/helm-operation-6l6ld                        0/2     Completed   0          107s
pod/helm-operation-8tk2v                        0/2     Completed   0          2m2s
pod/helm-operation-blnrr                        0/2     Completed   0          2m49s
pod/helm-operation-hdcmt                        0/2     Completed   0          3m19s
pod/helm-operation-m74c7                        0/2     Completed   0          97s
pod/helm-operation-qzzr4                        0/2     Completed   0          2m30s
pod/helm-operation-s9jh5                        0/2     Completed   0          3m
pod/helm-operation-tq7ts                        0/2     Completed   0          2m41s
pod/rancher-99d599967-ftjkk                     1/1     Running     0          4m15s
pod/rancher-webhook-79798674c5-6w28t            1/1     Running     0          2m27s
pod/system-upgrade-controller-56696956b-trq5c   1/1     Running     0          104s

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/rancher           ClusterIP   10.43.255.80   &lt;none&gt;        80/TCP,443/TCP   4m15s
service/rancher-webhook   ClusterIP   10.43.7.238    &lt;none&gt;        443/TCP          2m27s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rancher                     1/1     1            1           4m15s
deployment.apps/rancher-webhook             1/1     1            1           2m27s
deployment.apps/system-upgrade-controller   1/1     1            1           104s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/rancher-99d599967                     1         1         1       4m15s
replicaset.apps/rancher-webhook-79798674c5            1         1         1       2m27s
replicaset.apps/system-upgrade-controller-56696956b   1         1         1       104s</screen>
<para>Cuando vamos a <literal>https://192.168.100.50.sslip.io</literal> e
iniciamos sesión con la contraseña <literal>adminadminadmin</literal> que
establecimos anteriormente, accedemos al panel de control de Rancher:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width="100%"/>
</imageobject>
<textobject><phrase>rancher en entorno aislado</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="suse-security-install">
<title>Instalación de SUSE Security</title>
<para>A diferencia de la instalación de Rancher, la instalación de SUSE Security
no requiere ningún tratamiento especial en EIB. EIB instalará en un entorno
aislado automáticamente todas las imágenes requeridas por su componente
subyacente NeuVector.</para>
<para>Vamos a crear el archivo de definición:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>También crearemos un archivo de valores de Helm para NeuVector:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file eib-iso-definition.yaml</screen>
<para>El resultado debe ser parecido a esto:</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 4 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████| (5/5, 13 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Después de que se haya aprovisionado un nodo con la imagen creada, podemos
verificar la instalación de SUSE Security:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>El resultado debería ser similar a lo siguiente, donde se muestra que todo
se ha desplegado correctamente:</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/neuvector-cert-upgrader-job-bxbnz           0/1     Completed   0          3m39s
pod/neuvector-controller-pod-7d854bfdc7-nhxjf   1/1     Running     0          3m44s
pod/neuvector-enforcer-pod-ct8jm                1/1     Running     0          3m44s

NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.234.241   &lt;none&gt;        443/TCP                         3m44s
service/neuvector-svc-controller          ClusterIP   None            &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   3m44s
service/neuvector-svc-crd-webhook         ClusterIP   10.43.50.190    &lt;none&gt;        443/TCP                         3m44s

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          3m44s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           3m44s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-7d854bfdc7   1         1         1       3m44s

NAME                                        SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-cert-upgrader-pod   0 0 1 1 *   &lt;none&gt;     True      0        &lt;none&gt;          3m44s
cronjob.batch/neuvector-updater-pod         0 0 * * *   &lt;none&gt;     False     0        &lt;none&gt;          3m44s

NAME                                    STATUS     COMPLETIONS   DURATION   AGE
job.batch/neuvector-cert-upgrader-job   Complete   1/1           7s         3m39s</screen>
</section>
<section xml:id="suse-storage-install">
<title>Instalación de SUSE Storage</title>
<para>La <link
xl:href="https://longhorn.io/docs/1.8.1/deploy/install/airgap/">documentación
oficial</link> de Longhorn contiene un archivo
<literal>longhorn-images.txt</literal> que muestra todas las imágenes
necesarias para una instalación en entornos aislados. Vamos a incluir sus
homólogos duplicados del registro de contenedores de Rancher en nuestro
archivo de definición. Vamos a crear el archivo:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: [reg-code]
    packageList:
      - open-iscsi
kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 106.2.0+up1.8.1
      - name: longhorn-crd
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        version: 106.2.0+up1.8.1
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.2.0
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.13.2
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.2.0
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.13.0
    - name: registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.15.0
    - name: registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.52
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.8.1</screen>
<note>
<para>Observará que el archivo de definición muestra el paquete
<literal>open-iscsi</literal>. Este paquete es obligatorio, ya que Longhorn
se basa en un daemon <literal>iscsiadm</literal> que se ejecuta en los
diferentes nodos para proporcionar volúmenes persistentes a Kubernetes.</para>
</note>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file eib-iso-definition.yaml</screen>
<para>El resultado debe ser parecido a esto:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 20956 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Una vez aprovisionado un nodo con la imagen creada, podemos verificar la
instalación de Longhorn:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>El resultado debería ser similar a lo siguiente, donde se muestra que todo
se ha desplegado correctamente:</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/csi-attacher-787fd9c6c8-sf42d                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-tb82p                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-zhc6s                       1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-b2v9s                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-hwllt                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-mlrpk                    1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-t54zk                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-vdt5d                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-x9kh4                        1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-r62gr                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-vrwjn                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-z65nb                    1/1     Running   0          2m28s
pod/engine-image-ei-4623b511-9vhkb                      1/1     Running   0          3m13s
pod/instance-manager-6f95fd57d4a4cd0459e469d75a300552   1/1     Running   0          2m43s
pod/longhorn-csi-plugin-gx98x                           3/3     Running   0          2m28s
pod/longhorn-driver-deployer-55f9c88499-fbm6q           1/1     Running   0          3m28s
pod/longhorn-manager-dpdp7                              2/2     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-gg5hq                        1/1     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-s49jc                        1/1     Running   0          3m28s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.77.89    &lt;none&gt;        9502/TCP   3m28s
service/longhorn-backend              ClusterIP   10.43.56.17    &lt;none&gt;        9500/TCP   3m28s
service/longhorn-conversion-webhook   ClusterIP   10.43.54.73    &lt;none&gt;        9501/TCP   3m28s
service/longhorn-frontend             ClusterIP   10.43.22.82    &lt;none&gt;        80/TCP     3m28s
service/longhorn-recovery-backend     ClusterIP   10.43.45.143   &lt;none&gt;        9503/TCP   3m28s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-4623b511   1         1         1       1            1           &lt;none&gt;          3m13s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          2m28s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          3m28s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           2m28s
deployment.apps/csi-provisioner            3/3     3            3           2m28s
deployment.apps/csi-resizer                3/3     3            3           2m28s
deployment.apps/csi-snapshotter            3/3     3            3           2m28s
deployment.apps/longhorn-driver-deployer   1/1     1            1           3m28s
deployment.apps/longhorn-ui                2/2     2            2           3m28s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-787fd9c6c8               3         3         3       2m28s
replicaset.apps/csi-provisioner-74486b95c6            3         3         3       2m28s
replicaset.apps/csi-resizer-859d4557fd                3         3         3       2m28s
replicaset.apps/csi-snapshotter-6f69c6c8cc            3         3         3       2m28s
replicaset.apps/longhorn-driver-deployer-55f9c88499   1         1         1       3m28s
replicaset.apps/longhorn-ui-59c85fcf94                2         2         2       3m28s</screen>
</section>
<section xml:id="kubevirt-install">
<title>Instalación de KubeVirt y CDI</title>
<para>Los charts de Helm tanto para KubeVirt como para CDI solo instalan sus
operadores respectivos. Depende de los operadores desplegar el resto de
sistemas, lo que significa que tendremos que incluir todas las imágenes de
contenedor necesarias en nuestro archivo de definición. Vamos a crearlo:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: kubevirt
        repositoryName: suse-edge
        version: 303.0.0+up0.5.0
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi
        repositoryName: suse-edge
        version: 303.0.0+up0.5.0
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.6/cdi-uploadproxy:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-uploadserver:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-apiserver:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-controller:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-importer:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/cdi-cloner:1.60.1-150600.3.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-api:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-controller:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-launcher:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-handler:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-exportproxy:1.3.1-150600.5.9.1
    - name: registry.suse.com/suse/sles/15.6/virt-exportserver:1.3.1-150600.5.9.1</screen>
<para>Vamos a crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file eib-iso-definition.yaml</screen>
<para>El resultado debe ser parecido a esto:</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 48 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 4 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>Una vez aprovisionado un nodo con la imagen construida, podemos verificar la
instalación tanto de KubeVirt como de CDI.</para>
<para>Verifique KubeVirt:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>El resultado debería ser similar a lo siguiente, donde se muestra que todo
se ha desplegado correctamente:</para>
<screen language="console" linenumbering="unnumbered">NAME                                  READY   STATUS    RESTARTS   AGE
pod/virt-api-59cb997648-mmt67         1/1     Running   0          2m34s
pod/virt-controller-69786b785-7cc96   1/1     Running   0          2m8s
pod/virt-controller-69786b785-wq2dz   1/1     Running   0          2m8s
pod/virt-handler-2l4dm                1/1     Running   0          2m8s
pod/virt-operator-7c444cff46-nps4l    1/1     Running   0          3m1s
pod/virt-operator-7c444cff46-r25xq    1/1     Running   0          3m1s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.167.109   &lt;none&gt;        443/TCP   2m36s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m36s
service/virt-api                      ClusterIP   10.43.18.202    &lt;none&gt;        443/TCP   2m36s
service/virt-exportproxy              ClusterIP   10.43.142.188   &lt;none&gt;        443/TCP   2m36s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   2m8s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m34s
deployment.apps/virt-controller   2/2     2            2           2m8s
deployment.apps/virt-operator     2/2     2            2           3m1s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-59cb997648         1         1         1       2m34s
replicaset.apps/virt-controller-69786b785   2         2         2       2m8s
replicaset.apps/virt-operator-7c444cff46    2         2         2       3m1s

NAME                            AGE    PHASE
kubevirt.kubevirt.io/kubevirt   3m1s   Deployed</screen>
<para>Verifique CDI:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>El resultado debería ser similar a lo siguiente, donde se muestra que todo
se ha desplegado correctamente:</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-5598c9bf47-pqfxw     1/1     Running   0          3m44s
pod/cdi-deployment-7cbc5db7f8-g46z7    1/1     Running   0          3m44s
pod/cdi-operator-777c865745-2qcnj      1/1     Running   0          3m48s
pod/cdi-uploadproxy-646f4cd7f7-fzkv7   1/1     Running   0          3m44s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.2.224    &lt;none&gt;        443/TCP    3m44s
service/cdi-prometheus-metrics   ClusterIP   10.43.237.13   &lt;none&gt;        8080/TCP   3m44s
service/cdi-uploadproxy          ClusterIP   10.43.114.91   &lt;none&gt;        443/TCP    3m44s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m44s
deployment.apps/cdi-deployment    1/1     1            1           3m44s
deployment.apps/cdi-operator      1/1     1            1           3m48s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m44s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-5598c9bf47     1         1         1       3m44s
replicaset.apps/cdi-deployment-7cbc5db7f8    1         1         1       3m44s
replicaset.apps/cdi-operator-777c865745      1         1         1       3m48s
replicaset.apps/cdi-uploadproxy-646f4cd7f7   1         1         1       3m44s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>Solución de problemas</title>
<para>Si tiene algún problema al crear las imágenes o desea probar y depurar el
proceso, consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">documentación
original</link>.</para>
</section>
</chapter>
<chapter xml:id="guides-kiwi-builder-images">
<title>Creación de imágenes actualizadas de SUSE Linux Micro con Kiwi</title>
<para>En esta sección se explica cómo generar imágenes actualizadas de SUSE Linux
Micro para utilizarlas con Edge Image Builder, con Cluster API (CAPI) +
Metal<superscript>3</superscript>, o para escribir la imagen de disco
directamente en un dispositivo de bloques. Este proceso resulta útil en
situaciones en las que es necesario incluir los parches más recientes en las
imágenes de arranque iniciales del sistema inicial (para minimizar la
transferencia de parches tras la instalación), o para situaciones en las que
se usa CAPI, donde se prefiere reinstalar el sistema operativo con una
imagen nueva en lugar de actualizar los hosts in situ.</para>
<para>Este proceso hace uso de <link
xl:href="https://osinside.github.io/kiwi/">Kiwi</link> para crear la
imagen. SUSE Edge incluye una versión en contenedor que simplifica todo el
proceso con una utilidad de ayuda integrada que permite especificar el
<emphasis role="strong">perfil</emphasis> de destino necesario. El perfil
define el tipo de imagen de destino que se requiere. A continuación se
muestran los tipos más comunes:</para>
<itemizedlist>
<listitem>
<para>"<emphasis role="strong">Base</emphasis>": una imagen de disco de SUSE Linux
Micro con un conjunto de paquetes reducido (incluye Podman).</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-SelfInstall</emphasis>": una imagen de
autoinstalación basada en la imagen "Base" anterior.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-RT</emphasis>": igual que "Base", pero usa un
kernel en tiempo real (rt).</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Base-RT-SelfInstall</emphasis>": una imagen de
autoinstalación basada en la imagen "Base-RT" anterior.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Default</emphasis>": una imagen de disco de SUSE
Linux Micro basada en la imagen "Base" anterior, pero con algunas
herramientas más, como la pila de virtualización, Cockpit y salt-minion.</para>
</listitem>
<listitem>
<para>"<emphasis role="strong">Default-SelfInstall</emphasis>": una imagen de
autoinstalación basada en la imagen "Default" anterior.</para>
</listitem>
</itemizedlist>
<para>Consulte la documentación de <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-deployment-images/index.html#alp-images-installer-type">SUSE
Linux Micro 6.1</link> para obtener más detalles.</para>
<para>Este proceso funciona tanto para arquitecturas AMD64/Intel 64 como AArch64,
aunque no todos los perfiles de imagen están disponibles para ambas
arquitecturas. Por ejemplo, en SUSE Edge 3.3, donde se utiliza SUSE Linux
Micro 6.1, no hay disponible actualmente ningún perfil con un kernel en
tiempo real (es decir, "Base-RT" o "Base-RT-SelfInstall") para AArch64.</para>
<note>
<para>Es preciso usar un host de creación con la misma arquitectura de las
imágenes que se están creando. En otras palabras, para crear una imagen
AArch64, hay que utilizar un host de creación AArch64; y lo mismo ocurre
para AMD64/Intel 64. Actualmente, no se admiten las creaciones cruzadas.</para>
</note>
<section xml:id="id-prerequisites-10">
<title>Requisitos previos</title>
<para>El creador de imágenes Kiwi requiere lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Un host ("sistema de creación") SUSE Linux Micro 6.1 con la misma
arquitectura que la imagen que se va a crear.</para>
</listitem>
<listitem>
<para>El sistema de creación debe haberse registrado vía
<literal>SUSEConnect</literal> (el registro se utiliza para extraer los
paquetes más recientes de los repositorios de SUSE).</para>
</listitem>
<listitem>
<para>Una conexión a Internet que pueda usarse para extraer los paquetes
necesarios. Si se conecta a través de proxy y el host de creación debe estar
preconfigurado.</para>
</listitem>
<listitem>
<para>SELinux debe estar inhabilitado en el host de creación (ya que el etiquetado
de SELinux tiene lugar en el contenedor y puede entrar en conflicto con la
directiva del host).</para>
</listitem>
<listitem>
<para>Al menos 10 GB de espacio libre en disco para alojar la imagen de
contenedor, la raíz de creación y las imágenes de salida resultantes.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started-2">
<title>Inicio</title>
<para>Debido a ciertas limitaciones, actualmente es necesario inhabilitar
SELinux. Conéctese al host de creación de imágenes de SUSE Linux Micro 6.1 y
asegúrese de que SELinux está inhabilitado:</para>
<screen language="console" linenumbering="unnumbered"># setenforce 0</screen>
<para>Cree un directorio de salida que se compartirá con el contenedor de creación
de Kiwi para guardar las imágenes resultantes:</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/output</screen>
<para>Obtenga la imagen más reciente de Kiwi Builder del registro de SUSE:</para>
<screen language="console" linenumbering="unnumbered"># podman pull registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0
(...)</screen>
</section>
<section xml:id="id-building-the-default-image">
<title>Creación de la imagen por defecto</title>
<para>Este es el comportamiento por defecto del contenedor de imágenes de Kiwi si
no se proporcionan argumentos durante la ejecución de la imagen del
contenedor. El comando siguiente ejecuta <literal>Podman</literal> con dos
directorios asignados al contenedor:</para>
<itemizedlist>
<listitem>
<para>El directorio del repositorio de paquetes
<literal>/etc/zypp/repos.d</literal> de SUSE Linux Micro del host
subyacente.</para>
</listitem>
<listitem>
<para>El directorio de salida <literal>~/output</literal> creado arriba.</para>
</listitem>
</itemizedlist>
<para>El contenedor de imágenes de Kiwi requiere que se ejecute el guion de ayuda
<literal>build-image</literal> como:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<note>
<para>Es de esperar que, si ejecuta este guion por primera vez, <emphasis
role="strong">falle</emphasis> poco después de comenzar con el error
"<emphasis role="strong">ERROR: Early loop device test failed, please retry
the container run.</emphasis>" (La prueba del dispositivo de bucle temprano
ha fallado. Vuelva a intentar la ejecución del contenedor). Esto es un
síntoma de que se están creando dispositivos de bucle en el sistema host
subyacente que no son inmediatamente visibles dentro de la imagen del
contenedor. Solo tiene que ejecutar de nuevo el comando y el guion
continuará sin más problemas.</para>
</note>
<para>Al cabo de unos minutos, las imágenes se encuentran en el directorio de
salida local:</para>
<screen language="console" linenumbering="unnumbered">(...)
INFO: Image build successful, generated images are available in the 'output' directory.

# ls -1 output/
SLE-Micro.x86_64-6.1.changes
SLE-Micro.x86_64-6.1.packages
SLE-Micro.x86_64-6.1.raw
SLE-Micro.x86_64-6.1.verified
build
kiwi.result
kiwi.result.json</screen>
</section>
<section xml:id="id-building-images-with-other-profiles">
<title>Creación de imágenes con otros perfiles</title>
<para>Para crear perfiles de imagen distintos, se utiliza la opción de comando
"<emphasis role="strong">-p</emphasis>" en el guion de ayuda de la imagen
del contenedor de Kiwi. Por ejemplo, para crear la imagen ISO "<emphasis
role="strong">Default-SelfInstall</emphasis>":</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall
(...)</screen>
<note>
<para>Para evitar la pérdida de datos, Kiwi negará a ejecutarse si hay imágenes en
el directorio <literal>output</literal>. Es necesario eliminar el contenido
del directorio de salida antes de proceder con <literal>rm -f
output/*</literal>.</para>
</note>
<para>Como alternativa, puede crear una imagen ISO de autoinstalación con el
kernel en tiempo real ("<emphasis role="strong">kernel-rt</emphasis>"):</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0 build-image -p Base-RT-SelfInstall
(...)</screen>
</section>
<section xml:id="id-building-images-with-large-sector-sizes">
<title>Creación de imágenes con sectores de gran tamaño</title>
<para>Hay hardware que requiere una imagen con un tamaño de sectores grandes; por
ejemplo, de <emphasis role="strong">4096 bytes</emphasis> en lugar de los
512 bytes estándar. El creador en contenedor de Kiwi permite generar
imágenes con bloques grandes especificando el parámetro "<emphasis
role="strong">-b</emphasis>". Por ejemplo, para crear una imagen "<emphasis
role="strong">Default-SelfInstall</emphasis>" con sectores grandes:</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall -b
(...)</screen>
</section>
<section xml:id="id-using-a-custom-kiwi-image-definition-file">
<title>Uso de un archivo de definición de imagen de Kiwi personalizado</title>
<para>En ciertos casos de uso avanzados, es posible usar un archivo de definición
de imagen de Kiwi personalizado (<literal>SL-Micro.kiwi</literal>) junto con
los guiones de postcreación necesarios. Para ello, habrá que anular las
definiciones predeterminadas preempaquetadas por el equipo de SUSE Edge.</para>
<para>Cree un directorio nuevo y asígnelo a la imagen de contenedor donde busca el
guion de ayuda (<literal>/micro-sdk/defs</literal>):</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/mydefs/
# cp /path/to/SL-Micro.kiwi ~/mydefs/
# cp /path/to/config.sh ~/mydefs/
# podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output -v ~/mydefs/:/micro-sdk/defs/ \
    -it registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<warning>
<para>Esto solo es necesario para casos de uso avanzados y puede causar problemas
de compatibilidad. Póngase en contacto con su representante de SUSE para
obtener más ayuda.</para>
</warning>
<para>Para obtener los archivos de definición de imagen de Kiwi por defecto
incluidos en el contenedor, se pueden utilizar los siguientes comandos:</para>
<screen language="console" linenumbering="unnumbered">$ podman create --name kiwi-builder registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi .
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi.4096 .
$ podman rm kiwi-builder
$ ls ./SL-Micro.*
(...)</screen>
</section>
</chapter>
<chapter xml:id="guides-clusterclass-example">
<title>Uso de ClusterClass para desplegar clústeres descendentes</title>
<section xml:id="id-introduction">
<title>Introducción</title>
<para>El aprovisionamiento de clústeres de Kubernetes es una tarea compleja que
requiere una gran experiencia a la hora de configurar los componentes del
clúster. A medida que las configuraciones se vuelven más complejas, o que
las exigencias de los diferentes proveedores introducen numerosas
definiciones de recursos específicas para cada uno, la creación de clústeres
puede resultar abrumadora. Afortunadamente, la API de clústeres de
Kubernetes (Cluster API, CAPI) ofrece un enfoque elegante y claro aún más
reforzado gracias a ClusterClass. Esta función introduce un modelo basado en
plantillas que permite definir una clase de clúster reutilizable que
encapsula la complejidad y promueve la coherencia.</para>
</section>
<section xml:id="id-what-is-clusterclass">
<title>¿Qué es ClusterClass?</title>
<para>El proyecto CAPI introdujo la función ClusterClass como un cambio de
paradigma en la gestión del ciclo de vida de los clústeres de Kubernetes
mediante la adopción de una metodología basada en plantillas para la
instanciación de clústeres. En lugar de definir los recursos de forma
independiente para cada clúster, los usuarios definen una ClusterClass, que
sirve como un modelo completo y reutilizable. Esta representación abstracta
encapsula el estado y la configuración deseados de un clúster de Kubernetes,
lo que permite la creación rápida y coherente de múltiples clústeres que se
ajustan a las especificaciones definidas. Esta abstracción reduce la carga
de configuración, lo que da como resultado manifiestos de despliegue más
manejables. Esto significa que los componentes centrales de un clúster de
carga de trabajo se definen a nivel de clase, lo que permite a los usuarios
utilizar estas plantillas como variantes de clústeres de Kubernetes que se
pueden reutilizar una o varias veces para el aprovisionamiento de
clústeres. La implementación de ClusterClass ofrece varias ventajas clave
que abordan los retos inherentes a la gestión tradicional de CAPI a gran
escala:</para>
<itemizedlist>
<listitem>
<para>Reducción sustancial de la complejidad y la redundancia de YAML</para>
</listitem>
<listitem>
<para>Procesos de mantenimiento y actualización optimizados</para>
</listitem>
<listitem>
<para>Mejora en la coherencia y la estandarización entre despliegues</para>
</listitem>
<listitem>
<para>Mejora en la escalabilidad y las capacidades de automatización</para>
</listitem>
<listitem>
<para>Gestión declarativa y control de versiones robusto</para>
</listitem>
</itemizedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="clusterclass.png" width="100%"/>
</imageobject>
<textobject><phrase>clusterclass</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-example-of-current-capi-provisioning-file">
<title>Ejemplo de archivo de aprovisionamiento de CAPI actual</title>
<para>El despliegue de un clúster de Kubernetes que aproveche Cluster API (CAPI) y
el proveedor RKE2 requiere la definición de varios recursos
personalizados. Estos recursos definen el estado deseado del clúster y su
infraestructura subyacente, lo que permite a CAPI orquestar el ciclo de vida
del aprovisionamiento y gestión. El fragmento de código siguiente ilustra
los tipos de recursos que deben configurarse:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Cluster</emphasis>: este recurso encapsula
configuraciones generales, incluida la topología de red que regirá la
comunicación entre nodos y la detección de servicios. Además, establece los
vínculos esenciales con la especificación del plano de control y el recurso
del proveedor de infraestructura designado, informando así a CAPI sobre la
arquitectura de clúster deseada y la infraestructura subyacente sobre la que
se aprovisionará.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3Cluster</emphasis>: este recurso define los
atributos a nivel de infraestructura exclusivos de Metal3; por ejemplo, el
punto final externo a través del cual se podrá acceder al servidor de API de
Kubernetes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlane</emphasis>: este recurso define las
características y el comportamiento de los nodos de plano de control del
clúster. Dentro de esta especificación, se configuran parámetros como el
número deseado de réplicas del plano de control (fundamental para garantizar
una alta disponibilidad y tolerancia a fallos), la versión específica de la
distribución de Kubernetes (alineada con la versión elegida de RKE2) y la
estrategia para distribuir actualizaciones en los componentes de plano de
control. Además, este recurso dicta la interfaz de red de contenedores (CNI)
que se empleará dentro del clúster y facilita la inyección de
configuraciones específicas del agente, a menudo aprovechando Ignition para
el aprovisionamiento automatizado y sin interrupciones de los agentes RKE2
en los nodos de plano de control.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3MachineTemplate</emphasis>: este recurso actúa
como un modelo para la creación de las instancias de computación
individuales que formarán los nodos de trabajador del clúster de Kubernetes,
definiendo la imagen que se utilizará.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3DataTemplate</emphasis>: como complemento de
Metal3MachineTemplate, el recurso Metal3DataTemplate permite especificar
metadatos adicionales para los equipos recién aprovisionados.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: emea-spa-cluster-3
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: emea-spa-cluster-3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  controlPlaneEndpoint:
    host: 192.168.122.203
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: emea-spa-cluster-3
  replicas: 1
  version: v1.32.4+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  registrationMethod: "control-plane-endpoint"
  registrationAddress: 192.168.122.203
  serverConfig:
    cni: cilium
    cniMultusEnable: true
    tlsSan:
      - 192.168.122.203
      - https://192.168.122.203.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 303.0.0+up0.2.1
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 303.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - 192.168.122.203/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          node: group-3
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
</section>
<section xml:id="id-transforming-the-capi-provisioning-file-to-clusterclass">
<title>Transformación del archivo de aprovisionamiento de CAPI en ClusterClass</title>
<section xml:id="id-clusterclass-definition">
<title>Definición de ClusterClass</title>
<para>El siguiente código define un recurso ClusterClass, una plantilla
declarativa para desplegar de forma coherente un tipo específico de clúster
de Kubernetes. Esta especificación incluye configuraciones comunes de
infraestructura y plano de control, lo que permite un aprovisionamiento
eficiente y una gestión uniforme del ciclo de vida en toda una flota de
clústeres. En el siguiente ejemplo de ClusterClass, algunas variables se
sustituirán durante el proceso de instanciación del clúster por los valores
reales. En el ejemplo se utilizan las siguientes variables:</para>
<itemizedlist>
<listitem>
<para><literal>controlPlaneMachineTemplate</literal>: este es el nombre que define
la referencia de la plantilla del equipo de plano de control que se va a
utilizar.</para>
</listitem>
<listitem>
<para><literal>controlPlaneEndpointHost</literal>: este es el nombre de host o la
dirección IP del punto final del plano de control.</para>
</listitem>
<listitem>
<para><literal>tlsSan</literal>: este es el nombre alternativo del sujeto TLS para
el punto final del plano de control.</para>
</listitem>
</itemizedlist>
<para>El archivo de definición de ClusterClass se basa en los tres recursos
siguientes:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">ClusterClass</emphasis>: este recurso encapsula toda
la definición de ClusterClass, incluyendo el plano de control y las
plantillas de infraestructura. Además, incluye la lista de variables que se
sustituirán durante el proceso de instanciación.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2ControlPlaneTemplate</emphasis>: este recurso
define la plantilla del plano de control y especifica la configuración
deseada para los nodos de plano de control. Incluye parámetros como el
número de réplicas, la versión de Kubernetes y la CNI que se va a
utilizar. Además, algunos parámetros se sustituirán por los valores
correctos durante el proceso de instanciación.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3ClusterTemplate</emphasis>: este recurso
define la plantilla de infraestructura y especifica la configuración deseada
para la infraestructura subyacente. Incluye parámetros como el punto final
del plano de control y el indicador noCloudProvider. Además, algunos
parámetros se sustituirán por los valores correctos durante el proceso de
instanciación.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlaneTemplate
metadata:
  name: example-controlplane-type2
  namespace: emea-spa
spec:
  template:
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: example-controlplane    # This will be replaced by the patch applied in each cluster instances
        namespace: emea-spa
      replicas: 1
      version: v1.32.4+rke2r1
      rolloutStrategy:
        type: "RollingUpdate"
        rollingUpdate:
          maxSurge: 1
      registrationMethod: "control-plane-endpoint"
      registrationAddress: "default"  # This will be replaced by the patch applied in each cluster instances
      serverConfig:
        cni: cilium
        cniMultusEnable: true
        tlsSan:
          - "default"  # This will be replaced by the patch applied in each cluster instances
      agentConfig:
        format: ignition
        additionalUserData:
          config: |
            default
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3ClusterTemplate
metadata:
  name: example-cluster-template-type2
  namespace: emea-spa
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: "default"  # This will be replaced by the patch applied in each cluster instances
        port: 6443
      noCloudProvider: true
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: example-clusterclass-type2
  namespace: emea-spa
spec:
  variables:
    - name: controlPlaneMachineTemplate
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: controlPlaneEndpointHost
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: tlsSan
      required: true
      schema:
        openAPIV3Schema:
          type: array
          items:
            type: string
  infrastructure:
    ref:
      kind: Metal3ClusterTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: example-cluster-template-type2
  controlPlane:
    ref:
      kind: RKE2ControlPlaneTemplate
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      name: example-controlplane-type2
  patches:
    - name: setControlPlaneMachineTemplate
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/infrastructureRef/name"
              valueFrom:
                variable: controlPlaneMachineTemplate
    - name: setControlPlaneEndpoint
      definitions:
        - selector:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: Metal3ClusterTemplate
            matchResources:
              infrastructureCluster: true  # Added to select InfraCluster
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/controlPlaneEndpoint/host"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setRegistrationAddress
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/registrationAddress"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setTlsSan
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/serverConfig/tlsSan"
              valueFrom:
                variable: tlsSan
    - name: updateAdditionalUserData
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/agentConfig/additionalUserData"
              valueFrom:
                template: |
                  config: |
                    variant: fcos
                    version: 1.4.0
                    storage:
                      files:
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: endpoint-copier-operator
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                                targetNamespace: endpoint-copier-operator
                                version: 303.0.0+up0.2.1
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: metallb
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/metallb
                                targetNamespace: metallb-system
                                version: 303.0.0+up0.14.9
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: metallb.io/v1beta1
                              kind: IPAddressPool
                              metadata:
                                name: kubernetes-vip-ip-pool
                                namespace: metallb-system
                              spec:
                                addresses:
                                  - {{ .controlPlaneEndpointHost }}/32
                                serviceAllocation:
                                  priority: 100
                                  namespaces:
                                    - default
                                  serviceSelectors:
                                    - matchExpressions:
                                      - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                              ---
                              apiVersion: metallb.io/v1beta1
                              kind: L2Advertisement
                              metadata:
                                name: ip-pool-l2-adv
                                namespace: metallb-system
                              spec:
                                ipAddressPools:
                                  - kubernetes-vip-ip-pool
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: v1
                              kind: Service
                              metadata:
                                name: kubernetes-vip
                                namespace: default
                                labels:
                                  serviceType: kubernetes-vip
                              spec:
                                ports:
                                - name: rke2-api
                                  port: 9345
                                  protocol: TCP
                                  targetPort: 9345
                                - name: k8s-api
                                  port: 6443
                                  protocol: TCP
                                  targetPort: 6443
                                type: LoadBalancer
                    systemd:
                      units:
                        - name: rke2-preinstall.service
                          enabled: true
                          contents: |
                            [Unit]
                            Description=rke2-preinstall
                            Wants=network-online.target
                            Before=rke2-install.service
                            ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                            [Service]
                            Type=oneshot
                            User=root
                            ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                            ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                            ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                            ExecStartPost=/bin/sh -c "umount /mnt"
                            [Install]
                            WantedBy=multi-user.target</screen>
</section>
<section xml:id="id-cluster-instance-definition">
<title>Definición de la instancia del clúster</title>
<para>En el contexto de ClusterClass, una instancia de clúster es una instancia
concreta y en ejecución de un clúster creado a partir de una ClusterClass
definida. Representa un despliegue concreto con sus configuraciones,
recursos y estado operativo únicos, que se derivan directamente del modelo
especificado en la ClusterClass. Esto incluye el conjunto específico de
equipos, configuraciones de red y componentes de Kubernetes asociados que se
están ejecutando activamente. Comprender la instancia del clúster es
fundamental para gestionar su ciclo de vida, realizar actualizaciones,
ejecutar operaciones de escalado y supervisar un clúster desplegado concreto
que se ha aprovisionado utilizando el marco ClusterClass.</para>
<para>Para definir una instancia de clúster, debemos definir los siguientes
recursos:</para>
<itemizedlist>
<listitem>
<para>Cluster</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate</para>
</listitem>
<listitem>
<para>Metal3DataTemplate</para>
</listitem>
</itemizedlist>
<para>Las variables definidas previamente en la plantilla (archivo de definición
de ClusterClass) se sustituirán por los valores finales para esta
instanciación del clúster:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  topology:
    class: example-clusterclass-type2  # Correct way to reference ClusterClass
    version: v1.32.4+rke2r1
    controlPlane:
      replicas: 1
    variables:                         # Variables to be replaced for this cluster instance
      - name: controlPlaneMachineTemplate
        value: emea-spa-cluster-3-machinetemplate
      - name: controlPlaneEndpointHost
        value: 192.168.122.203
      - name: tlsSan
        value:
          - 192.168.122.203
          - https://192.168.122.203.sslip.io
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3-machinetemplate
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          cluster-type: type2
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine</screen>
<para>Este enfoque permite un proceso más ágil, ya que es posible desplegar un
clúster con solo 3 recursos una vez que se ha definido ClusterClass.</para>
</section>
</section>
</chapter>
</part>
<part xml:id="tips-and-tricks">
<title>Consejos y trucos</title>
<partintro>
<para>Consejos y trucos para componentes de Edge</para>
</partintro>
<chapter xml:id="id-edge-image-builder">
<title>Edge Image Builder</title>
<section xml:id="id-common">
<title>Comunes</title>
<itemizedlist>
<listitem>
<para>Si se encuentra en un entorno que no sea Linux y sigue estas instrucciones
para crear una imagen, es probable que esté ejecutando
<literal>Podman</literal> a través de una máquina virtual. De forma
predeterminada, esta máquina virtual estará configurada para tener asignada
una pequeña cantidad de recursos del sistema, lo que puede provocar
inestabilidad en <literal>Edge Image Builder</literal> durante operaciones
que consumen muchos recursos, como el proceso de resolución de RPM. Deberá
ajustar los recursos de la máquina de Podman, ya sea utilizando Podman
Desktop (icono de configuración → icono de edición de máquina de Podman) o
directamente mediante el <link
xl:href="https://docs.podman.io/en/stable/markdown/podman-machine-set.1.html">comando</link>
<literal>podman-machine-set</literal>.</para>
</listitem>
<listitem>
<para>En este momento, <literal>Edge Image Builder</literal> no puede crear
imágenes en una configuración de arquitectura cruzada, es decir, debe
ejecutarlo en:</para>
<itemizedlist>
<listitem>
<para>Sistemas AArch64 (como Apple Silicon) para crear imágenes
<literal>aarch64</literal> de SL Micro</para>
</listitem>
<listitem>
<para>Sistemas AMD64/Intel 64 para crear imágenes <literal>x86_64</literal> de SL
Micro</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-kubernetes">
<title>Kubernetes</title>
<itemizedlist>
<listitem>
<para>Para crear clústeres de Kubernetes de varios nodos, es necesario ajustar la
sección <literal>kubernetes</literal> del archivo de definición para:</para>
<itemizedlist>
<listitem>
<para>Mostrar todos los nodos de servidor y agente de
<literal>kubernetes.nodes</literal></para>
</listitem>
<listitem>
<para>Establecer una dirección IP virtual que se utilizaría para que todos los
nodos no inicializadores se unan al clúster en
<literal>kubernetes.network.apiVIP</literal></para>
</listitem>
<listitem>
<para>Opcionalmente, configurar un host de API para especificar una dirección de
dominio de acceso al clúster en
<literal>kubernetes.network.apiHost</literal>. Para obtener más información
sobre esta configuración, consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">sección
sobre Kubernetes de la documentación</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Edge Image Builder</literal> se basa en los nombres de host de los
diferentes nodos para determinar su tipo de Kubernetes
(<literal>server</literal> o <literal>agent</literal>). Aunque esta
configuración se gestiona en el archivo de definición, para la configuración
general de red de las máquinas podemos utilizar la configuración DHCP, tal y
como se describe en el <xref linkend="components-nmc"/>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="id-elemental">
<title>Elemental</title>
<section xml:id="id-common-2">
<title>Comunes</title>
<section xml:id="id-expose-rancher-service">
<title>Exposición del servicio Rancher</title>
<para>Cuando se utiliza RKE2 o K3s, es necesario exponer los servicios (Rancher en
este contexto) desde el clúster de gestión, ya que no están expuestos de
forma predeterminada. En RKE2, hay un controlador Ingress NGINX, y en k3s
utiliza Traefik. El flujo de trabajo actual sugiere utilizar MetalLB para
anunciar un servicio (con el modo L2 o con BGP) y el controlador de Ingress
correspondiente para crear un objeto Ingress a través de
<literal>HelmChartConfig</literal>, ya que la creación de un nuevo objeto
Ingress anularía la configuración existente.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Instale Rancher Prime (mediante Helm) y configure los valores necesarios.</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-192.168.64.101.sslip.io
replicas: 1
bootstrapPassword: Admin
global.cattle.psp.enabled: "false"</screen>
<tip>
<para>Siga las instrucciones para la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">instalación
de Rancher</link> de la documentación para obtener más detalles.</para>
</tip>
</listitem>
<listitem>
<para>Cree un servicio LoadBalancer para exponer Rancher.</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local
EOF</screen>
</listitem>
<listitem>
<para>Cree un pool de direcciones IP para el servicio utilizando la dirección IP
que hemos configurado anteriormente en los valores de Helm.</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.64.101/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}
EOF</screen>
</listitem>
<listitem>
<para>Crear un anuncio L2 para el pool de direcciones IP.</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ingress-ippool
EOF</screen>
</listitem>
<listitem>
<para>Asegúrese de que Elemental se ha instalado correctamente.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Instale el operador de Elemental y la interfaz del usuario de Elemental en
los nodos de gestión.</para>
</listitem>
<listitem>
<para>Añada la configuración de Elemental en el nodo descendente junto con un
código de registro, ya que eso hará que Edge Image Builder incluya la opción
de registro remoto para el equipo.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<tip>
<para>Consulte la <xref linkend="install-elemental"/> y la <xref
linkend="configure-elemental"/> para obtener información adicional y
ejemplos.</para>
</tip>
</section>
</section>
<section xml:id="id-hardware-specific">
<title>Específicos del hardware</title>
<section xml:id="id-trusted-platform-module">
<title>Trusted Platform Module</title>
<para>Es necesario gestionar correctamente la configuración del módulo de
plataforma segura <link
xl:href="https://elemental.docs.rancher.com/tpm/">Trusted Platform
Module</link> (TPM). Si no se hace, se producirán errores como este:</para>
<screen language="console" linenumbering="unnumbered">Nov 25 18:17:06 eled elemental-register[4038]: Error: registering machine: cannot generate authentication token: opening tpm for getting attestation data: TPM device not available</screen>
<para>Esto se puede mitigar con uno de estos enfoques:</para>
<itemizedlist>
<listitem>
<para>Habilite TPM en la configuración de la máquina virtual</para>
</listitem>
</itemizedlist>
<para><emphasis>Ejemplo con UTM en MacOS</emphasis></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="tpm.png" width="100%"/> </imageobject>
<textobject><phrase>TPM</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para>Emule TPM utilizando un valor negativo para la semilla de TPM en el recurso
<literal>MachineRegistration</literal>.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: true
        emulated-tpm-seed: -1</screen>
<itemizedlist>
<listitem>
<para>Inhabilite TPM en el recurso <literal>MachineRegistration</literal>.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: false</screen>
</section>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>Integración de productos de otros fabricantes</title>
<partintro>
<para>Integración de herramientas de terceros</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link xl:href="https://nats.io/">NATS</link> es una tecnología de
conectividad diseñada para un mundo cada vez más hiperconectado. Se trata de
una tecnología única que permite a las aplicaciones comunicarse de forma
segura a través de cualquier combinación de proveedores de la nube,
instalaciones locales, dispositivos periféricos, Web y móviles. NATS consta
de una familia de productos de código abierto estrechamente integrados, pero
que pueden desplegarse de forma independiente y sencilla. NATS se utiliza en
miles de empresas de todo el mundo y abarca casos de uso que incluyen
microservicios, edge computing, dispositivos móviles e IoT. También se puede
utilizar para ampliar o sustituir la mensajería tradicional.</para>
<section xml:id="id-architecture">
<title>Arquitectura</title>
<para>NATS es una infraestructura que permite el intercambio de datos entre
aplicaciones en forma de mensajes.</para>
<section xml:id="id-nats-client-applications">
<title>Aplicaciones cliente de NATS</title>
<para>Las bibliotecas cliente NATS permiten que las aplicaciones publiquen, se
suscriban, soliciten y respondan entre diferentes instancias. Estas
aplicaciones se denominan generalmente <literal>aplicaciones
cliente</literal>.</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>Infraestructura de servicios NATS</title>
<para>Los servicios NATS son proporcionados por uno o varios procesos de servidor
NATS configurados para interconectarse entre sí y ofrecer una
infraestructura. Esta infraestructura puede escalarse desde un único proceso
de servidor NATS que se ejecuta en un dispositivo final hasta un
superclúster global público compuesto por muchos clústeres que abarca todos
los principales proveedores de nube y todas las regiones del mundo.</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>Diseño de mensajería sencillo</title>
<para>Con NATS, las aplicaciones pueden comunicarse fácilmente enviando y
recibiendo mensajes. Estos mensajes se dirigen e identifican mediante
cadenas de asunto y no dependen de la ubicación de la red. Los datos se
cifran y se estructuran como mensajes, y son enviados por un editor. El
mensaje es recibido, descifrado y procesado por uno o varios suscriptores.</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATS cuenta con un sistema de persistencia distribuida integrado llamado
JetStream. JetStream se creó para resolver los problemas de transmisión
identificados de la tecnología actual: complejidad, fragilidad y falta de
escalabilidad. JetStream también resuelve el problema del acoplamiento entre
el editor y el suscriptor (los suscriptores deben estar activos y en
funcionamiento para recibir el mensaje cuando se publica). Para obtener más
información sobre NATS JetStream, consulte <link
xl:href="https://docs.nats.io/nats-concepts/jetstream">este
documento</link>.</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>Instalación</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>Instalación de NATS sobre K3s</title>
<para>NATS está diseñado para múltiples arquitecturas, por lo que se puede
instalar fácilmente en K3s (<xref linkend="components-k3s"/>).</para>
<para>Vamos a crear un archivo de valores para sobrescribir los valores
predeterminados de NATS.</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>Ahora, se instala NATS mediante Helm:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>Con el archivo <literal>values.yaml</literal> anterior, los componentes
siguientes estarán en el espacio de nombres <literal>nats</literal>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Una versión de alta disponibilidad de NATS Statefulset que incluye tres
contenedores: el servidor de NATS, el recargador de configuración y sidecars
de métricas.</para>
</listitem>
<listitem>
<para>Un contenedor de caja de NATS, que incluye un conjunto de utilidades
<literal>NATS</literal> que se pueden utilizar para verificar la
configuración.</para>
</listitem>
<listitem>
<para>JetStream también aprovecha su interfaz final de clave-valor incluida con
<literal>PVC</literal> vinculados a los pods.</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>Prueba de la configuración</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>Cree una suscripción de prueba:</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>Envíe un mensaje de prueba:</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>Limpieza</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>NATS como interfaz final para K3s</title>
<para>Uno de los componentes que aprovecha K3s es <link
xl:href="https://github.com/k3s-io/kine">KINE</link>, que es un shim que
permite sustituir etcd por interfaces finales de almacenamiento alternativas
destinadas originalmente a bases de datos relacionales. Dado que JetStream
proporciona una API de clave-valor, esto hace posible utilizar NATS como
interfaz final para el clúster K3s.</para>
<para>Ya existe una solicitud para simplificar la integración de NATS en K3s, pero
los cambios aún <link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">no se
han incluido</link> en las versiones de K3s.</para>
<para>Por eso, el binario de K3s debe crearse manualmente.</para>
<section xml:id="id-building-k3s">
<title>Creación de K3s</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>El siguiente comando añade <literal>nats</literal> en las etiquetas de
creación para habilitar la integración de NATS en K3s:</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>Sustituya &lt;node-ip&gt; con la IP real del nodo donde se iniciará K3s:</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>Para crear K3s localmente se necesita el complemento buildx Docker CLI. Se
puede <link
xl:href="https://github.com/docker/buildx#manual-download">instalar
manualmente</link> si <literal>$ make local</literal> falla.</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>Instalación de la interfaz de línea de comandos de NATS</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>Ejecución de NATS como interfaz final de K3s</title>
<para>Vamos a usar <literal>ssh</literal> en el nodo y a ejecutar K3s con el
indicador <literal>--datastore-endpoint</literal> apuntando a
<literal>nats</literal>.</para>
<note>
<para>El siguiente comando inicia K3s como un proceso en primer plano, por lo que
es posible consultar fácilmente en el registro si hay algún problema. Para
no bloquear el terminal actual, se puede añadir el indicador
<literal>&amp;</literal> antes del comando para iniciarlo como un proceso en
segundo plano.</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>Para hacer permanente el servidor K3s con la interfaz final de NATS en su
máquina virtual <literal>slemicro</literal>, puede ejecutar el siguiente
guion, que crea un servicio <literal>systemd</literal> con la configuración
necesaria.</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>Solución de problemas</title>
<para>Los siguientes comandos se pueden ejecutar en el nodo para verificar que
todo lo relacionado con la transmisión funciona correctamente:</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-suse-linux-micro">
<title>GPU NVIDIA en SUSE Linux Micro</title>
<section xml:id="id-intro-2">
<title>Introducción</title>
<para>Esta guía muestra cómo implementar compatibilidad con GPU NVIDIA a nivel de
host mediante <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">controladores de
código abierto</link> creados previamente en SUSE Linux Micro 6.1. Estos
controladores están integrados en el sistema operativo, en lugar de cargarse
de forma dinámica mediante el operador <link
xl:href="https://github.com/NVIDIA/gpu-operator">GPU Operator</link> de
NVIDIA. Esta configuración es muy recomendable para los clientes que desean
integrar previamente todos los artefactos necesarios para el despliegue en
la imagen y en cuando sea necesario seleccionar dinámicamente la versión del
controlador; es decir, que el usuario pueda seleccionar la versión del
controlador a través de Kubernetes. Al principio de esta guía se explica
cómo desplegar los componentes adicionales en un sistema que ya ha sido
desplegado previamente, y continúa con una sección donde se describe cómo
integrar esta configuración en el despliegue inicial mediante Edge Image
Builder. Si no desea repasar los conceptos básicos y configurarlo todo
manualmente, pase directamente a esa sección.</para>
<para>Es importante destacar que la asistencia para estos controladores la
proporcionan tanto SUSE como NVIDIA en estrecha colaboración, y que el
controlador ha sido creado y lo proporciona SUSE como parte de los
repositorios de paquetes. Sin embargo, si tiene alguna duda o pregunta sobre
la combinación en la que utiliza los controladores, solicite ayuda adicional
a sus gestores de cuentas de SUSE o NVIDIA. Si tiene previsto utilizar <link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE), asegúrese de que está utilizando una <link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">GPU
certificada para NVAIE</link>, lo que <emphasis>puede</emphasis> requerir el
uso de controladores de NVIDIA. Si no está seguro, consulte con su
representante de NVIDIA.</para>
<para>Esta guía <emphasis>no</emphasis> incluye más información sobre la
integración del operador GPU de NVIDIA. Aunque no trata sobre la integración
del operador de GPU de NVIDIA para Kubernetes, puede seguir la mayoría de
los pasos de esta guía para configurar el sistema operativo subyacente y,
simplemente, habilitar el operador de GPU para que utilice los controladores
<emphasis>preinstalados</emphasis> mediante el indicador
<literal>driver.enabled=false</literal> en el chart de Helm del operador de
GPU NVIDIA, donde bastará con seleccionar los controladores instalados en el
host. Encontrará instrucciones más completas de NVIDIA <link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">en
este documento</link>.</para>
</section>
<section xml:id="id-prerequisites-11">
<title>Requisitos previos</title>
<para>Si está siguiendo esta guía, se da por hecho que ya dispone de lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Al menos un host con SUSE Linux Micro 6.1 instalado; puede ser físico o
virtual.</para>
</listitem>
<listitem>
<para>Sus hosts están vinculados a una suscripción obligatoria para poder acceder
al paquete. Hay una versión de evaluación disponible <link
xl:href="https://www.suse.com/download/sle-micro/">aquí</link>.</para>
</listitem>
<listitem>
<para>Una <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">GPU
NVIDIA compatible</link> instalada (o transferida
<emphasis>completamente</emphasis> a la máquina virtual en la que se ejecute
SUSE Linux Micro).</para>
</listitem>
<listitem>
<para>Acceso al usuario root: estas instrucciones dan por sentado que usted es el
usuario root y que <emphasis>no</emphasis> está derivando sus privilegios
mediante <literal>sudo</literal>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>Instalación manual</title>
<para>En esta sección, va a instalar los controladores de NVIDIA directamente en
el sistema operativo SUSE Linux Micro, ya que el controlador de código
abierto de NVIDIA ahora forma parte de los repositorios de paquetes
principales de SUSE Linux Micro. De esta forma, basta con instalar los
paquetes RPM necesarios. No es necesario compilar ni descargar paquetes
ejecutables. A continuación, le guiaremos a través del despliegue de la
generación "G06" del controlador, que admite las GPU más recientes (consulte
<link xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">este
documento</link> para obtener más información). Debe seleccionar la
generación adecuada del controlador de GPU de NVIDIA de su sistema. Para las
GPU modernas, el controlador "G06" es la opción más habitual.</para>
<para>Antes de comenzar, tenga en cuenta que, además del controlador de código
abierto de NVIDIA que SUSE incluye en SUSE Linux Micro, es posible que
necesite componentes adicionales de NVIDIA para su configuración. Por
ejemplo, bibliotecas OpenGL, kits de herramientas CUDA, utilidades de línea
de comandos como <literal>nvidia-smi</literal> y componentes de integración
de contenedores como <literal>nvidia-container-toolkit</literal>. SUSE no
suministra muchos de estos componentes, ya sea porque son software propiedad
de NVIDIA o porque no tiene sentido que los suministremos nosotros en lugar
de NVIDIA. Por lo tanto, como parte de las instrucciones, configuraremos
repositorios adicionales que nos den acceso a dichos componentes y veremos
algunos ejemplos de uso de estas herramientas, de modo que el sistema
resultante será totalmente funcional. Es importante distinguir entre los
repositorios de SUSE y los de NVIDIA, ya que en ocasiones puede haber una
discrepancia entre las versiones de los paquetes que NVIDIA pone a
disposición y las que SUSE ha creado. Esto suele ocurrir cuando SUSE lanza
una nueva versión del controlador de código abierto y pasan un par de días
antes de que los paquetes equivalentes estén disponibles en los repositorios
de NVIDIA.</para>
<para>Le recomendamos que se asegure de que la versión del controlador que
selecciona sea compatible con su GPU y cumpla con los requisitos de CUDA que
pueda tener. Compruebe lo siguiente:</para>
<itemizedlist>
<listitem>
<para>Las <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">notas de
la versión de CUDA</link></para>
</listitem>
<listitem>
<para>Que la versión del controlador que tiene previsto desplegar tenga una
versión equivalente en el <link
xl:href="https://download.nvidia.com/suse/sle15sp6/x86_64/">repositorio de
NVIDIA</link> y que dispone de versiones equivalentes de los paquetes para
los componentes relacionados</para>
</listitem>
</itemizedlist>
<tip>
<para>Para localizar las versiones del controlador de código abierto de NVIDIA,
ejecute <literal>zypper se -s nvidia-open-driver</literal> en el equipo de
destino <emphasis>o</emphasis> busque "nvidia-open-driver" en el Centro de
servicios al cliente de SUSE en <link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.1&amp;arch=x86_64">SUSE
Linux Micro 6.1 para AMD64/Intel 64</link>.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width="100%"/>
</imageobject>
<textobject><phrase>Centro de servicios al cliente de SUSE</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>Cuando haya confirmado que hay una versión equivalente disponible en los
repositorios de NVIDIA, podrá instalar los paquetes en el sistema operativo
host. Para ello, debemos abrir una sesión
<literal>transactional-update</literal>, que crea una nueva instantánea de
lectura/escritura del sistema operativo subyacente para que podamos realizar
cambios en la plataforma inmutable (para obtener más instrucciones sobre
<literal>transactional-update</literal>, consulte <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-transactional-updates/transactional-updates.html">este
documento</link>):</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>Cuando acceda a la shell <literal>transactional-update</literal>, añada un
repositorio de paquetes adicional de NVIDIA. Esto nos permite incorporar
utilidades adicionales, por ejemplo, <literal>nvidia-smi</literal>:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-suse-main
zypper --gpg-auto-import-keys refresh</screen>
<para>A continuación, puede instalar el controlador y
<literal>nvidia-compute-utils</literal> para obtener utilidades
adicionales. Si no necesita las utilidades, puede omitirlas, pero para fines
de prueba, vale la pena instalarlas en esta etapa:</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>Si la instalación falla, podría deberse a una incompatibilidad entre la
versión del controlador seleccionada y la que NVIDIA incluye en sus
repositorios. Consulte la sección anterior para verificar que las versiones
coincidan. Pruebe a instalar una versión diferente del controlador. Por
ejemplo, si los repositorios de NVIDIA tienen una versión anterior, puede
especificar <literal>nvidia-open-driver-G06-signed-kmp=550.54.14</literal>
en su comando de instalación para especificar una versión que coincida.</para>
</note>
<para>A continuación, si <emphasis>no</emphasis> utiliza una GPU compatible
(recuerde que la lista está <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">aquí</link>),
puede comprobar si el controlador funciona habilitando la compatibilidad a
nivel de módulo, pero los resultados pueden variar. Omita este paso si
utiliza una GPU <emphasis>compatible</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>Ahora que ha instalado estos paquetes, es el momento de salir de la sesión
<literal>transactional-update</literal>:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>Asegúrese de haber salido de la sesión
<literal>transactional-update</literal> antes de continuar.</para>
</note>
<para>Después de instalar los controladores, reinicie. Dado que SUSE Linux Micro
es un sistema operativo inmutable, es necesario reiniciar en la nueva
instantánea que ha creado en el paso anterior. Los controladores solo se
instalan en esta nueva instantánea, por lo que no es posible cargarlos sin
reiniciar en ella, lo cual se realiza automáticamente. Ejecute el comando de
reinicio cuando esté listo:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>Una vez que el sistema se haya reiniciado correctamente, vuelva a iniciar
sesión y use la herramienta <literal>nvidia-smi</literal> para comprobar que
el controlador se ha cargado correctamente y que puede acceder y mostrar sus
GPU:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>El resultado de este comando debería mostrar algo similar a lo siguiente,
teniendo en cuenta que en este ejemplo tenemos dos GPU:</para>
<screen language="shell" linenumbering="unnumbered">+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>Con esto concluye el proceso de instalación y verificación de los
controladores NVIDIA en su sistema SUSE Linux Micro.</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>Validación adicional de la instalación manual</title>
<para>En esta fase, lo único que hemos podido verificar es que, a nivel del host,
se puede acceder al dispositivo NVIDIA y que los controladores se cargan
correctamente. Sin embargo, si queremos asegurarnos de que funcionan, una
prueba sencilla sería validar que la GPU puede recibir instrucciones de una
aplicación del espacio de usuarios, idealmente a través de un contenedor y
mediante la biblioteca CUDA, ya que eso es lo que normalmente utilizaría una
carga de trabajo real. Para ello, podemos realizar una modificación
adicional en el sistema operativo del host instalando
<literal>nvidia-container-toolkit</literal> (<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container Toolkit</link>). En primer lugar, abra otra shell
<literal>transactional-update</literal>, teniendo en cuenta que podríamos
haberlo hecho en una sola transacción en el paso anterior. Veremos cómo
hacerlo de forma totalmente automatizada en una sección posterior:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>A continuación, instale el paquete
<literal>nvidia-container-toolkit</literal> desde el repositorio de NVIDIA
Container Toolkit:</para>
<itemizedlist>
<listitem>
<para>El archivo <literal>nvidia-container-toolkit.repo</literal> que aparece a
continuación contiene un repositorio estable
(<literal>nvidia-container-toolkit</literal>) y otro experimental
(<literal>nvidia-container-toolkit-experimental</literal>). Se recomienda
utilizar el repositorio estable para uso en producción. El repositorio
experimental está inhabilitado de forma predeterminada.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>Cuando esté listo, puede salir de la shell
<literal>transactional-update</literal>:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>... y reiniciar el equipo en la nueva instantánea:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>Como antes, debe asegurarse de que ha salido de
<literal>transactional-shell</literal> y ha reiniciado el equipo para que
los cambios surtan efecto.</para>
</note>
<para>Una vez reiniciada el equipo, compruebe que el sistema puede mostrar
correctamente los dispositivos que usan NVIDIA Container Toolkit. El
resultado debe ser detallado, con mensajes INFO y WARN, pero sin mensajes
ERROR:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>Esto garantiza que cualquier contenedor iniciado en el equipo pueda emplear
los dispositivos GPU NVIDIA que se hayan detectado. Cuando esté listo,
ejecute un contenedor basado en Podman. Ejecutarlo con
<literal>Podman</literal> es una buena forma de validar el acceso al
dispositivo NVIDIA desde dentro de un contenedor, lo que debería dar
confianza para hacer lo mismo con Kubernetes en una fase posterior. Otorgue
a <literal>Podman</literal> acceso a los dispositivos NVIDIA etiquetados que
se gestionaron con el comando anterior, basándose en <link
xl:href="https://registry.suse.com/repositories/bci-bci-base-15sp6">SLE
BCI</link> y ejecute el comando Bash:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>Ahora ejecutará comandos desde un contenedor de Podman temporal. No tiene
acceso a su sistema subyacente y es efímero, por lo que cualquier acción que
realicemos aquí no será permanente y no debería poder dañar nada en el host
subyacente. Como ahora estamos en un contenedor, podemos instalar las
bibliotecas CUDA necesarias, comprobando de nuevo la versión correcta de
CUDA para su controlador <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">aquí</link>,
aunque el resultado anterior de <literal>nvidia-smi</literal> debería
mostrar la versión de CUDA necesaria. En el ejemplo siguiente, vamos a
instalar <emphasis>CUDA 12.3</emphasis> y a extraer muchos ejemplos,
demostraciones y kits de desarrollo para que pueda validar completamente la
GPU:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-suse
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>Cuando se haya instalado correctamente, no salga del
contenedor. Ejecutaremos el ejemplo de CUDA <literal>deviceQuery</literal>,
que valida de forma exhaustiva el acceso a la GPU a través de CUDA, y desde
el propio contenedor:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>Si todo va bien, debería ver un resultado similar al siguiente. Fíjese en el
mensaje <literal>Result = PASS</literal> al final del comando y en que, en
el resultado, el sistema identifica correctamente dos GPU, mientras que es
posible que su entorno solo tenga una:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>Desde aquí, puede continuar ejecutando cualquier otra carga de trabajo
CUDA. Utilice compiladores y cualquier otro aspecto del ecosistema CUDA para
realizar más pruebas. Cuando haya terminado, puede salir del contenedor,
pero tenga en cuenta que todo lo que haya instalado en él es efímero (por lo
que se perderá) y no ha afectado al sistema operativo subyacente:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Implementación con Kubernetes</title>
<para>Ahora que hemos probado la instalación y el uso del controlador de código
abierto de NVIDIA en SUSE Linux Micro, veamos cómo configurar Kubernetes en
el mismo equipo. Esta guía no le guiará a través del despliegue de
Kubernetes, pero se entiende que ha instalado <link
xl:href="https://k3s.io/">K3s</link> o <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> y que su
kubeconfig se ha configurado en consecuencia, de modo que puede ejecutar
como superusuario los comandos estándares de <literal>kubectl</literal>. Se
sobrentiende que su nodo forma un clúster de un solo nodo, aunque los pasos
básicos deberían ser similares para clústeres de varios nodos. En primer
lugar, asegúrese de que su acceso a <literal>kubectl</literal> funciona:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>Esto debería mostrar algo similar a lo siguiente:</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.32.4+rke2r1</screen>
<para>Debería ver que su instalación de k3s/rke2 ha detectado NVIDIA Container
Toolkit en el host y ha configurado automáticamente la integración del
entorno de ejecución de NVIDIA en <literal>containerd</literal> (la interfaz
de entorno de ejecución de contenedores que utiliza k3s/rke2). Confírmelo
comprobando el archivo <literal>config.toml</literal> de containerd:</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>El resultado debe ser similar a lo siguiente. La ubicación equivalente de
K3s es
<literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>:</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>Si estas entradas no están presentes, es posible que la detección haya
fallado. Podría deberse a que el equipo o los servicios de Kubernetes no se
han reiniciado. Si es necesario, añádalos manualmente como se indica
anteriormente.</para>
</note>
<para>A continuación, debemos configurar <literal>RuntimeClass</literal> de NVIDIA
como entorno de ejecución adicional al predeterminado de Kubernetes, lo que
garantiza que cualquier solicitud de pods que haga el usuario y que necesite
acceder a la GPU pueda utilizar NVIDIA Container Toolkit para hacerlo, a
través de <literal>nvidia-container-runtime</literal>, tal y como se
configura en <literal>containerd</literal>:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>El siguiente paso es configurar el <link
xl:href="https://github.com/NVIDIA/k8s-device-plugin">complemento de
dispositivos de NVIDIA</link>. Este complemento configura Kubernetes para
aprovechar las GPU NVIDIA como recursos que se pueden utilizar dentro del
clúster, en combinación con NVIDIA Container Toolkit. Esta herramienta
detecta inicialmente todas las capacidades del host subyacente, incluidas
las GPU, los controladores y otras (como GL) y, luego, permite solicitar
recursos de GPU y consumirlos como parte de sus aplicaciones.</para>
<para>Primero, debe añadir y actualizar el repositorio de Helm para el complemento
de dispositivos de NVIDIA:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>Ahora puede instalar el complemento de dispositivos de NVIDIA:</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>Después de unos minutos, verá un nuevo pod en ejecución que completará la
detección en los nodos disponibles y los etiquetará con el número de GPU que
se han detectado:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>Ahora ya está listo para crear un pod de NVIDIA que intentará utilizar esta
GPU. Probemos con el contenedor CUDA Benchmark:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>Si todo ha ido bien, puede consultar los registros y ver la información del
benchmark:</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance)
	-numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
	-device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>Por último, si sus aplicaciones requieren OpenGL, puede instalar las
bibliotecas de OpenGL de NVIDIA necesarias en el nivel del host. El
complemento de dispositivos de NVIDIA y NVIDIA Container Toolkit las pondrán
a disposición de los contenedores. Para ello, instale el paquete de la
siguiente manera:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>Debe rearrancar el sistema para que este paquete esté disponible para sus
aplicaciones. El complemento de dispositivos de NVIDIA debería volver a
detectarlo automáticamente mediante NVIDIA Container Toolkit.</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Unificación mediante Edge Image Builder</title>
<para>Ya ha demostrado la plena funcionalidad de sus aplicaciones y GPU en SUSE
Linux Micro y ahora desea utilizar <xref linkend="components-eib"/> para
unificarlo todo en una imagen de disco ISO o RAW
desplegable/consumible. Esta guía no explica cómo usar Edge Image Builder,
pero proporciona las configuraciones necesarias para crear dicha imagen. A
continuación encontrará un ejemplo de definición de imagen, junto con los
archivos de configuración de Kubernetes necesarios, para garantizar que
todos los componentes requeridos se desplieguen. Esta es la estructura de
directorio de Edge Image Builder para el ejemplo que se muestra a
continuación:</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>Exploremos esos archivos. En primer lugar, hay una definición de imagen de
muestra para un clúster de un solo nodo que ejecuta K3s y que despliega las
utilidades y los paquetes OpenGL (<literal>eib-config-iso.yaml</literal>):</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: [snip]
kubernetes:
  version: v1.32.4+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>Esto es solo un ejemplo. Deberá personalizarlo según sus necesidades y
expectativas. Además, si utiliza SUSE Linux Micro, debe proporcionar su
propio código <literal>sccRegistrationCode</literal> para resolver las
dependencias del paquete y conseguir los controladores de NVIDIA.</para>
</note>
<para>Además, debemos añadir componentes adicionales para que Kubernetes los
cargue durante el arranque. El directorio de EIB necesita primero un
directorio <literal>kubernetes</literal>, con subdirectorios para la
configuración, los valores del chart de Helm y cualquier manifiesto
adicional que sea necesario:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>Ahora vamos a configurar Kubernetes (opcional) eligiendo una CNI (por
defecto es Cilium si no se selecciona ninguna) y habilitando SELinux:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>Ahora, asegúrese de que se haya creado RuntimeClass de NVIDIA en el clúster
de Kubernetes:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>Usamos la versión integrada de Helm Controller para desplegar el complemento
de dispositivos de NVIDIA a través del propio Kubernetes. Proporcionemos la
clase de tiempo de ejecución en el archivo de valores para el chart:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>Antes de continuar, debemos obtener la clave pública del RPM de NVIDIA
Container Toolkit:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>Todos los artefactos necesarios, incluidos el binario de Kubernetes, las
imágenes de contenedores, los charts de Helm (y cualquier imagen
referenciada), se aislarán automáticamente, lo que significa que, en el
momento del despliegue, los sistemas no deberían requerir conexión a
Internet de forma predeterminada. Ahora solo tiene que obtener la imagen ISO
de SUSE Linux Micro de la <link
xl:href="https://www.suse.com/download/sle-micro/">página de descargas de
SUSE</link> (y colocarla en el directorio
<literal>base-images</literal>). Puede generar la imagen ISO con Edge Image
Builder. Para completar el ejemplo, este es el comando que se utilizó para
crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
build --definition-file eib-config-iso.yaml</screen>
<para>Para obtener más información, consulte la <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md">documentación</link>
de Edge Image Builder.</para>
</section>
<section xml:id="id-resolving-issues">
<title>Resolución de problemas</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smi no encuentra la GPU</title>
<para>Utilice <literal>dmesg</literal> para comprobar los mensajes del kernel. Si
se indica que no es posible asignar <literal>NvKMSKapDevice</literal>,
aplique la solución alternativa para las GPU no compatibles:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>NOTA</emphasis>: para que los cambios surtan efecto, si cambia la
configuración del módulo del kernel en el paso anterior, deberá volver a
cargar el módulo del kernel o rearrancar el sistema.</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="day-2-operations">
<title>Operaciones de día 2</title>
<partintro>
<para>En esta sección se explica cómo los administradores pueden gestionar
diferentes tareas operativas de "día dos" tanto en los clústeres de gestión
como en los descendentes.</para>
</partintro>
<chapter xml:id="day2-migration">
<title>Migración a Edge 3.3</title>
<para>En esta sección se explica cómo migrar los clústeres de
<literal>gestión</literal> y <literal>descendentes</literal> de
<literal>Edge 3.2</literal> a <literal>Edge 3.3.0</literal>.</para>
<important>
<para>Realice siempre las migraciones de clústeres desde la última versión
<literal>z-stream</literal> de <literal>Edge 3.2</literal>.</para>
<para>Migre siempre a la versión <literal>Edge 3.3.0</literal>. Para
actualizaciones posteriores a la migración, consulte las secciones sobre los
clústeres de gestión (<xref linkend="day2-mgmt-cluster"/>) y los clústeres
descendentes (<xref linkend="day2-downstream-clusters"/>).</para>
</important>
<section xml:id="day2-migration-mgmt">
<title>Clúster de gestión</title>
<para>En esta sección se tratan los temas siguientes:</para>
<para><xref linkend="day2-migration-mgmt-prereq"/>: pasos previos que deben
completarse antes de iniciar la migración.</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller"/>: cómo migrar un
clúster de <literal>gestión</literal> con <xref
linkend="components-upgrade-controller"/>.</para>
<para><xref linkend="day2-migration-mgmt-fleet"/>: cómo migrar un clúster de
<literal>gestión</literal> con <xref linkend="components-fleet"/>.</para>
<section xml:id="day2-migration-mgmt-prereq">
<title>Requisitos previos</title>
<section xml:id="id-upgrade-the-bare-metal-operator-crds">
<title>Actualización de las CRD del operador bare metal</title>
<note>
<para>Solo se aplica a clústeres que requieren una actualización del chart de
<xref linkend="components-metal3"/>.</para>
</note>
<para>El chart de Helm de <literal>Metal<superscript>3</superscript></literal>
incluye las CRD de <link
xl:href="https://book.metal3.io/bmo/introduction.html">Bare Metal Operator
(BMO)</link> obtenidas del directorio <link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you">CRD</link>
de Helm.</para>
<para>Sin embargo, este enfoque tiene ciertas limitaciones, en particular la
imposibilidad de actualizar las CRD de este directorio utilizando Helm. Para
obtener más información, consulte la <link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations">documentación
de Helm</link>.</para>
<para>Como resultado, antes de actualizar Metal<superscript>3</superscript> a una
versión compatible de <literal>Edge 3.3.0</literal>, los usuarios deben
actualizar manualmente las CRD de BMO subyacentes.</para>
<para>En un equipo con <literal>Helm</literal> instalado y
<literal>kubectl</literal> configurado que apunte al clúster de
<literal>gestión</literal>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Aplique manualmente las CRD de BMO:</para>
<screen language="bash" linenumbering="unnumbered">helm show crds oci://registry.suse.com/edge/charts/metal3 --version 303.0.7+up0.11.5 | kubectl apply -f -</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller">
<title>Upgrade Controller</title>
<important>
<para><literal>Upgrade Controller</literal> actualmente solo admite migraciones de
versiones de Edge para clústeres <emphasis role="strong">de gestión en
entornos que no sean aislados</emphasis>.</para>
</important>
<para>En esta sección se tratan los siguientes temas:</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-prereq"/>: requisitos
previos específicos de <literal>Upgrade Controller</literal>.</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-migration"/>: pasos
para migrar un clúster de <literal>gestión</literal> a una versión nueva de
Edge mediante <literal>Upgrade Controller</literal>.</para>
<section xml:id="day2-migration-mgmt-upgrade-controller-prereq">
<title>Requisitos previos</title>
<section xml:id="id-edge-3-3-upgrade-controller">
<title>Upgrade Controller de Edge 3.3</title>
<para>Antes de usar <literal>Upgrade Controller</literal>, debe asegurarse de que
está ejecutando una versión que sea compatible con la migración a la versión
de Edge deseada.</para>
<para>Para ello:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Si ya ha desplegado <literal>Upgrade Controller</literal> desde una versión
anterior de Edge, actualice su chart:</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade upgrade-controller -n upgrade-controller-system oci://registry.suse.com/edge/charts/upgrade-controller --version 303.0.1+up0.1.1</screen>
</listitem>
<listitem>
<para>Si <emphasis role="strong">no</emphasis> ha desplegado <literal>Upgrade
Controller</literal>, siga la <xref
linkend="components-upgrade-controller-installation"/>.</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller-migration">
<title>Pasos de la migración</title>
<para>Migrar un clúster de <literal>gestión</literal> con <literal>Upgrade
Controller</literal> es básicamente igual a ejecutar una actualización.</para>
<para>La única diferencia es que su recurso <literal>UpgradePlan</literal>
<emphasis role="strong">debe</emphasis> especificar la versión
<literal>3.3.0</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  # Change to the namespace of your Upgrade Controller
  namespace: CHANGE_ME
spec:
  releaseVersion: 3.3.0</screen>
<para>Para obtener información sobre cómo usar el recurso
<literal>UpgradePlan</literal> mencionado para realizar una migración,
consulte el proceso de actualización de Upgrade Controller (<xref
linkend="management-day2-upgrade-controller"/>).</para>
</section>
</section>
<section xml:id="day2-migration-mgmt-fleet">
<title>Fleet</title>
<note>
<para>Siempre que sea posible, use las instrucciones de la <xref
linkend="day2-migration-mgmt-upgrade-controller"/> para la migración.</para>
<para>Consulte esta sección solo para los casos no cubiertos por la migración con
<literal>Upgrade Controller</literal>.</para>
</note>
<para>Migrar un clúster de <literal>gestión</literal> con <literal>Fleet</literal>
es básicamente igual a ejecutar una actualización.</para>
<para>Estas son las diferencias <emphasis role="strong">principales</emphasis>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Fleet <emphasis role="strong">se debe usar</emphasis> desde la versión <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">release-3.3.0</link>
del repositorio <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>Los charts cuya actualización esté programada, <emphasis
role="strong">deben</emphasis> actualizarse a versiones compatibles con la
versión <literal>Edge 3.3.0</literal>. Para ver una lista de los componentes
de <literal>Edge 3.3.0</literal>, consulte la <xref
linkend="release-notes-3-3-0"/>.</para>
</listitem>
</orderedlist>
<important>
<para>Para garantizar que la migración a <literal>Edge 3.3.0</literal> se realice
correctamente, es importante que los usuarios cumplan con los puntos
descritos anteriormente.</para>
</important>
<para>Teniendo en cuenta los puntos anteriores, los usuarios pueden seguir la
documentación de Fleet (<xref linkend="management-day2-fleet"/>) sobre el
clúster de <literal>gestión</literal>, que incluye todos los pasos
necesarios para realizar la migración.</para>
</section>
</section>
<section xml:id="day2-migration-downstream">
<title>Clústeres descendentes</title>
<para><xref linkend="day2-migration-downstream-fleet"/>: cómo migrar un clúster
<literal>descendente</literal> con <xref linkend="components-fleet"/>.</para>
<section xml:id="day2-migration-downstream-fleet">
<title>Fleet</title>
<para>Migrar un clúster <literal>descendente</literal> con
<literal>Fleet</literal> es básicamente igual a ejecutar una actualización.</para>
<para>Estas son las diferencias <emphasis role="strong">principales</emphasis>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Fleet <emphasis role="strong">se debe usar</emphasis> desde la versión <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">release-3.3.0</link>
del repositorio <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>Los charts cuya actualización esté programada, <emphasis
role="strong">deben</emphasis> actualizarse a versiones compatibles con la
versión <literal>Edge 3.3.0</literal>. Para ver una lista de los componentes
de <literal>Edge 3.3.0</literal>, consulte la <xref
linkend="release-notes-3-3-0"/>.</para>
</listitem>
</orderedlist>
<important>
<para>Para garantizar que la migración a <literal>Edge 3.3.0</literal> se realice
correctamente, es importante que los usuarios cumplan con los puntos
descritos anteriormente.</para>
</important>
<para>Teniendo en cuenta los puntos anteriores, los usuarios pueden seguir la
documentación de Fleet (<xref linkend="downstream-day2-fleet"/>) sobre los
clústeres <literal>descendentes</literal> para obtener una guía completa
sobre los pasos necesarios para realizar la migración.</para>
</section>
</section>
</chapter>
<chapter xml:id="day2-mgmt-cluster">
<title>Clúster de gestión</title>
<para>Actualmente, hay dos formas de realizar operaciones de "día 2" en su clúster
de <literal>gestión</literal>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante <xref linkend="components-upgrade-controller"/>: <xref
linkend="management-day2-upgrade-controller"/></para>
</listitem>
<listitem>
<para>Mediante <xref linkend="components-fleet"/>: <xref
linkend="management-day2-fleet"/></para>
</listitem>
</orderedlist>
<section xml:id="management-day2-upgrade-controller">
<title>Upgrade Controller</title>
<important>
<para><literal>Upgrade Controller</literal> actualmente solo admite operaciones de
<literal>día 2</literal> para clústeres <emphasis role="strong">en entornos
no aislados</emphasis>.</para>
</important>
<para>En esta sección se explica cómo realizar las distintas operaciones de
<literal>día 2</literal> relacionadas con la actualización del clúster de
<literal>gestión</literal> de una versión de la plataforma Edge a otra.</para>
<para>Las operaciones de <literal>día 2</literal> se automatizan con Upgrade
Controller (<xref linkend="components-upgrade-controller"/>) e incluyen:</para>
<itemizedlist>
<listitem>
<para>Actualización del sistema operativo SUSE Linux Micro (<xref
linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Actualización de kubernetes con <xref linkend="components-rke2"/> o <xref
linkend="components-k3s"/></para>
</listitem>
<listitem>
<para>Actualización de componentes adicionales de SUSE (SUSE Rancher Prime, SUSE
Security, etc.)</para>
</listitem>
</itemizedlist>
<section xml:id="id-prerequisites-12">
<title>Requisitos previos</title>
<para>Antes de actualizar su clúster de <literal>gestión</literal>, deben
cumplirse los siguientes requisitos previos:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Nodos registrados en el SCC</literal>: asegúrese de que los
sistemas operativos de los nodos del clúster estén registrados con una clave
de suscripción que admita la versión del sistema operativo especificada en
la versión de Edge (<xref linkend="release-notes"/>) a la que desea
actualizar.</para>
</listitem>
<listitem>
<para><literal>Upgrade Controller</literal>: asegúrese de que <literal>Upgrade
Controller</literal> se haya desplegado en su clúster de
<literal>gestión</literal>. Para conocer los pasos de instalación, consulte
la <xref linkend="components-upgrade-controller-installation"/>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade">
<title>Actualización</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine la versión de Edge (<xref linkend="release-notes"/>) a la que
desea actualizar el clúster de <literal>gestión</literal>.</para>
</listitem>
<listitem>
<para>En el clúster de <literal>gestión</literal>, despliegue un recurso
<literal>UpgradePlan</literal> que especifique la <literal>versión</literal>
deseada. <literal>UpgradePlan</literal> debe desplegarse en el espacio de
nombres de <literal>Upgrade Controller</literal>.</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n &lt;upgrade_controller_namespace&gt; -f - &lt;&lt;EOF
apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
spec:
  # Version retrieved from release notes
  releaseVersion: 3.X.Y
EOF</screen>
<note>
<para>Puede haber casos en los que desee realizar configuraciones adicionales
sobre el recurso <literal>UpgradePlan</literal>. Para conocer todas las
configuraciones posibles, consulte la <xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>.</para>
</note>
</listitem>
<listitem>
<para>El despliegue de <literal>UpgradePlan</literal> en el espacio de nombres de
<literal>Upgrade Controller</literal> iniciará el <literal>proceso de
actualización</literal>.</para>
<note>
<para>Para obtener más información sobre el <literal>proceso de
actualización</literal> real, consulte la <xref
linkend="components-upgrade-controller-how"/>.</para>
<para>Para obtener más información sobre cómo hacer un seguimiento del
<literal>proceso de actualización</literal>, consulte la <xref
linkend="components-upgrade-controller-how-track"/>.</para>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet">
<title>Fleet</title>
<para>Esta sección ofrece información sobre cómo realizar operaciones de "día 2"
utilizando el componente Fleet (<xref linkend="components-fleet"/>).</para>
<para>En esta sección se tratan los siguientes temas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-components"/>: los componentes
predeterminados usados para todas las operaciones de "día 2".</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-determine-use-case"/>: proporciona una
descripción general de los recursos personalizados de Fleet que se usarán y
su idoneidad para diferentes casos de uso de operaciones de "día 2".</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-upgrade-workflow"/>: proporciona una guía del
flujo de trabajo para ejecutar operaciones de "día 2" con Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade"/>: describe cómo realizar
actualizaciones del sistema operativo con Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade"/>: describe cómo realizar
actualizaciones de la versión de Kubernetes con Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade"/>: describe cómo realizar
actualizaciones de charts de Helm con Fleet.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-components">
<title>Componentes</title>
<para>A continuación, encontrará una descripción de los componentes
predeterminados que deben configurarse en su clúster de
<literal>gestión</literal> para poder realizar correctamente las operaciones
de "día 2" con Fleet.</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<para>(<emphasis role="strong">Opcional</emphasis>) Es el responsable de gestionar
los <literal>clústeres descendentes</literal> y de desplegar <literal>System
Upgrade Controller</literal> en su <literal>clúster de gestión</literal>.</para>
<para>Para obtener más información, consulte el <xref
linkend="components-rancher"/>.</para>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System Upgrade Controller (SUC)</title>
<para><emphasis role="strong">System Upgrade Controller</emphasis> es responsable
de ejecutar tareas en nodos específicos según los datos de configuración
proporcionados mediante un recurso personalizado, denominado
<literal>plan</literal>.</para>
<para><emphasis role="strong">SUC</emphasis> se utiliza activamente para
actualizar el sistema operativo y la distribución de Kubernetes.</para>
<para>Para obtener más información sobre el componente <emphasis
role="strong">SUC</emphasis> y cómo encaja en la pila de Edge, consulte el
<xref linkend="components-system-upgrade-controller"/>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-determine-use-case">
<title>Determinación de su caso de uso</title>
<para>Fleet usa dos tipos de <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> para permitir la gestión de los recursos de Kubernetes
y Helm.</para>
<para>A continuación, encontrará información sobre la finalidad de estos recursos
y los casos de uso para los que son más adecuados en el contexto de las
operaciones de "día 2".</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para>Un <literal>GitRepo</literal> es un recurso de Fleet (<xref
linkend="components-fleet"/>) que representa un repositorio Git desde el que
<literal>Fleet</literal> puede crear <literal>Bundles</literal>. Cada
<literal>Bundle</literal> se crea en función de las rutas de configuración
definidas dentro del recurso <literal>GitRepo</literal>. Para obtener más
información, consulte la documentación de <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>.</para>
<para>En el contexto de las operaciones de "día 2", los recursos
<literal>GitRepo</literal> se suelen usar para desplegar
<literal>SUC</literal> o <literal>planes de SUC</literal> en entornos
<emphasis role="strong">no aislados</emphasis> que emplean un enfoque de
<emphasis>GitOps de Fleet</emphasis>.</para>
<para>Como alternativa, los recursos <literal>GitRepo</literal> también se pueden
utilizar para desplegar <literal>SUC</literal> o <literal>planes de
SUC</literal> en entornos <emphasis role="strong">aislados</emphasis>,
<emphasis role="strong">siempre que se refleje la configuración del
repositorio a través de un servidor Git local</emphasis>.</para>
</section>
<section xml:id="id-bundle">
<title>Bundle</title>
<para>Los <literal>Bundles</literal> contienen recursos <emphasis
role="strong">sin procesar</emphasis> de Kubernetes que se desplegarán en el
clúster de destino. Por lo general, se crean a partir de un recurso
<literal>GitRepo</literal>, pero hay casos en los que se pueden desplegar
manualmente. Para obtener más información, consulte la documentación de
<link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>.</para>
<para>En el contexto de las operaciones de "día 2", los recursos
<literal>Bundle</literal> se suelen usar para desplegar
<literal>SUC</literal> o <literal>planes de SUC</literal> en entornos
<emphasis role="strong">aislados</emphasis> que no utilizan algún tipo de
procedimiento <emphasis>GitOps local</emphasis> (por ejemplo, un <emphasis
role="strong">servidor Git local</emphasis>).</para>
<para>Alternativamente, si su caso de uso no permite un flujo de trabajo
<emphasis>GitOps</emphasis> (por ejemplo, porque use un repositorio Git),
también se pueden utilizar recursos <literal>Bundle</literal> para desplegar
<literal>SUC</literal> o <literal>planes de SUC</literal> en entornos
<emphasis role="strong">no aislados</emphasis>.</para>
</section>
</section>
<section xml:id="management-day2-upgrade-workflow">
<title>Flujo de trabajo de día 2</title>
<para>A continuación, se describe el flujo de trabajo de "día 2" que se debe
seguir al actualizar un clúster de gestión a una versión específica de Edge.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Actualización del sistema operativo (<xref
linkend="management-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Actualización de la versión de Kubernetes (<xref
linkend="management-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Actualización del chart de Helm (<xref
linkend="management-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade">
<title>Actualización del sistema operativo</title>
<para>En esta sección se describe cómo realizar una actualización del sistema
operativo utilizando <xref linkend="components-fleet"/> y <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>En esta sección se tratan los siguientes temas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-components"/>: componentes
adicionales usados por el proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-overview"/>: descripción
general del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-requirements"/>: requisitos
del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-plan-deployment"/>:
información sobre cómo desplegar <literal>planes de SUC</literal>,
responsables de iniciar el proceso de actualización.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-os-upgrade-components">
<title>Componentes</title>
<para>Esta sección trata sobre los componentes personalizados que el proceso de
<literal>actualización del sistema operativo</literal> usa en lugar de los
componentes predeterminados de "día 2" (<xref
linkend="management-day2-fleet-components"/>).</para>
<section xml:id="management-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>La actualización del sistema operativo en un nodo específico se gestiona
mediante un servicio <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>.</para>
<para>En función del tipo de actualización que requiera el sistema operativo de
una versión de Edge a otra, se creará un servicio diferente:</para>
<itemizedlist>
<listitem>
<para>Para las versiones de Edge que requieren la misma versión del sistema
operativo (por ejemplo, la <literal>6.0</literal>), se creará el servicio
<literal>os-pkg-update.service</literal>. Use <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para realizar una <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">actualización
normal del paquete</link>.</para>
</listitem>
<listitem>
<para>Para las versiones de Edge que requieran una migración de la versión del
sistema operativo (por ejemplo, de la <literal>6.0</literal> a la
<literal>6.1</literal>), se creará el servicio
<literal>os-migration.service</literal>. Use <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para realizar:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Una <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">actualización
normal del paquete</link> que garantice que todos los paquetes estén
actualizados para mitigar cualquier fallo en la migración relacionado con
versiones antiguas del paquete.</para>
</listitem>
<listitem>
<para>Una migración del sistema operativo mediante el comando <literal>zypper
migration</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>Los servicios mencionados anteriormente se envían a cada nodo a través de un
<literal>plan de SUC</literal>, que debe estar ubicado en el clúster de
gestión que necesita una actualización del sistema operativo.</para>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-overview">
<title>Descripción general</title>
<para>La actualización del sistema operativo para los nodos del clúster de gestión
se realiza con <literal>Fleet</literal> y <literal>System Upgrade Controller
(SUC)</literal>.</para>
<para><emphasis role="strong">Fleet</emphasis> se usa para desplegar y gestionar
<literal>planes de SUC</literal> en el clúster deseado.</para>
<note>
<para>Los <literal>planes de SUC</literal> son <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que describen los pasos que debe seguir
<literal>SUC</literal> para ejecutar una tarea específica en un conjunto de
nodos. Para ver un ejemplo de cómo es un <literal>plan de SUC</literal>,
consulte el <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositorio
original</link>.</para>
</note>
<para>Los <literal>planes de SUC de sistema operativo</literal> se envían a cada
clúster desplegando un recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> o <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> a un <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espacio
de trabajo</link> de Fleet específico. Fleet recupera el
<literal>GitRepo/Bundle</literal> desplegado y despliega su contenido (los
<literal>planes de SUC de sistema operativo</literal>) en los clústeres
deseados.</para>
<note>
<para>Los recursos <literal>GitRepo/Bundle</literal> siempre se despliegan en el
<literal>clúster de gestión</literal>. Que se use un recurso
<literal>GitRepo</literal> o <literal>Bundle</literal> depende del caso de
uso. Consulte la <xref linkend="management-day2-fleet-determine-use-case"/>
para obtener más información.</para>
</note>
<para>Los <literal>planes de SUC de sistema operativo</literal> describen el
siguiente flujo de trabajo:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Use siempre el comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
en los nodos antes de las actualizaciones del sistema operativo.</para>
</listitem>
<listitem>
<para>Actualice siempre los nodos <literal>control-plane</literal> antes que los
nodos <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Actualice siempre el clúster en un <emphasis role="strong">único</emphasis>
nodo cada vez.</para>
</listitem>
</orderedlist>
<para>Una vez desplegados los <literal>planes de SUC de sistema
operativo</literal>, el flujo de trabajo es el siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconcilia los <literal>planes de SUC de sistema operativo</literal>
desplegados y crea un <literal>trabajo de Kubernetes</literal> en <emphasis
role="strong">cada nodo</emphasis>.</para>
</listitem>
<listitem>
<para>El <literal>trabajo de Kubernetes</literal> crea un servicio systemd.service
(<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>)
para la actualización de paquetes o para la migración del sistema operativo.</para>
</listitem>
<listitem>
<para>El servicio <literal>systemd.service</literal> creado activa el proceso de
actualización del sistema operativo en el nodo específico.</para>
<important>
<para>Cuando finaliza el proceso de actualización del sistema operativo, el nodo
correspondiente se <literal>rearranca</literal> para aplicar las
actualizaciones en el sistema.</para>
</important>
</listitem>
</orderedlist>
<para>A continuación, encontrará un diagrama de la descripción anterior:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>actualización de sistema operativo de gestión de día 2 con fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-os-upgrade-requirements">
<title>Requisitos</title>
<para><emphasis>Generales:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Equipo registrado en el Centro de servicios al
cliente de SUSE</emphasis>. Todos los nodos del clúster de gestión deben
estar registrados en <literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>. Esto
es necesario para que el servicio <literal>systemd.service</literal>
respectivo pueda conectarse correctamente al repositorio de RPM deseado.</para>
<important>
<para>Para las versiones de Edge que requieren una migración de la versión del
sistema operativo (por ejemplo, de la <literal>6.0</literal> a la
<literal>6.1</literal>), asegúrese de que su clave del Centro de servicios
al cliente de SUSE admita la migración a la nueva versión.</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">Asegúrese de que las tolerancias del plan de SUC
coincidan con las tolerancias de los nodos</emphasis>. Si los nodos de su
clúster de Kubernetes tienen <emphasis role="strong">intolerancias
(taints)</emphasis> personalizadas, asegúrese de añadir <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerancias
(tolerations)</link> para ellas en los <emphasis role="strong">planes de
SUC</emphasis>. De forma predeterminada, los <emphasis role="strong">planes
de SUC</emphasis> solo tienen tolerancias para los nodos de <emphasis
role="strong">plano de control</emphasis>. Las tolerancias predeterminadas
son:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Cualquier tolerancia adicional debe añadirse en la sección
<literal>.spec.tolerations</literal> de cada plan. Los <emphasis
role="strong">planes de SUC</emphasis> relacionados con la actualización del
sistema operativo se pueden encontrar en el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
en
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>.
<emphasis role="strong">Asegúrese de usar planes que tengan una etiqueta de
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de repositorio válida.</emphasis></para>
<para>Esto es un ejemplo de definición de tolerancias personalizadas para el plan
de SUC de <emphasis role="strong">plano de control</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>En entornos aislados:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Duplique los repositorios RPM de
SUSE</emphasis>. Los repositorios RPM del sistema operativo deben duplicarse
y guardarse de forma local para que el servicio
<literal>systemd.service</literal> pueda acceder a ellos. Para ello, utilice
<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>
o <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment">
<title>Actualización del sistema operativo - Despliegue del plan de SUC</title>
<important>
<para>En entornos que se hayan actualizado previamente mediante este
procedimiento, los usuarios deben asegurarse de que se haya completado
<emphasis role="strong">uno</emphasis> de los pasos siguientes:</para>
<itemizedlist>
<listitem>
<para><literal>Elimine cualquier plan de SUC desplegado anteriormente relacionado
con versiones anteriores de Edge del clúster de gestión</literal>. Puede
hacerlo eliminando el clúster deseado de la <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuración
de destino</link> del <literal>GitRepo/Bundle</literal> o eliminando por
completo el recurso <literal>GitRepo/Bundle</literal>.</para>
</listitem>
<listitem>
<para><literal>Reutilice el recurso GitRepo/Bundle existente</literal>. Puede
hacerlo haciendo que la revisión del recurso apunte a una nueva etiqueta que
contenga los recursos de Fleet correctos para la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Esto se hace con el fin de evitar conflictos entre los <literal>planes de
SUC</literal> para versiones anteriores de Edge.</para>
<para>Si los usuarios intentan actualizar mientras hay <literal>planes de
SUC</literal> en el clúster de gestión, verán el siguiente error de Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Como se menciona en la <xref
linkend="management-day2-fleet-os-upgrade-overview"/>, las actualizaciones
del sistema operativo se realizan incluyendo <literal>planes de
SUC</literal> en el clúster deseado de las siguientes formas:</para>
<itemizedlist>
<listitem>
<para>Recurso <literal>GitRepo</literal> de Fleet: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>.</para>
</listitem>
<listitem>
<para>Recurso <literal>Bundle</literal> de Fleet: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle"/>.</para>
</listitem>
</itemizedlist>
<para>Para determinar qué recurso se debe usar, consulte la <xref
linkend="management-day2-fleet-determine-use-case"/>.</para>
<para>Si desea desplegar los <literal>planes de SUC de sistema operativo</literal>
desde una herramienta GitOps de terceros, consulte la <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>Despliegue del plan de SUC - Recurso GitRepo</title>
<para>Es posible desplegar un recurso <emphasis role="strong">GitRepo</emphasis>,
que incluye los <literal>planes de SUC de sistema operativo</literal>, de
estas formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización del sistema
operativo de los nodos de su clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>Creación de GitRepo - Interfaz de usuario de Rancher</title>
<para>Para crear un recurso <literal>GitRepo</literal> con la interfaz de usuario
de Rancher, siga la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentación
oficial</link>.</para>
<para>El equipo de Edge mantiene una <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">flota
(fleet)</link> lista para usar. Dependiendo de su entorno, se puede utilizar
directamente o como plantilla.</para>
<important>
<para>Use siempre esta flota desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge válida.</para>
</important>
<para>Para los casos prácticos en los que no es necesario incluir cambios
personalizados en los <literal>planes de SUC</literal> que incluye la flota,
los usuarios pueden usar directamente la flota <literal>os-upgrade</literal>
desde el repositorio <literal>suse-edge/fleet-examples</literal>.</para>
<para>Si fuera necesario realizar cambios personalizados (por ejemplo, para añadir
tolerancias personalizadas), los usuarios deben usar la flota
<literal>os-upgrade</literal> desde un repositorio independiente, lo que les
permitirá añadir los cambios a los planes de SUC según sea necesario.</para>
<para>Hay un ejemplo de cómo se puede configurar un recurso
<literal>GitRepo</literal> para utilizar la flota del repositorio
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/gitrepos/day2/os-upgrade-gitrepo.yaml">aquí</link>.</para>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>Creación de GitRepo - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga el recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edite la configuración de <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Elimine la sección <literal>spec.targets</literal>: solo se necesita para
clústeres descendentes.</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Dirija el espacio de nombres de <literal>GitRepo</literal> al espacio de
nombres <literal>fleet-local</literal>. Esto se hace para desplegar el
recurso en el clúster de gestión.</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique el recurso <emphasis role="strong">GitRepo</emphasis> a su
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Compruebe que el recurso <emphasis role="strong">GitRepo</emphasis> creado
se encuentra en el espacio de nombres <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-local

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.3.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>Despliegue del plan de SUC - Recurso Bundle</title>
<para>Es posible desplegar un recurso <emphasis role="strong">Bundle</emphasis>,
que incluye los <literal>planes de SUC de sistema operativo</literal>
válidos, de estas formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización del sistema
operativo de los nodos de su clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>Creación de Bundle - Interfaz de usuario de Rancher</title>
<para>El equipo de Edge mantiene un <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle</link>
listo para usar que puede emplear en los pasos que se indican a
continuación.</para>
<important>
<para>Use siempre este bundle desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
válida de Edge.</para>
</important>
<para>Para crear un bundle con la interfaz de usuario de Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En la esquina superior izquierda, haga clic en <emphasis role="strong">☰ →
Continuous Delivery</emphasis> (☰ → Entrega continua).</para>
</listitem>
<listitem>
<para>Diríjase a <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avanzado > Bundles).</para>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create from YAML</emphasis> (Crear desde
YAML).</para>
</listitem>
<listitem>
<para>Desde aquí, puede crear el Bundle de las siguientes maneras:</para>
<note>
<para>Puede haber casos en los que sea necesario incluir cambios personalizados en
los <literal>planes de SUC</literal> que incluye el Bundle (por ejemplo,
para añadir tolerancias personalizadas). Asegúrese de incluir esos cambios
en el Bundle que se generará con los pasos siguientes.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copiando manualmente el <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">contenido
del Bundle</link> desde <literal>suse-edge/fleet-examples</literal> a la
página <emphasis role="strong">Create from YAML</emphasis> (Crear desde
YAML).</para>
</listitem>
<listitem>
<para>Clonando el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
desde la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada y seleccionando la opción<emphasis role="strong">Read from
File</emphasis> (Leer desde archivo) en la página <emphasis
role="strong">Create from YAML</emphasis> (Crear desde YAML). Desde ahí,
diríjase a la ubicación del Bundle
(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)
y seleccione el archivo del Bundle. Esto rellenará automáticamente la página
<emphasis role="strong">Create from YAML</emphasis> (Crear desde YAML) con
el contenido del Bundle.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Edite el Bundle en la interfaz de usuario de Rancher:</para>
<itemizedlist>
<listitem>
<para>Cambie el <emphasis role="strong">espacio de nombres</emphasis> del
<literal>bundle</literal> para que dirija al espacio de nombres
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>Cambie los clústeres de <emphasis role="strong">destino</emphasis> para que
el <literal>bundle</literal> apunte a su clúster de gestión
<literal>local</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Hay casos prácticos en los que el clúster <literal>local</literal> podría
tener un nombre diferente.</para>
<para>Para recuperar el nombre del clúster <literal>local</literal>, ejecute el
siguiente comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create</emphasis> (Crear).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>Creación del Bundle - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga el recurso <emphasis role="strong">Bundle</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edite la configuración del <literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Cambie los clústeres de <emphasis role="strong">destino</emphasis> para que
el <literal>bundle</literal> apunte a su clúster de gestión
<literal>local</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Hay casos prácticos en los que el clúster <literal>local</literal> podría
tener un nombre diferente.</para>
<para>Para recuperar el nombre del clúster <literal>local</literal>, ejecute el
siguiente comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Cambie el <emphasis role="strong">espacio de nombres</emphasis> del
<literal>bundle</literal> para que dirija al espacio de nombres
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique el recurso <emphasis role="strong">Bundle</emphasis> al
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Compruebe que el recurso <emphasis role="strong">Bundle</emphasis> se ha
creado en el espacio de nombres <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-local</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>Despliegue del plan de SUC - Flujo de trabajo de GitOps de terceros</title>
<para>Puede haber casos prácticos en los que los usuarios deseen incorporar los
<literal>planes de SUC de sistema operativo</literal> a su propio flujo de
trabajo de GitOps de terceros (por ejemplo, <literal>Flux</literal>).</para>
<para>Para obtener los recursos de actualización del sistema operativo que
necesita, determine primero la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge del repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
que desea utilizar.</para>
<para>Después, los recursos se encuentran en
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>,
donde:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> es un recurso de plan de SUC para
los nodos de <emphasis role="strong">plano de control</emphasis>.</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> es un recurso de plan de SUC para los
nodos de <emphasis role="strong">trabajador</emphasis>.</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> es un secreto que contiene el guion
<literal>upgrade.sh</literal>, que se encarga de crear el servicio
systemd.service (<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>).</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> es un mapa de configuración que contiene
las configuraciones que utiliza el guion <literal>upgrade.sh</literal>.</para>
</listitem>
</itemizedlist>
<important>
<para>Estos recursos de <literal>plan</literal> son interpretados por
<literal>System Upgrade Controller</literal> y se deben desplegar en cada
clúster descendente que desee actualizar. Para obtener información sobre el
despliegue de SUC, consulte la <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para comprender mejor cómo usar su flujo de trabajo de GitOps para desplegar
los <emphasis role="strong">planes de SUC</emphasis> para actualizar el
sistema operativo, puede resultar útil echar un vistazo a la descripción
general (<xref linkend="management-day2-fleet-os-upgrade-overview"/>).</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade">
<title>Actualización de la versión de Kubernetes</title>
<para>En esta sección se describe cómo realizar una actualización de Kubernetes
utilizando <xref linkend="components-fleet"/> y <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>En esta sección se tratan los siguientes temas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-components"/>: componentes
adicionales usados por el proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-overview"/>: descripción
general del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-requirements"/>: requisitos
del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment"/>:
información sobre cómo desplegar <literal>planes de SUC</literal>,
responsables de iniciar el proceso de actualización.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-k8s-upgrade-components">
<title>Componentes</title>
<para>Esta sección trata sobre los componentes personalizados que el proceso de
<literal>actualización de K8s</literal> usa en lugar de los componentes
predeterminados de "día 2" (<xref
linkend="management-day2-fleet-components"/>).</para>
<section xml:id="management-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>Es la imagen de contenedor responsable de actualizar la versión de RKE2 de
un nodo específico.</para>
<para>Se incluye a través de un pod creado por <emphasis
role="strong">SUC</emphasis> basado en un <emphasis role="strong">plan de
SUC</emphasis>. El plan debe estar ubicado en cada <emphasis
role="strong">clúster</emphasis> que necesite una actualización de RKE2.</para>
<para>Para obtener más información sobre cómo la imagen
<literal>rke2-upgrade</literal> realiza la actualización, consulte la <link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">documentación
original</link>.</para>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Es la imagen de contenedor responsable de actualizar la versión de K3s de un
nodo específico.</para>
<para>Se incluye a través de un pod creado por <emphasis
role="strong">SUC</emphasis> basado en un <emphasis role="strong">plan de
SUC</emphasis>. El plan debe estar ubicado en cada <emphasis
role="strong">clúster</emphasis> que necesite una actualización de K3s.</para>
<para>Para obtener más información sobre cómo realiza la imagen
<literal>k3s-upgrade</literal> la actualización, consulte la <link
xl:href="https://github.com/k3s-io/k3s-upgrade">documentación
original</link>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-overview">
<title>Descripción general</title>
<para>La actualización de la distribución de Kubernetes para los nodos del clúster
de gestión se realiza con <literal>Fleet</literal> y <literal>System Upgrade
Controller (SUC)</literal>.</para>
<para><literal>Fleet</literal> se usa para desplegar y gestionar <literal>planes
de SUC</literal> en el clúster deseado.</para>
<note>
<para>Los <literal>planes de SUC</literal> son <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que describen los pasos que debe seguir <emphasis
role="strong">SUC</emphasis> para ejecutar una tarea específica en un
conjunto de nodos. Para ver un ejemplo de un <literal>plan de SUC</literal>,
consulte el <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositorio
original</link>.</para>
</note>
<para>Los <literal>planes de SUC de K8s</literal> se incluyen en cada clúster
desplegando un recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> o <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> en un <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espacio
de trabajo</link> específico de Fleet. Fleet recupera el
<literal>GitRepo/Bundle</literal> desplegado y despliega su contenido (los
<literal>planes de SUC de K8s</literal>) en los clústeres deseados.</para>
<note>
<para>Los recursos <literal>GitRepo/Bundle</literal> siempre se despliegan en el
<literal>clúster de gestión</literal>. Que se use un recurso
<literal>GitRepo</literal> o <literal>Bundle</literal> depende del caso de
uso. Consulte la <xref linkend="management-day2-fleet-determine-use-case"/>
para obtener más información.</para>
</note>
<para>Los <literal>planes de SUC de K8s</literal> describen el siguiente flujo de
trabajo:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Use siempre el comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
en los nodos antes de las actualizaciones de K8s.</para>
</listitem>
<listitem>
<para>Actualice siempre los nodos <literal>control-plane</literal> antes que los
nodos <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Actualice siempre los nodos de <literal>control-plane</literal> (plano de
control) de <emphasis role="strong">uno en uno</emphasis> y los nodos
<literal>worker</literal> (trabajador) de <emphasis role="strong">dos en
dos</emphasis>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegados los <literal>planes de SUC de K8s</literal>, el flujo de
trabajo es el siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconcilia los <literal>planes de SUC de K8s</literal> desplegados y
crea un <literal>trabajo de Kubernetes</literal> en <emphasis
role="strong">cada nodo</emphasis>.</para>
</listitem>
<listitem>
<para>Dependiendo de la distribución de Kubernetes, el trabajo creará un pod que
ejecutará la imagen de contenedor rke2-upgrade (<xref
linkend="management-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>) o
k3s-upgrade (<xref
linkend="management-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>).</para>
</listitem>
<listitem>
<para>El pod creado seguirá el siguiente flujo de trabajo:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Sustituye el binario <literal>rke2/k3s</literal> existente en el nodo por el
de la imagen <literal>rke2-upgrade/k3s-upgrade</literal>.</para>
</listitem>
<listitem>
<para>Detiene el proceso <literal>rke2/k3s</literal> en ejecución.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Al detener el proceso <literal>rke2/k3s</literal>, se activa un reinicio y
se lanza un nuevo proceso que ejecuta el binario actualizado, lo que da como
resultado una versión actualizada de la distribución de Kubernetes.</para>
</listitem>
</orderedlist>
<para>A continuación, encontrará un diagrama de la descripción anterior:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>actualización de k8s de gestión de día2 con fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-requirements">
<title>Requisitos</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Haga una copia de seguridad de su distribución de
Kubernetes:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Para los <emphasis role="strong">clústeres RKE2</emphasis>, consulte <link
xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2 Backup and
Restore</link> (Copia de seguridad y restauración de RKE2).</para>
</listitem>
<listitem>
<para>Para los <emphasis role="strong">clústeres K3s</emphasis>, consulte <link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and
Restore</link> (Copia de seguridad y restauración de K3s).</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Asegúrese de que las tolerancias del plan de SUC
coincidan con las tolerancias de los nodos</emphasis>. Si los nodos del
clúster de Kubernetes tienen <emphasis role="strong">intolerancias
(taints)</emphasis> personalizadas, asegúrese de añadir <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerancias
(tolerations)</link> para ellas en los <emphasis role="strong">planes de
SUC</emphasis>. Por defecto, los <emphasis role="strong">planes de
SUC</emphasis> solo tienen tolerancias para los nodos de <emphasis
role="strong">plano de control</emphasis>. Las tolerancias predeterminadas
son:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Cualquier tolerancia adicional debe añadirse en la sección
<literal>.spec.tolerations</literal> de cada plan. Los <emphasis
role="strong">planes de SUC</emphasis> relacionados con la actualización de
la versión de Kubernetes se encuentran en el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
en:</para>
<itemizedlist>
<listitem>
<para>Para <emphasis role="strong">RKE2</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>Para <emphasis role="strong">K3s</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Asegúrese de usar los planes de una etiqueta de
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
del repositorio válida.</emphasis></para>
<para>Este es un ejemplo de definición de tolerancias personalizadas para el plan
de SUC de <emphasis role="strong">plano de control</emphasis> de RKE2:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment">
<title>Actualización de K8s - Despliegue del plan de SUC</title>
<important>
<para>En entornos que se hayan actualizado previamente mediante este
procedimiento, los usuarios deben asegurarse de que se haya completado
<emphasis role="strong">uno</emphasis> de los pasos siguientes:</para>
<itemizedlist>
<listitem>
<para><literal>Elimine cualquier plan de SUC desplegado anteriormente relacionado
con versiones anteriores de Edge del clúster de gestión</literal>. Puede
hacerlo eliminando el clúster deseado de la <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuración
de destino</link> del <literal>GitRepo/Bundle</literal> o eliminando por
completo el recurso <literal>GitRepo/Bundle</literal>.</para>
</listitem>
<listitem>
<para><literal>Reutilice el recurso GitRepo/Bundle existente</literal>. Puede
hacerlo haciendo que la revisión del recurso apunte a una nueva etiqueta que
contenga los recursos de Fleet correctos para la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Esto se hace con el fin de evitar conflictos entre los <literal>planes de
SUC</literal> para versiones anteriores de Edge.</para>
<para>Si los usuarios intentan actualizar mientras hay <literal>planes de
SUC</literal> en el clúster de gestión, verán el siguiente error de Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Como se menciona en la <xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>, las actualizaciones
de Kubernetes se realizan incluyendo <literal>planes de SUC</literal> en el
clúster deseado de las siguientes formas:</para>
<itemizedlist>
<listitem>
<para>Recurso GitRepo de Fleet (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Recurso Bundle de Fleet (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>Para determinar qué recurso se debe usar, consulte la <xref
linkend="management-day2-fleet-determine-use-case"/>.</para>
<para>Si desea desplegar los <literal>planes de SUC de K8s</literal> desde una
herramienta GitOps de terceros, consulte la <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>Despliegue del plan de SUC - Recurso GitRepo</title>
<para>Es posible desplegar un recurso <emphasis role="strong">GitRepo</emphasis>,
que incluye los <literal>planes de SUC de K8s</literal> necesarios, de las
siguientes formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización de
Kubernetes de los nodos del clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>Creación de GitRepo - Interfaz de usuario de Rancher</title>
<para>Para crear un recurso <literal>GitRepo</literal> con la interfaz de usuario
de Rancher, siga la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentación
oficial</link>.</para>
<para>El equipo de Edge mantiene flotas listas para usar para las distribuciones
de Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>
y <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>.
Según su entorno, estas flotas se pueden usar directamente o como plantilla.</para>
<important>
<para>Utilice siempre estas flotas desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge válida.</para>
</important>
<para>Cuando no sea necesario incluir cambios personalizados en los
<literal>planes de SUC</literal> que incluyen estas flotas, los usuarios
pueden consultar directamente las flotas en el repositorio
<literal>suse-edge/fleet-examples</literal>.</para>
<para>Si fuera necesario realizar cambios personalizados (por ejemplo, para añadir
tolerancias personalizadas), los usuarios deben consultar las flotas desde
un repositorio independiente, lo que les permitirá añadir los cambios a los
planes de SUC según sea necesario.</para>
<para>Ejemplos de configuración para un recurso <literal>GitRepo</literal>
utilizando las flotas del repositorio
<literal>suse-edge/fleet-examples</literal>:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>Creación de GitRepo - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga el recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clústeres <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para clústeres <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite la configuración de <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Elimine la sección <literal>spec.targets</literal>: solo se necesita para
clústeres descendentes.</para>
<itemizedlist>
<listitem>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Dirija el espacio de nombres de <literal>GitRepo</literal> al espacio de
nombres <literal>fleet-local</literal>. Esto se hace para desplegar el
recurso en el clúster de gestión.</para>
<itemizedlist>
<listitem>
<para>Para RKE2:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para K3s:</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique los recursos <emphasis role="strong">GitRepo</emphasis> a su
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Compruebe que el recurso <emphasis role="strong">GitRepo</emphasis> creado
se encuentra en el espacio de nombres <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-local

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-local

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>Despliegue del plan de SUC - Recurso Bundle</title>
<para>Es posible desplegar un recurso <emphasis role="strong">Bundle</emphasis>,
que incluye los <literal>planes de SUC de actualización de
Kubernetes</literal> necesarios, de una de estas formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización de
Kubernetes de los nodos del clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>Creación de Bundle - Interfaz de usuario de Rancher</title>
<para>El equipo de Edge mantiene bundles listos para usar para las distribuciones
de Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>
y <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>.
Dependiendo de su entorno, estos bundles se pueden utilizar directamente o
como plantilla.</para>
<important>
<para>Use siempre este bundle desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
válida de Edge.</para>
</important>
<para>Para crear un bundle con la interfaz de usuario de Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En la esquina superior izquierda, haga clic en <emphasis role="strong">☰ →
Continuous Delivery</emphasis> (☰ → Entrega continua).</para>
</listitem>
<listitem>
<para>Diríjase a <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avanzado > Bundles).</para>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create from YAML</emphasis> (Crear desde
YAML).</para>
</listitem>
<listitem>
<para>Desde aquí, puede crear el Bundle de las siguientes maneras:</para>
<note>
<para>Puede haber casos en los que sea necesario incluir cambios personalizados en
los <literal>planes de SUC</literal> que incluye el Bundle (por ejemplo,
para añadir tolerancias personalizadas). Asegúrese de incluir esos cambios
en el Bundle que se generará con los pasos siguientes.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie manualmente el contenido del Bundle para <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>
o <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>
desde <literal>suse-edge/fleet-examples</literal> en la página <emphasis
role="strong">Create from YAML</emphasis> (Crear desde YAML).</para>
</listitem>
<listitem>
<para>Clone el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
desde la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada y seleccione la opción <emphasis role="strong">Read from
File</emphasis> (Leer desde archivo) en la página <emphasis
role="strong">Create from YAML</emphasis> (Crear desde YAML). Desde ahí,
diríjase al Bundle que necesita
(<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>
para RKE2 y
<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>
para K3s). La página <emphasis role="strong">Create from YAML</emphasis>
(Crear desde YAML) se rellenará automáticamente con el contenido del
paquete.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Edite el Bundle en la interfaz de usuario de Rancher:</para>
<itemizedlist>
<listitem>
<para>Cambie el <emphasis role="strong">espacio de nombres</emphasis> del
<literal>bundle</literal> para que dirija al espacio de nombres
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>Cambie los clústeres de <emphasis role="strong">destino</emphasis> para que
el <literal>bundle</literal> apunte a su clúster de gestión
<literal>local</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Hay casos prácticos en los que el clúster <literal>local</literal> podría
tener un nombre diferente.</para>
<para>Para recuperar el nombre del clúster <literal>local</literal>, ejecute el
siguiente comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create</emphasis> (Crear).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>Creación del Bundle - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga los recursos <emphasis role="strong">Bundle</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clústeres <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Para clústeres <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite la configuración del <literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Cambie los clústeres de <emphasis role="strong">destino</emphasis> para que
el <literal>bundle</literal> apunte a su clúster de gestión
<literal>local</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>Hay casos prácticos en los que el clúster <literal>local</literal> podría
tener un nombre diferente.</para>
<para>Para recuperar el nombre del clúster <literal>local</literal>, ejecute el
siguiente comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Cambie el <emphasis role="strong">espacio de nombres</emphasis> del
<literal>bundle</literal> para que dirija al espacio de nombres
<literal>fleet-local</literal>.</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique los recursos <emphasis role="strong">Bundle</emphasis> al
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Compruebe que el recurso <emphasis role="strong">Bundle</emphasis> se ha
creado en el espacio de nombres <literal>fleet-local</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-local

# For K3s
kubectl get bundles k3s-upgrade -n fleet-local

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>Despliegue del plan de SUC - Flujo de trabajo de GitOps de terceros</title>
<para>Puede haber casos prácticos en los que los usuarios deseen incorporar los
<literal>planes de SUC de actualización de Kubernetes</literal> a su propio
flujo de trabajo de GitOps de terceros (por ejemplo,
<literal>Flux</literal>).</para>
<para>Para obtener los recursos de actualización de K8s que necesita, primero
determine la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge del repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
que desea utilizar.</para>
<para>Después, los recursos se encuentran en:</para>
<itemizedlist>
<listitem>
<para>Para actualizar un clúster RKE2:</para>
<itemizedlist>
<listitem>
<para>Para nodos <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para nodos <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Para actualizar un clúster K3s:</para>
<itemizedlist>
<listitem>
<para>Para nodos <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para nodos <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>Estos recursos de <literal>plan</literal> son interpretados por
<literal>System Upgrade Controller</literal> y se deben desplegar en cada
clúster descendente que desee actualizar. Para obtener información sobre el
despliegue de SUC, consulte la <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para comprender mejor cómo usar su flujo de trabajo de GitOps para desplegar
los <emphasis role="strong">planes de SUC</emphasis> para actualizar la
versión de Kubernetes, puede resultar útil echar un vistazo a la descripción
general (<xref linkend="management-day2-fleet-k8s-upgrade-overview"/>) del
procedimiento de actualización mediante <literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade">
<title>Actualización de charts de Helm</title>
<para>Esta sección cubre lo siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-air-gap"/>: contiene
información sobre cómo incluir charts e imágenes OCI relacionados con Edge
en su registro privado.</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure"/>: contiene
información sobre diferentes casos de actualización de charts de Helm y su
procedimiento de actualización.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-air-gap">
<title>Preparación para entornos aislados</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet">
<title>Asegúrese de tener acceso a los recursos de Fleect del chart de Helm</title>
<para>Dependiendo de lo que admita su entorno, puede elegir una de estas opciones:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Aloje los recursos de Fleet de su chart en un servidor Git local al que
pueda acceder su <literal>clúster de gestión</literal>.</para>
</listitem>
<listitem>
<para>Use la interfaz de línea de comandos de Fleet para <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">convertir
un chart de Helm en un Bundle</link> que podrá utilizar directamente sin
necesidad de alojarlo en ningún sitio. Puede descargar la interfaz de línea
de comandos de la página de la <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.12.2">versión</link>.
Para los usuarios de Mac, hay una versión propia de <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Busque los recursos necesarios para su versión de Edge</title>
<orderedlist numeration="arabic">
<listitem>
<para>Vaya a la página de la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de "día 2", busque la versión de Edge a la que desea actualizar su chart y
haga clic en <emphasis role="strong">Assets</emphasis> (Recursos).</para>
</listitem>
<listitem>
<para>En la sección <emphasis role="strong">Assets</emphasis> (Recursos),
descargue los archivos siguientes:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Archivo de versión</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Descripción</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Extrae las imágenes especificadas en el archivo
<literal>edge-release-images.txt</literal> y las empaqueta en un archivo
".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Extrae las imágenes del chart OCI relacionadas con la versión específica de
Edge y las empaqueta en un archivo ".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Carga imágenes desde un archivo ".tar.gz", las vuelve a etiquetar y las
envía a un registro privado.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Toma un directorio que contiene paquetes de charts OCI ".tgz" de Edge y los
carga en un registro privado.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contiene una lista de imágenes de charts OCI relacionadas con una versión
específica de Edge.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contiene una lista de imágenes relacionadas con una versión específica de
Edge.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive">
<title>Cree el archivo de imágenes de la versión de Edge</title>
<para><emphasis>En un equipo con acceso a Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Permita que <literal>edge-save-images.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Genere el archivo de imagen:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esto creará un archivo listo para cargar llamado
<literal>edge-images.tar.gz</literal>.</para>
<note>
<para>Si se especifica la opción <literal>-i|--images</literal>, el nombre del
archivo puede ser diferente.</para>
</note>
</listitem>
<listitem>
<para>Copie este archivo a su equipo <emphasis role="strong">aislado</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive">
<title>Cree el archivo de imágenes del chart OCI de Edge</title>
<para><emphasis>En un equipo con acceso a Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Permita que <literal>edge-save-oci-artefacts.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Genere el archivo de imagen del chart OCI:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esto creará un archivo denominado <literal>oci-artefacts.tar.gz</literal>.</para>
<note>
<para>Si se especifica la opción <literal>-a|--archive</literal>, el nombre del
archivo puede ser distinto.</para>
</note>
</listitem>
<listitem>
<para>Copie este archivo a su equipo <emphasis role="strong">aislado</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine">
<title>Cargue las imágenes de la versión de Edge en su equipo aislado</title>
<para><emphasis>En su equipo aislado:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Inicie sesión en su registro privado (si es necesario):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Permita que <literal>edge-load-images.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Ejecute el guion pasando el archivo <literal>edge-images.tar.gz</literal>
<emphasis role="strong">copiado</emphasis> anteriormente:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>Esto cargará todas las imágenes desde <literal>edge-images.tar.gz</literal>,
las volverá a etiquetar y las enviará al registro especificado en la opción
<literal>--registry</literal>.</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine">
<title>Cargue las imágenes del chart OCI de Edge en su equipo aislado</title>
<para><emphasis>En su equipo aislado:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Inicie sesión en su registro privado (si es necesario):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Permita que <literal>edge-load-oci-artefacts.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Desempaquete el archivo <literal>oci-artefacts.tar.gz</literal> copiado:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>Esto generará un directorio con el formato de nombre
<literal>edge-release-oci-tgz-&lt;fecha&gt;</literal>.</para>
</listitem>
<listitem>
<para>Pase este directorio al guion <literal>edge-load-oci-artefacts.sh</literal>
para cargar las imágenes del chart OCI de Edge a su registro privado:</para>
<note>
<para>Este guion presupone que la interfaz de línea de comandos de
<literal>Helm</literal> ya está preinstalada en su entorno. Para obtener
instrucciones sobre la instalación de Helm, consulte <link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>
(Instalación de Helm).</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution">
<title>Configure el registro privado en su distribución de Kubernetes</title>
<para>Para RKE2, consulte <link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration</link> (Configuración del registro privado)</para>
<para>Para K3s, consulte <link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration</link> (Configuración del registro privado)</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure">
<title>Procedimiento de actualización</title>
<para>Esta sección se centra en los siguientes casos prácticos del procedimiento
de actualización de Helm:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>Los charts de Helm desplegados manualmente no se pueden actualizar de forma
fiable. Recomendamos volver a desplegar el chart de Helm utilizando el
método <xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/>.</para>
</important>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>Tengo un nuevo clúster y me gustaría desplegar y gestionar un chart de Helm
de Edge</title>
<para>En esta sección se trata lo siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>Prepare los recursos de Fleet para su chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Adquiera los recursos de Fleet del chart desde la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge que desee usar.</para>
</listitem>
<listitem>
<para>Diríjase a la flota del chart de Helm
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Si tiene intención de utilizar un flujo de trabajo
de GitOps</emphasis>, copie el directorio Fleet del chart en el repositorio
Git desde donde realizará la operación de GitOps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, si el chart de Helm
requiere que se configuren sus <emphasis role="strong">valores</emphasis>,
edite la configuración de <literal>.helm.values</literal> dentro del archivo
<literal>fleet.yaml</literal> del directorio copiado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, puede haber casos en los
que sea necesario añadir recursos adicionales a la flota de su chart para
que se adapte mejor a su entorno. Para obtener información sobre cómo
mejorar su directorio de Fleet, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository
Contents</link> (Contenido del repositorio Git).</para>
</listitem>
</orderedlist>
<note>
<para>En algunos casos, el tiempo de espera predeterminado que Fleet usa para las
operaciones de Helm puede ser insuficiente, lo que da lugar al siguiente
error:</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>Si fuera el caso, añada la propiedad <link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>
en la configuración de <literal>helm</literal> de su archivo
<literal>fleet.yaml</literal>.</para>
</note>
<para>Esto es un <emphasis role="strong">ejemplo</emphasis> del aspecto que
tendría el chart de Helm de <literal>longhorn</literal>:</para>
<itemizedlist>
<listitem>
<para>Estructura del repositorio Git del usuario:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>Contenido de <literal>fleet.yaml</literal> con datos de
<literal>Longhorn</literal> del usuario:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "106.2.0+up1.8.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>Estos son solo valores de ejemplo que se utilizan para mostrar
configuraciones personalizadas en el chart de
<literal>longhorn</literal>. <emphasis role="strong">NO</emphasis> deben
considerarse directrices de despliegue para el chart de
<literal>longhorn</literal>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>Despliegue la flota en su chart</title>
<para>Puede desplegar la flota de su chart usando GitRepo (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>)
o Bundle (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>).</para>
<note>
<para>Al desplegar su flota, si recibe un mensaje con la indicación
<literal>Modified</literal> (Modificado), asegúrese de añadir la entrada
<literal>comparePatches</literal> correspondiente a la sección
<literal>diff</literal> del Fleet. Para obtener más información, consulte
<link xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to
Ignore Modified GitRepos</link> (Generación de diffs para ignorar recursos
GitRepo modificados).</para>
</note>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>El recurso <link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> de Fleet
incluye información sobre cómo acceder a los recursos de Fleet de su chart y
a qué clústeres deben aplicarse esos recursos.</para>
<para>El recurso <literal>GitRepo</literal> se puede desplegar mediante la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">interfaz
de usuario de Rancher</link> o, manualmente, <link
xl:href="https://fleet.rancher.io/tut-deployment">desplegando</link> el
recurso en el <literal>clúster de gestión</literal>.</para>
<para>Recurso <literal>GitRepo</literal> de <emphasis
role="strong">Longhorn</emphasis> de ejemplo para el despliegue <emphasis
role="strong">manual</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-local
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url</screen>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>Bundle</title>
<para>Los recursos <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> contienen los
recursos sin procesar de Kubernetes que debe desplegar Fleet. Normalmente,
se recomienda utilizar el enfoque de <literal>GitRepo</literal>, pero en
entornos aislados o que no admitan un servidor Git local, los
<literal>Bundles</literal> pueden ayudarle a propagar su chart de Helm de
Fleet a los clústeres de destino.</para>
<para>Es posible desplegar un <literal>Bundle</literal> mediante la interfaz de
usuario de Rancher seleccionando <literal>Continuous Delivery → Advanced →
Bundles → Create from YAML</literal> (Entrega continua → Avanzado → Bundles
→ Crear desde YAML)) o desplegando manualmente el recurso
<literal>Bundle</literal> en el espacio de nombres de Fleet correcto. Para
obtener información sobre los espacios de nombres de Fleet, consulte la
<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentación
original</link>.</para>
<para>Es posible crear <literal>Bundles</literal> para los charts de Helm de Edge
<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">convirtiendo
un chart de Helm en un Bundle de Fleet</link>.</para>
<para>A continuación, hay un ejemplo de cómo crear un recurso
<literal>Bundle</literal> a partir de plantillas de flota de un chart de
Helm de <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
y <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
y cómo desplegar manualmente este Bundle en su <literal>clúster de
gestión</literal>.</para>
<note>
<para>Para ilustrar el flujo de trabajo, el siguiente ejemplo usa la estructura de
directorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Diríjase a la plantilla de flota de chart de <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>Cree un archivo <literal>targets.yaml</literal> que indicará a Fleet en qué
clústeres debe desplegar el chart de Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
<note>
<para>Hay casos prácticos en los que el clúster local podría tener un nombre
diferente.</para>
<para>Para recuperar el nombre del clúster local, ejecute el siguiente comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Convierta la flota de chart de Helm de <literal>Longhorn</literal> en un
recurso Bundle mediante <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<note>
<para>Puede obtener la interfaz de línea de comandos de Fleet
(<literal>fleet-linux-amd64</literal>) de la página <emphasis
role="strong">Assets</emphasis> (Recursos) de su <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.12.2">versión</link>.</para>
<para>Para usuarios de Mac, hay una versión propia de <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>Diríjase a la plantilla de flota de chart <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>Cree un archivo <literal>targets.yaml</literal> que indicará a Fleet en qué
clústeres debe desplegar el chart de Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>Convierta la flota de chart de Helm de la <literal>CRD de Longhorn</literal>
en un recurso Bundle mediante <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>Despliegue los archivos <literal>longhorn-bundle.yaml</literal> y
<literal>longhorn-crd-bundle.yaml</literal> en su <literal>clúster de
gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>Siga estos pasos para asegurarse de que <literal>SUSE Storage</literal> se
despliega en todos los clústeres de gestión especificados.</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>Gestione el chart de Helm desplegado</title>
<para>Cuando se complete el despliegue con Fleet, para actualizaciones de charts
de Helm, consulte la <xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>Quiero actualizar un chart de Helm gestionado por Fleet</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine la versión a la que debe actualizar su chart para que sea
compatible con la versión deseada de Edge. Puede consultar la versión del
chart de Helm para cada versión de Edge en las notas de la versión (<xref
linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>En su repositorio Git supervisado por Fleet, edite el archivo
<literal>fleet.yaml</literal> del chart de Helm con la <emphasis
role="strong">versión</emphasis> y el <emphasis
role="strong">repositorio</emphasis> correctos del chart, tal y como se
indica en las notas de la versión (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>Después de confirmar y enviar los cambios al repositorio, se activará una
actualización del chart de Helm deseado.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>Quiero actualizar un chart de Helm desplegado mediante EIB</title>
<para><xref linkend="components-eib"/> despliega charts de Helm creando un recurso
<literal>HelmChart</literal> y usando el <literal>helm-controller</literal>
introducido por la función de integración de Helm <link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>.</para>
<para>Para garantizar que un chart de Helm desplegado mediante
<literal>EIB</literal> se actualice correctamente, los usuarios deben
realizar una actualización de los recursos <literal>HelmChart</literal>
correspondientes.</para>
<para>A continuación, encontrará más información:</para>
<itemizedlist>
<listitem>
<para>La descripción general (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>)
del proceso de actualización.</para>
</listitem>
<listitem>
<para>Los pasos necesarios para la actualización (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>).</para>
</listitem>
<listitem>
<para>Un ejemplo (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>)
que muestra la actualización de un chart de <link
xl:href="https://longhorn.io">Longhorn</link> con el método explicado.</para>
</listitem>
<listitem>
<para>Cómo usar el proceso de actualización con una herramienta GitOps diferente
(<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>Descripción general</title>
<para>Los charts de Helm que se despliegan mediante <literal>EIB</literal> se
actualizan usando una <literal>flota</literal> llamada <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>.</para>
<para>Esta <literal>flota</literal> procesa los datos <emphasis
role="strong">proporcionados por el usuario</emphasis> para <emphasis
role="strong">actualizar</emphasis> un conjunto específico de recursos
HelmChart.</para>
<para>Al actualizar estos recursos se activa <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>,
que <emphasis role="strong">actualiza</emphasis> los charts de Helm
asociados con los recursos <literal>HelmChart</literal> modificados.</para>
<para>Solo se espera de los usuarios que:</para>
<orderedlist numeration="arabic">
<listitem>
<para><link xl:href="https://helm.sh/docs/helm/helm_pull/">Extraigan</link>
localmente los archivos de cada chart de Helm que deba actualizarse.</para>
</listitem>
<listitem>
<para>Pasen estos archivos al guion <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal>, que incluirá los datos de
estos archivos en la flota <literal>eib-charts-upgrader</literal>.</para>
</listitem>
<listitem>
<para>Desplieguen la flota <literal>eib-charts-upgrader</literal> a su
<literal>clúster de gestión</literal>. Esto se hace con un recurso
<literal>GitRepo</literal> o <literal>Bundle</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, <literal>eib-charts-upgrader</literal>, con la ayuda de
Fleet, incluirá sus recursos en el clúster de gestión deseado.</para>
<para>Estos recursos incluyen:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Un conjunto de <literal>secretos</literal> que guardan los datos del chart
de Helm <emphasis role="strong">proporcionados por el usuario</emphasis>.</para>
</listitem>
<listitem>
<para>Un <literal>trabajo de Kubernetes</literal> que desplegará un
<literal>pod</literal> que montará los <literal>secretos</literal>
mencionados anteriormente y, basándose en ellos, <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">parcheará</link>
los recursos HelmChart correspondientes.</para>
</listitem>
</orderedlist>
<para>Como se mencionó anteriormente, esto activará
<literal>helm-controller</literal>, que llevará a cabo la actualización real
del chart de Helm.</para>
<para>A continuación, encontrará un diagrama de la descripción anterior:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-management-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>actualización eib del helm de gestión de día 2 de fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>Pasos para la actualización</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clone el repositorio <literal>suse-edge/fleet-examples</literal> de la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">etiqueta</link>
de versión correcta.</para>
</listitem>
<listitem>
<para>Cree un directorio en el que almacenarán los archivos de los charts de Helm
extraídos.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>En el directorio recién creado, <link
xl:href="https://helm.sh/docs/helm/helm_pull/">extraiga</link> los archivos
de los charts de Helm que desea actualizar:</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>En la página <emphasis role="strong">Assets</emphasis> (Recursos) de la
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">etiqueta
de versión</link> deseada, descargue el guion
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</listitem>
<listitem>
<para>Ejecute el guion <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>Para cada archivo de chart del directorio <literal>--archive-dir</literal>,
el guion genera un archivo <literal>Kubernetes Secret YAML</literal> que
contiene los datos de actualización de chart y lo guardar en el directorio
<literal>base/secrets</literal> de la flota especificada por
<literal>--fleet-path</literal>.</para>
<para>El guion <literal>generate-chart-upgrade-data.sh</literal> también aplica
modificaciones adicionales a la flota para garantizar que la carga de
trabajo desplegada por la flota utilice correctamente los archivos
<literal>Kubernetes Secret YAML</literal> generados.</para>
<important>
<para>Los usuarios no deben cambiar nada de lo que genera el guion
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</important>
</listitem>
</orderedlist>
<para>Los pasos siguientes dependen del entorno en el que se esté ejecutando:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En un entorno que admita GitOps (por ejemplo, que no esté aislado, o que
esté aislado pero permita la asistencia de un servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie la flota <literal>fleets/day2/eib-charts-upgrader</literal> en el
repositorio que usará para GitOps.</para>
<note>
<para>Asegúrese de que la flota incluye los cambios realizados por el guion
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</note>
</listitem>
<listitem>
<para>Configure un recurso <literal>GitRepo</literal> que se usará para incluir
todos los recursos de la flota <literal>eib-charts-upgrader</literal>.</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Para la configuración y el despliegue de <literal>GitRepo</literal> mediante
la interfaz de usuario de Rancher, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI</link> (Acceso a Fleet en la interfaz de usuario de
Rancher).</para>
</listitem>
<listitem>
<para>Para la configuración y el despliegue manuales de
<literal>GitRepo</literal>, consulte <link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a
Deployment</link> (Creación de un despliegue).</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>En un entorno que no admita GitOps (por ejemplo, un entorno aislado que no
permita el uso de un servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Descargue el binario de <literal>fleet-cli</literal> de la página de <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.12.2">versión</link>
de <literal>rancher/fleet</literal> (<literal>fleet-linux-amd64</literal>
para Linux). Los usuarios de Mac pueden usar una versión propia: <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Diríjase a la flota <literal>eib-charts-upgrader</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Cree un archivo <literal>targets.yaml</literal> que indicará a Fleet dónde
debe desplegar los recursos:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To map the local(management) cluster
- clusterName: local
EOF</screen>
<note>
<para>Hay casos prácticos en los que el clúster <literal>local</literal> podría
tener un nombre diferente.</para>
<para>Para recuperar el nombre del clúster <literal>local</literal>, ejecute el
siguiente comando:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>Utilice <literal>fleet-cli</literal> para convertir la flota en un recurso
<literal>Bundle</literal>:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>Esto creará un Bundle (<literal>bundle.yaml</literal>) que contendrá todos
los recursos de plantilla de la flota
<literal>eib-charts-upgrader</literal>.</para>
<para>Para obtener más información sobre el comando <literal>fleet
apply</literal>, consulte <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>.</para>
<para>Para obtener más información sobre cómo convertir flotas en Bundles,
consulte <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Conversión de un chart de Helm en un
Bundle).</para>
</listitem>
<listitem>
<para>Despliegue el <literal>Bundle</literal>. Puede hacerlo de dos formas:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Mediante la interfaz de usuario de Rancher: seleccione <emphasis
role="strong">Continuous Delivery → Advanced → Bundles → Create from
YAML</emphasis> (Entrega continua → Avanzado → Bundles → Crear desde YAML) y
pegue el contenido de <literal>bundle.yaml</literal> o haga clic en la
opción <literal>Read from File</literal> (Leer desde archivo) y pase el
archivo.</para>
</listitem>
<listitem>
<para>Manualmente: despliegue el archivo <literal>bundle.yaml</literal>
manualmente dentro de su <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Al ejecutar estos pasos, se desplegará correctamente el recurso
<literal>GitRepo/Bundle</literal>. Fleet recogerá el recurso y su contenido
se desplegará en los clústeres de destino que el usuario haya especificado
en los pasos anteriores. Para obtener una descripción general del proceso,
consulte la <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>.</para>
<para>Para obtener información sobre cómo realizar un seguimiento del proceso de
actualización, consulte la <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>.</para>
<important>
<para>Cuando haya verificado que el chart se ha actualizado correctamente, elimine
el recurso <literal>Bundle/GitRepo</literal>.</para>
<para>Esto eliminará los recursos de actualización que ya no son necesarios del
clúster de <literal>gestión</literal>, lo que garantizará que no se
produzcan conflictos entre versiones en el futuro.</para>
</important>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>Ejemplo</title>
<note>
<para>El siguiente ejemplo muestra cómo actualizar un chart de Helm desplegado
mediante <literal>EIB</literal> de una versión a otra en un clúster de
<literal>gestión</literal>. Tenga en cuenta que las versiones utilizadas en
este ejemplo <emphasis role="strong">no</emphasis> son recomendaciones. Para
obtener recomendaciones de versiones específicas para una versión de Edge,
consulte las notas de la versión (<xref linkend="release-notes"/>).</para>
</note>
<para><emphasis>Caso práctico:</emphasis></para>
<itemizedlist>
<listitem>
<para>Se ejecuta un clúster de <literal>gestión</literal> en una versión anterior
de <link xl:href="https://longhorn.io">Longhorn</link>.</para>
</listitem>
<listitem>
<para>El clúster se ha desplegado mediante EIB, utilizando el siguiente
<emphasis>fragmento</emphasis> de definición de imagen:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal> debe actualizarse a una versión compatible
con la versión Edge 3.3.1. Esto significa que debe actualizarse a
<literal>106.2.0+up1.8.1</literal>.</para>
</listitem>
<listitem>
<para>Se entiende que el <literal>clúster de gestión</literal> está <emphasis
role="strong">aislado</emphasis>, sin asistencia para un servidor Git local
y con una configuración de Rancher operativa.</para>
</listitem>
</itemizedlist>
<para>Siga los pasos de actualización (<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>):</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clone el repositorio <literal>suse-edge/fleet-example</literal> desde la
etiqueta <literal>release-3.3.0</literal>.</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.3.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Cree un directorio donde se almacenará el archivo de actualización de
<literal>Longhorn</literal>.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Extraiga la versión del archivo de chart de <literal>Longhorn</literal>
deseada:</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.8.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 106.2.0+up1.8.1

# Pull the Longhorn 1.8.1 chart archive
helm pull rancher-charts/longhorn --version 106.2.0+up1.8.1</screen>
</listitem>
<listitem>
<para>Fuera del directorio <literal>archives</literal>, descargue el guion
<literal>generate-chart-upgrade-data.sh</literal> de la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">etiqueta</link>
de la versión <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>La configuración del directorio debería ser similar a:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-106.2.0+up1.8.1.tgz
│   └── longhorn-crd-106.2.0+up1.8.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>Ejecute el guion <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>La estructura del directorio tras la ejecución del guion debería ser similar
a esto:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-106.2.0+up1.8.1.tgz
│   └── longhorn-crd-106.2.0+up1.8.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Los archivos modificados en Git deberían tener este aspecto:</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
	modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
	modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
	fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
	fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>Cree un <literal>Bundle</literal> para la flota
<literal>eib-charts-upgrader</literal>:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>En primer lugar, diríjase a la flota:</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>A continuación, cree un archivo <literal>targets.yaml</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>Después, use el binario de <literal>fleet-cli</literal> para convertir la
flota en un Bundle:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Despliegue el Bundle mediante la interfaz de usuario de Rancher:</para>
<figure>
<title>Despliegue del Bundle mediante la interfaz de Rancher</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>ejemplo 1 de actualización del chart de helm de día 2</phrase></textobject>
</mediaobject></figure>
<para>Ahora, seleccione <emphasis role="strong">Read from File</emphasis> (Leer
desde archivo) y busque el archivo <literal>bundle.yaml</literal> en su
sistema.</para>
<para>El <literal>Bundle</literal> se rellenará automáticamente en la interfaz de
usuario de Rancher.</para>
<para>Seleccione <emphasis role="strong">Create</emphasis> (Crear).</para>
</listitem>
<listitem>
<para>Cuando haya finalizado el despliegue correctamente, el Bundle deberá tener
un aspecto similar al siguiente:</para>
<figure>
<title>Bundle desplegado correctamente</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>ejemplo 2 de actualización de chart del helm de día 2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>Cuando haya finalizado el despliegue del <literal>Bundle</literal>
correctamente, para supervisar el proceso de actualización:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Verifique los registros del <literal>pod de actualización</literal>:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_management.png" width="100%"/>
</imageobject>
<textobject><phrase>ejemplo 3 de actualización de chart de helm de día 2 gestión</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Ahora, verifique los registros del pod creado para la actualización por
helm-controller:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>El nombre del pod tendrá este formato:
<literal>helm-install-longhorn-&lt;sufijo aleatorio&gt;</literal></para>
</listitem>
<listitem>
<para>El pod estará en el espacio de nombres donde se desplegó el recurso
<literal>HelmChart</literal>. En nuestro caso,
<literal>kube-system</literal>.</para>
<figure>
<title>Registros de chart de Longhorn actualizado correctamente</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_management.png" width="100%"/>
</imageobject>
<textobject><phrase>ejemplo 4 de actualización de chart de helm de día 2 gestión</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Compruebe que la versión de <literal>HelmChart</literal> se ha
actualizado. Para ello, diríjase a la sección <literal>HelmCharts</literal>
de Rancher seleccionando <literal>More Resources → HelmCharts</literal> (Más
recursos → HelmCharts). Seleccione el espacio de nombres donde se desplegó
el chart; en este ejemplo, sería <literal>kube-system</literal>.</para>
</listitem>
<listitem>
<para>Por último, compruebe que los pods de Longhorn se estén ejecutando.</para>
</listitem>
</orderedlist>
<para>Después de realizar las validaciones anteriores, se puede entender con
seguridad que el chart de Helm de Longhorn se ha actualizado a la versión
<literal>106.2.0+up1.8.1</literal>.</para>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>Actualización del chart de Helm con una herramienta GitOps de terceros</title>
<para>Puede darse el caso de que los usuarios deseen utilizar este procedimiento
de actualización con un flujo de trabajo de GitOps que no sea Fleet (por
ejemplo, <literal>Flux</literal>).</para>
<para>Para generar los recursos necesarios para el procedimiento de actualización,
puede usar el guion <literal>generate-chart-upgrade-data.sh</literal> para
rellenar la flota <literal>eib-charts-upgrader</literal> con los datos
proporcionados por el usuario. Para obtener más información sobre cómo
hacerlo, consulte la <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
<para>Cuando haya completado la configuración, puede utilizar <link
xl:href="https://kustomize.io">kustomize</link> para generar una solución
totalmente funcional que puede desplegar en su clúster:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>Si desea incluir la solución en su flujo de trabajo de GitOps, puede
eliminar el archivo <literal>fleet.yaml</literal> y usar lo que queda como
una configuración válida de <literal>Kustomize</literal>. No olvide ejecutar
primero el guion <literal>generate-chart-upgrade-data.sh</literal>, para
poder rellenar la configuración de <literal>Kustomize</literal> con los
datos de los charts de Helm a los que desea actualizar.</para>
<para>Para comprender cómo se pretende usar este flujo de trabajo, puede resultar
útil consultar la <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>
y la <xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="day2-downstream-clusters">
<title>Clústeres descendentes</title>
<important>
<para>Los pasos siguientes no se aplican a clústeres
<literal>descendentes</literal> gestionados por SUSE Edge for Telco (<xref
linkend="atip"/>). Para obtener información sobre cómo actualizar estos
clústeres, consulte la <xref linkend="atip-lifecycle-downstream"/>.</para>
</important>
<para>Esta sección describe las formas posibles de realizar operaciones de "día 2"
para diferentes partes de su clúster <literal>descendente</literal>.</para>
<section xml:id="downstream-day2-fleet">
<title>Fleet</title>
<para>Esta sección ofrece información sobre cómo realizar operaciones de "día 2"
utilizando el componente Fleet (<xref linkend="components-fleet"/>).</para>
<para>En esta sección se tratan los siguientes temas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-components"/>: los componentes
predeterminados usados para todas las operaciones de "día 2".</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-determine-use-case"/>: proporciona una
descripción general de los recursos personalizados de Fleet que se usarán y
su idoneidad para diferentes casos de uso de operaciones de "día 2".</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-upgrade-workflow"/>: proporciona una guía del
flujo de trabajo para ejecutar operaciones de "día 2" con Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade"/>: describe cómo realizar
actualizaciones del sistema operativo con Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade"/>: describe cómo realizar
actualizaciones de la versión de Kubernetes con Fleet.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade"/>: describe cómo realizar
actualizaciones de charts de Helm con Fleet.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-components">
<title>Componentes</title>
<para>A continuación, encontrará una descripción de los componentes
predeterminados que deben configurarse en el clúster
<literal>descendente</literal> para poder realizar correctamente operaciones
de "día 2" con Fleet.</para>
<section xml:id="id-system-upgrade-controller-suc-2">
<title>System Upgrade Controller (SUC)</title>
<note>
<para><emphasis role="strong">Debe</emphasis> desplegarse en cada clúster
descendente.</para>
</note>
<para><emphasis role="strong">System Upgrade Controller</emphasis> es responsable
de ejecutar tareas en nodos específicos según los datos de configuración
proporcionados mediante un recurso personalizado, denominado
<literal>plan</literal>.</para>
<para><emphasis role="strong">SUC</emphasis> se utiliza activamente para
actualizar el sistema operativo y la distribución de Kubernetes.</para>
<para>Para obtener más información sobre el componente <emphasis
role="strong">SUC</emphasis> y cómo encaja en la pila de Edge, consulte el
<xref linkend="components-system-upgrade-controller"/>.</para>
<para>Para obtener información sobre cómo desplegar <emphasis
role="strong">SUC</emphasis>, determine primero su caso práctico <xref
linkend="downstream-day2-fleet-determine-use-case"/>) y, a continuación,
consulte las secciones Instalación de System Upgrade Controller - GitRepo
(<xref linkend="components-system-upgrade-controller-fleet-gitrepo"/>) o
Instalación de System Upgrade Controller - Bundle (<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>).</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-determine-use-case">
<title>Determinación de su caso de uso</title>
<para>Fleet usa dos tipos de <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> para permitir la gestión de los recursos de Kubernetes
y Helm.</para>
<para>A continuación, encontrará información sobre la finalidad de estos recursos
y los casos de uso para los que son más adecuados en el contexto de las
operaciones de "día 2".</para>
<section xml:id="id-gitrepo-2">
<title>GitRepo</title>
<para>Un <literal>GitRepo</literal> es un recurso de Fleet (<xref
linkend="components-fleet"/>) que representa un repositorio Git desde el que
<literal>Fleet</literal> puede crear <literal>Bundles</literal>. Cada
<literal>Bundle</literal> se crea en función de las rutas de configuración
definidas dentro del recurso <literal>GitRepo</literal>. Para obtener más
información, consulte la documentación de <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>.</para>
<para>En el contexto de las operaciones de "día 2", los recursos
<literal>GitRepo</literal> se suelen usar para desplegar
<literal>SUC</literal> o <literal>planes de SUC</literal> en entornos
<emphasis role="strong">no aislados</emphasis> que emplean un enfoque de
<emphasis>GitOps de Fleet</emphasis>.</para>
<para>Como alternativa, los recursos <literal>GitRepo</literal> también se pueden
utilizar para desplegar <literal>SUC</literal> o <literal>planes de
SUC</literal> en entornos <emphasis role="strong">aislados</emphasis>,
<emphasis role="strong">siempre que se refleje la configuración del
repositorio a través de un servidor Git local</emphasis>.</para>
</section>
<section xml:id="id-bundle-2">
<title>Bundle</title>
<para>Los <literal>Bundles</literal> contienen recursos <emphasis
role="strong">sin procesar</emphasis> de Kubernetes que se desplegarán en el
clúster de destino. Por lo general, se crean a partir de un recurso
<literal>GitRepo</literal>, pero hay casos en los que se pueden desplegar
manualmente. Para obtener más información, consulte la documentación de
<link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>.</para>
<para>En el contexto de las operaciones de "día 2", los recursos
<literal>Bundle</literal> se suelen usar para desplegar
<literal>SUC</literal> o <literal>planes de SUC</literal> en entornos
<emphasis role="strong">aislados</emphasis> que no utilizan algún tipo de
procedimiento <emphasis>GitOps local</emphasis> (por ejemplo, un <emphasis
role="strong">servidor Git local</emphasis>).</para>
<para>Alternativamente, si su caso de uso no permite un flujo de trabajo
<emphasis>GitOps</emphasis> (por ejemplo, porque use un repositorio Git),
también se pueden utilizar recursos <literal>Bundle</literal> para desplegar
<literal>SUC</literal> o <literal>planes de SUC</literal> en entornos
<emphasis role="strong">no aislados</emphasis>.</para>
</section>
</section>
<section xml:id="downstream-day2-upgrade-workflow">
<title>Flujo de trabajo de día 2</title>
<para>A continuación, se describe el flujo de trabajo de "día 2" que se debe
seguir al actualizar un clúster descendente a una versión específica de
Edge.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Actualización del sistema operativo (<xref
linkend="downstream-day2-fleet-os-upgrade"/>)</para>
</listitem>
<listitem>
<para>Actualización de la versión de Kubernetes (<xref
linkend="downstream-day2-fleet-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Actualización del chart de Helm (<xref
linkend="downstream-day2-fleet-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade">
<title>Actualización del sistema operativo</title>
<para>En esta sección se describe cómo realizar una actualización del sistema
operativo utilizando <xref linkend="components-fleet"/> y <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>En esta sección se tratan los siguientes temas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-components"/>: componentes
adicionales usados por el proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-overview"/>: descripción
general del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-requirements"/>: requisitos
del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment"/>:
información sobre cómo desplegar <literal>planes de SUC</literal>,
responsables de iniciar el proceso de actualización.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-os-upgrade-components">
<title>Componentes</title>
<para>Esta sección trata sobre los componentes personalizados que el proceso de
<literal>actualización del sistema operativo</literal> usa en lugar de los
componentes predeterminados de "día 2" (<xref
linkend="downstream-day2-fleet-components"/>).</para>
<section xml:id="downstream-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>La actualización del sistema operativo en un nodo específico se gestiona
mediante un servicio <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>.</para>
<para>En función del tipo de actualización que requiera el sistema operativo de
una versión de Edge a otra, se creará un servicio diferente:</para>
<itemizedlist>
<listitem>
<para>Para las versiones de Edge que requieren la misma versión del sistema
operativo (por ejemplo, la <literal>6.0</literal>), se creará el servicio
<literal>os-pkg-update.service</literal>. Use <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para realizar una <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">actualización
normal del paquete</link>.</para>
</listitem>
<listitem>
<para>Para las versiones de Edge que requieran una migración de la versión del
sistema operativo (por ejemplo, de la <literal>6.0</literal> a la
<literal>6.1</literal>), se creará el servicio
<literal>os-migration.service</literal>. Use <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
para realizar:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Una <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">actualización
normal del paquete</link> que garantice que todos los paquetes estén
actualizados para mitigar cualquier fallo en la migración relacionado con
versiones antiguas del paquete.</para>
</listitem>
<listitem>
<para>Una migración del sistema operativo mediante el comando <literal>zypper
migration</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>Los servicios mencionados se incluyen a cada nodo mediante un <literal>plan
de SUC</literal> que debe estar ubicado en el clúster descendente que
necesita una actualización del sistema operativo.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-overview">
<title>Descripción general</title>
<para>La actualización del sistema operativo para los nodos del clúster
descendente se realiza con <literal>Fleet</literal> y <literal>System
Upgrade Controller (SUC)</literal>.</para>
<para><emphasis role="strong">Fleet</emphasis> se usa para desplegar y gestionar
<literal>planes de SUC</literal> en el clúster deseado.</para>
<note>
<para>Los <literal>planes de SUC</literal> son <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que describen los pasos que debe seguir
<literal>SUC</literal> para ejecutar una tarea específica en un conjunto de
nodos. Para ver un ejemplo de cómo es un <literal>plan de SUC</literal>,
consulte el <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositorio
original</link>.</para>
</note>
<para>Los <literal>planes de SUC de sistema operativo</literal> se envían a cada
clúster desplegando un recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> o <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> a un <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espacio
de trabajo</link> de Fleet específico. Fleet recupera el
<literal>GitRepo/Bundle</literal> desplegado y despliega su contenido (los
<literal>planes de SUC de sistema operativo</literal>) en los clústeres
deseados.</para>
<note>
<para>Los recursos <literal>GitRepo/Bundle</literal> siempre se despliegan en el
<literal>clúster de gestión</literal>. Que se use un recurso
<literal>GitRepo</literal> o <literal>Bundle</literal> depende del caso de
uso. Consulte la <xref linkend="downstream-day2-fleet-determine-use-case"/>
para obtener más información.</para>
</note>
<para>Los <literal>planes de SUC de sistema operativo</literal> describen el
siguiente flujo de trabajo:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Use siempre el comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
en los nodos antes de las actualizaciones del sistema operativo.</para>
</listitem>
<listitem>
<para>Actualice siempre los nodos <literal>control-plane</literal> antes que los
nodos <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Actualice siempre el clúster en un <emphasis role="strong">único</emphasis>
nodo cada vez.</para>
</listitem>
</orderedlist>
<para>Una vez desplegados los <literal>planes de SUC de sistema
operativo</literal>, el flujo de trabajo es el siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconcilia los <literal>planes de SUC de sistema operativo</literal>
desplegados y crea un <literal>trabajo de Kubernetes</literal> en <emphasis
role="strong">cada nodo</emphasis>.</para>
</listitem>
<listitem>
<para>El <literal>trabajo de Kubernetes</literal> crea un servicio systemd.service
(<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>)
para la actualización de paquetes o para la migración del sistema operativo.</para>
</listitem>
<listitem>
<para>El servicio <literal>systemd.service</literal> creado activa el proceso de
actualización del sistema operativo en el nodo específico.</para>
<important>
<para>Cuando finaliza el proceso de actualización del sistema operativo, el nodo
correspondiente se <literal>rearranca</literal> para aplicar las
actualizaciones en el sistema.</para>
</important>
</listitem>
</orderedlist>
<para>A continuación, encontrará un diagrama de la descripción anterior:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>actualización del sistema operativo descendente día 2 fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-requirements">
<title>Requisitos</title>
<para><emphasis>Generales:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Equipo registrado en el Centro de servicios al
cliente de SUSE</emphasis>: todos los nodos del clúster descendentes deben
estar registrados en <literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>. Es
necesario para que el servicio <literal>systemd.service</literal> respectivo
pueda conectarse correctamente al repositorio RPM deseado.</para>
<important>
<para>Para las versiones de Edge que requieren una migración de la versión del
sistema operativo (por ejemplo, de la <literal>6.0</literal> a la
<literal>6.1</literal>), asegúrese de que su clave del Centro de servicios
al cliente de SUSE admita la migración a la nueva versión.</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">Asegúrese de que las tolerancias del plan de SUC
coincidan con las tolerancias de los nodos</emphasis>. Si los nodos de su
clúster de Kubernetes tienen <emphasis role="strong">intolerancias
(taints)</emphasis> personalizadas, asegúrese de añadir <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerancias
(tolerations)</link> para ellas en los <emphasis role="strong">planes de
SUC</emphasis>. De forma predeterminada, los <emphasis role="strong">planes
de SUC</emphasis> solo tienen tolerancias para los nodos de <emphasis
role="strong">plano de control</emphasis>. Las tolerancias predeterminadas
son:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Cualquier tolerancia adicional debe añadirse en la sección
<literal>.spec.tolerations</literal> de cada plan. Los <emphasis
role="strong">planes de SUC</emphasis> relacionados con la actualización del
sistema operativo se pueden encontrar en el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
en
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>.
<emphasis role="strong">Asegúrese de usar planes que tengan una etiqueta de
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de repositorio válida.</emphasis></para>
<para>Esto es un ejemplo de definición de tolerancias personalizadas para el plan
de SUC de <emphasis role="strong">plano de control</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>En entornos aislados:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Duplique los repositorios RPM de
SUSE</emphasis>. Los repositorios RPM del sistema operativo deben duplicarse
y guardarse de forma local para que el servicio
<literal>systemd.service</literal> pueda acceder a ellos. Para ello, utilice
<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>
o <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment">
<title>Actualización del sistema operativo - Despliegue del plan de SUC</title>
<important>
<para>En entornos que se hayan actualizado previamente mediante este
procedimiento, los usuarios deben asegurarse de que se haya completado
<emphasis role="strong">uno</emphasis> de los pasos siguientes:</para>
<itemizedlist>
<listitem>
<para><literal>Elimine cualquier plan de SUC desplegado anteriormente relacionado
con versiones anteriores de Edge del clúster descendente</literal>. Esto se
puede hacer eliminando el clúster deseado de la <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuración
de destino</link> de <literal>GitRepo/Bundle</literal> existente o
eliminando por completo el recurso <literal>GitRepo/Bundle</literal>.</para>
</listitem>
<listitem>
<para><literal>Reutilice el recurso GitRepo/Bundle existente</literal>. Puede
hacerlo haciendo que la revisión del recurso apunte a una nueva etiqueta que
contenga los recursos de Fleet correctos para la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Esto se hace con el fin de evitar conflictos entre los <literal>planes de
SUC</literal> para versiones anteriores de Edge.</para>
<para>Si los usuarios intentan actualizar, mientras haya <literal>planes de
SUC</literal> existentes en el clúster descendente, verán el siguiente error
de Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Como se menciona en la <xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>, las actualizaciones
del sistema operativo se realizan incluyendo <literal>planes de
SUC</literal> en el clúster deseado de las siguientes formas:</para>
<itemizedlist>
<listitem>
<para>Recurso <literal>GitRepo</literal> de Fleet: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>.</para>
</listitem>
<listitem>
<para>Recurso <literal>Bundle</literal> de Fleet: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle"/>.</para>
</listitem>
</itemizedlist>
<para>Para determinar qué recurso se debe usar, consulte la <xref
linkend="downstream-day2-fleet-determine-use-case"/>.</para>
<para>Si desea desplegar los <literal>planes de SUC de sistema operativo</literal>
desde una herramienta GitOps de terceros, consulte la <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>Despliegue del plan de SUC - Recurso GitRepo</title>
<para>Es posible desplegar un recurso <emphasis role="strong">GitRepo</emphasis>,
que incluye los <literal>planes de SUC de sistema operativo</literal>, de
estas formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización del sistema
operativo de los nodos de su clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>Creación de GitRepo - Interfaz de usuario de Rancher</title>
<para>Para crear un recurso <literal>GitRepo</literal> con la interfaz de usuario
de Rancher, siga la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentación
oficial</link>.</para>
<para>El equipo de Edge mantiene una <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">flota
(fleet)</link> lista para usar. Dependiendo de su entorno, se puede utilizar
directamente o como plantilla.</para>
<important>
<para>Use siempre esta flota desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge válida.</para>
</important>
<para>Para los casos prácticos en los que no es necesario incluir cambios
personalizados en los <literal>planes de SUC</literal> que incluye la flota,
los usuarios pueden usar directamente la flota <literal>os-upgrade</literal>
desde el repositorio <literal>suse-edge/fleet-examples</literal>.</para>
<para>Si fuera necesario realizar cambios personalizados (por ejemplo, para añadir
tolerancias personalizadas), los usuarios deben usar la flota
<literal>os-upgrade</literal> desde un repositorio independiente, lo que les
permitirá añadir los cambios a los planes de SUC según sea necesario.</para>
<para>Hay un ejemplo de cómo se puede configurar un recurso
<literal>GitRepo</literal> para utilizar la flota del repositorio
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/gitrepos/day2/os-upgrade-gitrepo.yaml">aquí</link>.</para>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>Creación de GitRepo - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga el recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edite la configuración de <emphasis role="strong">GitRepo</emphasis>. En
<literal>spec.targets</literal>, especifique la lista de destinos que
desee. De forma predeterminada, los recursos <literal>GitRepo</literal> de
<literal>suse-edge/fleet-examples</literal> <emphasis
role="strong">NO</emphasis> están asignados a ningún clúster descendente.</para>
<itemizedlist>
<listitem>
<para>Para que coincidan todos los clústeres, cambie el <emphasis
role="strong">destino</emphasis> predeterminado de
<literal>GitRepo</literal> a:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Como alternativa, si desea una selección de clústeres más detallada,
consulte <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique el recurso <emphasis role="strong">GitRepo</emphasis> a su
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Consulte el recurso <emphasis role="strong">GitRepo</emphasis> creado en el
espacio de nombres <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.3.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>Despliegue del plan de SUC - Recurso Bundle</title>
<para>Es posible desplegar un recurso <emphasis role="strong">Bundle</emphasis>,
que incluye los <literal>planes de SUC de sistema operativo</literal>
válidos, de estas formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización del sistema
operativo de los nodos de su clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>Creación de Bundle - Interfaz de usuario de Rancher</title>
<para>El equipo de Edge mantiene un <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">bundle</link>
listo para usar que puede emplear en los pasos que se indican a
continuación.</para>
<important>
<para>Use siempre este bundle desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
válida de Edge.</para>
</important>
<para>Para crear un bundle con la interfaz de usuario de Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En la esquina superior izquierda, haga clic en <emphasis role="strong">☰ →
Continuous Delivery</emphasis> (☰ → Entrega continua).</para>
</listitem>
<listitem>
<para>Diríjase a <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avanzado > Bundles).</para>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create from YAML</emphasis> (Crear desde
YAML).</para>
</listitem>
<listitem>
<para>Desde aquí, puede crear el Bundle de las siguientes maneras:</para>
<note>
<para>Puede haber casos en los que sea necesario incluir cambios personalizados en
los <literal>planes de SUC</literal> que incluye el Bundle (por ejemplo,
para añadir tolerancias personalizadas). Asegúrese de incluir esos cambios
en el Bundle que se generará con los pasos siguientes.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copiando manualmente el <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">contenido
del Bundle</link> desde <literal>suse-edge/fleet-examples</literal> a la
página <emphasis role="strong">Create from YAML</emphasis> (Crear desde
YAML).</para>
</listitem>
<listitem>
<para>Clonando el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
desde la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada y seleccionando la opción<emphasis role="strong">Read from
File</emphasis> (Leer desde archivo) en la página <emphasis
role="strong">Create from YAML</emphasis> (Crear desde YAML). Desde ahí,
diríjase a la ubicación del Bundle
(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)
y seleccione el archivo del Bundle. Esto rellenará automáticamente la página
<emphasis role="strong">Create from YAML</emphasis> (Crear desde YAML) con
el contenido del Bundle.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Cambie los clústeres de <emphasis role="strong">destino</emphasis> para el
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Para que coincidan todos los clústeres descendentes, cambie el valor
<literal>.spec.targets</literal> predeterminado del Bundle a:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Para obtener asignaciones de clústeres descendentes más detalladas, consulte
<link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create</emphasis> (Crear).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>Creación del Bundle - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga el recurso <emphasis role="strong">Bundle</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edite las configuraciones de <emphasis role="strong">destino</emphasis> del
<literal>Bundle</literal>. En <literal>spec.targets</literal>, proporcione
la lista de objetivos que desee. De forma predeterminada, los recursos
<literal>Bundle</literal> de <literal>suse-edge/fleet-examples</literal>
<emphasis role="strong">NO</emphasis> están asignados a ningún clúster
descendente.</para>
<itemizedlist>
<listitem>
<para>Para que coincidan con todos los clústeres descendentes, cambie el valor de
<emphasis role="strong">destino</emphasis> predeterminado del
<literal>Bundle</literal> a:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Como alternativa, si desea una selección de clústeres más detallada,
consulte <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique el recurso <emphasis role="strong">Bundle</emphasis> al
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>Consulte el recurso <emphasis role="strong">Bundle</emphasis> creado en el
espacio de nombres <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-default</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>Despliegue del plan de SUC - Flujo de trabajo de GitOps de terceros</title>
<para>Puede haber casos prácticos en los que los usuarios deseen incorporar los
<literal>planes de SUC de sistema operativo</literal> a su propio flujo de
trabajo de GitOps de terceros (por ejemplo, <literal>Flux</literal>).</para>
<para>Para obtener los recursos de actualización del sistema operativo que
necesita, determine primero la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge del repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
que desea utilizar.</para>
<para>Después, los recursos se encuentran en
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>,
donde:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> es un recurso de plan de SUC para
los nodos de <emphasis role="strong">plano de control</emphasis>.</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> es un recurso de plan de SUC para los
nodos de <emphasis role="strong">trabajador</emphasis>.</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> es un secreto que contiene el guion
<literal>upgrade.sh</literal>, que se encarga de crear el servicio
systemd.service (<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>).</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> es un mapa de configuración que contiene
las configuraciones que utiliza el guion <literal>upgrade.sh</literal>.</para>
</listitem>
</itemizedlist>
<important>
<para>Estos recursos de <literal>plan</literal> son interpretados por
<literal>System Upgrade Controller</literal> y se deben desplegar en cada
clúster descendente que desee actualizar. Para obtener información sobre el
despliegue de SUC, consulte la <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para comprender mejor cómo usar su flujo de trabajo de GitOps para desplegar
los <emphasis role="strong">planes de SUC</emphasis> para actualizar el
sistema operativo, puede resultar útil echar un vistazo a la descripción
general (<xref linkend="downstream-day2-fleet-os-upgrade-overview"/>).</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade">
<title>Actualización de la versión de Kubernetes</title>
<important>
<para>Esta sección trata sobre las actualizaciones de Kubernetes para clústeres
descendentes que <emphasis role="strong">NO</emphasis> se hayan creado
mediante una instancia de Rancher (<xref
linkend="components-rancher"/>). Para obtener información sobre cómo
actualizar la versión de Kubernetes de los clústeres creados con
<literal>Rancher</literal>, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes</link> (Actualización y reversión de
Kubernetes).</para>
</important>
<para>En esta sección se describe cómo realizar una actualización de Kubernetes
utilizando <xref linkend="components-fleet"/> y <xref
linkend="components-system-upgrade-controller"/>.</para>
<para>En esta sección se tratan los siguientes temas:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-components"/>: componentes
adicionales usados por el proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/>: descripción
general del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-requirements"/>: requisitos
del proceso de actualización.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment"/>:
información sobre cómo desplegar <literal>planes de SUC</literal>,
responsables de iniciar el proceso de actualización.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components">
<title>Componentes</title>
<para>Esta sección trata sobre los componentes personalizados que el proceso de
<literal>actualización de K8s</literal> usa en lugar de los componentes
predeterminados de "día 2" (<xref
linkend="downstream-day2-fleet-components"/>).</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>Es la imagen de contenedor responsable de actualizar la versión de RKE2 de
un nodo específico.</para>
<para>Se incluye a través de un pod creado por <emphasis
role="strong">SUC</emphasis> basado en un <emphasis role="strong">plan de
SUC</emphasis>. El plan debe estar ubicado en cada <emphasis
role="strong">clúster</emphasis> que necesite una actualización de RKE2.</para>
<para>Para obtener más información sobre cómo la imagen
<literal>rke2-upgrade</literal> realiza la actualización, consulte la <link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">documentación
original</link>.</para>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Es la imagen de contenedor responsable de actualizar la versión de K3s de un
nodo específico.</para>
<para>Se incluye a través de un pod creado por <emphasis
role="strong">SUC</emphasis> basado en un <emphasis role="strong">plan de
SUC</emphasis>. El plan debe estar ubicado en cada <emphasis
role="strong">clúster</emphasis> que necesite una actualización de K3s.</para>
<para>Para obtener más información sobre cómo realiza la imagen
<literal>k3s-upgrade</literal> la actualización, consulte la <link
xl:href="https://github.com/k3s-io/k3s-upgrade">documentación
original</link>.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-overview">
<title>Descripción general</title>
<para>La actualización de la distribución de Kubernetes para los nodos del clúster
descendentes se realiza con <literal>Fleet</literal> y <literal>System
Upgrade Controller (SUC)</literal>.</para>
<para><literal>Fleet</literal> se usa para desplegar y gestionar <literal>planes
de SUC</literal> en el clúster deseado.</para>
<note>
<para>Los <literal>planes de SUC</literal> son <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">recursos
personalizados</link> que describen los pasos que debe seguir <emphasis
role="strong">SUC</emphasis> para ejecutar una tarea específica en un
conjunto de nodos. Para ver un ejemplo de un <literal>plan de SUC</literal>,
consulte el <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">repositorio
original</link>.</para>
</note>
<para>Los <literal>planes de SUC de K8s</literal> se incluyen en cada clúster
desplegando un recurso <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> o <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> en un <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">espacio
de trabajo</link> específico de Fleet. Fleet recupera el
<literal>GitRepo/Bundle</literal> desplegado y despliega su contenido (los
<literal>planes de SUC de K8s</literal>) en los clústeres deseados.</para>
<note>
<para>Los recursos <literal>GitRepo/Bundle</literal> siempre se despliegan en el
<literal>clúster de gestión</literal>. Que se use un recurso
<literal>GitRepo</literal> o <literal>Bundle</literal> depende del caso de
uso. Consulte la <xref linkend="downstream-day2-fleet-determine-use-case"/>
para obtener más información.</para>
</note>
<para>Los <literal>planes de SUC de K8s</literal> describen el siguiente flujo de
trabajo:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Use siempre el comando <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
en los nodos antes de las actualizaciones de K8s.</para>
</listitem>
<listitem>
<para>Actualice siempre los nodos <literal>control-plane</literal> antes que los
nodos <literal>worker</literal>.</para>
</listitem>
<listitem>
<para>Actualice siempre los nodos de <literal>control-plane</literal> (plano de
control) de <emphasis role="strong">uno en uno</emphasis> y los nodos
<literal>worker</literal> (trabajador) de <emphasis role="strong">dos en
dos</emphasis>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegados los <literal>planes de SUC de K8s</literal>, el flujo de
trabajo es el siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC reconcilia los <literal>planes de SUC de K8s</literal> desplegados y
crea un <literal>trabajo de Kubernetes</literal> en <emphasis
role="strong">cada nodo</emphasis>.</para>
</listitem>
<listitem>
<para>Dependiendo de la distribución de Kubernetes, el trabajo creará un pod que
ejecutará la imagen de contenedor rke2-upgrade (<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>) o
k3s-upgrade (<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>).</para>
</listitem>
<listitem>
<para>El pod creado seguirá el siguiente flujo de trabajo:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Sustituye el binario <literal>rke2/k3s</literal> existente en el nodo por el
de la imagen <literal>rke2-upgrade/k3s-upgrade</literal>.</para>
</listitem>
<listitem>
<para>Detiene el proceso <literal>rke2/k3s</literal> en ejecución.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Al detener el proceso <literal>rke2/k3s</literal>, se activa un reinicio y
se lanza un nuevo proceso que ejecuta el binario actualizado, lo que da como
resultado una versión actualizada de la distribución de Kubernetes.</para>
</listitem>
</orderedlist>
<para>A continuación, encontrará un diagrama de la descripción anterior:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>actualización k8s descendente de día 2 con fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-requirements">
<title>Requisitos</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Haga una copia de seguridad de su distribución de
Kubernetes:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Para los <emphasis role="strong">clústeres RKE2</emphasis>, consulte <link
xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2 Backup and
Restore</link> (Copia de seguridad y restauración de RKE2).</para>
</listitem>
<listitem>
<para>Para los <emphasis role="strong">clústeres K3s</emphasis>, consulte <link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and
Restore</link> (Copia de seguridad y restauración de K3s).</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Asegúrese de que las tolerancias del plan de SUC
coincidan con las tolerancias de los nodos</emphasis>. Si los nodos del
clúster de Kubernetes tienen <emphasis role="strong">intolerancias
(taints)</emphasis> personalizadas, asegúrese de añadir <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerancias
(tolerations)</link> para ellas en los <emphasis role="strong">planes de
SUC</emphasis>. Por defecto, los <emphasis role="strong">planes de
SUC</emphasis> solo tienen tolerancias para los nodos de <emphasis
role="strong">plano de control</emphasis>. Las tolerancias predeterminadas
son:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Cualquier tolerancia adicional debe añadirse en la sección
<literal>.spec.tolerations</literal> de cada plan. Los <emphasis
role="strong">planes de SUC</emphasis> relacionados con la actualización de
la versión de Kubernetes se encuentran en el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
en:</para>
<itemizedlist>
<listitem>
<para>Para <emphasis role="strong">RKE2</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>Para <emphasis role="strong">K3s</emphasis>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Asegúrese de usar los planes de una etiqueta de
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
del repositorio válida.</emphasis></para>
<para>Este es un ejemplo de definición de tolerancias personalizadas para el plan
de SUC de <emphasis role="strong">plano de control</emphasis> de RKE2:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment">
<title>Actualización de K8s - Despliegue del plan de SUC</title>
<important>
<para>En entornos que se hayan actualizado previamente mediante este
procedimiento, los usuarios deben asegurarse de que se haya completado
<emphasis role="strong">uno</emphasis> de los pasos siguientes:</para>
<itemizedlist>
<listitem>
<para><literal>Elimine cualquier plan de SUC desplegado anteriormente relacionado
con versiones anteriores de Edge del clúster descendente</literal>. Esto se
puede hacer eliminando el clúster deseado de la <link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">configuración
de destino</link> de <literal>GitRepo/Bundle</literal> existente o
eliminando por completo el recurso <literal>GitRepo/Bundle</literal>.</para>
</listitem>
<listitem>
<para><literal>Reutilice el recurso GitRepo/Bundle existente</literal>. Puede
hacerlo haciendo que la revisión del recurso apunte a una nueva etiqueta que
contenga los recursos de Fleet correctos para la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada de <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
</itemizedlist>
<para>Esto se hace con el fin de evitar conflictos entre los <literal>planes de
SUC</literal> para versiones anteriores de Edge.</para>
<para>Si los usuarios intentan actualizar, mientras haya <literal>planes de
SUC</literal> existentes en el clúster descendente, verán el siguiente error
de Fleet:</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>Como se menciona en la <xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>, las actualizaciones
de Kubernetes se realizan incluyendo <literal>planes de SUC</literal> en el
clúster deseado de las siguientes formas:</para>
<itemizedlist>
<listitem>
<para>Recurso GitRepo de Fleet (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>)</para>
</listitem>
<listitem>
<para>Recurso Bundle de Fleet (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>)</para>
</listitem>
</itemizedlist>
<para>Para determinar qué recurso se debe usar, consulte la <xref
linkend="downstream-day2-fleet-determine-use-case"/>.</para>
<para>Si desea desplegar los <literal>planes de SUC de K8s</literal> desde una
herramienta GitOps de terceros, consulte la <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>Despliegue del plan de SUC - Recurso GitRepo</title>
<para>Es posible desplegar un recurso <emphasis role="strong">GitRepo</emphasis>,
que incluye los <literal>planes de SUC de K8s</literal> necesarios, de las
siguientes formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización de
Kubernetes de los nodos del clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>Creación de GitRepo - Interfaz de usuario de Rancher</title>
<para>Para crear un recurso <literal>GitRepo</literal> con la interfaz de usuario
de Rancher, siga la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">documentación
oficial</link>.</para>
<para>El equipo de Edge mantiene flotas listas para usar para las distribuciones
de Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>
y <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>.
Según su entorno, estas flotas se pueden usar directamente o como plantilla.</para>
<important>
<para>Utilice siempre estas flotas desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge válida.</para>
</important>
<para>Cuando no sea necesario incluir cambios personalizados en los
<literal>planes de SUC</literal> que incluyen estas flotas, los usuarios
pueden consultar directamente las flotas en el repositorio
<literal>suse-edge/fleet-examples</literal>.</para>
<para>Si fuera necesario realizar cambios personalizados (por ejemplo, para añadir
tolerancias personalizadas), los usuarios deben consultar las flotas desde
un repositorio independiente, lo que les permitirá añadir los cambios a los
planes de SUC según sea necesario.</para>
<para>Ejemplos de configuración para un recurso <literal>GitRepo</literal>
utilizando las flotas del repositorio
<literal>suse-edge/fleet-examples</literal>:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>Creación de GitRepo - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga el recurso <emphasis role="strong">GitRepo</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clústeres <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Para clústeres <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite la configuración de <emphasis role="strong">GitRepo</emphasis>. En
<literal>spec.targets</literal>, especifique la lista de destinos que
desee. De forma predeterminada, los recursos <literal>GitRepo</literal> de
<literal>suse-edge/fleet-examples</literal> <emphasis
role="strong">NO</emphasis> están asignados a ningún clúster descendente.</para>
<itemizedlist>
<listitem>
<para>Para que coincidan todos los clústeres, cambie el <emphasis
role="strong">destino</emphasis> predeterminado de
<literal>GitRepo</literal> a:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Como alternativa, si desea una selección de clústeres más detallada,
consulte <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique los recursos <emphasis role="strong">GitRepo</emphasis> a su
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Consulte el recurso <emphasis role="strong">GitRepo</emphasis> creado en el
espacio de nombres <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>Despliegue del plan de SUC - Recurso Bundle</title>
<para>Es posible desplegar un recurso <emphasis role="strong">Bundle</emphasis>,
que incluye los <literal>planes de SUC de actualización de
Kubernetes</literal> necesarios, de una de estas formas:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Mediante la <literal>interfaz de usuario de Rancher</literal>: <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>
(si <literal>Rancher</literal> está disponible).</para>
</listitem>
<listitem>
<para>Desplegando manualmente (<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>)
el recurso en el <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, para supervisar el proceso de actualización de
Kubernetes de los nodos del clúster de destino, consulte la <xref
linkend="components-system-upgrade-controller-monitor-plans"/>.</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>Creación de Bundle - Interfaz de usuario de Rancher</title>
<para>El equipo de Edge mantiene bundles listos para usar para las distribuciones
de Kubernetes <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>
y <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>.
Dependiendo de su entorno, estos bundles se pueden utilizar directamente o
como plantilla.</para>
<important>
<para>Use siempre este bundle desde una etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
válida de Edge.</para>
</important>
<para>Para crear un bundle con la interfaz de usuario de Rancher:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En la esquina superior izquierda, haga clic en <emphasis role="strong">☰ →
Continuous Delivery</emphasis> (☰ → Entrega continua).</para>
</listitem>
<listitem>
<para>Diríjase a <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis> (Avanzado > Bundles).</para>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create from YAML</emphasis> (Crear desde
YAML).</para>
</listitem>
<listitem>
<para>Desde aquí, puede crear el Bundle de las siguientes maneras:</para>
<note>
<para>Puede haber casos en los que sea necesario incluir cambios personalizados en
los <literal>planes de SUC</literal> que incluye el Bundle (por ejemplo,
para añadir tolerancias personalizadas). Asegúrese de incluir esos cambios
en el Bundle que se generará con los pasos siguientes.</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie manualmente el contenido del Bundle para <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>
o <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>
desde <literal>suse-edge/fleet-examples</literal> en la página <emphasis
role="strong">Create from YAML</emphasis> (Crear desde YAML).</para>
</listitem>
<listitem>
<para>Clone el repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
desde la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
deseada y seleccione la opción <emphasis role="strong">Read from
File</emphasis> (Leer desde archivo) en la página <emphasis
role="strong">Create from YAML</emphasis> (Crear desde YAML). Desde ahí,
diríjase al Bundle que necesita
(<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>
para RKE2 y
<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>
para K3s). La página <emphasis role="strong">Create from YAML</emphasis>
(Crear desde YAML) se rellenará automáticamente con el contenido del
paquete.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Cambie los clústeres de <emphasis role="strong">destino</emphasis> para el
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>Para que coincidan todos los clústeres descendentes, cambie el valor
<literal>.spec.targets</literal> predeterminado del Bundle a:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Para obtener asignaciones de clústeres descendentes más detalladas, consulte
<link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Seleccione <emphasis role="strong">Create</emphasis> (Crear).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>Creación del Bundle - Manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Extraiga los recursos <emphasis role="strong">Bundle</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Para clústeres <emphasis role="strong">RKE2</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Para clústeres <emphasis role="strong">K3s</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.3.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edite las configuraciones de <emphasis role="strong">destino</emphasis> del
<literal>Bundle</literal>. En <literal>spec.targets</literal>, proporcione
la lista de objetivos que desee. De forma predeterminada, los recursos
<literal>Bundle</literal> de <literal>suse-edge/fleet-examples</literal>
<emphasis role="strong">NO</emphasis> están asignados a ningún clúster
descendente.</para>
<itemizedlist>
<listitem>
<para>Para que coincidan con todos los clústeres descendentes, cambie el valor de
<emphasis role="strong">destino</emphasis> predeterminado del
<literal>Bundle</literal> a:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Como alternativa, si desea una selección de clústeres más detallada,
consulte <link xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to
Downstream Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Aplique los recursos <emphasis role="strong">Bundle</emphasis> al
<literal>clúster de gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>Consulte el recurso <emphasis role="strong">Bundle</emphasis> creado en el
espacio de nombres <literal>fleet-default</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>Despliegue del plan de SUC - Flujo de trabajo de GitOps de terceros</title>
<para>Puede haber casos prácticos en los que los usuarios deseen incorporar los
<literal>planes de SUC de actualización de Kubernetes</literal> a su propio
flujo de trabajo de GitOps de terceros (por ejemplo,
<literal>Flux</literal>).</para>
<para>Para obtener los recursos de actualización de K8s que necesita, primero
determine la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge del repositorio <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
que desea utilizar.</para>
<para>Después, los recursos se encuentran en:</para>
<itemizedlist>
<listitem>
<para>Para actualizar un clúster RKE2:</para>
<itemizedlist>
<listitem>
<para>Para nodos <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para nodos <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Para actualizar un clúster K3s:</para>
<itemizedlist>
<listitem>
<para>Para nodos <literal>control-plane</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>Para nodos <literal>worker</literal>:
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-worker.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>Estos recursos de <literal>plan</literal> son interpretados por
<literal>System Upgrade Controller</literal> y se deben desplegar en cada
clúster descendente que desee actualizar. Para obtener información sobre el
despliegue de SUC, consulte la <xref
linkend="components-system-upgrade-controller-install"/>.</para>
</important>
<para>Para comprender mejor cómo usar su flujo de trabajo de GitOps para desplegar
los <emphasis role="strong">planes de SUC</emphasis> para actualizar la
versión de Kubernetes, puede resultar útil echar un vistazo a la descripción
general (<xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/>) del
procedimiento de actualización utilizando <literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade">
<title>Actualización de charts de Helm</title>
<para>Esta sección cubre lo siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-air-gap"/>: contiene
información sobre cómo incluir charts e imágenes OCI relacionados con Edge
en su registro privado.</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure"/>: contiene
información sobre diferentes casos de actualización de charts de Helm y su
procedimiento de actualización.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-air-gap">
<title>Preparación para entornos aislados</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet-2">
<title>Asegúrese de tener acceso a los recursos de Fleect del chart de Helm</title>
<para>Dependiendo de lo que admita su entorno, puede elegir una de estas opciones:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Aloje los recursos de Fleet de su chart en un servidor Git local al que
pueda acceder su <literal>clúster de gestión</literal>.</para>
</listitem>
<listitem>
<para>Use la interfaz de línea de comandos de Fleet para <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">convertir
un chart de Helm en un Bundle</link> que podrá utilizar directamente sin
necesidad de alojarlo en ningún sitio. Puede descargar la interfaz de línea
de comandos de la página de la <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.12.2">versión</link>.
Para los usuarios de Mac, hay una versión propia de <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version-2">
<title>Busque los recursos necesarios para su versión de Edge</title>
<orderedlist numeration="arabic">
<listitem>
<para>Vaya a la página de la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de "día 2", busque la versión de Edge a la que desea actualizar su chart y
haga clic en <emphasis role="strong">Assets</emphasis> (Recursos).</para>
</listitem>
<listitem>
<para>En la sección <emphasis role="strong">Assets</emphasis> (Recursos),
descargue los archivos siguientes:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Archivo de versión</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Descripción</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Extrae las imágenes especificadas en el archivo
<literal>edge-release-images.txt</literal> y las empaqueta en un archivo
".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Extrae las imágenes del chart OCI relacionadas con la versión específica de
Edge y las empaqueta en un archivo ".tar.gz".</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Carga imágenes desde un archivo ".tar.gz", las vuelve a etiquetar y las
envía a un registro privado.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>Toma un directorio que contiene paquetes de charts OCI ".tgz" de Edge y los
carga en un registro privado.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contiene una lista de imágenes de charts OCI relacionadas con una versión
específica de Edge.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>Contiene una lista de imágenes relacionadas con una versión específica de
Edge.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive-2">
<title>Cree el archivo de imágenes de la versión de Edge</title>
<para><emphasis>En un equipo con acceso a Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Permita que <literal>edge-save-images.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Genere el archivo de imagen:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esto creará un archivo listo para cargar llamado
<literal>edge-images.tar.gz</literal>.</para>
<note>
<para>Si se especifica la opción <literal>-i|--images</literal>, el nombre del
archivo puede ser diferente.</para>
</note>
</listitem>
<listitem>
<para>Copie este archivo a su equipo <emphasis role="strong">aislado</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive-2">
<title>Cree el archivo de imágenes del chart OCI de Edge</title>
<para><emphasis>En un equipo con acceso a Internet:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Permita que <literal>edge-save-oci-artefacts.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Genere el archivo de imagen del chart OCI:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>Esto creará un archivo denominado <literal>oci-artefacts.tar.gz</literal>.</para>
<note>
<para>Si se especifica la opción <literal>-a|--archive</literal>, el nombre del
archivo puede ser distinto.</para>
</note>
</listitem>
<listitem>
<para>Copie este archivo a su equipo <emphasis role="strong">aislado</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine-2">
<title>Cargue las imágenes de la versión de Edge en su equipo aislado</title>
<para><emphasis>En su equipo aislado:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Inicie sesión en su registro privado (si es necesario):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Permita que <literal>edge-load-images.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Ejecute el guion pasando el archivo <literal>edge-images.tar.gz</literal>
<emphasis role="strong">copiado</emphasis> anteriormente:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>Esto cargará todas las imágenes desde <literal>edge-images.tar.gz</literal>,
las volverá a etiquetar y las enviará al registro especificado en la opción
<literal>--registry</literal>.</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine-2">
<title>Cargue las imágenes del chart OCI de Edge en su equipo aislado</title>
<para><emphasis>En su equipo aislado:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Inicie sesión en su registro privado (si es necesario):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Permita que <literal>edge-load-oci-artefacts.sh</literal> se pueda ejecutar:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Desempaquete el archivo <literal>oci-artefacts.tar.gz</literal> copiado:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>Esto generará un directorio con el formato de nombre
<literal>edge-release-oci-tgz-&lt;fecha&gt;</literal>.</para>
</listitem>
<listitem>
<para>Pase este directorio al guion <literal>edge-load-oci-artefacts.sh</literal>
para cargar las imágenes del chart OCI de Edge a su registro privado:</para>
<note>
<para>Este guion presupone que la interfaz de línea de comandos de
<literal>Helm</literal> ya está preinstalada en su entorno. Para obtener
instrucciones sobre la instalación de Helm, consulte <link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>
(Instalación de Helm).</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution-2">
<title>Configure el registro privado en su distribución de Kubernetes</title>
<para>Para RKE2, consulte <link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration</link> (Configuración del registro privado)</para>
<para>Para K3s, consulte <link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration</link> (Configuración del registro privado)</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure">
<title>Procedimiento de actualización</title>
<para>Esta sección se centra en los siguientes casos prácticos del procedimiento
de actualización de Helm:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>Los charts de Helm desplegados manualmente no se pueden actualizar de forma
fiable. Recomendamos volver a desplegar el chart de Helm utilizando el
método <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/>.</para>
</important>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>Tengo un nuevo clúster y me gustaría desplegar y gestionar un chart de Helm
de Edge</title>
<para>En esta sección se trata lo siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>.</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>.</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>Prepare los recursos de Fleet para su chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Adquiera los recursos de Fleet del chart desde la etiqueta de <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">versión</link>
de Edge que desee usar.</para>
</listitem>
<listitem>
<para>Diríjase a la flota del chart de Helm
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Si tiene intención de utilizar un flujo de trabajo
de GitOps</emphasis>, copie el directorio Fleet del chart en el repositorio
Git desde donde realizará la operación de GitOps.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, si el chart de Helm
requiere que se configuren sus <emphasis role="strong">valores</emphasis>,
edite la configuración de <literal>.helm.values</literal> dentro del archivo
<literal>fleet.yaml</literal> del directorio copiado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Opcionalmente</emphasis>, puede haber casos en los
que sea necesario añadir recursos adicionales a la flota de su chart para
que se adapte mejor a su entorno. Para obtener información sobre cómo
mejorar su directorio de Fleet, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository
Contents</link> (Contenido del repositorio Git).</para>
</listitem>
</orderedlist>
<note>
<para>En algunos casos, el tiempo de espera predeterminado que Fleet usa para las
operaciones de Helm puede ser insuficiente, lo que da lugar al siguiente
error:</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>Si fuera el caso, añada la propiedad <link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>
en la configuración de <literal>helm</literal> de su archivo
<literal>fleet.yaml</literal>.</para>
</note>
<para>Esto es un <emphasis role="strong">ejemplo</emphasis> del aspecto que
tendría el chart de Helm de <literal>longhorn</literal>:</para>
<itemizedlist>
<listitem>
<para>Estructura del repositorio Git del usuario:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>Contenido de <literal>fleet.yaml</literal> con datos de
<literal>Longhorn</literal> del usuario:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "106.2.0+up1.8.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>Estos son solo valores de ejemplo que se utilizan para mostrar
configuraciones personalizadas en el chart de
<literal>longhorn</literal>. <emphasis role="strong">NO</emphasis> deben
considerarse directrices de despliegue para el chart de
<literal>longhorn</literal>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>Despliegue la flota en su chart</title>
<para>Puede desplegar la flota de su chart usando GitRepo (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>)
o Bundle (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>).</para>
<note>
<para>Al desplegar su flota, si recibe un mensaje con la indicación
<literal>Modified</literal> (Modificado), asegúrese de añadir la entrada
<literal>comparePatches</literal> correspondiente a la sección
<literal>diff</literal> del Fleet. Para obtener más información, consulte
<link xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to
Ignore Modified GitRepos</link> (Generación de diffs para ignorar recursos
GitRepo modificados).</para>
</note>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>El recurso <link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> de Fleet
incluye información sobre cómo acceder a los recursos de Fleet de su chart y
a qué clústeres deben aplicarse esos recursos.</para>
<para>El recurso <literal>GitRepo</literal> se puede desplegar mediante la <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">interfaz
de usuario de Rancher</link> o, manualmente, <link
xl:href="https://fleet.rancher.io/tut-deployment">desplegando</link> el
recurso en el <literal>clúster de gestión</literal>.</para>
<para>Recurso <literal>GitRepo</literal> de <emphasis
role="strong">Longhorn</emphasis> de ejemplo para el despliegue <emphasis
role="strong">manual</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>Bundle</title>
<para>Los recursos <link
xl:href="https://fleet.rancher.io/bundle-add">Bundle</link> contienen los
recursos sin procesar de Kubernetes que debe desplegar Fleet. Normalmente,
se recomienda utilizar el enfoque de <literal>GitRepo</literal>, pero en
entornos aislados o que no admitan un servidor Git local, los
<literal>Bundles</literal> pueden ayudarle a propagar su chart de Helm de
Fleet a los clústeres de destino.</para>
<para>Es posible desplegar un <literal>Bundle</literal> mediante la interfaz de
usuario de Rancher seleccionando <literal>Continuous Delivery → Advanced →
Bundles → Create from YAML</literal> (Entrega continua → Avanzado → Bundles
→ Crear desde YAML)) o desplegando manualmente el recurso
<literal>Bundle</literal> en el espacio de nombres de Fleet correcto. Para
obtener información sobre los espacios de nombres de Fleet, consulte la
<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">documentación
original</link>.</para>
<para>Es posible crear <literal>Bundles</literal> para los charts de Helm de Edge
<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">convirtiendo
un chart de Helm en un Bundle de Fleet</link>.</para>
<para>A continuación, hay un ejemplo de cómo crear un recurso
<literal>Bundle</literal> a partir de plantillas de flota de un chart de
Helm de <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
y <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
y cómo desplegar manualmente este Bundle en su <literal>clúster de
gestión</literal>.</para>
<note>
<para>Para ilustrar el flujo de trabajo, el siguiente ejemplo usa la estructura de
directorio <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>.</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>Diríjase a la plantilla de flota de chart de <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>Cree un archivo <literal>targets.yaml</literal> que indicará a Fleet en qué
clústeres debe desplegar el chart de Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>Para una selección más detallada de clústeres descendentes, consulte <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link> (Asignación a clústeres descendentes).</para>
</listitem>
<listitem>
<para>Convierta la flota de chart de Helm de <literal>Longhorn</literal> en un
recurso Bundle mediante <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<note>
<para>Puede obtener la interfaz de línea de comandos de Fleet
(<literal>fleet-linux-amd64</literal>) de la página <emphasis
role="strong">Assets</emphasis> (Recursos) de su <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.12.2">versión</link>.</para>
<para>Para usuarios de Mac, hay una versión propia de <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>Diríjase a la plantilla de flota de chart <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>:</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>Cree un archivo <literal>targets.yaml</literal> que indicará a Fleet en qué
clústeres debe desplegar el chart de Helm:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
</listitem>
<listitem>
<para>Convierta la flota de chart de Helm de la <literal>CRD de Longhorn</literal>
en un recurso Bundle mediante <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link>.</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>Despliegue los archivos <literal>longhorn-bundle.yaml</literal> y
<literal>longhorn-crd-bundle.yaml</literal> en su <literal>clúster de
gestión</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>Seguir estos pasos garantizará que <literal>SUSE Storage</literal> se
despliegue en todos los clústeres descendentes especificados.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>Gestione el chart de Helm desplegado</title>
<para>Cuando se complete el despliegue con Fleet, para actualizaciones de charts
de Helm, consulte la <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>Quiero actualizar un chart de Helm gestionado por Fleet</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine la versión a la que debe actualizar su chart para que sea
compatible con la versión deseada de Edge. Puede consultar la versión del
chart de Helm para cada versión de Edge en las notas de la versión (<xref
linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>En su repositorio Git supervisado por Fleet, edite el archivo
<literal>fleet.yaml</literal> del chart de Helm con la <emphasis
role="strong">versión</emphasis> y el <emphasis
role="strong">repositorio</emphasis> correctos del chart, tal y como se
indica en las notas de la versión (<xref linkend="release-notes"/>).</para>
</listitem>
<listitem>
<para>Después de confirmar y enviar los cambios al repositorio, se activará una
actualización del chart de Helm deseado.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>Quiero actualizar un chart de Helm desplegado mediante EIB</title>
<para><xref linkend="components-eib"/> despliega charts de Helm creando un recurso
<literal>HelmChart</literal> y usando el <literal>helm-controller</literal>
introducido por la función de integración de Helm <link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>.</para>
<para>Para garantizar que un chart de Helm desplegado mediante
<literal>EIB</literal> se actualice correctamente, los usuarios deben
realizar una actualización de los recursos <literal>HelmChart</literal>
correspondientes.</para>
<para>A continuación, encontrará más información:</para>
<itemizedlist>
<listitem>
<para>La descripción general (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>)
del proceso de actualización.</para>
</listitem>
<listitem>
<para>Los pasos necesarios para la actualización (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>).</para>
</listitem>
<listitem>
<para>Un ejemplo (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>)
que muestra la actualización de un chart de <link
xl:href="https://longhorn.io">Longhorn</link> usando el método explicado.</para>
</listitem>
<listitem>
<para>Cómo usar el proceso de actualización con una herramienta GitOps diferente
(<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>Descripción general</title>
<para>Los charts de Helm que se despliegan mediante <literal>EIB</literal> se
actualizan usando una <literal>flota</literal> llamada <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.3.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>.</para>
<para>Esta <literal>flota</literal> procesa los datos <emphasis
role="strong">proporcionados por el usuario</emphasis> para <emphasis
role="strong">actualizar</emphasis> un conjunto específico de recursos
HelmChart.</para>
<para>Al actualizar estos recursos se activa <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>,
que <emphasis role="strong">actualiza</emphasis> los charts de Helm
asociados con los recursos <literal>HelmChart</literal> modificados.</para>
<para>Solo se espera de los usuarios que:</para>
<orderedlist numeration="arabic">
<listitem>
<para><link xl:href="https://helm.sh/docs/helm/helm_pull/">Extraigan</link>
localmente los archivos de cada chart de Helm que deba actualizarse.</para>
</listitem>
<listitem>
<para>Pasen estos archivos al guion <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal>, que incluirá los datos de
estos archivos en la flota <literal>eib-charts-upgrader</literal>.</para>
</listitem>
<listitem>
<para>Desplieguen la flota <literal>eib-charts-upgrader</literal> a su
<literal>clúster de gestión</literal>. Esto se hace con un recurso
<literal>GitRepo</literal> o <literal>Bundle</literal>.</para>
</listitem>
</orderedlist>
<para>Una vez desplegado, <literal>eib-charts-upgrader</literal>, con ayuda de
Fleet, incluirá sus recursos en el clúster descendente deseado.</para>
<para>Estos recursos incluyen:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Un conjunto de <literal>secretos</literal> que guardan los datos del chart
de Helm <emphasis role="strong">proporcionados por el usuario</emphasis>.</para>
</listitem>
<listitem>
<para>Un <literal>trabajo de Kubernetes</literal> que desplegará un
<literal>pod</literal> que montará los <literal>secretos</literal>
mencionados anteriormente y, basándose en ellos, <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">parcheará</link>
los recursos HelmChart correspondientes.</para>
</listitem>
</orderedlist>
<para>Como se mencionó anteriormente, esto activará
<literal>helm-controller</literal>, que llevará a cabo la actualización real
del chart de Helm.</para>
<para>A continuación, encontrará un diagrama de la descripción anterior:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-downstream-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>actualización eib de helm descendente de día 2 con fleet</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>Pasos para la actualización</title>
<orderedlist numeration="arabic">
<listitem>
<para>Clone el repositorio <literal>suse-edge/fleet-examples</literal> de la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">etiqueta</link>
de versión correcta.</para>
</listitem>
<listitem>
<para>Cree un directorio en el que almacenarán los archivos de los charts de Helm
extraídos.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>En el directorio recién creado, <link
xl:href="https://helm.sh/docs/helm/helm_pull/">extraiga</link> los archivos
de los charts de Helm que desea actualizar:</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>En la página <emphasis role="strong">Assets</emphasis> (Recursos) de la
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">etiqueta
de versión</link> deseada, descargue el guion
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</listitem>
<listitem>
<para>Ejecute el guion <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>Para cada archivo de chart del directorio <literal>--archive-dir</literal>,
el guion genera un archivo <literal>Kubernetes Secret YAML</literal> que
contiene los datos de actualización de chart y lo guardar en el directorio
<literal>base/secrets</literal> de la flota especificada por
<literal>--fleet-path</literal>.</para>
<para>El guion <literal>generate-chart-upgrade-data.sh</literal> también aplica
modificaciones adicionales a la flota para garantizar que la carga de
trabajo desplegada por la flota utilice correctamente los archivos
<literal>Kubernetes Secret YAML</literal> generados.</para>
<important>
<para>Los usuarios no deben cambiar nada de lo que genera el guion
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</important>
</listitem>
</orderedlist>
<para>Los pasos siguientes dependen del entorno en el que se esté ejecutando:</para>
<orderedlist numeration="arabic">
<listitem>
<para>En un entorno que admita GitOps (por ejemplo, que no esté aislado, o que
esté aislado pero permita la asistencia de un servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Copie la flota <literal>fleets/day2/eib-charts-upgrader</literal> en el
repositorio que usará para GitOps.</para>
<note>
<para>Asegúrese de que la flota incluye los cambios realizados por el guion
<literal>generate-chart-upgrade-data.sh</literal>.</para>
</note>
</listitem>
<listitem>
<para>Configure un recurso <literal>GitRepo</literal> que se usará para incluir
todos los recursos de la flota <literal>eib-charts-upgrader</literal>.</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Para la configuración y el despliegue de <literal>GitRepo</literal> mediante
la interfaz de usuario de Rancher, consulte <link
xl:href="https://ranchermanager.docs.rancher.com/v2.11/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI</link> (Acceso a Fleet en la interfaz de usuario de
Rancher).</para>
</listitem>
<listitem>
<para>Para la configuración y el despliegue manuales de
<literal>GitRepo</literal>, consulte <link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a
Deployment</link> (Creación de un despliegue).</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>En un entorno que no admita GitOps (por ejemplo, un entorno aislado que no
permita el uso de un servidor Git local):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Descargue el binario de <literal>fleet-cli</literal> de la página de <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.12.2">versión</link>
de <literal>rancher/fleet</literal> (<literal>fleet-linux-amd64</literal>
para Linux). Los usuarios de Mac pueden usar una versión propia: <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>.</para>
</listitem>
<listitem>
<para>Diríjase a la flota <literal>eib-charts-upgrader</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>Cree un archivo <literal>targets.yaml</literal> que indicará a Fleet dónde
debe desplegar los recursos:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To match all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>Para obtener información sobre cómo asignar clústeres de destino, consulte
la <link xl:href="https://fleet.rancher.io/gitrepo-targets">documentación
original</link>.</para>
</listitem>
<listitem>
<para>Utilice <literal>fleet-cli</literal> para convertir la flota en un recurso
<literal>Bundle</literal>:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>Esto creará un Bundle (<literal>bundle.yaml</literal>) que contendrá todos
los recursos de plantilla de la flota
<literal>eib-charts-upgrader</literal>.</para>
<para>Para obtener más información sobre el comando <literal>fleet
apply</literal>, consulte <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>.</para>
<para>Para obtener más información sobre cómo convertir flotas en Bundles,
consulte <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">Convert
a Helm Chart into a Bundle</link> (Conversión de un chart de Helm en un
Bundle).</para>
</listitem>
<listitem>
<para>Despliegue el <literal>Bundle</literal>. Puede hacerlo de dos formas:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Mediante la interfaz de usuario de Rancher: seleccione <emphasis
role="strong">Continuous Delivery → Advanced → Bundles → Create from
YAML</emphasis> (Entrega continua → Avanzado → Bundles → Crear desde YAML) y
pegue el contenido de <literal>bundle.yaml</literal> o haga clic en la
opción <literal>Read from File</literal> (Leer desde archivo) y pase el
archivo.</para>
</listitem>
<listitem>
<para>Manualmente: despliegue el archivo <literal>bundle.yaml</literal>
manualmente dentro de su <literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Al ejecutar estos pasos, se desplegará correctamente el recurso
<literal>GitRepo/Bundle</literal>. Fleet recogerá el recurso y su contenido
se desplegará en los clústeres de destino que el usuario haya especificado
en los pasos anteriores. Para obtener una descripción general del proceso,
consulte la <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>.</para>
<para>Para obtener información sobre cómo realizar un seguimiento del proceso de
actualización, consulte la <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>.</para>
<important>
<para>Cuando haya verificado que el chart se ha actualizado correctamente, elimine
el recurso <literal>Bundle/GitRepo</literal>.</para>
<para>Esto eliminará los recursos de actualización que ya no son necesarios de su
clúster <literal>descendente</literal>, lo que garantizará que no se
produzcan conflictos entre versiones en el futuro.</para>
</important>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>Ejemplo</title>
<note>
<para>El siguiente ejemplo muestra cómo actualizar un chart de Helm desplegado con
<literal>EIB</literal> de una versión a otra en un clúster
<literal>descendente</literal>. Tenga en cuenta que las versiones utilizadas
en este ejemplo <emphasis role="strong">no</emphasis> son
recomendaciones. Para obtener recomendaciones de versiones específicas para
una versión de Edge, consulte las notas de la versión (<xref
linkend="release-notes"/>).</para>
</note>
<para><emphasis>Caso práctico:</emphasis></para>
<itemizedlist>
<listitem>
<para>Un clúster denominado <literal>doc-example</literal> ejecuta una versión
anterior de <link xl:href="https://longhorn.io">Longhorn</link>.</para>
</listitem>
<listitem>
<para>El clúster se ha desplegado mediante EIB, utilizando el siguiente
<emphasis>fragmento</emphasis> de definición de imagen:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal> debe actualizarse a una versión compatible
con la versión Edge 3.3.1. Esto significa que debe actualizarse a
<literal>106.2.0+up1.8.1</literal>.</para>
</listitem>
<listitem>
<para>Se sobrentiende que el <literal>clúster de gestión</literal> encargado de
gestionar <literal>doc-example</literal> está <emphasis
role="strong">aislado</emphasis>, sin asistencia para un servidor Git local
y con una configuración de Rancher operativa.</para>
</listitem>
</itemizedlist>
<para>Siga los pasos de actualización (<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>):</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clone el repositorio <literal>suse-edge/fleet-example</literal> desde la
etiqueta <literal>release-3.3.0</literal>.</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.3.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>Cree un directorio donde se almacenará el archivo de actualización de
<literal>Longhorn</literal>.</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>Extraiga la versión del archivo de chart de <literal>Longhorn</literal>
deseada:</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.8.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 106.2.0+up1.8.1

# Pull the Longhorn 1.8.1 chart archive
helm pull rancher-charts/longhorn --version 106.2.0+up1.8.1</screen>
</listitem>
<listitem>
<para>Fuera del directorio <literal>archives</literal>, descargue el guion
<literal>generate-chart-upgrade-data.sh</literal> de la <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.3.0">etiqueta</link>
de la versión <literal>suse-edge/fleet-examples</literal>.</para>
</listitem>
<listitem>
<para>La configuración del directorio debería ser similar a:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-106.2.0+up1.8.1.tgz
│   └── longhorn-crd-106.2.0+up1.8.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>Ejecute el guion <literal>generate-chart-upgrade-data.sh</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>La estructura del directorio tras la ejecución del guion debería ser similar
a esto:</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-106.2.0+up1.8.1.tgz
│   └── longhorn-crd-106.2.0+up1.8.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>Los archivos modificados en Git deberían tener este aspecto:</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
	modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
	modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
	fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
	fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>Cree un <literal>Bundle</literal> para la flota
<literal>eib-charts-upgrader</literal>:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>En primer lugar, diríjase a la flota:</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>A continuación, cree un archivo <literal>targets.yaml</literal>:</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: doc-example
EOF</screen>
</listitem>
<listitem>
<para>Después, use el binario de <literal>fleet-cli</literal> para convertir la
flota en un Bundle:</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
<listitem>
<para>Ahora, transfiera <literal>bundle.yaml</literal> a su equipo del
<literal>clúster de gestión</literal>.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Despliegue el Bundle mediante la interfaz de usuario de Rancher:</para>
<figure>
<title>Despliegue del Bundle mediante la interfaz de Rancher</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>ejemplo 1 de actualización del chart de helm de día 2</phrase></textobject>
</mediaobject></figure>
<para>Ahora, seleccione <emphasis role="strong">Read from File</emphasis> (Leer
desde archivo) y busque el archivo <literal>bundle.yaml</literal> en su
sistema.</para>
<para>El <literal>Bundle</literal> se rellenará automáticamente en la interfaz de
usuario de Rancher.</para>
<para>Seleccione <emphasis role="strong">Create</emphasis> (Crear).</para>
</listitem>
<listitem>
<para>Cuando haya finalizado el despliegue correctamente, el Bundle deberá tener
un aspecto similar al siguiente:</para>
<figure>
<title>Bundle desplegado correctamente</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>ejemplo 2 de actualización de chart del helm de día 2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>Cuando haya finalizado el despliegue del <literal>Bundle</literal>
correctamente, para supervisar el proceso de actualización:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Verifique los registros del <literal>pod de actualización</literal>:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>ejemplo 3 de actualización de chart de helm de día dos descendente</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Ahora, verifique los registros del pod creado para la actualización por
helm-controller:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>El nombre del pod tendrá este formato:
<literal>helm-install-longhorn-&lt;sufijo aleatorio&gt;</literal></para>
</listitem>
<listitem>
<para>El pod estará en el espacio de nombres donde se desplegó el recurso
<literal>HelmChart</literal>. En nuestro caso,
<literal>kube-system</literal>.</para>
<figure>
<title>Registros de chart de Longhorn actualizado correctamente</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>ejemplo 4 de actualización de chart de helm de día dos descendente</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Compruebe que la versión de <literal>HelmChart</literal> se ha
actualizado. Para ello, diríjase a la sección <literal>HelmCharts</literal>
de Rancher seleccionando <literal>More Resources → HelmCharts</literal> (Más
recursos → HelmCharts). Seleccione el espacio de nombres donde se desplegó
el chart; en este ejemplo, sería <literal>kube-system</literal>.</para>
</listitem>
<listitem>
<para>Por último, compruebe que los pods de Longhorn se estén ejecutando.</para>
</listitem>
</orderedlist>
<para>Después de realizar las validaciones anteriores, se puede entender con
seguridad que el chart de Helm de Longhorn se ha actualizado a la versión
<literal>106.2.0+up1.8.1</literal>.</para>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>Actualización del chart de Helm con una herramienta GitOps de terceros</title>
<para>Puede darse el caso de que los usuarios deseen utilizar este procedimiento
de actualización con un flujo de trabajo de GitOps que no sea Fleet (por
ejemplo, <literal>Flux</literal>).</para>
<para>Para generar los recursos necesarios para el procedimiento de actualización,
puede usar el guion <literal>generate-chart-upgrade-data.sh</literal> para
rellenar la flota <literal>eib-charts-upgrader</literal> con los datos
proporcionados por el usuario. Para obtener más información sobre cómo
hacerlo, consulte la <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
<para>Cuando haya completado la configuración, puede utilizar <link
xl:href="https://kustomize.io">kustomize</link> para generar una solución
totalmente funcional que puede desplegar en su clúster:</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>Si desea incluir la solución en su flujo de trabajo de GitOps, puede
eliminar el archivo <literal>fleet.yaml</literal> y usar lo que queda como
una configuración válida de <literal>Kustomize</literal>. No olvide ejecutar
primero el guion <literal>generate-chart-upgrade-data.sh</literal>, para
poder rellenar la configuración de <literal>Kustomize</literal> con los
datos de los charts de Helm a los que desea actualizar.</para>
<para>Para comprender cómo se pretende usar este flujo de trabajo, puede resultar
útil consultar la <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>
y la <xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>.</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-product-documentation">
<title>Documentación del producto</title>
<partintro>
<para>Aquí encontrará la documentación de SUSE Edge for Telco</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Edge for Telco</title>
<para>SUSE Edge for Telco (antes conocido como Adaptive Telco Infrastructure
Platform/ATIP) es una plataforma de edge computing optimizada para las
telecomunicaciones que permite a las empresas de ese sector innovar y
acelerar la modernización de sus redes.</para>
<para>SUSE Edge for Telco es una pila completa de nube para telecomunicaciones
diseñada para alojar CNF (construcciones de nube) como 5G Packet Core y
Cloud RAN.</para>
<itemizedlist>
<listitem>
<para>Automatiza la distribución sin intervención y la gestión del ciclo de vida
de configuraciones complejas de pilas periféricas dirigidas a las
telecomunicaciones.</para>
</listitem>
<listitem>
<para>Garantiza continuamente la calidad del hardware para telecomunicaciones,
utilizando configuraciones y cargas de trabajo específicas para el sector.</para>
</listitem>
<listitem>
<para>Sus componentes están específicamente diseñados para usarse en edge
computing, lo que ocupan menos espacio y ofrecen un mayor rendimiento por
vatio.</para>
</listitem>
<listitem>
<para>Mantiene una estrategia de plataforma flexible con API independientes del
proveedor y 100 % de código abierto.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>Concepto y arquitectura</title>
<para>SUSE Edge for Telco es una plataforma diseñada para alojar aplicaciones de
telecomunicaciones modernas y nativas de la nube a gran escala, desde el
núcleo hasta la periferia.</para>
<para>Esta página explica la arquitectura y los componentes usados en SUSE Edge
for Telco.</para>
<section xml:id="id-suse-edge-for-telco-architecture">
<title>Arquitectura de SUSE Edge for Telco</title>
<para>El siguiente diagrama muestra la arquitectura general de SUSE Edge for
Telco:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.svg" width="100%"/>
</imageobject>
<textobject><phrase>arquitectura de producto atip 1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-2">
<title>Componentes</title>
<para>Hay dos bloques diferentes: la pila de gestión y la pila de entorno de
ejecución:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Pila de gestión</emphasis>: es la parte de SUSE Edge
for Telco que se emplea para gestionar el aprovisionamiento y el ciclo de
vida de las pilas de entornos de ejecución. Incluye los siguientes
componentes:</para>
<itemizedlist>
<listitem>
<para>Gestión de múltiples clústeres en entornos de nube pública y privada con
Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Asistencia bare metal con proveedores de infraestructura Metal3 (<xref
linkend="components-metal3"/>), MetalLB (<xref
linkend="components-metallb"/>) y <literal>CAPI</literal> (Cluster API)</para>
</listitem>
<listitem>
<para>Aislamiento completo de inquilinos e integraciones <literal>IDP</literal>
(proveedor de identidad)</para>
</listitem>
<listitem>
<para>Gran mercado de integraciones y extensiones de terceros</para>
</listitem>
<listitem>
<para>API independiente del proveedor y un rico ecosistema de proveedores</para>
</listitem>
<listitem>
<para>Control de las actualizaciones transaccionales de SUSE Linux Micro</para>
</listitem>
<listitem>
<para>Motor GitOps para gestionar el ciclo de vida de los clústeres mediante
repositorios Git con Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Pila de entorno de ejecución</emphasis>: es la parte
de SUSE Edge for Telco que se emplea para ejecutar las cargas de trabajo.</para>
<itemizedlist>
<listitem>
<para>Kubernetes con distribuciones seguras y ligeras como K3s (<xref
linkend="components-k3s"/>) y RKE2 (<xref linkend="components-rke2"/>)
(<literal>RKE2</literal> está reforzado, certificado y optimizado para su
uso gubernamental y en sectores regulados).</para>
</listitem>
<listitem>
<para>SUSE Security (<xref linkend="components-suse-security"/>) para habilitar
funciones de seguridad como el análisis de vulnerabilidades de imágenes, la
inspección profunda de paquetes y el control automático del tráfico dentro
del clúster.</para>
</listitem>
<listitem>
<para>Almacenamiento en bloques con SUSE Storage (<xref
linkend="components-suse-storage"/>) para permitir una forma sencilla y
fácil de utilizar una solución de almacenamiento nativa en la nube.</para>
</listitem>
<listitem>
<para>Sistema operativo optimizado con SUSE Linux Micro (<xref
linkend="components-slmicro"/>) para habilitar un sistema operativo seguro,
ligero e inmutable (sistema de archivos transaccional) para ejecutar
contenedores. SUSE Linux Micro está disponible en arquitecturas AArch64 y
AMD64/Intel 64, y también es compatible con un <literal>kernel en tiempo
real</literal> para casos prácticos de telecomunicaciones y periféricos.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>Ejemplo de flujos de despliegue</title>
<para>A continuación se muestran ejemplos generales de flujos de trabajo para
comprender la relación entre los componentes de gestión y los componentes de
entorno de ejecución.</para>
<para>El aprovisionamiento de red dirigida es el flujo de trabajo que permite el
despliegue de un nuevo clúster descendente con todos los componentes
preconfigurados y listos para ejecutar cargas de trabajo sin intervención
manual.</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>Ejemplo 1: despliegue de un nuevo clúster de gestión con todos los
componentes instalados</title>
<para>Use Edge Image Builder (<xref linkend="components-eib"/>) para crear una
imagen <literal>ISO</literal> nueva con la pila de gestión incluida. A
continuación, puede utilizar esta imagen <literal>ISO</literal> para
instalar un clúster de gestión nuevo en máquinas virtuales o en hardware
físico.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png" width="100%"/>
</imageobject>
<textobject><phrase>arquitectura de producto atip 2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Para obtener más información sobre cómo desplegar un nuevo clúster de
gestión, consulte el capítulo sobre el clúster de gestión de SUSE Edge for
Telco (<xref linkend="atip-management-cluster"/>).</para>
</note>
<note>
<para>Para obtener más información sobre cómo usar Edge Image Builder, consulte el
capítulo sobre Edge Image Builder (<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>Ejemplo 2: despliegue de un clúster descendente de un solo nodo con perfiles
de telecomunicaciones para permitir la ejecución de cargas de trabajo de
telecomunicaciones</title>
<para>Cuando tengamos el clúster de gestión instalado y en funcionamiento,
podremos utilizarlo para desplegar un clúster descendente de un solo nodo
con todas las capacidades de telecomunicaciones habilitadas y configuradas
mediante el flujo de trabajo de aprovisionamiento de red dirigida.</para>
<para>El siguiente diagrama muestra el flujo de trabajo general para desplegarlo:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png" width="100%"/>
</imageobject>
<textobject><phrase>arquitectura de producto atip 3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Para obtener más información sobre cómo desplegar un clúster descendente,
consulte el capítulo sobre aprovisionamiento automatizado de SUSE Edge for
Telco (<xref linkend="atip-automated-provisioning"/>).</para>
</note>
<note>
<para>Para obtener más información sobre las funciones de telecomunicaciones,
consulte el capítulo sobre funciones de telecomunicaciones de SUSE Edge for
Telco (<xref linkend="atip-features"/>).</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>Ejemplo 3: despliegue de un clúster descendente de alta disponibilidad con
MetalLB como equilibrador de carga</title>
<para>Cuando tengamos el clúster de gestión instalado y en funcionamiento,
podremos utilizarlo para desplegar un clúster descendente de alta
disponibilidad con <literal>MetalLB</literal> como equilibrador de carga
mediante el flujo de trabajo de aprovisionamiento de red dirigida.</para>
<para>El siguiente diagrama muestra el flujo de trabajo general para desplegarlo:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png" width="100%"/>
</imageobject>
<textobject><phrase>arquitectura de producto atip 4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>Para obtener más información sobre cómo desplegar un clúster descendente,
consulte el capítulo sobre aprovisionamiento automatizado de SUSE Edge for
Telco (<xref linkend="atip-automated-provisioning"/>).</para>
</note>
<note>
<para>Para obtener más información sobre <literal>MetalLB</literal>, consulte el
<xref linkend="components-metallb"/>.</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>Requisitos y supuestos</title>
<section xml:id="id-hardware">
<title>Hardware</title>
<para>Estos son los requisitos de hardware de SUSE Edge for Telco:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Clúster de gestión</emphasis>: el clúster de gestión
tiene componentes como <literal>SUSE Linux Micro</literal>,
<literal>RKE2</literal>, <literal>SUSE Rancher Prime</literal> y
<literal>Metal<superscript>3</superscript></literal>, y se utiliza para
gestionar varios clústeres descendentes. Dependiendo del número de clústeres
descendentes que se vayan a gestionar, los requisitos de hardware para el
servidor pueden variar.</para>
<itemizedlist>
<listitem>
<para>Los requisitos mínimos para el servidor (<literal>MV</literal> o
<literal>bare metal</literal>) son:</para>
<itemizedlist>
<listitem>
<para>RAM: 8 GB como mínimo (se recomiendan al menos 16 GB)</para>
</listitem>
<listitem>
<para>CPU: 2 como mínimo (se recomiendan al menos 4 CPU)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Clústeres descendentes</emphasis>: los clústeres
descendentes son los clústeres desplegados para ejecutar cargas de trabajo
de telecomunicaciones. Hay requisitos específicos para habilitar ciertas
capacidades de telecomunicaciones como <literal>SR-IOV</literal>,
<literal>Optimización del rendimiento de la CPU</literal>, etc.</para>
<itemizedlist>
<listitem>
<para>SR-IOV: para adjuntar funciones virtuales (VF) en modo de encaminamiento a
varias CNF/VNF, la tarjeta de interfaz de red debe admitir SR-IOV y
VT-d/AMD-Vi debe estar habilitado en el BIOS.</para>
</listitem>
<listitem>
<para>Procesadores CPU: para ejecutar cargas de trabajo específicas de
telecomunicaciones, el modelo de procesador CPU debe adaptarse para permitir
la mayoría de las funciones disponibles en esta tabla de referencia (<xref
linkend="atip-features"/>).</para>
</listitem>
<listitem>
<para>Requisitos de firmware para la instalación con medios virtuales:</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Hardware del servidor</para></entry>
<entry align="left" valign="top"><para>Modelo de BMC</para></entry>
<entry align="left" valign="top"><para>Gestión</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Hardware Dell</para></entry>
<entry align="left" valign="top"><para>15.ª generación</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Hardware Supermicro</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - Redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Hardware HPE</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="id-network">
<title>Red</title>
<para>Como referencia para la arquitectura de red, el siguiente diagrama muestra
una arquitectura de red típica para un entorno de telecomunicaciones:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirements1.svg" width="100%"/>
</imageobject>
<textobject><phrase>requisitos de producto atip 1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>La arquitectura de red se basa en los siguientes componentes:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Red de gestión</emphasis>: esta red se usa para la
gestión de los nodos del clúster descendentes. Se utiliza para la gestión
fuera de banda. Por lo general, esta red también está conectada a un
conmutador de gestión independiente, pero puede conectarse al mismo
conmutador de servicio mediante VLAN para aislar el tráfico.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Red de plano de control</emphasis>: esta red se usa
para la comunicación entre los nodos del clúster descendente y los servicios
que se ejecutan en ellos. Esta red también se utiliza para la comunicación
entre los nodos y los servicios externos, como los servidores
<literal>DHCP</literal> o <literal>DNS</literal>. En algunos casos, para
entornos conectados, el conmutador/enrutador puede gestionar el tráfico a
través de Internet.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Otras redes</emphasis>: en algunos casos, los nodos
podrían conectarse a otras redes con fines específicos.</para>
</listitem>
</itemizedlist>
<note>
<para>Para utilizar el flujo de trabajo de aprovisionamiento de red dirigida, el
clúster de gestión debe tener conectividad de red con el controlador de
gestión de la placa base (BMC) del servidor del clúster descendente, de modo
que se puedan automatizar la preparación y el aprovisionamiento del host.</para>
</note>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>Servicios (DHCP, DNS, etc.)</title>
<para>Algunos servicios externos como <literal>DHCP</literal>,
<literal>DNS</literal>, etc. podrían ser necesarios dependiendo del tipo de
entorno en el que se desplieguen:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Entorno conectado</emphasis>: en este caso, los
nodos estarán conectados a Internet (a través de protocolos de enrutamiento
L3) y el cliente proporcionará los servicios externos.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Entorno desconectado/aislado</emphasis>: en este
caso, los nodos no tendrán conectividad IP a Internet y se necesitarán
servicios adicionales para duplicar localmente el contenido requerido por el
flujo de trabajo de aprovisionamiento de red dirigida.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Servidor de archivos</emphasis>: se utiliza para
almacenar las imágenes del sistema operativo que se aprovisionarán en los
nodos del clúster descendente durante el flujo de trabajo de
aprovisionamiento de red dirigida. El chart de Helm de
<literal>Metal<superscript>3</superscript></literal> puede desplegar un
servidor multimedia para almacenar las imágenes del sistema operativo,
consulte la siguiente sección (<xref linkend="metal3-media-server"/>), pero
también es posible utilizar un servidor Web local existente.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-systemd-services">
<title>Inhabilitación de servicios systemd</title>
<para>Para las cargas de trabajo de telecomunicaciones, es importante inhabilitar
o configurar adecuadamente algunos de los servicios que se ejecutan en los
nodos para evitar cualquier impacto en el rendimiento de la carga de trabajo
que se ejecuta en los nodos (latencia).</para>
<itemizedlist>
<listitem>
<para><literal>rebootmgr</literal> es un servicio que permite configurar una
estrategia de rearranque cuando el sistema tenga actualizaciones
pendientes. Para las cargas de trabajo de telecomunicaciones, es muy
importante inhabilitar o configurar correctamente el servicio
<literal>rebootmgr</literal> para evitar el rearranque de los nodos en caso
de actualizaciones programadas por el sistema, con el fin de evitar
cualquier impacto en los servicios que se ejecutan en los nodos.</para>
</listitem>
</itemizedlist>
<note>
<para>Para obtener más información sobre <literal>rebootmgr</literal>, consulte el
<link xl:href="https://github.com/SUSE/rebootmgr">repositorio GitHub de
rebootmgr</link>.</para>
</note>
<para>Verifique la estrategia que se está utilizando ejecutando:</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>Puede inhabilitarla ejecutando:</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>O con el comando <literal>rebootmgrctl</literal>:</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para>Esta configuración para establecer la estrategia de
<literal>rebootmgr</literal> se puede automatizar mediante el flujo de
trabajo de aprovisionamiento de red dirigida. Para obtener más información,
consulte el capítulo correspondiente (<xref
linkend="atip-automated-provisioning"/>).</para>
</note>
<itemizedlist>
<listitem>
<para><literal>transactional-update</literal> es un servicio que permite
actualizaciones automáticas controladas por el sistema. Para las cargas de
trabajo de telecomunicaciones, es importante inhabilitar las actualizaciones
automáticas para evitar cualquier impacto en los servicios que se ejecutan
en los nodos.</para>
</listitem>
</itemizedlist>
<para>Para inhabilitar las actualizaciones automáticas, puede ejecutar:</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable transactional-update.timer
systemctl --now disable transactional-update-cleanup.timer</screen>
<itemizedlist>
<listitem>
<para><literal>fstrim</literal> es un servicio que permite recortar los sistemas
de archivos automáticamente cada semana. Para las cargas de trabajo de
telecomunicaciones, es importante inhabilitar el recorte automático para
evitar cualquier impacto en los servicios que se ejecutan en los nodos.</para>
</listitem>
</itemizedlist>
<para>Para inhabilitar el recorte automático, puede ejecutar:</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable fstrim.timer</screen>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>Configuración del clúster de gestión</title>
<section xml:id="id-introduction-2">
<title>Introducción</title>
<para>El clúster de gestión es la parte de SUSE Edge for Telco que se utiliza para
gestionar el aprovisionamiento y el ciclo de vida de las pilas de entorno de
ejecución. Desde un punto de vista técnico, el clúster de gestión contiene
los siguientes componentes:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro</literal> como sistema operativo. Dependiendo del
caso práctico, será posible personalizar algunas configuraciones como la
red, el almacenamiento, los usuarios y los argumentos del kernel.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> como clúster de Kubernetes. Dependiendo del caso
práctico, se puede configurar para utilizar complementos de CNI específicos,
como <literal>Multus</literal>, <literal>Cilium</literal>,
<literal>Calico</literal>, etc.</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> como plataforma de gestión del ciclo de vida de
los clústeres.</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal> como componente para
gestionar el ciclo de vida de los nodos bare metal.</para>
</listitem>
<listitem>
<para><literal>CAPI</literal> como componente para gestionar el ciclo de vida de
los clústeres de Kubernetes (clústeres descendentes). El <literal>proveedor
CAPI de RKE2</literal> se utiliza para gestionar el ciclo de vida de los
clústeres RKE2.</para>
</listitem>
</itemizedlist>
<para>Con todos los componentes mencionados, el clúster de gestión puede gestionar
el ciclo de vida de los clústeres descendentes usando un enfoque declarativo
para gestionar la infraestructura y las aplicaciones.</para>
<note>
<para>Para obtener más información sobre <literal>SUSE Linux Micro</literal>,
consulte: SUSE Linux Micro (<xref linkend="components-slmicro"/>)</para>
<para>Para obtener más información sobre <literal>RKE2</literal>, consulte: RKE2
(<xref linkend="components-rke2"/>)</para>
<para>Para obtener más información sobre <literal>Rancher</literal>, consulte:
Rancher (<xref linkend="components-rancher"/>)</para>
<para>Para obtener más información sobre
<literal>Metal<superscript>3</superscript></literal>, consulte: Metal3
(<xref linkend="components-metal3"/>)</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>Pasos para configurar el clúster de gestión</title>
<para>Los siguientes pasos son necesarios para configurar el clúster de gestión
(usando un único nodo):</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png" width="100%"/>
</imageobject>
<textobject><phrase>clúster gestión de producto atip 1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Estos son los pasos principales para configurar el clúster de gestión usando
un enfoque declarativo:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Preparación de imágenes para entornos conectados
(<xref linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>. El
primer paso es preparar los manifiestos y los archivos con todas las
configuraciones necesarias para su uso en entornos conectados.</para>
<itemizedlist>
<listitem>
<para>Estructura de directorios para entornos conectados (<xref
linkend="mgmt-cluster-directory-structure"/>). Este paso crea una estructura
de directorios que Edge Image Builder utilizará para almacenar los archivos
de configuración y la propia imagen.</para>
</listitem>
<listitem>
<para>Archivo de definición del clúster de gestión (<xref
linkend="mgmt-cluster-image-definition-file"/>). El archivo
<literal>mgmt-cluster.yaml</literal> es el archivo de definición principal
del clúster de gestión. Contiene la siguiente información sobre la imagen
que se va a crear:</para>
<itemizedlist>
<listitem>
<para>Información de la imagen: la información relacionada con la imagen que se
creará utilizando la imagen base.</para>
</listitem>
<listitem>
<para>Sistema operativo: las configuraciones del sistema operativo que se
utilizarán en la imagen.</para>
</listitem>
<listitem>
<para>Kubernetes: los charts de Helm y los repositorios, la versión de Kubernetes,
la configuración de red y los nodos que se utilizarán en el clúster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Carpeta personalizada (<xref linkend="mgmt-cluster-custom-folder"/>). La
carpeta <literal>custom</literal> contiene los archivos de configuración y
los guiones que utilizará Edge Image Builder para desplegar un clúster de
gestión totalmente funcional.</para>
<itemizedlist>
<listitem>
<para>Archivos: contiene los archivos de configuración que utilizará el clúster de
gestión.</para>
</listitem>
<listitem>
<para>Guiones: contiene los guiones que utilizará el clúster de gestión.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Carpeta de Kubernetes (<xref linkend="mgmt-cluster-kubernetes-folder"/>). La
carpeta <literal>kubernetes</literal> contiene los archivos de configuración
que utilizará el clúster de gestión.</para>
<itemizedlist>
<listitem>
<para>Manifiestos: contiene los manifiestos que utilizará el clúster de gestión.</para>
</listitem>
<listitem>
<para>Helm: contiene los archivos de valores de Helm que utilizará el clúster de
gestión.</para>
</listitem>
<listitem>
<para>Config: contiene los archivos de configuración que utilizará el clúster de
gestión.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Carpeta de red (<xref linkend="mgmt-cluster-network-folder"/>). La carpeta
<literal>network</literal> contiene los archivos de configuración de red que
utilizarán los nodos del clúster de gestión.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Preparación de imágenes para entornos aislados
(<xref linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>. El
paso consiste en mostrar las diferencias para preparar los manifiestos y los
archivos que se utilizarán en un entorno aislado.</para>
<itemizedlist>
<listitem>
<para>Modificaciones en el archivo de definición (<xref
linkend="mgmt-cluster-image-definition-file-airgap"/>). El archivo
<literal>mgmt-cluster.yaml</literal> debe modificarse para incluir la
sección <literal>embeddedArtifactRegistry</literal> con el campo
<literal>images</literal> establecido en todas las imágenes de contenedor
que se incluirán en la imagen de salida de EIB.</para>
</listitem>
<listitem>
<para>Modificaciones en la carpeta custom (<xref
linkend="mgmt-cluster-custom-folder-airgap"/>). La carpeta
<literal>custom</literal> debe modificarse para incluir los recursos
necesarios para ejecutar el clúster de gestión en un entorno aislado.</para>
<itemizedlist>
<listitem>
<para>Guion de registro: el guion <literal>custom/scripts/99-register.sh</literal>
debe eliminarse cuando se utiliza un entorno aislado.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Modificaciones en la carpeta de valores de Helm (<xref
linkend="mgmt-cluster-helm-values-folder-airgap"/>). La carpeta
<literal>helm/values</literal> debe modificarse para incluir la
configuración necesaria para ejecutar el clúster de gestión en un entorno
aislado.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Creación de la imagen (<xref
linkend="mgmt-cluster-image-creation"/>)</emphasis>. Este paso abarca la
creación de la imagen mediante la herramienta Edge Image Builder (tanto para
escenarios conectados como en entornos aislados). Compruebe los requisitos
previos (<xref linkend="components-eib"/>) para ejecutar la herramienta Edge
Image Builder en su sistema.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Aprovisionamiento del clúster de gestión (<xref
linkend="mgmt-cluster-provision"/>)</emphasis>. Este paso abarca el
aprovisionamiento del clúster de gestión que usa la imagen creada en el paso
anterior (tanto para entornos conectados como aislados). Este paso se puede
realizar con un ordenador portátil, un servidor, una máquina virtual o
cualquier otro sistema AMD64/Intel 64 con un puerto USB.</para>
</listitem>
</orderedlist>
<note>
<para>Para obtener más información, consulte la sección sobre Edge Image Builder
(<xref linkend="components-eib"/>) y la guía de inicio rápido de Edge Image
Builder (<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>Preparación de la imagen para entornos conectados</title>
<para>Edge Image Builder se utiliza para crear la imagen del clúster de
gestión. En este documento se describe la configuración mínima necesaria
para configurar el clúster de gestión.</para>
<para>Edge Image Builder se ejecuta dentro de un contenedor, por lo que se
requiere un entorno de ejecución de contenedores como <link
xl:href="https://podman.io">Podman</link> o <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>. En esta guía, se
entiende que Podman está disponible.</para>
<para>Además, como requisito previo para desplegar un clúster de gestión de alta
disponibilidad, es necesario reservar tres direcciones IP en la red:</para>
<itemizedlist>
<listitem>
<para><literal>apiVIP</literal> para la dirección IP virtual de API (utilizada
para acceder al servidor API de Kubernetes).</para>
</listitem>
<listitem>
<para><literal>ingressVIP</literal> para la dirección IP virtual de Ingress
(utilizada, por ejemplo, por la interfaz de usuario de Rancher).</para>
</listitem>
<listitem>
<para><literal>metal3VIP</literal> para la dirección IP virtual de Metal3.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure">
<title>Estructura del directorio</title>
<para>Al ejecutar EIB, se monta un directorio desde el host, por lo que lo primero
que hay que hacer es crear una estructura de directorios que EIB utilizará
para almacenar los archivos de configuración y la propia imagen. Este
directorio tiene la siguiente estructura:</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>La imagen
<literal>SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso</literal> debe
descargarse desde el <link xl:href="https://scc.suse.com/">Centro de
servicios al cliente de SUSE</link> o desde la <link
xl:href="https://www.suse.com/download/sle-micro/">página de descargas de
SUSE</link>, y debe estar ubicada en la carpeta
<literal>base-images</literal>.</para>
<para>Debe comprobar la suma de comprobación SHA256 de la imagen para asegurarse
de que no ha sido manipulada. La suma de comprobación se encuentra en la
misma ubicación en la que se descargó la imagen.</para>
<para>Un ejemplo de estructura del directorios se puede encontrar en el <link
xl:href="https://github.com/suse-edge/atip">repositorio GitHub de SUSE Edge,
en la carpeta "telco-examples"</link>.</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>Archivo de definición del clúster de gestión</title>
<para>El archivo de definición principal del clúster de gestión es
<literal>mgmt-cluster.yaml</literal>. Contiene la siguiente información:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - git
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.15.3
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 106.2.0+up1.8.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 106.2.0+up1.8.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3
        version: 303.0.7+up0.11.5
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 303.0.4+up0.20.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.11.2
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: $API_HOST
    apiVIP: $API_VIP
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>Para explicar los campos y valores del archivo de definición
<literal>mgmt-cluster.yaml</literal>, lo hemos dividido en las siguientes
secciones.</para>
<itemizedlist>
<listitem>
<para>Sección de imagen (archivo de definición):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>Donde <literal>baseImage</literal> es la imagen original que descargó del
Centro de servicios al cliente de SUSE o de la página de descargas de
SUSE. <literal>outputImageName</literal> es el nombre de la nueva imagen que
se utilizará para aprovisionar el clúster de gestión.</para>
<itemizedlist>
<listitem>
<para>Sección de sistema operativo (archivo de definición):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Donde <literal>installDevice</literal> es el dispositivo que se utilizará
para instalar el sistema operativo, <literal>username</literal> y
<literal>encryptedPassword</literal> son las credenciales que se utilizarán
para acceder al sistema, <literal>packageList</literal> es la lista de
paquetes que se instalarán (<literal>jq</literal> es necesario internamente
durante el proceso de instalación), y <literal>sccRegistrationCode</literal>
es el código de registro utilizado para obtener los paquetes y las
dependencias en el momento de la creación, que se pueden obtener en el
Centro de servicios al cliente de SUSE. La contraseña cifrada se puede
generar utilizando el comando <literal>openssl</literal> de la siguiente
manera:</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>El resultado es algo similar a esto:</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Sección de Kubernetes (archivo de definición):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.15.3
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 106.2.0+up1.8.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 106.2.0+up1.8.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3
        version: 303.0.7+up0.11.5
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 303.0.4+up0.20.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.11.2
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>La sección <literal>helm</literal> contiene la lista de charts de Helm que
se instalarán, los repositorios que se utilizarán y la configuración de la
versión de todos ellos.</para>
<para>La sección <literal>network</literal> contiene la configuración de la red,
como <literal>apiHost</literal> y <literal>apiVIP</literal>, que utilizará
el componente <literal>RKE2</literal>. <literal>apiVIP</literal> debe ser
una dirección IP que no se use en la red y no debe formar parte del pool
DHCP (en caso de que se utilice DHCP). Además, si se usa
<literal>apiVIP</literal> en un clúster de varios nodos, se hace para
acceder al servidor de API de Kubernetes. <literal>apiHost</literal> es la
resolución de nombres para <literal>apiVIP</literal> que utilizará el
componente <literal>RKE2</literal>.</para>
<para>La sección <literal>nodes</literal> contiene la lista de nodos que se
utilizarán en el clúster. En este ejemplo, se utiliza un
clúster de un solo nodo, pero se puede ampliar a un clúster de varios nodos
añadiendo más nodos a la lista (descomentando las líneas).</para>
<note>
<itemizedlist>
<listitem>
<para>Los nombres de los nodos deben ser únicos en el clúster.</para>
</listitem>
<listitem>
<para>Si lo desea, puede usar el campo <literal>initializer</literal> para
especificar el host de arranque. Si no lo hace, será el primer nodo de la
lista.</para>
</listitem>
<listitem>
<para>Si se requiere una configuración de red, los nombres de los nodos deben ser
los mismos que los nombres de host definidos en la carpeta network (<xref
linkend="mgmt-cluster-network-folder"/>).</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Carpeta custom</title>
<para>La carpeta <literal>custom</literal> contiene las siguientes subcarpetas:</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para>La carpeta <literal>custom/files</literal> contiene los archivos de
configuración que utilizará el clúster de gestión.</para>
</listitem>
<listitem>
<para>La carpeta <literal>custom/scripts</literal> contiene los guiones que
utilizará el clúster de gestión.</para>
</listitem>
</itemizedlist>
<para>La carpeta <literal>custom/files</literal> contiene los siguientes archivos:</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>: contiene los parámetros de configuración
para <literal>Metal<superscript>3</superscript></literal>,
<literal>Rancher</literal> y <literal>MetalLB</literal>. Solo debe modificar
este archivo si desea cambiar los espacios de nombres que se van a utilizar.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>: contiene la configuración del componente
<literal>Metal<superscript>3</superscript></literal> que se va a utilizar
(no es necesario modificarlo). En futuras versiones, este guion se
sustituirá por <literal>Rancher Turtles</literal> para facilitar su uso.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic-bmo -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
	EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
	EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>: contiene la configuración del componente
<literal>Rancher</literal> que se va a utilizar (no es necesario
modificarlo).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>: contiene la configuración para
crear el servicio systemd que ejecutará los guiones durante el primer
arranque (no es necesario modificarlo).</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single one
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>La carpeta <literal>custom/scripts</literal> contiene los siguientes
archivos:</para>
<itemizedlist>
<listitem>
<para>Guion <literal>99-alias.sh</literal>: contiene el alias que usará el clúster
de gestión para cargar el archivo kubeconfig en el primer arranque (no es
necesario modificarlo).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para>Guion <literal>99-mgmt-setup.sh</literal>: contiene la configuración para
copiar los guiones durante el primer arranque (no es necesario modificarlo).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para>Guion <literal>99-register.sh</literal>: contiene la configuración para
registrar el sistema usando el código de registro del Centro de servicios al
cliente de SUSE. Los elementos <literal>${SCC_ACCOUNT_EMAIL}</literal> y
<literal>${SCC_REGISTRATION_CODE}</literal> deben configurarse correctamente
para registrar el sistema con su cuenta.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
	zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Carpeta kubernetes</title>
<para>La carpeta <literal>kubernetes</literal> contiene las siguientes
subcarpetas:</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para>La carpeta <literal>kubernetes/config</literal> contiene los siguientes
archivos:</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>: de forma predeterminada, el complemento de
<literal>CNI</literal> instalado por defecto es <literal>Cilium</literal>,
por lo que no es necesario crear esta carpeta ni este archivo. En caso de
que necesite personalizar el complemento de <literal>CNI</literal>, puede
usar el archivo <literal>server.yaml</literal> que se encuentra en la
carpeta <literal>kubernetes/config</literal>. Contiene la siguiente
información:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</listitem>
</itemizedlist>
<note>
<para>Se trata de un archivo opcional para definir ciertas personalizaciones de
Kubernetes, como los complementos de CNI que se utilizarán o muchas opciones
que puede consultar en la <link
xl:href="https://docs.rke2.io/install/configuration">documentación
oficial</link>.</para>
</note>
<para>La carpeta <literal>kubernetes/manifests</literal> contiene los siguientes
archivos:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>: contiene la configuración para
crear el servicio <literal>Ingress</literal> para el clúster de gestión (no
es necesario modificarlo).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>: contiene la configuración para
crear el espacio de nombres <literal>NeuVector</literal> (no es necesario
modificarlo).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>: contiene la configuración para crear
el <literal>L2Advertisement</literal> para el componente
<literal>MetalLB</literal> (no es necesario modificarlo).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>: contiene la configuración para crear
el <literal>IPAddressPool</literal> para el componente
<literal>rke2-ingress-nginx</literal>. <literal>${INGRESS_VIP}</literal>
debe configurarse correctamente para definir la dirección IP reservada para
usarse por el componente <literal>rke2-ingress-nginx</literal>.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para>La carpeta <literal>kubernetes/helm/values</literal> contiene los siguientes
archivos:</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>: contiene la configuración para crear el
componente <literal>Rancher</literal>. <literal>${INGRESS_VIP}</literal>
debe configurarse correctamente para definir la dirección IP que utilizará
el componente <literal>Rancher</literal>. La URL para acceder al componente
<literal>Rancher</literal> será
<literal>https://rancher-${INGRESS_VIP}.sslip.io</literal>.</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global.cattle.psp.enabled: "false"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>: contiene la configuración para crear el
componente <literal>NeuVector</literal> (no es necesario modificarlo).</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>: contiene la configuración para crear el
componente
<literal>Metal<superscript>3</superscript></literal>.
<literal>${METAL3_VIP}</literal> debe configurarse correctamente para
definir la dirección IP que utilizará el componente
<literal>Metal<superscript>3</superscript></literal>.</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
<para xml:id="arm64-mgmt-cluster">Si desea desplegar clústeres descendentes arm64 utilizando este clúster de
gestión x86_64, debe añadir la línea <literal>deployArchitecture:
arm64</literal> siguiente a la sección <literal>global</literal> del archivo
<literal>metal3.yaml</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
  deployArchitecture: arm64
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note>
<para>En la versión actual, existe una limitación con respecto al uso de
<literal>deployArchitecture: arm64</literal>. En concreto, si habilita el
despliegue de clústeres arm64 descendentes mediante esta directiva, el
clúster de gestión solo podrá desplegar posteriormente esta
arquitectura. Para desplegar clústeres en ambas arquitecturas (x86_64 y
arm64), deberá aprovisionar dos clústeres de gestión independientes. Esta
limitación se eliminará en una versión futura.</para>
</note>
<note xml:id="metal3-media-server">
<para>El servidor de medios es una función opcional incluida en
Metal<superscript>3</superscript> (inhabilitada de forma
predeterminada). Para utilizar esta función de Metal3, es necesario
configurarla en el manifiesto anterior. Para utilizar el servidor de medios
de Metal<superscript>3</superscript>, especifique la siguiente variable:</para>
<itemizedlist>
<listitem>
<para>Defina <literal>enable_metal3_media_server</literal> como
<literal>true</literal> para habilitar la función de servidor de medios en
la sección global.</para>
</listitem>
<listitem>
<para>Incluya la siguiente configuración sobre el servidor de medios, donde
${MEDIA_VOLUME_PATH} es la ruta al volumen de medios en el medio (por
ejemplo, <literal>/home/metal3/bmh-image-cache</literal>).</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>Se puede utilizar un servidor de medios externo para almacenar las imágenes
y, en caso de que desee usarlo con TLS, deberá modificar las siguientes
configuraciones:</para>
<itemizedlist>
<listitem>
<para>Defina <literal>true</literal> en <literal>additionalTrustedCAs</literal> en
el archivo <literal>metal3.yaml</literal> anterior para habilitar las CA de
confianza adicionales del servidor de medios externo.</para>
</listitem>
<listitem>
<para>Incluya la siguiente configuración de secreto en la carpeta
<literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal> para
almacenar el certificado de CA del servidor de medios externo.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para><literal>additional_ca_cert</literal> es el certificado de CA cifrado en
base64 del servidor de medios externo. Puede usar el siguiente comando para
cifrar el certificado y generar el secreto manualmente:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>: contiene la configuración para crear el
componente <literal>Cert-Manager</literal> (no es necesario modificarlo).</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: "true"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>Carpeta network</title>
<para>La carpeta <literal>network</literal> contiene tantos archivos como nodos
haya en el clúster de gestión. En nuestro caso, solo tenemos un nodo, por lo
que solo tenemos un archivo llamado
<literal>mgmt-cluster-node1.yaml</literal>. El nombre del archivo debe
coincidir con el nombre de host definido en el archivo de definición
<literal>mgmt-cluster.yaml</literal> en la sección network/node descrita
anteriormente.</para>
<para>Si necesita personalizar la configuración de red, por ejemplo, para utilizar
una dirección IP estática específica (si no se usa DHCP), puede utilizar el
archivo <literal>mgmt-cluster-node1.yaml</literal> que se encuentra en la
carpeta <literal>network</literal>. Contiene la siguiente información:</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: la dirección IP del gateway.</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: la dirección IP del servidor DNS.</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: la dirección MAC de la interfaz de red.</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: la dirección IP del clúster de gestión.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>Si desea utilizar DHCP para obtener la dirección IP, puede usar la siguiente
configuración (la dirección <literal>MAC</literal> debe configurarse
correctamente utilizando la variable <literal>${MGMT_MAC}</literal>):</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>Dependiendo del número de nodos del clúster de gestión, puede crear más
archivos como <literal>mgmt-cluster-node2.yaml</literal>,
<literal>mgmt-cluster-node3.yaml</literal>, etc. para configurar el resto de
los nodos.</para>
</listitem>
<listitem>
<para>La sección <literal>routes</literal> se utiliza para definir la tabla de
enrutamiento del clúster de gestión.</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>Preparación de la imagen para entornos aislados</title>
<para>En esta sección se describe cómo preparar la imagen para entornos
aislados. Muestra solo las diferencias con respecto a las secciones
anteriores. Para preparar la imagen para entornos aislados, es necesario
realizar los siguientes cambios con respecto a la sección sobre la
preparación de imagen en entornos conectados (<xref
linkend="mgmt-cluster-image-preparation-connected"/>):</para>
<itemizedlist>
<listitem>
<para>El archivo <literal>mgmt-cluster.yaml</literal> debe modificarse para
incluir la sección <literal>embeddedArtifactRegistry</literal> con el campo
<literal>images</literal> definido con todas las imágenes de contenedor que
se incluirán en la imagen de salida de EIB.</para>
</listitem>
<listitem>
<para>El archivo <literal>mgmt-cluster.yaml</literal> debe modificarse para
incluir el chart de Helm
<literal>rancher-turtles-airgap-resources</literal>.</para>
</listitem>
<listitem>
<para>El guion <literal>custom/scripts/99-register.sh</literal> debe eliminarse si
se utiliza un entorno aislado.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>Modificaciones del archivo de definición</title>
<para>El archivo <literal>mgmt-cluster.yaml</literal> debe modificarse para
incluir la sección <literal>embeddedArtifactRegistry</literal>. En ella, el
campo <literal>images</literal> debe contener la lista de todas las imágenes
de contenedor que se incluirán en la imagen de salida.</para>
<note>
<para>A continuación, se muestra un ejemplo del archivo
<literal>mgmt-cluster.yaml</literal> con la sección
<literal>embeddedArtifactRegistry</literal> incluida. Asegúrese de que las
imágenes indicadas contengan las versiones de los componentes que necesita.</para>
</note>
<para>También se debe añadir el chart de Helm
<literal>rancher-turtles-airgap-resources</literal>, que crea los recursos
descritos en la <link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/v0.19/en/getting-started/air-gapped-environment.html">documentación
de Rancher Turtles para entornos aislados</link>. Esto también requiere un
archivo de valores turtles.yaml para que el chart de rancher-turtles
especifique la configuración necesaria.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.32.4+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.15.3
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 106.2.0+up1.8.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 106.2.0+up1.8.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3
        version: 303.0.7+up0.11.5
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 303.0.4+up0.20.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources
        version: 303.0.4+up0.20.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 106.0.1+up2.8.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.11.2
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/rancher/hardened-cluster-autoscaler:v1.9.0-build20241203
    - name: registry.suse.com/rancher/hardened-cni-plugins:v1.6.2-build20250306
    - name: registry.suse.com/rancher/hardened-coredns:v1.12.1-build20250401
    - name: registry.suse.com/rancher/hardened-k8s-metrics-server:v0.7.2-build20250110
    - name: registry.suse.com/rancher/hardened-multus-cni:v4.2.0-build20250326
    - name: registry.suse.com/rancher/klipper-helm:v0.9.5-build20250306
    - name: registry.suse.com/rancher/mirrored-cilium-cilium:v1.17.3
    - name: registry.suse.com/rancher/mirrored-cilium-operator-generic:v1.17.3
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.13.0
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.2.0
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.13.2
    - name: registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.2.0
    - name: registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.15.0
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.8.1
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.8.1
    - name: registry.suse.com/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0
    - name: registry.suse.com/rancher/neuvector-compliance-config:1.0.5
    - name: registry.suse.com/rancher/neuvector-controller:5.4.4
    - name: registry.suse.com/rancher/neuvector-enforcer:5.4.4
    - name: registry.suse.com/rancher/nginx-ingress-controller:v1.12.1-hardened3
    - name: registry.rancher.com/rancher/cluster-api-addon-provider-fleet:v0.10.0
    - name: registry.rancher.com/rancher/cluster-api-operator:v0.17.0
    - name: registry.rancher.com/rancher/fleet-agent:v0.12.3
    - name: registry.rancher.com/rancher/fleet:v0.12.3
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.7-build20250425
    - name: registry.rancher.com/rancher/rancher-webhook:v0.7.2
    - name: registry.rancher.com/rancher/rancher/turtles:v0.20.0
    - name: registry.rancher.com/rancher/rancher:v2.11.2
    - name: registry.rancher.com/rancher/shell:v0.4.1
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.15.2
    - name: registry.suse.com/rancher/cluster-api-controller:v1.9.5
    - name: registry.suse.com/rancher/cluster-api-provider-metal3:v1.9.3
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.16.1
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.16.1
    - name: registry.suse.com/rancher/hardened-sriov-network-operator:v1.5.0-build20250425
    - name: registry.suse.com/rancher/ip-address-manager:v1.9.4
    - name: registry.rancher.com/rancher/kubectl:v1.32.2</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>Modificaciones de la carpeta custom</title>
<itemizedlist>
<listitem>
<para>El guion <literal>custom/scripts/99-register.sh</literal> debe eliminarse si
se utiliza un entorno aislado. Como se puede ver en la estructura del
directorio, el guion <literal>99-register.sh</literal> no está incluido en
la carpeta <literal>custom/scripts</literal>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-helm-values-folder-airgap">
<title>Modificaciones de la carpeta de valores de Helm</title>
<itemizedlist>
<listitem>
<para>El archivo <literal>turtles.yaml</literal> contiene la configuración
necesaria para especificar el funcionamiento con aislamiento para Rancher
Turtles. Tenga en cuenta que esto depende de la instalación del chart de
rancher-turtles-airgap-resources.</para>
<screen language="yaml" linenumbering="unnumbered">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"</screen>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>Creación de la imagen</title>
<para>Una vez preparada la estructura de directorios siguiendo las secciones
anteriores (tanto para escenarios conectados como aislados), ejecute el
comando siguiente para crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>Esto crea el archivo de imagen ISO de salida que, en nuestro caso, según la
definición de imagen descrita anteriormente, es
<literal>eib-mgmt-cluster-image.iso</literal>.</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>Aprovisionamiento del clúster de gestión</title>
<para>La imagen anterior contiene todos los componentes explicados anteriormente y
se puede utilizar para aprovisionar el clúster de gestión mediante una
máquina virtual o un servidor bare metal (usando la función de medios
virtuales).</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>Configuración de funciones de telecomunicaciones</title>
<para>Esta sección documenta y explica la configuración de las funciones
específicas para telecomunicaciones en clústeres desplegados mediante SUSE
Edge for Telco.</para>
<para>Se utiliza el método de despliegue de aprovisionamiento de red dirigida,
descrito en la sección sobre aprovisionamiento automatizado (<xref
linkend="atip-automated-provisioning"/>).</para>
<para>En esta sección se tratan los siguientes temas:</para>
<itemizedlist>
<listitem>
<para>Imagen del kernel en tiempo real (<xref
linkend="kernel-image-for-real-time"/>): imagen que se usará para el kernel
en tiempo real.</para>
</listitem>
<listitem>
<para>Argumentos del kernel para baja latencia y alto rendimiento (<xref
linkend="kernel-args"/>): los argumentos que debe utilizar el kernel en
tiempo real para conseguir el máximo rendimiento y una baja latencia al
ejecutar cargas de trabajo de telecomunicaciones.</para>
</listitem>
<listitem>
<para>Fijación de CPU mediante Tuned y argumentos del kernel (<xref
linkend="cpu-tuned-configuration"/>): aislamiento de las CPU mediante
argumentos del kernel y el perfil Tuned.</para>
</listitem>
<listitem>
<para>Configuración de la CNI (<xref linkend="cni-configuration"/>): la
configuración de la CNI que usará el clúster de Kubernetes.</para>
</listitem>
<listitem>
<para>Configuración de SR-IOV (<xref linkend="sriov"/>): la configuración de
SR-IOV que usarán las cargas de trabajo de Kubernetes.</para>
</listitem>
<listitem>
<para>Configuración de DPDK (<xref linkend="dpdk"/>): la configuración de DPDK que
usará el sistema.</para>
</listitem>
<listitem>
<para>Tarjeta de aceleración vRAN (<xref linkend="acceleration"/>): configuración
de la tarjeta de aceleración que usarán las cargas de trabajo de Kubernetes.</para>
</listitem>
<listitem>
<para>Páginas enormes (<xref linkend="huge-pages"/>): configuración de páginas
enormes (huge) que usarán las cargas de trabajo de Kubernetes.</para>
</listitem>
<listitem>
<para>Fijación de CPU en Kubernetes (<xref linkend="cpu-pinning-kubernetes"/>):
configuración de Kubernetes y las aplicaciones para aprovechar la fijación
de CPU.</para>
</listitem>
<listitem>
<para>Configuración de programación compatible con NUMA (<xref
linkend="numa-aware-scheduling"/>): configuración de programación compatible
con NUMA que usarán las cargas de trabajo de Kubernetes.</para>
</listitem>
<listitem>
<para>Configuración de MetalLB (<xref linkend="metal-lb-configuration"/>):
configuración de MetalLB que usarán las cargas de trabajo de Kubernetes.</para>
</listitem>
<listitem>
<para>Configuración del registro privado (<xref linkend="private-registry"/>):
configuración del registro privado que usarán las cargas de trabajo de
Kubernetes.</para>
</listitem>
<listitem>
<para>Configuración del protocolo de tiempo de precisión (<xref
linkend="ptp-configuration"/>): archivos de configuración para ejecutar
perfiles PTP de telecomunicaciones.</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>Imagen del kernel en tiempo real</title>
<para>Usar una imagen de kernel en tiempo real no es necesariamente mejor que usar
un kernel estándar. Se trata de un kernel diferente, optimizado para un caso
práctico concreto. El kernel en tiempo real está optimizado para reducir la
latencia a costa del rendimiento. No se recomienda el uso del kernel en
tiempo real para fines generales, pero en nuestro caso, es el kernel
recomendado para cargas de trabajo de telecomunicaciones, donde la latencia
es un factor clave.</para>
<para>Hay cuatro funciones principales:</para>
<itemizedlist>
<listitem>
<para>Ejecución determinista:</para>
<para>Se consigue una mayor previsibilidad: se garantiza que los procesos
empresariales críticos se completan a tiempo, siempre, y ofrece un servicio
de alta calidad, incluso con cargas elevadas del sistema. Al proteger los
recursos clave del sistema para los procesos de alta prioridad, se garantiza
una mayor previsibilidad para las aplicaciones en las que el tiempo es
importante.</para>
</listitem>
<listitem>
<para>Baja fluctuación:</para>
<para>La baja fluctuación basada en la tecnología altamente determinista ayuda a
mantener las aplicaciones sincronizadas con el mundo real. Esto ayuda a los
servicios que necesitan cálculos continuos y repetidos.</para>
</listitem>
<listitem>
<para>Herencia de prioridad:</para>
<para>La herencia de prioridad hace referencia a la capacidad de un proceso de
subir de prioridad si existe un proceso de mayor prioridad que requiere que
el de menor prioridad finalice antes para poder completar su tarea. SUSE
Linux Enterprise Real Time resuelve estos problemas de inversión de
prioridad para los procesos críticos.</para>
</listitem>
<listitem>
<para>Interrupciones de subprocesos:</para>
<para>En un sistema operativo de uso general, no se pueden interrumpir los
procesos que se ejecutan en modo de interrupción. Con SUSE Linux Enterprise
Real Time, estas interrupciones se encapsulan en subprocesos del kernel, que
son interrumpibles. De esta forma, los procesos de mayor prioridad definidos
por el usuario pueden interrumpir tanto interrupciones duras como suaves.</para>
<para>En nuestro caso, si ha instalado una imagen en tiempo real como
<literal>SUSE Linux Micro RT</literal>, el kernel en tiempo real ya está
instalado. Puede descargar la imagen del kernel en tiempo real del <link
xl:href="https://scc.suse.com/">Centro de servicios al cliente de
SUSE</link>.</para>
<note>
<para>Para obtener más información sobre el kernel en tiempo real, visite <link
xl:href="https://www.suse.com/products/realtime/">SUSE Real Time</link>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="kernel-args">
<title>Argumentos del kernel para baja latencia y alto rendimiento</title>
<para>Es importante configurar los argumentos del kernel para que el kernel en
tiempo real funcione correctamente y ofrezca el mejor rendimiento y una baja
latencia para ejecutar cargas de trabajo de telecomunicaciones. Hay algunos
conceptos importantes que hay que tener en cuenta al configurar los
argumentos del kernel para este caso práctico:</para>
<itemizedlist>
<listitem>
<para>Elimine <literal>kthread_cpus</literal> cuando use el kernel en tiempo real
de SUSE. Este parámetro controla en qué CPU se crean los subprocesos del
kernel. También controla qué CPU están permitidas para el PID 1 y para
cargar módulos del kernel (el ayudante del espacio de usuario kmod). Este
parámetro no se reconoce y no tiene ningún efecto.</para>
</listitem>
<listitem>
<para>Use <literal>isolcpus</literal>, <literal>nohz_full</literal>,
<literal>rcu_nocbs</literal> e <literal>irqaffinity</literal> para aislar
los núcleos de CPU. Para obtener una lista completa de las técnicas de
fijación de CPU, consulte el capítulo sobre fijación de CPU mediante Tuned y
argumentos del kernel (<xref linkend="cpu-tuned-configuration"/>).</para>
</listitem>
<listitem>
<para>Añade los indicadores <literal>domain,nohz,managed_irq</literal> al
argumento del kernel <literal>isolcpus</literal>. Si no se añade ningún
indicador, <literal>isolcpus</literal> equivale a especificar solo el
indicador <literal>domain</literal>. Esto aísla las CPU especificadas de la
programación, incluidas las tareas del kernel. El indicador
<literal>nohz</literal> detiene la marca del programador en las CPU
especificadas (si solo se puede ejecutar una tarea en una CPU), y el
indicador <literal>managed_irq</literal> evita el enrutamiento de
interrupciones externas gestionadas (dispositivos) en las CPU
especificadas. Tenga en cuenta que las líneas IRQ de los dispositivos NVMe
están totalmente gestionadas por el kernel y, como consecuencia, se
enrutarán a los núcleos no aislados (de mantenimiento). Por ejemplo, la
línea de comandos que se proporciona al final de esta sección dará como
resultado que solo se asignen cuatro colas (más una cola de
administración/control) en el sistema:</para>
<screen language="shell" linenumbering="unnumbered">for I in $(grep nvme0 /proc/interrupts | cut -d ':' -f1); do cat /proc/irq/${I}/effective_affinity_list; done | column
39      0       19      20      39</screen>
<para>Este comportamiento evita cualquier interrupción causada por la E/S del
disco en cualquier aplicación sensible al tiempo que se ejecute en los
núcleos aislados, pero puede requerir atención y un diseño cuidadoso para
las cargas de trabajo centradas en el almacenamiento.</para>
</listitem>
<listitem>
<para>Ajuste las marcas (ticks, interrupciones periódicas del temporizador del
kernel):</para>
<itemizedlist>
<listitem>
<para><literal>skew_tick=1</literal>: a veces, las marcas pueden ocurrir
simultáneamente. En lugar de que todas las CPU reciban su marca de
temporizador exactamente al mismo tiempo, <literal>skew_tick=1</literal>
hace que se produzcan en momentos ligeramente distintos. Esto ayuda a
reducir la fluctuación del sistema, lo que se traduce en tiempos de
respuesta a las interrupciones más consistentes y más bajos (un requisito
esencial para las aplicaciones sensibles a la latencia).</para>
</listitem>
<listitem>
<para><literal>nohz=on</literal>: detiene la marca del temporizador periódico en
las CPU inactivas.</para>
</listitem>
<listitem>
<para><literal>nohz_full=&lt;núcleos-cpu&gt;</literal>: detiene la marca del
temporizador periódico en las CPU especificadas que están dedicadas a
aplicaciones en tiempo real.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Inhabilite la gestión de excepciones de comprobación de máquina (MCE)
especificando <literal>mce=off</literal>. Las MCE son errores de hardware
detectados por el procesador y desactivarlas puede evitar que los registros
sean excesivamente grandes.</para>
</listitem>
<listitem>
<para>Añada <literal>nowatchdog</literal> para inhabilitar el watchdog de bloqueo
suave, que se implementa como un temporizador que se ejecuta en el contexto
de interrupción dura del temporizador. Cuando caduca (es decir, cuando se
detecta un bloqueo suave), muestra una advertencia (en el contexto de
interrupción dura) y ejecuta cualquier objetivo de latencia. Incluso si no
caduca nunca, pasa a la lista de temporizadores, lo que aumenta ligeramente
la sobrecarga de cada interrupción del temporizador. Esta opción también
inhabilita el watchdog de NMI, por lo que las NMI no pueden interferir.</para>
</listitem>
<listitem>
<para><literal>nmi_watchdog=0</literal> inhabilita el watchdog de NMI
(interrupción no enmascarable). Se puede omitir si se usa
<literal>nowatchdog</literal>.</para>
</listitem>
<listitem>
<para>RCU (Read-Copy-Update, leer-copiar-actualizar) es un mecanismo del kernel
que permite el acceso simultáneo y sin bloqueos de muchos lectores a datos
compartidos. Una retrollamada RCU, una función que se activa tras un
"periodo de gracia", garantiza que todos los lectores anteriores hayan
terminado, de modo que los datos antiguos puedan recuperarse de forma
segura. Ajustamos RCU, especialmente para cargas de trabajo sensibles, con
el fin de descargar estas retrollamadas de CPU dedicadas (fijadas), evitando
que las operaciones del kernel interfieran con tareas críticas y sensibles
al tiempo.</para>
<itemizedlist>
<listitem>
<para>Especifique las CPU fijadas en <literal>rcu_nocbs</literal> para que las
retrollamadas de RCU no se ejecuten en ellas. Esto ayuda a reducir la
fluctuación y la latencia de las cargas de trabajo en tiempo real.</para>
</listitem>
<listitem>
<para><literal>rcu_nocb_poll</literal> hace que las CPU sin retrollamada realicen
sondeos periódicos para comprobar si es necesario gestionar la
retrollamada. Esto puede reducir la sobrecarga de interrupciones.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_cpu_stall_suppress=1</literal> suprime las
advertencias de bloqueo de la CPU por RCU, que en ocasiones pueden ser
falsos positivos en sistemas en tiempo real con mucha carga.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_expedited=1</literal> acelera el período de gracia
para las operaciones RCU, lo que hace que las secciones críticas de lectura
sean más receptivas.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_normal_after_boot=1</literal>, cuando se usa con
rcu_expedited, permite que RCU vuelva al funcionamiento normal (no
acelerado) después del arranque del sistema.</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_task_stall_timeout=0</literal> inhabilita el detector
de bloqueo de tareas RCU, lo que evita posibles advertencias o
interrupciones del sistema provocadas por tareas RCU de larga duración.</para>
</listitem>
<listitem>
<para><literal>rcutree.kthread_prio=99</literal> establece la prioridad del
subproceso del kernel de retrollamadas de RCU al máximo posible (99), lo que
garantiza que se programen y gestionen las retrollamadas de RCU con rapidez
cuando sea necesario.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Añada <literal>ignition.platform.id=openstack</literal> para Metal3 y
Cluster API a fin de aprovisionar/desaprovisionar correctamente el
clúster. Esto lo utiliza el agente Python de Metal3, que tiene su origen en
Openstack Ironic.</para>
</listitem>
<listitem>
<para>Elimine <literal>intel_pstate=passive</literal>. Esta opción configura
<literal>intel_pstate</literal> para que funcione con reguladores cpufreq
genéricos; pero, para que esto funcione, inhabilita los estados P
gestionados por hardware (<literal>HWP</literal>) como efecto
secundario. Para reducir la latencia del hardware, no se recomienda utilizar
esta opción para cargas de trabajo en tiempo real.</para>
</listitem>
<listitem>
<para>Reemplace <literal>intel_idle.max_cstate=0 processor.max_cstate=1</literal>
por <literal>idle=poll</literal>. Para evitar las transiciones de estado C,
se usa la opción <literal>idle=poll</literal> para inhabilitar las
transiciones de estado C y mantener la CPU en el estado C más alto. La
opción <literal>intel_idle.max_cstate=0</literal> inhabilita
<literal>intel_idle</literal>, por lo que se usa
<literal>acpi_idle</literal>, y <literal>acpi_idle.max_cstate=1</literal>
establece el estado C máximo para acpi_idle. En arquitecturas AMD64/Intel
64, el primer estado C de ACPI es siempre <literal>POLL</literal>, pero usa
una función <literal>poll_idle()</literal>, que puede introducir una pequeña
latencia al leer el reloj periódicamente y reiniciar el bucle principal en
<literal>do_idle()</literal> después de un tiempo de espera (esto también
implica borrar y establecer el indicador de tarea
<literal>TIF_POLL</literal>). Por el contrario, <literal>idle=poll</literal>
se ejecuta en un bucle cerrado, esperando activamente a que se reprogramen
las tareas. Esto minimiza la latencia al salir del estado de inactividad,
pero a costa de mantener la CPU funcionando a toda velocidad en el
subproceso inactivo.</para>
</listitem>
<listitem>
<para>Desactive C1E en BIOS. Es importante desactivar el estado C1E en el BIOS
para evitar que la CPU entre en el estado C1E cuando está inactiva. El
estado C1E es un estado de bajo consumo que puede introducir latencia cuando
la CPU está inactiva.</para>
</listitem>
</itemizedlist>
<para>El resto de esta documentación trata sobre parámetros adicionales,
incluyendo las páginas enormes e IOMMU.</para>
<para>Se proporciona un ejemplo de argumentos del kernel para un servidor Intel de
32 núcleos, que incluye los ajustes mencionados anteriormente:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
<para>Aquí hay otro ejemplo de configuración para un servidor AMD de 64
núcleos. Entre los 128 procesadores lógicos (<literal>0-127</literal>), los
primeros 8 núcleos (<literal>0-7</literal>) se destinan a tareas de
mantenimiento, mientras que los 120 núcleos restantes
(<literal>8-127</literal>) se fijan para las aplicaciones:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=575291cf-74e8-42cf-8f2c-408a20dc00b8 skew_tick=1 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack amd_iommu=on iommu=pt irqaffinity=0-7 isolcpus=domain,nohz,managed_irq,8-127 nohz_full=8-127 rcu_nocbs=8-127 mce=off nohz=on net.ifnames=0 nowatchdog nmi_watchdog=0 nosoftlockup quiet rcu_nocb_poll rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
</section>
<section xml:id="cpu-tuned-configuration">
<title>Fijación de CPU mediante Tuned y argumentos del kernel</title>
<para><literal>tuned</literal> es una herramienta de ajuste del sistema que
supervisa las condiciones del sistema para optimizar el rendimiento usando
varios perfiles predefinidos. Una característica clave es su capacidad para
aislar los núcleos de la CPU para cargas de trabajo específicas, como
aplicaciones en tiempo real. Esto evita que el sistema operativo utilice
esos núcleos y aumente potencialmente la latencia.</para>
<para>Para habilitar y configurar esta función, lo primero es crear un perfil para
los núcleos de CPU que se deseen aislar. En este ejemplo, dedicamos 60 de
los 64 núcleos (<literal>1-30,33-62</literal>) a la aplicación y los 4
núcleos restantes para tareas de mantenimiento. Tenga en cuenta que el
diseño de las CPU aisladas depende en gran medida de las aplicaciones en
tiempo real.</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>A continuación, debemos modificar la opción GRUB para aislar los núcleos de
CPU y otros parámetros importantes para el uso de las CPU. Es importante
personalizar las opciones siguientes con las especificaciones reales de su
hardware:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Parámetro</entry>
<entry align="left" valign="top">Valor</entry>
<entry align="left" valign="top">Descripción</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Aísla los núcleos 1-30 y 33-62. <literal>domain</literal> indica que estas
CPU forman parte del dominio de aislamiento. <literal>nohz</literal>
habilita el funcionamiento sin marcas en estas CPU aisladas cuando están
inactivas para reducir las interrupciones. <literal>managed_irq</literal>
aísla las CPU fijadas para que no sean objeto de IRQ. Esto contempla
<literal>irqaffinity=0-7</literal>, que ya dirige la mayoría de las IRQ a
los núcleos de mantenimiento.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Esta opción permite al kernel distribuir las interrupciones del temporizador
entre las CPU aisladas.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Si está habilitada, la interrupción periódica del temporizador del kernel
(la "marca") se detendrá en cualquier núcleo de CPU que esté inactivo. Esto
beneficia principalmente a las CPU de mantenimiento
(<literal>0,31,32,63</literal>). De este modo se ahorra energía y se reducen
los despertares innecesarios en esos núcleos de uso general.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>En el caso de los núcleos aislados, esto detiene la marca incluso si la CPU
está ejecutando una sola tarea activa. Es decir, hace que la CPU funcione en
modo totalmente sin marcas (o "dyntick"). El kernel solo proporcionará
interrupciones del temporizador cuando sean realmente necesarias.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Esta opción descarga el procesamiento de retrollamada de RCU de los núcleos
de CPU especificados.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocb_poll</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Cuando esta opción está definida, las CPU sin retrollamada de RCU realizarán
sondeos periódicos para comprobar si es necesario gestionar las
retrollamadas, en lugar de que se activen explícitamente por otras CPU. Esto
puede reducir la sobrecarga de interrupciones.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Esta opción permite al kernel ejecutar las interrupciones en los núcleos de
mantenimiento.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>Esto minimiza la latencia al salir del estado inactivo, pero a costa de
mantener la CPU funcionando a toda velocidad en el subproceso inactivo.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>Esta opción inhabilita únicamente el watchdog de NMI. Se puede omitir si se
ha definido <literal>nowatchdog</literal>.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Esta opción inhabilita el watchdog de bloqueo suave, que se implementa como
un temporizador que se ejecuta en el contexto de interrupción dura del
temporizador.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Los siguientes comandos modifican la configuración de GRUB y aplican los
cambios mencionados anteriormente para que estén presentes en el próximo
arranque:</para>
<para>Edite el archivo <literal>/etc/default/grub</literal> con los parámetros
anteriores. El archivo tendrá este aspecto:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Actualice la configuración de GRUB:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>Para comprobar que los parámetros se han aplicado después del rearranque,
puede utilizar el siguiente comando para comprobar la línea de comandos del
kernel:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<para>Puede usar otro guion para ajustar la configuración de la CPU, que
básicamente realiza los siguientes pasos:</para>
<itemizedlist>
<listitem>
<para>Definir el regulador de la CPU como <literal>performance</literal>.</para>
</listitem>
<listitem>
<para>Desactivar la migración del temporizador a las CPU aisladas.</para>
</listitem>
<listitem>
<para>Migrar los subprocesos kdaemon a las CPU de mantenimiento.</para>
</listitem>
<listitem>
<para>Definir la latencia de las CPU aisladas en el valor más bajo posible.</para>
</listitem>
<listitem>
<para>Retrasar las actualizaciones de vmstat a 300 segundos.</para>
</listitem>
</itemizedlist>
<para>El guion está disponible en el <link
xl:href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.3/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh">repositorio
de ejemplos de SUSE Edge for Telco</link>.</para>
</section>
<section xml:id="cni-configuration">
<title>Configuración de la CNI</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal> es el complemento de CNI predeterminado para SUSE
Edge for Telco. Para habilitar Cilium en el clúster RKE2 como complemento
predeterminado hay que configurar lo siguiente en el archivo
<literal>/etc/rancher/rke2/config.yaml</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>Esto también se puede especificar con argumentos de línea de comandos, es
decir, con <literal>--cni=cilium</literal> en la línea del servidor del
archivo <literal>/etc/systemd/system/rke2-server</literal>.</para>
<para>Para usar el operador de red <literal>SR-IOV</literal> descrito en la
siguiente sección (<xref linkend="option2-sriov-helm"/>), use
<literal>Multus</literal> con otro complemento de CNI, como
<literal>Cilium</literal> o <literal>Calico</literal>, como complemento
secundario.</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
<note>
<para>Para obtener más información sobre los complementos de CNI, visite <link
xl:href="https://docs.rke2.io/install/network_options">Network
Options</link> (Opciones de red).</para>
</note>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOV permite que un dispositivo, como un adaptador de red, divida el
acceso a sus recursos entre varias funciones de hardware
<literal>PCIe</literal>. Hay diferentes formas de desplegar
<literal>SR-IOV</literal>, y aquí mostramos dos:</para>
<itemizedlist>
<listitem>
<para>Opción 1: usar los complementos de dispositivo de CNI
<literal>SR-IOV</literal> y un mapa de configuración para configurarlo
correctamente.</para>
</listitem>
<listitem>
<para>Opción 2 (recomendada): usar el chart de Helm de <literal>SR-IOV</literal>
en Rancher Prime para facilitar este despliegue.</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">Opción 1: Instalación de complementos de dispositivo
de CNI SR-IOV y un mapa de configuración para configurarlo
correctamente</emphasis></para>
<itemizedlist>
<listitem>
<para>Prepare el mapa de configuración para el complemento de dispositivo</para>
</listitem>
</itemizedlist>
<para>Obtenga la información para rellenar el mapa de configuración desde el
comando <literal>lspci</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>El mapa de configuración es un archivo <literal>JSON</literal> que describe
los dispositivos utilizando filtros para detectarlos, y crea grupos para las
interfaces. Lo importante es comprender qué son los filtros y los
grupos. Los filtros se usan para detectar los dispositivos y los grupos,
para crear las interfaces.</para>
<para>Es posible establecer filtros como:</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (tarjeta Accelerator)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (controlador)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (nombre de interfaz física)</para>
</listitem>
</itemizedlist>
<para>También es posible definir filtros con una sintaxis de interfaz más
compleja, por ejemplo:</para>
<itemizedlist>
<listitem>
<para>pfNames: <literal>["eth1#1,2,3,4,5,6"]</literal> o
<literal>[eth1#1-6]</literal> (nombre de interfaz física)</para>
</listitem>
</itemizedlist>
<para>Para los grupos, es posible crear un grupo para la tarjeta
<literal>FEC</literal> y otro para la tarjeta <literal>Intel</literal>, o
incluso crear un prefijo en función de nuestro caso de uso:</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>Hay muchas combinaciones posibles para descubrir y crear el grupo de
recursos necesario para asignar <literal>funciones virtuales</literal> a los
pods.</para>
<note>
<para>Para obtener más información sobre los filtros y los grupos, consulte <link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sr-iov
network device plug-in</link> (Complemento de dispositivo de red sr-iov).</para>
</note>
<para>Después de configurar los filtros y los grupos que coincidan con las
interfaces del hardware y el caso de uso, el mapa de configuración siguiente
muestra un ejemplo útil:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para>Prepare el archivo <literal>daemonset</literal> para desplegar el
complemento de dispositivo.</para>
</listitem>
</itemizedlist>
<para>El complemento de dispositivo admite varias arquitecturas
(<literal>arm</literal>, <literal>amd</literal> o
<literal>ppc64le</literal>), por lo que se puede usar el mismo archivo para
diferentes arquitecturas, desplegando un <literal>daemonset</literal> para
cada arquitectura.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>Después de aplicar el mapa de configuración y el
<literal>daemonset</literal>, se desplegará el complemento de dispositivo y
se detectarán las interfaces, que estarán disponibles para los pods.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Compruebe qué interfaces se han detectado y están disponibles en los nodos
para los pods:</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para>La tarjeta <literal>FEC</literal> es
<literal>intel.com/intel_fec_5g</literal> y su valor es 1.</para>
</listitem>
<listitem>
<para>La función virtual <literal>VF</literal> es
<literal>intel.com/intel_sriov_odu</literal> o
<literal>intel.com/intel_sriov_oru</literal> si la despliega con un
complemento de dispositivo y el mapa de configuración sin charts de Helm.</para>
</listitem>
</itemizedlist>
<important>
<para>Si no hay interfaces, no tiene mucho sentido continuar, ya que la interfaz
no estará disponible para los pods. Revise primero el mapa de configuración
y los filtros para resolver el problema.</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">Opción 2 (recomendada): instalación con Rancher
mediante el chart de Helm para la CNI SR-IOV y los complementos de
dispositivo</emphasis></para>
<itemizedlist>
<listitem>
<para>Consiga Helm si no está presente:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>Instale SR-IOV.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">helm install sriov-crd oci://registry.suse.com/edge/charts/sriov-crd -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/charts/sriov-network-operator -n sriov-network-operator</screen>
<itemizedlist>
<listitem>
<para>Compruebe los crd y los pods de los recursos desplegados:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>Compruebe la etiqueta de los nodos.</para>
</listitem>
</itemizedlist>
<para>Con todos los recursos en ejecución, la etiqueta aparece automáticamente en
su nodo:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para>Revise el <literal>daemonset</literal> para comprobar si las entradas
<literal>sriov-network-config-daemon</literal> y
<literal>sriov-rancher-nfd-worker</literal> nuevas están activas y listas:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>En unos minutos (la actualización puede tardar hasta 10 minutos), los nodos
se detectan y se configuran capacidad de <literal>SR-IOV</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>Compruebe las interfaces detectadas.</para>
</listitem>
</itemizedlist>
<para>Las interfaces detectadas deben ser la dirección PCI del dispositivo de
red. Compruebe esta información con el comando <literal>lspci</literal> en
el host.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>Si su interfaz no se detecta, asegúrese de que esté presente en el siguiente
mapa de configuración:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>Si su dispositivo no aparece, edite el mapa de configuración y añada los
valores correctos que se deban detectar (puede que sea necesario reiniciar
el daemonset <literal>sriov-network-config-daemon</literal>).</para>
</note>
<itemizedlist>
<listitem>
<para>Cree la directiva <literal>NetworkNodePolicy</literal> para configurar las
<literal>funciones virtuales</literal>.</para>
</listitem>
</itemizedlist>
<para>Se crearán algunas <literal>funciones virtuales</literal>
(<literal>numVfs</literal>) desde el dispositivo
(<literal>rootDevices</literal>) y se configurarán con el controlador
<literal>deviceType</literal> y el <literal>MTU</literal>:</para>
<note>
<para>El campo <literal>resourceName</literal> no debe contener caracteres
especiales y debe ser único en todo el clúster. En el ejemplo se usa
<literal>deviceType: vfio-pci</literal> porque <literal>dpdk</literal> se
usará junto a <literal>sr-iov</literal>. Si no utiliza
<literal>dpdk</literal>, el valor de deviceType debe ser
<literal>deviceType: netdevice</literal> (valor predeterminado).</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>Valide las configuraciones:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>Cree la red sr-iov (opcional, si se necesita una red diferente):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>Compruebe la red creada:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal> (Data Plane Development Kit, kit de desarrollo del
plano de datos) es un conjunto de bibliotecas y controladores para el
procesamiento rápido de paquetes. Se utiliza para acelerar las cargas de
trabajo de procesamiento de paquetes que se ejecutan en una amplia variedad
de arquitecturas de CPU. DPDK incluye bibliotecas de plano de datos y
controladores de controlador de interfaz de red (<literal>NIC</literal>)
optimizados para lo siguiente:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Un gestor de colas implementa colas sin bloqueo.</para>
</listitem>
<listitem>
<para>Un gestor de búferes preasigna búferes de tamaño fijo.</para>
</listitem>
<listitem>
<para>Un gestor de memoria asigna pools de objetos en la memoria y usa un anillo
para almacenar los objetos libres; esto garantiza que los objetos se
distribuyan de forma equitativa en todos los canales
<literal>DRAM</literal>.</para>
</listitem>
<listitem>
<para>Los controladores en modo de sondeo (Poll mode drivers,
<literal>PMD</literal>) están diseñados para funcionar sin notificaciones
asíncronas, lo que reduce la sobrecarga.</para>
</listitem>
<listitem>
<para>Un marco de paquetes como un conjunto de bibliotecas que sirven de ayuda
para desarrollar el procesamiento de paquetes.</para>
</listitem>
</orderedlist>
<para>Los siguientes pasos muestran cómo habilitar <literal>DPDK</literal> y cómo
crear <literal>funciones virtuales</literal> a partir de las
<literal>NIC</literal> que se utilizarán en las interfaces de
<literal>DPDK</literal>:</para>
<itemizedlist>
<listitem>
<para>Instale el paquete de <literal>DPDK</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Parámetros del kernel:</para>
</listitem>
</itemizedlist>
<para>Para usar DPDK, emplee algunos controladores para habilitar ciertos
parámetros en el kernel:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Parámetro</entry>
<entry align="left" valign="top">Valor</entry>
<entry align="left" valign="top">Descripción</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Esta opción permite el uso del controlador <literal>vfio</literal> para las
interfaces de DPDK.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu o amd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Esta opción permite el uso de <literal>vfio</literal> para las
<literal>funciones virtuales</literal>.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Para habilitar los parámetros, añádalos al archivo
<literal>/etc/default/grub</literal>:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Actualice la configuración de GRUB y rearranque el sistema para aplicar los
cambios:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Cargue el módulo del kernel <literal>vfio-pci</literal> y habilite
<literal>SR-IOV</literal> en las tarjetas <literal>NIC</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Cree algunas funciones virtuales (<literal>VF</literal>) a partir de las
tarjetas <literal>NIC</literal>.</para>
</listitem>
</itemizedlist>
<para>Para crear <literal>funciones virtuales</literal>, por ejemplo, para dos
<literal>NIC</literal> distintas, se requieren los siguientes comandos:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>Vincule las nuevas funciones virtuales con el controlador
<literal>vfio-pci</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>Compruebe que la configuración se haya aplicado correctamente:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>Aceleración vRAN (<literal>Intel ACC100/ACC200</literal>)</title>
<para>A medida que los proveedores de servicios de comunicaciones pasan de las
redes 4G a las 5G, muchos están adoptando arquitecturas de redes de acceso
por radio virtualizadas (<literal>vRAN</literal>) para obtener una mayor
capacidad de canal y facilitar el despliegue de servicios y aplicaciones
basados en la periferia. Las soluciones vRAN son perfectas para ofrecer
servicios de baja latencia con la flexibilidad de aumentar o disminuir la
capacidad en función del volumen de tráfico en tiempo real y la demanda de
la red.</para>
<para>Una de las cargas de trabajo de 4G y 5G que más recursos informáticos
consume es la <literal>FEC</literal> de la capa 1 (<literal>L1</literal>) de
la RAN, que resuelve los errores de transmisión de datos en canales de
comunicación poco fiables o con ruido. La tecnología <literal>FEC</literal>
detecta y corrige un número limitado de errores en los datos 4G o 5G, lo que
elimina la necesidad de retransmisión. Dado que la transacción de
aceleración de <literal>FEC</literal> no contiene información sobre el
estado de las celdas, se puede virtualizar fácilmente, de forma que se puede
aprovechar la agrupación y facilitar la migración de celdas.</para>
<itemizedlist>
<listitem>
<para>Parámetros del kernel</para>
</listitem>
</itemizedlist>
<para>Para habilitar la aceleración <literal>vRAN</literal>, es preciso habilitar
los siguientes parámetros del kernel (si aún no están presentes):</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Parámetro</entry>
<entry align="left" valign="top">Valor</entry>
<entry align="left" valign="top">Descripción</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Esta opción permite el uso de vfio para las interfaces DPDK.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu o amd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Esta opción permite el uso de vfio para las funciones virtuales.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modifique el archivo GRUB <literal>/etc/default/grub</literal> para
añadirlos a la línea de comandos del kernel:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>Actualice la configuración de GRUB y rearranque el sistema para aplicar los
cambios:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>Para verificar que los parámetros se han aplicado después del rearranque,
compruebe la línea de comandos:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Cargue los módulos del kernel vfio-pci para habilitar la aceleración
<literal>vRAN</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Obtenga información de la interfaz Acc100:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>Vincule la interfaz física (<literal>PF</literal>) con el controlador
<literal>vfio-pci</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>Cree las funciones virtuales (<literal>VF</literal>) a partir de la interfaz
física (<literal>PF</literal>).</para>
</listitem>
</itemizedlist>
<para>Cree dos <literal>VF</literal> desde la <literal>PF</literal> y vincúlelas
con <literal>vfio-pci</literal> así:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>Configure acc100 con el archivo de configuración propuesto:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>Compruebe las nuevas funciones virtuales creadas desde la interfaz física
FEC:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>Páginas enormes</title>
<para>Cuando un proceso utiliza <literal>RAM</literal>, la <literal>CPU</literal>
lo marca como utilizado por ese proceso. Por motivos de eficiencia, la
<literal>CPU</literal> asigna <literal>RAM</literal> en bloques de
<literal>4K</literal> bytes, que es el valor predeterminado en muchas
plataformas. Esos bloques se denominan páginas. Las páginas se pueden
intercambiar en el disco, etc.</para>
<para>Dado que el espacio de direcciones del proceso es virtual, la
<literal>CPU</literal> y el sistema operativo deben recordar qué páginas
pertenecen a cada proceso y dónde se almacena cada página. Cuanto más
páginas haya, más tiempo llevará la búsqueda de asignación de
memoria. Cuando un proceso usa <literal>1 GB</literal> de memoria, eso
supone 262 144 entradas que buscar (<literal>1 GB</literal> / <literal>4
K</literal>). Si una entrada de la tabla de páginas consume 8 bytes, eso
supone <literal>2 MB</literal> (262 144 * 8) que buscar.</para>
<para>La mayoría de las arquitecturas actuales de <literal>CPU</literal> admiten
páginas más grandes que las predeterminadas, lo que reduce el número de
entradas que la <literal>CPU/SO</literal> debe buscar.</para>
<itemizedlist>
<listitem>
<para>Parámetros del kernel</para>
</listitem>
</itemizedlist>
<para>Para habilitar estas páginas enormes (huge pages), debemos añadir los
siguientes parámetros del kernel. En este ejemplo, configuramos 40 páginas
de 1 GB, aunque el tamaño de la página enorme y el número exacto de ellas
deben adaptarse a los requisitos de memoria de su aplicación:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Parámetro</entry>
<entry align="left" valign="top">Valor</entry>
<entry align="left" valign="top">Descripción</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Esta opción permite establecer el tamaño de las páginas enormes en 1 G</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>Este es el número de páginas enormes definidas anteriormente</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Este es el valor predeterminado para obtener las páginas enormes</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modifique el archivo GRUB <literal>/etc/default/grub</literal> para añadir
estos parámetros en <literal>GRUB_CMDLINE_LINUX</literal>:</para>
<screen language="shell" linenumbering="unnumbered">default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0</screen>
<para>Actualice la configuración de GRUB y rearranque el sistema para aplicar los
cambios:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>Para comprobar que los parámetros se han aplicado después del rearranque,
puede consultar la línea de comandos:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Uso de páginas enormes</para>
</listitem>
</itemizedlist>
<para>Para usar las páginas enormes, debe montarlas:</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Despliegue una carga de trabajo de Kubernetes creando los recursos y los
volúmenes:</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-kubernetes">
<title>Fijación de CPU en Kubernetes</title>
<section xml:id="id-prerequisite">
<title>Requisito previo</title>
<para>Debe tener la <literal>CPU</literal> ajustada al perfil de rendimiento
descrito en esta sección (<xref linkend="cpu-tuned-configuration"/>).</para>
</section>
<section xml:id="id-configure-kubernetes-for-cpu-pinning">
<title>Configuración de Kubernetes para la fijación de CPU</title>
<para>Configure los argumentos de kubelet para implementar la gestión de la CPU en
el clúster <literal>RKE2</literal>. Añada el siguiente bloque de
configuración, como se muestra en el ejemplo siguiente, al archivo
<literal>/etc/rancher/rke2/config.yaml</literal>. Asegúrese de especificar
los núcleos de CPU de mantenimiento en los argumentos
<literal>kubelet-reserved</literal> y <literal>system-reserved</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=0,31,32,63"
- "system-reserved=cpu=0,31,32,63"</screen>
</section>
<section xml:id="id-leveraging-pinned-cpus-for-workloads">
<title>Aprovechamiento de CPU fijadas para cargas de trabajo</title>
<para>Hay tres formas de utilizar esa función utilizando la <literal>directiva
estática</literal> definida en kubelet, dependiendo de las solicitudes y los
límites que defina en su carga de trabajo:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Clase de calidad de servicio <literal>BestEffort</literal>: si no se define
ninguna solicitud o límite para la <literal>CPU</literal>, el pod se
programa en la primera <literal>CPU</literal> disponible en el sistema.</para>
<para>Este es un ejemplo de uso de la clase de calidad de servicio
<literal>BestEffort</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para>Clase de calidad de servicio <literal>Burstable</literal>: si se define una
solicitud para la CPU que no se ajusta a los límites establecidos, o si no
hay ninguna solicitud de CPU.</para>
<para>Ejemplos de uso de la clase de calidad de servicio
<literal>Burstable</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>o</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para>Clase de calidad de servicio <literal>Guaranteed</literal>: si define una
solicitud de CPU que es igual a los límites.</para>
<para>Ejemplo de uso de la clase de calidad de servicio
<literal>Guaranteed</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="numa-aware-scheduling">
<title>Programación compatible con NUMA</title>
<para>El acceso a la memoria no uniforme o arquitectura de memoria no uniforme
(<literal>NUMA</literal>) es un diseño de memoria física utilizado en la
arquitectura <literal>SMP</literal> (multiprocesadores) en la que el tiempo
de acceso a la memoria depende de la ubicación de la memoria con respecto al
procesador. En <literal>NUMA</literal>, un procesador puede acceder a su
propia memoria local más rápido que a la memoria no local; es decir, la
memoria local de otro procesador o la memoria compartida entre procesadores.</para>
<section xml:id="id-identifying-numa-nodes">
<title>Identificación de nodos NUMA</title>
<para>Para identificar los nodos <literal>NUMA</literal>, utilice el siguiente
comando en su sistema:</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>En este ejemplo, solo tenemos un nodo <literal>NUMA</literal> que muestra 64
<literal>CPU</literal>.</para>
<para><literal>NUMA</literal> debe estar habilitado en el
<literal>BIOS</literal>. Si <literal>dmesg</literal> no tiene registros de
la inicialización de NUMA durante el arranque, es posible que los mensajes
relacionados con <literal>NUMA</literal> del búfer del anillo del kernel se
hayan sobrescrito.</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>MetalLB</title>
<para><literal>MetalLB</literal> es una implementación de equilibrador de carga
para clústeres bare metal de Kubernetes que utiliza protocolos de
enrutamiento estándar como <literal>L2</literal> y <literal>BGP</literal>
como protocolos de anuncios. Se trata de un equilibrador de carga de red que
se puede utilizar para exponer servicios de un clúster de Kubernetes al
mundo exterior que necesite utilizar el tipo de servicios de Kubernetes
<literal>LoadBalancer</literal> con bare metal.</para>
<para>Para habilitar <literal>MetalLB</literal> en el clúster
<literal>RKE2</literal>, se deben seguir estos pasos:</para>
<itemizedlist>
<listitem>
<para>Instale <literal>MetalLB</literal> con este comando:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/metallb
  targetNamespace: metallb-system
  version: 303.0.0+up0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
  version: 303.0.0+up0.2.1
  createNamespace: true
EOF</screen>
<itemizedlist>
<listitem>
<para>Cree la configuración <literal>IpAddressPool</literal> y
<literal>L2advertisement</literal>:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para>Cree el servicio de punto final para exponer la <literal>IP
virtual</literal>:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para>Compruebe que se ha creado la <literal>IP virtual</literal> y que los pods
de <literal>MetalLB</literal> se están ejecutando:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>Configuración del registro privado</title>
<para>Es posible configurar <literal>Containerd</literal> para que se conecte a
registros privados y los use para extraer imágenes privadas en cada nodo.</para>
<para>Durante el inicio, <literal>RKE2</literal> comprueba si existe un archivo
<literal>registries.yaml</literal> en <literal>/etc/rancher/rke2/</literal>
y ordena a <literal>containerd</literal> que use cualquier registro definido
en el archivo. Si desea utilizar un registro privado, cree este archivo como
usuario root en cada nodo que vaya a utilizar el registro.</para>
<para>Para añadir el registro privado, cree el archivo
<literal>/etc/rancher/rke2/registries.yaml</literal> con el siguiente
contenido:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>o sin autenticación:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>Para que los cambios en el registro surtan efecto, debe configurar este
archivo antes de iniciar RKE2 en el nodo, o bien reiniciar RKE2 en cada nodo
configurado.</para>
<note>
<para>Para obtener más información, consulte <link
xl:href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html#_registries_configuration_file">containerd
registry configuration rke2</link> (Configuración del registro containerd en
RKE2).</para>
</note>
</section>
<section xml:id="ptp-configuration">
<title>Protocolo de tiempo de precisión (PTP)</title>
<para>El protocolo de tiempo de precisión (PTP) es un protocolo de red
desarrollado por el IEEE (Instituto de Ingenieros Eléctricos y Electrónicos)
para permitir la sincronización del tiempo en submicrosegundos en una red
informática. Desde su creación y durante las últimas dos décadas, el PTP se
ha utilizado en muchos sectores. Recientemente, se ha observado un aumento
de su adopción en las redes de telecomunicaciones como elemento fundamental
para las redes 5G. Aunque se trata de un protocolo relativamente sencillo,
su configuración puede variar significativamente en función de la
aplicación. Por ese motivo, se han definido y estandarizado múltiples
perfiles.</para>
<para>En esta sección solo se tratan los perfiles específicos de
telecomunicaciones. Por lo tanto, se dará por sentado que la NIC cuenta con
capacidad de marcación de tiempo y con un reloj de hardware PTP (PHC). Hoy
en día, todos los adaptadores de red para telecomunicaciones incluyen
compatibilidad con PTP en el hardware, pero puede verificarlo con el
siguiente comando:</para>
<screen language="console" linenumbering="unnumbered"># ethtool -T p1p1
Time stamping parameters for p1p1:
Capabilities:
        hardware-transmit
        software-transmit
        hardware-receive
        software-receive
        software-system-clock
        hardware-raw-clock
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
        off
        on
Hardware Receive Filter Modes:
        none
        all</screen>
<para>Sustituya <literal>p1p1</literal> por el nombre de la interfaz que se
utilizará para PTP.</para>
<para>Las siguientes secciones explican cómo instalar y configurar PTP en SUSE
Edge específicamente, pero se espera que el usuario esté familiarizado con
los conceptos básicos de PTP. Para obtener una breve descripción general de
PTP y la implementación incluida en SUSE Edge for Telco, consulte <link
xl:href="https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html">https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html</link>.</para>
<section xml:id="id-install-ptp-software-components">
<title>Instalación de los componentes de software de PTP</title>
<para>En SUSE Edge for Telco, la implementación de PTP la proporciona el paquete
<literal>linuxptp</literal>, que incluye dos componentes:</para>
<itemizedlist>
<listitem>
<para><literal>ptp4l</literal>: un daemon que controla el PHC en la NIC y ejecuta
el protocolo PTP</para>
</listitem>
<listitem>
<para><literal>phc2sys</literal>: un daemon que mantiene el reloj del sistema
sincronizado con el PHC sincronizado por PTP en la NIC</para>
</listitem>
</itemizedlist>
<para>Ambos daemons son necesarios para que la sincronización del sistema funcione
correctamente y deben estar configurados adecuadamente según su
configuración. Esto se explica en la <xref linkend="ptp-telco-config"/>.</para>
<para>La mejor manera, y la más fácil, de integrar PTP en su clúster descendente
es añadir el paquete <literal>linuxptp</literal> en
<literal>packageList</literal> en el archivo de definición de Edge Image
Builder (EIB). De esta forma, el software del plano de control de PTP se
instalará automáticamente durante el aprovisionamiento del clúster. Consulte
la documentación de EIB (<xref linkend="eib-configuring-rpm-packages"/>)
para obtener más información sobre la instalación de paquetes.</para>
<para>Esto es un ejemplo de manifiesto de EIB con <literal>linuxptp</literal>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: {micro-base-rt-image-raw}
  outputImageName: eibimage-slmicrort-telco.raw
operatingSystem:
  time:
    timezone: America/New_York
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
    enable:
      - ptp4l
      - phc2sys
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
      - open-iscsi
      - tuned
      - cpupower
      - linuxptp
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<note>
<para>El paquete <literal>linuxptp</literal> incluido en SUSE Edge for Telco no
habilita <literal>ptp4l</literal> ni <literal>phc2sys</literal> de forma
predeterminada. Si sus archivos de configuración específicos del sistema se
despliegan en el momento del aprovisionamiento (consulte la <xref
linkend="ptp-capi"/>), deben habilitarse. Para ello, añádalos a la sección
<literal>systemd</literal> del manifiesto, como en el ejemplo anterior.</para>
</note>
<para>Siga el proceso habitual para crear la imagen que se describe en la
documentación de EIB (<xref linkend="eib-how-to-build-image"/>) y utilícela
para desplegar su clúster. Si no tiene experiencia con EIB, comience por el
<xref linkend="components-eib"/>.</para>
</section>
<section xml:id="ptp-telco-config">
<title>Configuración de PTP para despliegues de telecomunicaciones</title>
<para>Muchas aplicaciones de telecomunicaciones requieren una sincronización
estricta de fase y tiempo con poca desviación. Por eso, se definieron dos
perfiles orientados a las telecomunicaciones: el ITU-T G.8275.1 y el ITU-T
G.8275.2. Ambos tienen una alta tasa de mensajes de sincronización y otras
características distintivas, como el uso de un algoritmo de reloj maestro
óptimo (BMCA) alternativo. Este comportamiento exige ajustes específicos en
el archivo de configuración utilizado por <literal>ptp4l</literal>, que se
proporcionan en las siguientes secciones como referencia.</para>
<note>
<itemizedlist>
<listitem>
<para>Ambas secciones solo cubren el caso de un reloj normal con una configuración
de recepción de hora.</para>
</listitem>
<listitem>
<para>Los perfiles de este tipo deben utilizarse en una infraestructura PTP bien
planificada.</para>
</listitem>
<listitem>
<para>Es posible que su red PTP específica requiera ajustes de configuración
adicionales. Asegúrese de revisar y adaptar los ejemplos proporcionados si
es necesario.</para>
</listitem>
</itemizedlist>
</note>
<section xml:id="id-ptp-profile-itu-t-g-8275-1">
<title>Perfil ITU-T G.8275.1 de PTP</title>
<para>El perfil G.8275.1 tiene las siguientes especificaciones:</para>
<itemizedlist>
<listitem>
<para>Se ejecuta directamente en Ethernet y requiere soporte de red completo (los
nodos/conmutadores adyacentes deben ser compatibles con PTP).</para>
</listitem>
<listitem>
<para>La configuración predeterminada del dominio es 24.</para>
</listitem>
<listitem>
<para>La comparación de conjuntos de datos se basa en el algoritmo G.8275.x y sus
valores <literal>localPriority</literal> después de
<literal>priority2</literal>.</para>
</listitem>
</itemizedlist>
<para>Copie el siguiente contenido en un archivo llamado
<literal>/etc/ptp4l-G.8275.1.conf</literal>:</para>
<screen linenumbering="unnumbered"># Telecom G.8275.1 example configuration
[global]
domainNumber                    24
priority2			255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             -3
logSyncInterval                 -4
logMinDelayReqInterval          -4
announceReceiptTimeout		3
serverOnly                      0
ptp_dst_mac                     01:80:C2:00:00:0E
network_transport               L2</screen>
<para>Una vez creado el archivo, debe hacerse referencia a él en
<literal>/etc/sysconfig/ptp4l</literal> para que el daemon se inicie
correctamente. Puede hacerlo cambiando la línea <literal>OPTIONS=</literal>
por:</para>
<screen linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"</screen>
<para>En concreto:</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> requiere el nombre del archivo de configuración que se
va a utilizar; en este caso, <literal>/etc/ptp4l-G.8275.1.conf</literal>.</para>
</listitem>
<listitem>
<para><literal>-i</literal> requiere el nombre de la interfaz que se va a
utilizar. Sustituya <literal>$IFNAME</literal> por el nombre real de la
interfaz.</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> permite identificar mejor la salida de
ptp4l en los registros del sistema y es opcional.</para>
</listitem>
</itemizedlist>
<para>Una vez completados los pasos anteriores, se debe (re)iniciar el daemon
<literal>ptp4l</literal>:</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>Compruebe el estado de sincronización observando los registros con:</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-ptp-profile-itu-t-g-8275-2">
<title>Perfil ITU-T G.8275.2 de PTP</title>
<para>El perfil G.8275.2 tiene las siguientes especificaciones:</para>
<itemizedlist>
<listitem>
<para>Funciona con IP y no requiere soporte de red completo (los
nodos/conmutadores adyacentes pueden no ser compatibles con PTP).</para>
</listitem>
<listitem>
<para>La configuración predeterminada del dominio es 44.</para>
</listitem>
<listitem>
<para>La comparación de conjuntos de datos se basa en el algoritmo G.8275.x y sus
valores <literal>localPriority</literal> después de
<literal>priority2</literal>.</para>
</listitem>
</itemizedlist>
<para>Copie el siguiente contenido en un archivo llamado
<literal>/etc/ptp4l-G.8275.2.conf</literal>:</para>
<screen linenumbering="unnumbered"># Telecom G.8275.2 example configuration
[global]
domainNumber                    44
priority2			255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             0
serverOnly                      0
hybrid_e2e                      1
inhibit_multicast_service       1
unicast_listen                  1
unicast_req_duration            60
logSyncInterval                 -5
logMinDelayReqInterval          -4
announceReceiptTimeout		2
#
# Customize the following for slave operation:
#
[unicast_master_table]
table_id                        1
logQueryInterval                2
UDPv4                           $PEER_IP_ADDRESS
[$IFNAME]
unicast_master_table            1</screen>
<para>Asegúrese de sustituir los siguientes marcadores de posición:</para>
<itemizedlist>
<listitem>
<para><literal>$PEER_IP_ADDRESS</literal>: la dirección IP del siguiente nodo PTP
con el que se va a comunicar, como el reloj maestro o el reloj fronterizo
que proporcionará la sincronización.</para>
</listitem>
<listitem>
<para><literal>$IFNAME</literal>: indica a <literal>ptp4l</literal> qué interfaz
debe utilizar para PTP.</para>
</listitem>
</itemizedlist>
<para>Una vez creado el archivo, debe referenciarse en
<literal>/etc/sysconfig/ptp4l</literal> junto con el nombre de la interfaz
que se utilizará para PTP para que el daemon se inicie correctamente. Puede
hacerlo cambiando la línea <literal>OPTIONS=</literal> por:</para>
<screen language="shell" linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.2.conf --message_tag ptp-8275.2"</screen>
<para>En concreto:</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> requiere el nombre del archivo de configuración que se
va a utilizar. En este caso, es <literal>/etc/ptp4l-G.8275.2.conf</literal>.</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> permite identificar mejor la salida de
ptp4l en los registros del sistema y es opcional.</para>
</listitem>
</itemizedlist>
<para>Una vez completados los pasos anteriores, se debe (re)iniciar el daemon
<literal>ptp4l</literal>:</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>Compruebe el estado de sincronización observando los registros con:</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-configuration-of-phc2sys">
<title>Configuración de phc2sys</title>
<para>Aunque no es obligatorio, se recomienda completar totalmente la
configuración de <literal>ptp4l</literal> antes de pasar a
<literal>phc2sys</literal>. <literal>phc2sys</literal> no requiere un
archivo de configuración y sus parámetros de ejecución se pueden controlar
únicamente a través de la variable <literal>OPTIONS=</literal> presente en
<literal>/etc/sysconfig/ptp4l</literal>, de forma similar a
<literal>ptp4l</literal>:</para>
<screen linenumbering="unnumbered">OPTIONS="-s $IFNAME -w"</screen>
<para>Donde <literal>$IFNAME</literal> es el nombre de la interfaz ya configurada
en ptp4l que se utilizará como fuente para el reloj del sistema. Esto se
utiliza para identificar la fuente de PHC.</para>
</section>
</section>
<section xml:id="ptp-capi">
<title>Integración de Cluster API</title>
<para>Cada vez que se despliega un clúster mediante un clúster de gestión y un
aprovisionamiento de red dirigido, tanto el archivo de configuración como
las dos variables de configuración de <literal>/etc/sysconfig</literal> se
pueden desplegar en el host en el momento del aprovisionamiento. A
continuación, se muestra un extracto de una definición de clúster, centrado
en un objeto <literal>RKE2ControlPlane</literal> modificado que despliega el
mismo archivo de configuración G.8275.1 en todos los hosts:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  serverConfig:
    cni: canal
  agentConfig:
    format: ignition
    cisProfile: cis
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /etc/ptp4l-G.8275.1.conf
              overwrite: true
              contents:
                inline: |
                  # Telecom G.8275.1 example configuration
                  [global]
                  domainNumber                    24
                  priority2                       255
                  dataset_comparison              G.8275.x
                  G.8275.portDS.localPriority     128
                  G.8275.defaultDS.localPriority  128
                  maxStepsRemoved                 255
                  logAnnounceInterval             -3
                  logSyncInterval                 -4
                  logMinDelayReqInterval          -4
                  announceReceiptTimeout          3
                  serverOnly                      0
                  ptp_dst_mac                     01:80:C2:00:00:0E
                  network_transport               L2
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/ptp4l
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): ptp4l settings
                  ## Type:           string
                  ## Default:        "-i eth0 -f /etc/ptp4l.conf"
                  ## ServiceRestart: ptp4l
                  #
                  # Arguments when starting ptp4l(8).
                  #
                  OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/phc2sys
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): phc2sys settings
                  ## Type:           string
                  ## Default:        "-s eth0 -w"
                  ## ServiceRestart: phc2sys
                  #
                  # Arguments when starting phc2sys(8).
                  #
                  OPTIONS="-s $IFNAME -w"
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Además de otras variables, la definición anterior debe completarse con el
nombre de la interfaz y con los demás objetos de Cluster API, como se
describe en el <xref linkend="atip-automated-provisioning"/>.</para>
<note>
<itemizedlist>
<listitem>
<para>Este enfoque solo resulta útil si el hardware del clúster es uniforme y se
necesita la misma configuración en todos los hosts, incluido el nombre de la
interfaz.</para>
</listitem>
<listitem>
<para>Existen enfoques alternativos que se tratarán en futuras versiones.</para>
</listitem>
</itemizedlist>
</note>
<para>En este punto, los hosts deberían tener una pila PTP operativa y en
ejecución, y comenzarán a negociar su función PTP.</para>
</section>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>Aprovisionamiento de red dirigida totalmente automatizado</title>
<section xml:id="id-introduction-3">
<title>Introducción</title>
<para>El aprovisionamiento de red dirigida es una función que permite automatizar
el aprovisionamiento de clústeres descendentes. Esta función resulta útil
cuando hay muchos clústeres descendentes que aprovisionar y se desea
automatizar el proceso.</para>
<para>Un clúster de gestión (<xref linkend="atip-management-cluster"/>) automatiza
el despliegue de los siguientes componentes:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro RT</literal> como sistema operativo. Dependiendo
del caso de uso, se pueden personalizar ajustes como las redes, el
almacenamiento, los usuarios y los argumentos del kernel.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> como clúster de Kubernetes. El complemento de
<literal>CNI</literal> predeterminado es
<literal>Cilium</literal>. Dependiendo del caso práctico, se pueden usar
determinados complementos de <literal>CNI</literal>, como
<literal>Cilium+Multus</literal>.</para>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal></para>
</listitem>
<listitem>
<para><literal>SUSE Security</literal></para>
</listitem>
<listitem>
<para><literal>MetalLB</literal> se puede usar como equilibrador de carga para
clústeres de varios nodos de alta disponibilidad.</para>
</listitem>
</itemizedlist>
<note>
<para>Para obtener más información sobre <literal>SUSE Linux Micro</literal>,
consulte el <xref linkend="components-slmicro"/>. Para obtener más
información sobre <literal>RKE2</literal>, consulte el <xref
linkend="components-rke2"/>. Para obtener más información sobre
<literal>SUSE Storage</literal>, consulte el <xref
linkend="components-suse-storage"/>. Para obtener más información sobre
<literal>SUSE Security</literal>, consulte el <xref
linkend="components-suse-security"/>.</para>
</note>
<para>Las siguientes secciones describen los diferentes flujos de trabajo de
aprovisionamiento de red dirigida y algunas características adicionales que
se pueden añadir al proceso de aprovisionamiento:</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
<note>
<para>Las siguientes secciones muestran cómo preparar los diferentes escenarios
para el flujo de trabajo de aprovisionamiento de red dirigida utilizando
SUSE Edge for Telco. Para ver ejemplos de las diferentes opciones de
configuración para el despliegue (incluidos entornos aislados, redes con
DHCP y sin DHCP, registros de contenedores privados, etc.), consulte el
<link
xl:href="https://github.com/suse-edge/atip/tree/release-3.3/telco-examples/edge-clusters">repositorio
de SUSE Edge for Telco</link>.</para>
</note>
</section>
<section xml:id="eib-edge-image-connected">
<title>Preparación de la imagen del clúster descendente para entornos conectados</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) se utiliza para
preparar una imagen base de SLEMicro modificada que se aprovisiona en los
hosts del clúster descendente.</para>
<para>Gran parte de la configuración se puede realizar con Edge Image Builder,
pero en esta guía cubrimos las configuraciones mínimas necesarias para
configurar el clúster descendente.</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>Requisitos previos para entornos conectados</title>
<itemizedlist>
<listitem>
<para>Se requiere un entorno de ejecución de contenedores como <link
xl:href="https://podman.io">Podman</link> o <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link> para ejecutar
Edge Image Builder.</para>
</listitem>
<listitem>
<para>La imagen base se creará con las instrucciones del <xref
linkend="guides-kiwi-builder-images"/> y el perfil
<literal>Base-SelfInstall</literal> (o
<literal>Base-RT-SelfInstall</literal> para el kernel en tiempo real). El
proceso es el mismo para ambas arquitecturas (x86-64 y aarch64).</para>
</listitem>
<listitem>
<para>Para desplegar clústeres descendentes aarch64, antes del despliegue del
clúster de gestión debe definir el parámetro <literal>deployArchitecture:
arm64</literal> en el archivo <literal>metal3.yaml</literal>, lo que se
explica en la documentación del clúster de gestión (<xref
linkend="arm64-mgmt-cluster"/>). Esto es necesario para garantizar que se
utiliza la arquitectura correcta para el clúster descendente.</para>
</listitem>
</itemizedlist>
<note>
<para>Es necesario utilizar un host de creación con la misma arquitectura que las
imágenes que se están creando. En otras palabras, para crear una imagen
<literal>aarch64</literal>, es necesario utilizar un host de creación
<literal>aarch64</literal>, y lo mismo ocurre para <literal>x86-64</literal>
(actualmente, no se admiten creaciones con arquitecturas cruzadas).</para>
</note>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>Configuración de la imagen para entornos conectados</title>
<para>Al ejecutar Edge Image Builder, se monta un directorio desde el host, por lo
que es necesario crear una estructura de directorios para almacenar los
archivos de configuración usados para definir la imagen de destino.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> es el archivo de
definición de la imagen. Consulte el <xref linkend="quickstart-eib"/> para
obtener más detalles.</para>
</listitem>
<listitem>
<para>La carpeta de imágenes base contendrá la imagen raw generada al seguir las
instrucciones del <xref linkend="guides-kiwi-builder-images"/> con el perfil
<literal>Base-SelfInstall</literal> (o
<literal>Base-RT-SelfInstall</literal> para el kernel en tiempo real) y
deberá copiarse/moverse a la carpeta <literal>base-images</literal>.</para>
</listitem>
<listitem>
<para>La carpeta <literal>network</literal> es opcional. Para obtener más
información, consulte la <xref linkend="add-network-eib"/>.</para>
</listitem>
<listitem>
<para>El directorio <literal>custom/scripts</literal> contiene guiones que se
deben ejecutar en el primer arranque:</para>
<orderedlist numeration="arabic">
<listitem>
<para>El guion <literal>01-fix-growfs.sh</literal> es necesario para cambiar el
tamaño de la partición raíz del sistema operativo durante el despliegue.</para>
</listitem>
<listitem>
<para>El guion <literal>02-performance.sh</literal> es opcional y se puede usar
para configurar el sistema con el fin de optimizar su rendimiento.</para>
</listitem>
<listitem>
<para>El guion <literal>03-sriov.sh</literal> es opcional y se puede usar para
configurar el sistema para SR-IOV.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>El directorio <literal>custom/files</literal> contiene los archivos
<literal>performance-settings.sh</literal> y
<literal>sriov-auto-filler.sh</literal> que se copiarán en la imagen durante
el proceso de creación de la imagen.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>Archivo de definición de la imagen del clúster descendente</title>
<para>El archivo <literal>downstream-cluster-config.yaml</literal> es el principal
archivo de configuración para la imagen del clúster descendente. A
continuación, se muestra un ejemplo mínimo de despliegue mediante
Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Donde <literal>$SCC_REGISTRATION_CODE</literal> es el código de registro
copiado del <link xl:href="https://scc.suse.com/">Centro de servicios al
cliente de SUSE</link>, y la lista de paquetes contiene
<literal>jq</literal>, que es necesario.</para>
<para><literal>$ROOT_PASSWORD</literal> es la contraseña cifrada del usuario root,
que puede ser útil para pruebas y depuración. Se puede generar con el
comando <literal>openssl passwd -6 PASSWORD</literal>.</para>
<para>En entornos de producción, se recomienda usar las claves SSH que se pueden
añadir al bloque de usuarios sustituyendo <literal>$USERKEY1</literal> por
las claves SSH reales.</para>
<note>
<para><literal>arch: x86_64</literal> es la arquitectura de la imagen. Para la
arquitectura arm64, utilice <literal>arch: aarch64</literal>.</para>
<para><literal>net.ifnames=1</literal> permite los <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">nombres
predecibles para las interfaces de red</link></para>
<para>Esto coincide con la configuración predeterminada para el chart de metal3,
pero el ajuste debe coincidir con el valor de
<literal>predictableNicNames</literal> configurado en el chart.</para>
<para>Tenga en cuenta también que
<literal>ignition.platform.id=openstack</literal> es obligatorio. Sin este
argumento, la configuración de SLEMicro mediante Ignition fallará en el
flujo automatizado de Metal<superscript>3</superscript>.</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Guion Growfs</title>
<para>Actualmente, se requiere un guion personalizado
(<literal>custom/scripts/01-fix-growfs.sh</literal>) para ampliar el sistema
de archivos y que coincida con el tamaño del disco en el primer arranque
después del aprovisionamiento. El guion <literal>01-fix-growfs.sh</literal>
contiene la siguiente información:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="add-custom-script-performance">
<title>Guion performance</title>
<para>El siguiente guion opcional
(<literal>custom/scripts/02-performance.sh</literal>) se puede usar para
configurar el sistema a fin de optimizar su rendimiento:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para>El contenido de <literal>custom/files/performance-settings. sh</literal> es
un guion que se puede usar para configurar el sistema con el fin de
optimizar su rendimiento, y se puede descargar desde el siguiente <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.3/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">enlace</link>.</para>
</section>
<section xml:id="add-custom-script-sriov">
<title>Guion SR-IOV</title>
<para>El siguiente guion opcional (<literal>custom/scripts/03-sriov.sh</literal>)
se puede usar para configurar el sistema para SR-IOV:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para>El contenido de <literal>custom/files/ sriov-auto-filler.sh</literal> es un
guion que se puede usar para configurar el sistema para SR-IOV, y se puede
descargar desde el siguiente <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.3/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">enlace</link>.</para>
<note>
<para>Use el mismo método para añadir sus propios guiones personalizados que se
ejecuten durante el proceso de aprovisionamiento. Para obtener más
información, consulte el <xref linkend="quickstart-eib"/>.</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>Configuración adicional para cargas de trabajo de telecomunicaciones</title>
<para>Para habilitar funciones de telecomunicaciones como <literal>dpdk</literal>,
<literal>sr-iov</literal> o <literal>FEC</literal>, es posible que se
requieran paquetes adicionales, como se muestra en el siguiente ejemplo.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.2
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $user1Key1
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>Donde <literal>$SCC_REGISTRATION_CODE</literal> es el código de registro
copiado del <link xl:href="https://scc.suse.com/">Centro de servicios al
cliente de SUSE</link>, y la lista de paquetes contiene los paquetes mínimos
que se utilizarán para los perfiles de telecomunicaciones.</para>
<note>
<para><literal>arch: x86_64</literal> es la arquitectura de la imagen. Para la
arquitectura arm64, utilice <literal>arch: aarch64</literal>.</para>
</note>
</section>
<section xml:id="add-network-eib">
<title>Guion adicional para la configuración avanzada de red</title>
<para>Si necesita configurar direcciones IP estáticas o escenarios de red más
avanzados, como se describe en la <xref
linkend="advanced-network-configuration"/>, se requiere la siguiente
configuración adicional.</para>
<para>En la carpeta <literal>network</literal>, cree el siguiente archivo
<literal>configure-network.sh</literal>. Este archivo consume los datos de
la unidad de configuración durante el primer arranque y configura la red del
host utilizando la <link
xl:href="https://github.com/suse-edge/nm-configurator">herramienta NM
Configurator</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>Creación de la imagen</title>
<para>Cuando se haya preparado la estructura de directorios siguiendo las
secciones anteriores, ejecute el siguiente comando para crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>Esto crea el archivo de imagen ISO de salida denominado
<literal>eibimage-output-telco.raw</literal>, basado en la definición
descrita anteriormente.</para>
<para>La imagen resultante debe estar disponible a través de un servidor Web, ya
sea el contenedor del servidor de medios habilitado en los pasos descritos
en la sección sobre el clúster de gestión (<xref
linkend="metal3-media-server"/>) o algún otro servidor al que se pueda
acceder localmente. En los ejemplos siguientes, nos referimos a este
servidor como <literal>imagecache.local:8080</literal>.</para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>Preparación de la imagen del clúster descendente para entornos aislados</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) se utiliza para
preparar una imagen base de SLEMicro modificada que se aprovisiona en los
hosts del clúster descendente.</para>
<para>Gran parte de la configuración se puede realizar con Edge Image Builder,
pero en esta guía tratamos las configuraciones mínimas necesarias para
configurar el clúster descendente en entornos aislados.</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>Requisitos previos para entornos aislados</title>
<itemizedlist>
<listitem>
<para>Se requiere un entorno de ejecución de contenedores como <link
xl:href="https://podman.io">Podman</link> o <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link> para ejecutar
Edge Image Builder.</para>
</listitem>
<listitem>
<para>La imagen base se creará con las instrucciones del <xref
linkend="guides-kiwi-builder-images"/> y el perfil
<literal>Base-SelfInstall</literal> (o
<literal>Base-RT-SelfInstall</literal> para el kernel en tiempo real). El
proceso es el mismo para ambas arquitecturas (x86-64 y aarch64).</para>
</listitem>
<listitem>
<para>Para desplegar clústeres descendentes aarch64, antes del despliegue del
clúster de gestión debe definir el parámetro <literal>deployArchitecture:
arm64</literal> en el archivo <literal>metal3.yaml</literal>, lo que se
explica en la documentación del clúster de gestión (<xref
linkend="arm64-mgmt-cluster"/>). Esto es necesario para garantizar que se
utiliza la arquitectura correcta para el clúster descendente.</para>
</listitem>
<listitem>
<para>Si desea utilizar SR-IOV o cualquier otra carga de trabajo que requiera una
imagen de contenedor, debe desplegar y configurar previamente un registro
privado local (con o sin TLS y/o autenticación). Este registro se utilizará
para almacenar las imágenes y las imágenes OCI del chart de Helm.</para>
</listitem>
</itemizedlist>
<note>
<para>Es necesario utilizar un host de creación con la misma arquitectura que las
imágenes que se están creando. En otras palabras, para crear una imagen
<literal>aarch64</literal>, es necesario utilizar un host de creación
<literal>aarch64</literal>, y lo mismo ocurre para <literal>x86-64</literal>
(actualmente, no se admiten creaciones con arquitecturas cruzadas).</para>
</note>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>Configuración de la imagen para entornos aislados</title>
<para>Al ejecutar Edge Image Builder, se monta un directorio desde el host, por lo
que es necesario crear una estructura de directorios para almacenar los
archivos de configuración usados para definir la imagen de destino.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal> es el archivo de
definición de la imagen. Para obtener más información, consulte el <xref
linkend="quickstart-eib"/>.</para>
</listitem>
<listitem>
<para>La carpeta de imágenes base contendrá la imagen raw generada al seguir las
instrucciones del <xref linkend="guides-kiwi-builder-images"/> con el perfil
<literal>Base-SelfInstall</literal> (o
<literal>Base-RT-SelfInstall</literal> para el kernel en tiempo real) y
deberá copiarse/moverse a la carpeta <literal>base-images</literal>.</para>
</listitem>
<listitem>
<para>La carpeta <literal>network</literal> es opcional. Para obtener más
información, consulte la <xref linkend="add-network-eib"/>.</para>
</listitem>
<listitem>
<para>El directorio <literal>custom/scripts</literal> contiene guiones que se
deben ejecutar en el primer arranque:</para>
<orderedlist numeration="arabic">
<listitem>
<para>El guion <literal>01-fix-growfs.sh</literal> es necesario para cambiar el
tamaño de la partición raíz del sistema operativo durante el despliegue.</para>
</listitem>
<listitem>
<para>El guion <literal>02-airgap.sh</literal> es necesario para copiar las
imágenes en el lugar correcto durante el proceso de creación de imágenes
para entornos aislados.</para>
</listitem>
<listitem>
<para>El guion <literal>03-performance.sh</literal> es opcional y se puede usar
para configurar el sistema con el fin de optimizar su rendimiento.</para>
</listitem>
<listitem>
<para>El guion <literal>04-sriov.sh</literal> es opcional y se puede usar para
configurar el sistema para SR-IOV.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>El directorio <literal>custom/files</literal> contiene las imágenes
<literal>rke2</literal> y <literal>cni</literal> que se copiarán en la
imagen durante el proceso de creación de la imagen. Además, se pueden
incluir los archivos opcionales <literal>performance-settings.sh</literal> y
<literal>sriov-auto-filler.sh</literal>.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>Archivo de definición de la imagen del clúster descendente</title>
<para>El archivo <literal>downstream-cluster-airgap-config.yaml</literal> es el
archivo de configuración principal para la imagen del clúster descendente y
su contenido se ha descrito en la sección anterior (<xref
linkend="add-telco-feature-eib"/>).</para>
</section>
<section xml:id="id-growfs-script">
<title>Guion Growfs</title>
<para>Actualmente, se requiere un guion personalizado
(<literal>custom/scripts/01-fix-growfs.sh</literal>) para ampliar el sistema
de archivos y que coincida con el tamaño del disco en el primer arranque
después del aprovisionamiento. El guion <literal>01-fix-growfs.sh</literal>
contiene la siguiente información:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>Guion air-gap</title>
<para>El siguiente guion (<literal>custom/scripts/02-airgap.sh</literal>) es
necesario para copiar las imágenes en el lugar correcto durante el proceso
de creación de la imagen:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="add-custom-script-performance2">
<title>Guion performance</title>
<para>El siguiente guion opcional
(<literal>custom/scripts/03-performance.sh</literal>) se puede usar para
configurar el sistema con el fin de optimizar su rendimiento:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para>El contenido de <literal>custom/files/performance-settings. sh</literal> es
un guion que se puede usar para configurar el sistema con el fin de
optimizar su rendimiento, y se puede descargar desde el siguiente <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.3/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">enlace</link>.</para>
</section>
<section xml:id="add-custom-script-sriov2">
<title>Guion SR-IOV</title>
<para>El siguiente guion opcional (<literal>custom/scripts/04-sriov.sh</literal>)
se puede usar para configurar el sistema para SR-IOV:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para>El contenido de <literal>custom/files/ sriov-auto-filler.sh</literal> es un
guion que se puede usar para configurar el sistema para SR-IOV, y se puede
descargar desde el siguiente <link
xl:href="https://github.com/suse-edge/atip/blob/release-3.3/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">enlace</link>.</para>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>Archivos personalizados para entornos aislados</title>
<para>El directorio <literal>custom/files</literal> contiene las imágenes
<literal>rke2</literal> y <literal>cni</literal> que se copiarán en la
imagen durante el proceso de creación de la imagen. Para generar fácilmente
las imágenes, prepárelas localmente utilizando el siguiente <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/edge-save-images.sh">guion</link>
y la lista de imágenes que encontrará <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/edge-release-rke2-images.txt">aquí</link>
para generar los artefactos que deben incluirse en
<literal>custom/files</literal>. Además, puede descargar el guion
<literal>rke2-install</literal> más reciente desde <link
xl:href="https://get.rke2.io/">aquí</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>Después de descargar las imágenes, la estructura de directorios debería
tener este aspecto:</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>Carga previa del registro privado con las imágenes necesarias para
situaciones de entornos aislados y SR-IOV (opcional)</title>
<para>Si desea utilizar SR-IOV en un entorno aislado o en cualquier otra imagen de
carga de trabajo, debe cargar previamente su registro privado local con las
imágenes siguiendo estos pasos:</para>
<itemizedlist>
<listitem>
<para>Descargar, extraer y enviar las imágenes OCI del chart de Helm al registro
privado</para>
</listitem>
<listitem>
<para>Descargar, extraer y enviar el resto de imágenes necesarias al registro
privado</para>
</listitem>
</itemizedlist>
<para>Los guiones siguientes se pueden usar para descargar, extraer y enviar las
imágenes al registro privado. Mostraremos un ejemplo para precargar las
imágenes de SR-IOV, pero también puede usar el mismo método para cargar
previamente cualquier otra imagen personalizada:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Realice la carga previa con imágenes OCI del chart de Helm para SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Debe crear una lista con las imágenes OCI del chart de Helm necesarias:</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:303.0.2+up1.5.0
edge/sriov-crd-chart:303.0.2+up1.5.0
EOF</screen>
</listitem>
<listitem>
<para>Genere un archivo tarball local utilizando el siguiente <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/edge-save-oci-artefacts.sh">guion</link>
y la lista creada anteriormente:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/charts/sriov-network-operator:303.0.2+up1.5.0
Pulled: registry.suse.com/edge/charts/sriov-crd:303.0.2+up1.5.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-303.0.2+up1.5.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-303.0.2+up1.5.0.tgz</screen>
</listitem>
<listitem>
<para>Suba el archivo tarball a su registro privado (por ejemplo,
<literal>myregistry:5000</literal>) usando el <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/edge-load-oci-artefacts.sh">guion</link>
siguiente para cargar previamente su registro con las imágenes OCI del chart
de Helm descargadas en el paso anterior:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Realice la carga previa del resto de las imágenes necesarias para SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>En este caso, debemos incluir las imágenes de contenedor sr-iov para cargas
de trabajo de telecomunicaciones (por ejemplo, como referencia, puede
obtenerlas de los <link
xl:href="https://github.com/suse-edge/charts/blob/main/charts/sriov-network-operator/1.5.0/values.yaml">valores
del chart de Helm</link>).</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</screen>
</listitem>
<listitem>
<para>Con el <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/edge-save-images.sh">guion</link>
siguiente y la lista creada anteriormente, debe generar localmente el
archivo tarball con las imágenes necesarias:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</screen>
</listitem>
<listitem>
<para>Suba el archivo tarball a su registro privado (por ejemplo,
<literal>myregistry:5000</literal>) usando el <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.3.0/scripts/day2/edge-load-images.sh">guion</link>
siguiente para precargar su registro privado con las imágenes descargadas en
el paso anterior:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>Creación de la imagen para entornos aislados</title>
<para>Cuando se haya preparado la estructura de directorios siguiendo las
secciones anteriores, ejecute el siguiente comando para crear la imagen:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.3/edge-image-builder:1.2.1 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>Esto crea el archivo de imagen ISO de salida denominado
<literal>eibimage-output-telco.raw</literal>, basado en la definición
descrita anteriormente.</para>
<para>La imagen resultante debe estar disponible a través de un servidor Web, ya
sea el contenedor del servidor de medios habilitado siguiendo los pasos de
la sección sobre el clúster de gestión (<xref
linkend="metal3-media-server"/>) o algún otro servidor al que se pueda
acceder localmente. En los ejemplos siguientes, nos referimos a este
servidor como <literal>imagecache.local:8080</literal>.</para>
</section>
</section>
<section xml:id="single-node">
<title>Aprovisionamiento de clústeres descendentes con aprovisionamiento de red
dirigida (nodo único)</title>
<para>En esta sección se describe el flujo de trabajo usado para automatizar el
aprovisionamiento de un clúster descendente de un solo nodo con el método de
aprovisionamiento de red dirigida. Se trata de la forma más sencilla de
automatizar el aprovisionamiento de un clúster descendente.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>La imagen generada con <literal>EIB</literal>, como se describe en la
sección anterior (<xref linkend="eib-edge-image-connected"/>), con la
configuración mínima para configurar el clúster descendente, debe ubicarse
en el clúster de gestión exactamente en la ruta que ha configurado en esta
sección (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>El servidor de gestión creado y disponible para su uso en las siguientes
secciones. Para obtener más información, consulte el <xref
linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Flujo de trabajo</emphasis></para>
<para>El diagrama siguiente muestra el flujo de trabajo usado para automatizar el
aprovisionamiento de un clúster descendente de un solo nodo con el método de
aprovisionamiento de red dirigida:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png" width="100%"/>
</imageobject>
<textobject><phrase>atip automatizado de un solo nodo 1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Hay dos pasos diferentes para automatizar el aprovisionamiento de un clúster
descendente de un solo nodo con el método de aprovisionamiento de red
dirigida:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Inscribir el host bare metal para que esté disponible para el proceso de
aprovisionamiento.</para>
</listitem>
<listitem>
<para>Proporcionar el host bare metal para instalar y configurar el sistema
operativo y el clúster de Kubernetes.</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">Inscripción del host bare metal</emphasis></para>
<para>El primer paso es inscribir el nuevo host bare metal en el clúster de
gestión para que esté disponible para su aprovisionamiento. Para ello, se
debe crear el archivo <literal>bmh-example.yaml</literal> en el clúster de
gestión, con el fin de especificar las credenciales de
<literal>BMC</literal> que se van a utilizar y el objeto
<literal>BaremetalHost</literal> que se va a inscribir:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>Donde:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal>: es el nombre de usuario del
<literal>BMC</literal> del nuevo host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal>: es la contraseña para el
<literal>BMC</literal> del nuevo host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal>: es la dirección <literal>MAC</literal> del
nuevo host bare metal que se va a utilizar.</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal>: es la <literal>URL</literal> del host
bare metal de <literal>BMC</literal> (por ejemplo,
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>).
Para obtener más información sobre las opciones disponibles en función de su
proveedor de hardware, consulte el siguiente <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">enlace</link>.</para>
</listitem>
</itemizedlist>
<note>
<para>Si no se ha especificado ninguna configuración de red para el host, ya sea
en el momento de la creación de la imagen o en la definición
<literal>BareMetalHost</literal>, se utilizará un mecanismo de configuración
automática (DHCP, DHCPv6 o SLAAC). Para obtener más detalles o
configuraciones complejas, consulte la <xref
linkend="advanced-network-configuration"/>.</para>
</note>
<para>Una vez creado el archivo, se debe ejecutar el siguiente comando en el
clúster de gestión para comenzar a inscribir el nuevo host bare metal en el
clúster de gestión:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>El nuevo objeto host bare metal se inscribirá y cambiará su estado de
"registering" (en registro) a "inspecting" (en inspección) y "available"
(disponible). Los cambios se pueden comprobar con el siguiente comando:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para>El objeto <literal>BaremetalHost</literal> permanece en estado
<literal>registering</literal> (en registro) hasta que se validan las
credenciales de <literal>BMC</literal>. Una vez validadas las credenciales,
el objeto <literal>BaremetalHost</literal> cambia su estado a
<literal>inspecting</literal> (en inspección), y este paso puede tardar
algún tiempo dependiendo del hardware (hasta 20 minutos). Durante la fase de
inspección, se recupera la información del hardware y se actualiza el objeto
de Kubernetes. Compruebe la información con el siguiente comando:
<literal>kubectl get bmh -o yaml</literal>.</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">Paso de aprovisionamiento</emphasis></para>
<para>Una vez que el host bare metal está inscrito y disponible, el paso siguiente
es aprovisionar el host bare metal para instalar y configurar el sistema
operativo y el clúster de Kubernetes. Para ello, se debe crear el archivo
<literal>capi-provisioning-example.yaml</literal> en el clúster de gestión
con la siguiente información (el archivo
<literal>capi-provisioning-example.yaml</literal> se puede generar uniendo
los bloques siguientes).</para>
<note>
<para>Solo los valores entre <literal>$\{…​\}</literal> deben sustituirse por los
valores reales.</para>
</note>
<para>El siguiente bloque es la definición del clúster, donde se puede configurar
la red utilizando los bloques <literal>pods</literal> y
<literal>services</literal>. Además, contiene las referencias al plano de
control y a los objetos de infraestructura (utilizando el proveedor de
<literal>Metal<superscript>3</superscript></literal>) que se van a utilizar.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<para>Para un despliegue con pods y servicios de doble pila, se puede utilizar la
siguiente definición en su lugar:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:bad:cafe::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:bad:bad:cafe::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<important>
<para>Los despliegues de IPv6 y doble pila se encuentran en fase de tecnología en
fase preliminar y no son compatibles oficialmente.</para>
</important>
<para>El objeto <literal>Metal3Cluster</literal> especifica el punto final del
plano de control (en sustitución de
<literal>${DOWNSTREAM_CONTROL_PLANE_IP}</literal>) que se va a configurar y
el objeto <literal>noCloudProvider</literal>, ya que se utiliza un nodo bare
metal.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IP}
    port: 6443
  noCloudProvider: true</screen>
<para>El objeto <literal>RKE2ControlPlane</literal> especifica la configuración de
plano de control que se va a utilizar y el objeto
<literal>Metal3MachineTemplate</literal> especifica la imagen de plano de
control que se va a utilizar. Además, contiene la información sobre el
número de réplicas que se utilizarán (en este caso, una) y el complemento de
<literal>CNI</literal> que se usará (en este caso,
<literal>Cilium</literal>). El bloque agentConfig contiene el formato de
<literal>Ignition</literal> que se va a usar y el objeto
<literal>additionalUserData</literal> que se va a usar para configurar el
nodo <literal>RKE2</literal> con información como un servicio systemd
denominado <literal>rke2-preinstall.service</literal> para sustituir
automáticamente <literal>BAREMETALHOST_UUID</literal> y
<literal>node-name</literal> durante el proceso de aprovisionamiento
utilizando la información de Ironic. Para habilitar Multus con Cilium, se
crea un archivo en el directorio de manifiestos del servidor
<literal>rke2</literal> llamado <literal>rke2-cilium-config.yaml</literal>
con la configuración que se va a usar. El último bloque de información
contiene la versión de Kubernetes que se va a
usar. <literal>${RKE2_VERSION}</literal> es la versión de
<literal>RKE2</literal> que se va a usar en sustitución de este valor (por
ejemplo, <literal>v1.32.4+rke2r1</literal>).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>El objeto <literal>Metal3MachineTemplate</literal> especifica la información
siguiente:</para>
<itemizedlist>
<listitem>
<para>La plantilla de datos (<literal>dataTemplate</literal>) que se usará como
referencia para la plantilla.</para>
</listitem>
<listitem>
<para>El selector de host (<literal>hostSelector</literal>) que se usará que
coincida con la etiqueta creada durante el proceso de inscripción.</para>
</listitem>
<listitem>
<para>La imagen (<literal>image</literal>) que se usará como referencia para la
imagen generada mediante <literal>EIB</literal> en la sección anterior
(<xref linkend="eib-edge-image-connected"/>), y la suma de comprobación
(<literal>checksum</literal>) y el tipo de suma
(<literal>checksumType</literal>) que se usarán para validar la imagen.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para>El objeto <literal>Metal3DataTemplate</literal> especifica los metadatos
(<literal>metaData</literal>) para el clúster descendente.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Una vez creado el archivo uniendo los bloques anteriores, se debe ejecutar
el comando siguiente en el clúster de gestión para iniciar el
aprovisionamiento del nuevo host bare metal:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>Aprovisionamiento de clústeres descendentes con aprovisionamiento de red
dirigida (varios nodos)</title>
<para>En esta sección se describe el flujo de trabajo usado para automatizar el
aprovisionamiento de un clúster descendente de varios nodos con el método de
aprovisionamiento de red dirigida y <literal>MetalLB</literal> como
equilibrador de carga. Esta es la forma más sencilla de automatizar el
aprovisionamiento de un clúster descendente. El siguiente diagrama muestra
el flujo de trabajo utilizado para automatizar el aprovisionamiento de un
clúster descendente de varios nodos con el método de aprovisionamiento de
red dirigida y <literal>MetalLB</literal>.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>La imagen generada con <literal>EIB</literal>, como se describe en la
sección anterior (<xref linkend="eib-edge-image-connected"/>), con la
configuración mínima para configurar el clúster descendente, debe ubicarse
en el clúster de gestión exactamente en la ruta que ha configurado en esta
sección (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>El servidor de gestión creado y disponible para su uso en las siguientes
secciones. Para obtener más información, consulte el <xref
linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Flujo de trabajo</emphasis></para>
<para>El diagrama siguiente muestra el flujo de trabajo usado para automatizar el
aprovisionamiento de un clúster descendente de varios nodos con el método de
aprovisionamiento de red dirigida:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png" width="100%"/>
</imageobject>
<textobject><phrase>atip automatizado de varios nodos 1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>Inscriba los tres hosts bare metal para que estén disponibles para el
proceso de aprovisionamiento.</para>
</listitem>
<listitem>
<para>Aprovisione los tres hosts bare metal para instalar y configurar el sistema
operativo y el clúster de Kubernetes con <literal>MetalLB</literal>.</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">Inscripción de los hosts bare metal</emphasis></para>
<para>El primer paso es inscribir los tres hosts bare metal en el clúster de
gestión para que estén disponibles para su aprovisionamiento. Para ello, se
deben crear los archivos <literal>bmh-example-node1.yaml</literal>,
<literal>bmh-example-node2.yaml</literal> y
<literal>bmh-example-node3.yaml</literal> en el clúster de gestión para
especificar las credenciales de <literal>BMC</literal> que se van a utilizar
y el objeto <literal>BaremetalHost</literal> que se va a inscribir en el
clúster de gestión.</para>
<note>
<itemizedlist>
<listitem>
<para>Solo los valores entre <literal>$\{…​\}</literal> deben sustituirse por los
valores reales.</para>
</listitem>
<listitem>
<para>Le guiaremos por el proceso para un solo host. Los mismos pasos se aplican a
los otros dos nodos.</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>Donde:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal>: es el nombre de usuario para el
BMC del primer host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal>: es la contraseña para el BMC del
primer host bare metal.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal>: es la dirección MAC del primer host
bare metal que se utilizará.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal>: es la URL del primer host BMC bare
metal (por ejemplo,
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>).
Para obtener más información sobre las opciones disponibles en función de su
proveedor de hardware, consulte el siguiente <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">enlace</link>.</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>Si no se ha especificado ninguna configuración de red para el host, ya sea
en el momento de la creación de la imagen o en la definición
<literal>BareMetalHost</literal>, se utilizará un mecanismo de configuración
automática (DHCP, DHCPv6 o SLAAC). Para obtener más detalles o
configuraciones complejas, consulte la <xref
linkend="advanced-network-configuration"/>.</para>
</listitem>
<listitem>
<para>Aún no se admiten clústeres de varios nodos con doble pila o solo IPv6.</para>
</listitem>
</itemizedlist>
</note>
<para>Una vez creado el archivo, se debe ejecutar el comando siguiente en el
clúster de gestión para comenzar a inscribir los hosts bare metal en el
clúster de gestión:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>Los nuevos objetos host bare metal se inscriben y cambian su estado de
"registering" (en registro) a "inspecting" (en inspección) y "available"
(disponible). Los cambios se pueden comprobar con el siguiente comando:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para>El objeto <literal>BaremetalHost</literal> permanece en estado
<literal>registering</literal> (en registro) hasta que se validan las
credenciales de <literal>BMC</literal>. Una vez validadas las credenciales,
el objeto <literal>BaremetalHost</literal> cambia su estado a
<literal>inspecting</literal> (en inspección), y este paso puede tardar
algún tiempo dependiendo del hardware (hasta 20 minutos). Durante la fase de
inspección, se recupera la información del hardware y se actualiza el objeto
de Kubernetes. Compruebe la información con el siguiente comando:
<literal>kubectl get bmh -o yaml</literal>.</para>
</note>
<para><emphasis role="strong">Paso de aprovisionamiento</emphasis></para>
<para>Una vez que los tres hosts bare metal se han inscrito y están disponibles,
el siguiente paso es aprovisionar los hosts bare metal para instalar y
configurar el sistema operativo y el clúster de Kubernetes, creando un
equilibrador de carga para gestionarlos. Para ello, se debe crear el archivo
<literal>capi-provisioning-example.yaml</literal> en el clúster de gestión
con la siguiente información (el archivo capi-provisioning-example.yaml se
puede generar uniendo los siguientes bloques).</para>
<note>
<itemizedlist>
<listitem>
<para>Solo los valores entre <literal>$\{…​\}</literal> deben sustituirse por los
valores reales.</para>
</listitem>
<listitem>
<para>La dirección <literal>IP virtual</literal> es una dirección IP reservada que
no se asigna a ningún nodo y se utiliza para configurar el equilibrador de
carga.</para>
</listitem>
</itemizedlist>
</note>
<para>A continuación, se muestra la definición del clúster, donde la red del
clúster se puede configurar utilizando los bloques <literal>pods</literal> y
<literal>services</literal>. Además, contiene las referencias al plano de
control y a los objetos de infraestructura (utilizando el proveedor de
<literal>Metal<superscript>3</superscript></literal>) que se van a utilizar.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<para>El objeto <literal>Metal3Cluster</literal> especifica el punto final de
plano de control que usa la dirección <literal>IP virtual</literal> ya
reservada (en sustitución de <literal>${DOWNSTREAM_VIP_ADDRESS}</literal>)
que se va a configurar y <literal>noCloudProvider</literal> porque se
utilizan los tres nodos bare metal.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS}
    port: 6443
  noCloudProvider: true</screen>
<para>El objeto <literal>RKE2ControlPlane</literal> especifica la configuración de
plano de control que se va a utilizar, y el objeto
<literal>Metal3MachineTemplate</literal> especifica la imagen de plano de
control que se va a utilizar.</para>
<itemizedlist>
<listitem>
<para>El número de réplicas que se utilizarán (en este caso, tres).</para>
</listitem>
<listitem>
<para>El modo de anuncios que utilizará el equilibrador de carga (en
<literal>address</literal> se usa la implementación L2), así como la
dirección que se va a usar (sustituyendo
<literal>${EDGE_VIP_ADDRESS}</literal> por la dirección <literal>IP
virtual</literal>).</para>
</listitem>
<listitem>
<para>El <literal>serverConfig</literal> con el complemento de
<literal>CNI</literal> que se va a usar (en este caso,
<literal>Cilium</literal>) y el <literal>tlsSan</literal> que se va a usar
para configurar la dirección <literal>IP virtual</literal>.</para>
</listitem>
<listitem>
<para>El bloque agentConfig contiene el formato de <literal>Ignition</literal> que
se va a usar y el bloque <literal>additionalUserData</literal> que se va a
usar para configurar el nodo <literal>RKE2</literal> con información como:</para>
<itemizedlist>
<listitem>
<para>El servicio systemd denominado <literal>rke2-preinstall.service</literal>
que sustituye automáticamente a <literal>BAREMETALHOST_UUID</literal> y
<literal>node-name</literal> durante el proceso de aprovisionamiento con la
información de Ironic.</para>
</listitem>
<listitem>
<para>El bloque <literal>storage</literal> que contiene los charts de Helm que se
van a usar para instalar <literal>MetalLB</literal> y
<literal>endpoint-copier-operator</literal>.</para>
</listitem>
<listitem>
<para>El archivo de recurso personalizado de <literal>metalLB</literal> con el
<literal>IPaddressPool</literal> y el <literal>L2Advertisement</literal> que
se van a usar (sustituyendo <literal>${EDGE_VIP_ADDRESS}</literal> por la
dirección <literal>IP virtual</literal>).</para>
</listitem>
<listitem>
<para>El archivo <literal>endpoint-svc.yaml</literal> que se va a usar para
configurar el servicio <literal>kubernetes-vip</literal> que utilizará
<literal>MetalLB</literal> para gestionar la dirección <literal>IP
virtual</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>El último bloque de información contiene la versión de Kubernetes que se va
a usar. <literal>${RKE2_VERSION}</literal> es la versión de
<literal>RKE2</literal> que se va a usar en sustitución de este valor (por
ejemplo, <literal>v1.32.4+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS}
      - https://${EDGE_VIP_ADDRESS}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 303.0.0+up0.2.1
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 303.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS}/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "Node-multinode-cluster"</screen>
<para>El objeto <literal>Metal3MachineTemplate</literal> especifica la información
siguiente:</para>
<itemizedlist>
<listitem>
<para>La plantilla de datos (<literal>dataTemplate</literal>) que se usará como
referencia para la plantilla.</para>
</listitem>
<listitem>
<para>El selector de host (<literal>hostSelector</literal>) que se usará que
coincida con la etiqueta creada durante el proceso de inscripción.</para>
</listitem>
<listitem>
<para>La imagen (<literal>image</literal>) que se va a usar como referencia para
la imagen generada con <literal>EIB</literal> en la sección anterior (<xref
linkend="eib-edge-image-connected"/>), y la suma de comprobación
(<literal>checksum</literal>) y el tipo de suma
(<literal>checksumType</literal>) que se van a usar para validar la imagen.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para>El objeto <literal>Metal3DataTemplate</literal> especifica los metadatos
(<literal>metaData</literal>) para el clúster descendente.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Una vez creado el archivo uniendo los bloques anteriores, se debe ejecutar
el comando siguiente en el clúster de gestión para iniciar el
aprovisionamiento de los tres nuevos hosts bare metal:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>Configuración de red avanzada</title>
<para>El flujo de trabajo de aprovisionamiento de red dirigida permite
configuraciones de red específicas en clústeres descendentes, como
direcciones IP estáticas, vinculación, VLAN, IPv6, etc.</para>
<para>Las secciones siguientes describen los pasos adicionales necesarios para
habilitar el aprovisionamiento de clústeres descendentes con el método de
configuración de red avanzada.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>La imagen generada con <literal>EIB</literal> debe incluir la carpeta de red
y el guion de esta sección (<xref linkend="add-network-eib"/>).</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuración</emphasis></para>
<para>Antes de continuar, consulte en las secciones siguientes los pasos
necesarios para inscribir y aprovisionar los hosts:</para>
<itemizedlist>
<listitem>
<para>Aprovisionamiento de clústeres descendentes con aprovisionamiento de red
dirigida (nodo único) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Aprovisionamiento de clústeres descendentes con aprovisionamiento de red
dirigida (varios nodos) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>Cualquier configuración de red avanzada debe aplicarse en el momento de la
inscripción mediante la definición de host <literal>BareMetalHost</literal>
y un secreto asociado que contenga un bloque <literal>networkData</literal>
con formato <literal>nmstate</literal>. El siguiente archivo de ejemplo
define un secreto que contiene el bloque <literal>networkData</literal>
necesario que solicita una <literal>IP</literal> estática y una
<literal>VLAN</literal> para el host del clúster descendente:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      identifier: mac-address
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>Como puede ver, el ejemplo muestra la configuración para habilitar la
interfaz con direcciones IP estáticas, así como la configuración para
habilitar la VLAN utilizando la interfaz base, una vez que las siguientes
variables se sustituyan por los valores reales de su infraestructura:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE1_INTERFACE}</literal>: es la interfaz de plano de
control que se usará para el clúster periférico (por ejemplo,
<literal>eth0</literal>). Si se incluye <literal>identificador: dirección
MAC</literal>, el nombre se comprueba automáticamente por su dirección MAC,
por lo que se puede utilizar cualquier nombre de interfaz.</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_IP}</literal>: es la dirección IP que se usará como
punto final para el clúster periférico (debe coincidir con el punto final
kubeapi-server).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_PREFIX}</literal>: es el CIDR que se usará para el
clúster periférico (por ejemplo, <literal>24</literal> si desea
<literal>/24</literal> o <literal>255.255.255.0</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_GATEWAY}</literal>: es el gateway que se usará para
el clúster periférico (por ejemplo, <literal>192.168.100.1</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_MAC}</literal>: es la dirección MAC que se usará
para la interfaz de plano de control (por ejemplo,
<literal>00:0c:29:3e:3e:3e</literal>).</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal>: es el servidor DNS que se usará para el
clúster periférico (por ejemplo, <literal>192.168.100.2</literal>).</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal>: es el ID de VLAN que se usará para el clúster
periférico (por ejemplo, <literal>100</literal>).</para>
</listitem>
</itemizedlist>
<para>Se puede usar cualquier otra definición compatible con
<literal>nmstate</literal> para configurar la red del clúster descendente y
adaptarla a los requisitos específicos. Por ejemplo, es posible especificar
una configuración estática de doble pila:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: ${CONTROLPLANE_MAC}
      ipv4:
        enabled: true
        dhcp: false
        address:
        - ip: ${CONTROLPLANE_IP_V4}
          prefix-length: ${CONTROLPLANE_PREFIX_V4}
      ipv6:
        enabled: true
        dhcp: false
        autoconf: false
        address:
        - ip: ${CONTROLPLANE_IP_V6}
          prefix-length: ${CONTROLPLANE_PREFIX_V6}
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V4}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
      - destination: ::/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V6}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
    dns-resolver:
      config:
        server:
        - ${DNS_SERVER_V4}
        - ${DNS_SERVER_V6}</screen>
<para>En el ejemplo anterior, sustituya las variables siguientes por los valores
reales de su infraestructura:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_IP_V4}</literal>: la dirección IPv4 que se asignará
al host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V4}</literal>: el prefijo IPv4 de la red a la
que pertenece la IP del host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP_V6}</literal>: la dirección IPv6 que se asignará
al host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V6}</literal>: el prefijo IPv6 de la red a la
que pertenece la IP del host</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V4}</literal>: la dirección IPv4 del gateway
para el tráfico que coincide con la ruta predeterminada</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V6}</literal>: la dirección IPv6 del gateway
para el tráfico que coincide con la ruta predeterminada</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal>: el nombre de la interfaz a la
que se asignarán las direcciones y que se usará para el tráfico de salida
que coincida con la ruta predeterminada, tanto para IPv4 como para IPv6</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER_V4}</literal> y/o <literal>${DNS_SERVER_V6}</literal>:
las direcciones IP de los servidores DNS que se van a usar, que pueden
especificarse como entradas únicas o múltiples. Se admiten direcciones IPv4
y/o IPv6</para>
</listitem>
</itemizedlist>
<important>
<para>Los despliegues de IPv6 y doble pila se encuentran en fase de tecnología en
fase preliminar y no son compatibles oficialmente.</para>
</important>
<note>
<para>En el <link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/edge-clusters">repositorio
de ejemplos de SUSE Edge for Telco</link> encontrará ejemplos más complejos,
incluidas configuraciones solo de IPv6 y de doble pila.</para>
</note>
<para>Por último, independientemente de los detalles de la configuración de red,
asegúrese de que se haga referencia al secreto añadiendo
<literal>preprovisioningNetworkDataName</literal> al objeto
<literal>BaremetalHost</literal> para inscribir correctamente el host en el
clúster de gestión.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<itemizedlist>
<listitem>
<para>Si necesita desplegar un clúster de varios nodos, debe realizar el mismo
proceso para cada nodo.</para>
</listitem>
<listitem>
<para>Actualmente, <literal>Metal3DataTemplate</literal>,
<literal>networkData</literal> y <literal>Metal3 IPAM</literal> no se
admiten; solo se admite plenamente la configuración mediante secretos
estáticos.</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="add-telco">
<title>Funciones para telecomunicaciones (DPDK, SR-IOV, aislamiento de CPU, páginas
enormes, NUMA, etc.)</title>
<para>El flujo de trabajo de aprovisionamiento de red dirigida permite automatizar
las funciones de telecomunicaciones que se usarán en los clústeres
descendentes para ejecutar cargas de trabajo de telecomunicaciones en esos
servidores.</para>
<para><emphasis role="strong">Requisitos</emphasis></para>
<itemizedlist>
<listitem>
<para>La imagen generada con <literal>EIB</literal>, que se describe en la sección
anterior (<xref linkend="eib-edge-image-connected"/>), debe estar en el
clúster de gestión exactamente en la ruta que ha configurado en esa sección
(<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>La imagen generada con <literal>EIB</literal> debe incluir los paquetes
específicos de telecomunicaciones que se indican en esa sección (<xref
linkend="add-telco-feature-eib"/>).</para>
</listitem>
<listitem>
<para>El servidor de gestión creado y disponible para su uso en las siguientes
secciones. Para obtener más información, consulte el <xref
linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuración</emphasis></para>
<para>Use las dos secciones siguientes como base para inscribir y aprovisionar los
hosts:</para>
<itemizedlist>
<listitem>
<para>Aprovisionamiento de clústeres descendentes con aprovisionamiento de red
dirigida (nodo único) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Aprovisionamiento de clústeres descendentes con aprovisionamiento de red
dirigida (varios nodos) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>Las funciones de telecomunicaciones que se tratan en esta sección son las
siguientes:</para>
<itemizedlist>
<listitem>
<para>DPDK y creación de funciones virtuales</para>
</listitem>
<listitem>
<para>Asignación de SR-IOV y funciones virtuales que usarán las cargas de trabajo</para>
</listitem>
<listitem>
<para>Aislamiento de CPU y optimización del rendimiento</para>
</listitem>
<listitem>
<para>Configuración de páginas enormes</para>
</listitem>
<listitem>
<para>Optimización de los parámetros del kernel</para>
</listitem>
</itemizedlist>
<note>
<para>Para obtener más información sobre las funciones de telecomunicaciones,
consulte el <xref linkend="atip-features"/>.</para>
</note>
<para>Todos los cambios necesarios para habilitar las funciones de
telecomunicaciones indicados arriba se encuentran en el bloque
<literal>RKE2ControlPlane</literal> del archivo de aprovisionamiento
<literal>capi-provisioning-example.yaml</literal>. El resto de la
información contenida en el archivo
<literal>capi-provisioning-example.yaml</literal> es la misma que se
proporciona en la sección sobre aprovisionamiento (<xref
linkend="single-node-provision"/>).</para>
<para>Para que el proceso quede claro, los cambios necesarios en ese bloque
(<literal>RKE2ControlPlane</literal>) para habilitar las funciones de
telecomunicaciones son los siguientes:</para>
<itemizedlist>
<listitem>
<para>Los comandos <literal>preRKE2Commands</literal> que se usarán para ejecutar
los comandos antes del proceso de instalación de <literal>RKE2</literal>. En
este caso, use el comando <literal>modprobe</literal> para habilitar los
módulos del kernel <literal>vfio-pci</literal> y <literal>SR-IOV</literal>.</para>
</listitem>
<listitem>
<para>El archivo de Ignition
<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>
se usará para definir las interfaces, los controladores y el número de
<literal>funciones virtuales</literal> que se crearán y expondrán a las
cargas de trabajo.</para>
<itemizedlist>
<listitem>
<para>Los valores del mapa de configuración
<literal>sriov-custom-auto-config</literal> son los únicos que deben
sustituirse por valores reales.</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal>: el nombre del recurso que se va a usar
para la primera interfaz de función física <literal>PF</literal> (por
ejemplo, <literal>sriov-resource-du1</literal>). Se añade al prefijo
<literal>rancher.io</literal> para usarse como etiqueta que utilizarán las
cargas de trabajo (por ejemplo,
<literal>rancher.io/sriov-resource-du1</literal>).</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal>: el nombre de la primera interfaz de
función física <literal>PF</literal> que se usará (por ejemplo,
<literal>eth0</literal>).</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal>: el nombre de la primera función física
<literal>PF</literal> que se usará. Con esto, puede generar filtros más
complejos (por ejemplo, <literal>eth0#2-5</literal>).</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal>: el nombre del controlador que se usará
para la primera interfaz de función virtual <literal>VF</literal> (por
ejemplo, <literal>vfio-pci</literal>).</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal>: el número de <literal>funciones
virtuales</literal> que se crearán para la primera interfaz de función
física <literal>PF</literal> (por ejemplo, <literal>8</literal>).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>El archivo <literal>/var/sriov-auto-filler.sh</literal> que se usará como
traductor entre el mapa de configuración general
<literal>sriov-custom-auto-config</literal> y el archivo
<literal>sriovnetworknodepolicy</literal>, que contiene la información de
hardware de bajo nivel. Este guion se ha creado para evitar al usuario la
complejidad que supone que tenga que conocer de antemano la información de
hardware. No es necesario realizar cambios en este archivo, pero debe estar
presente si necesitamos habilitar <literal>sr-iov</literal> y crear
<literal>funciones virtuales</literal>.</para>
</listitem>
<listitem>
<para>Los argumentos del kernel que se usarán para habilitar las siguientes
funciones:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Parámetro</para></entry>
<entry align="left" valign="top"><para>Valor</para></entry>
<entry align="left" valign="top"><para>Descripción</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Aísla los núcleos 1-30 y 33-62.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Permite al kernel distribuir las interrupciones del temporizador entre las
CPU aisladas.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Permite al kernel ejecutar la marca del temporizador en una sola CPU cuando
el sistema está inactivo.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>El parámetro de arranque del kernel es la interfaz principal actual para
configurar dynticks completos junto con el aislamiento de la CPU.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Permite al kernel ejecutar las retrollamadas de RCU en una sola CPU cuando
el sistema está inactivo.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Permite al kernel ejecutar las interrupciones en una sola CPU cuando el
sistema está inactivo.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>Minimiza la latencia al salir del estado inactivo.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Permite usar vfio para las interfaces dpdk.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Habilita el uso de vfio para funciones virtuales.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Permite establecer el tamaño de las páginas enormes en 1 GB.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>El número de páginas enormes definidas anteriormente.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Valor predeterminado para habilitar las páginas enormes.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>Inhabilita el watchdog.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>Inhabilita el watchdog de NMI.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>Los siguientes servicios systemd se utilizan para habilitar lo siguiente:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> para sustituir automáticamente
<literal>BAREMETALHOST_UUID</literal> y <literal>node-name</literal> durante
el proceso de aprovisionamiento utilizando la información de Ironic.</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> para habilitar los núcleos de
aislamiento de la <literal>CPU</literal> (por ejemplo,
<literal>1-30,33-62</literal>).</para>
</listitem>
<listitem>
<para><literal>performance-settings.service</literal> para habilitar la
optimización del rendimiento de la CPU.</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> para instalar el chart de
Helm <literal>sriov</literal>. Espere hasta que se creen los recursos
personalizados y ejecute <literal>/var/sriov-auto-filler.sh</literal> para
reemplazar los valores en el mapa de configuración
<literal>sriov-custom-auto-config</literal> y crear el
<literal>sriovnetworknodepolicy</literal> que utilizarán las cargas de
trabajo.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>${RKE2_VERSION}</literal> es la versión de <literal>RKE2</literal>
que se usará en sustitución de este valor (por ejemplo,
<literal>v1.32.4+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<para>Con todos estos cambios mencionados, el bloque
<literal>RKE2ControlPlane</literal> del archivo
<literal>capi-provisioning-example.yaml</literal> tendrá el siguiente
aspecto:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-crd
                    targetNamespace: sriov-network-operator
                    version: 303.0.2+up1.5.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-network-operator
                    targetNamespace: sriov-network-operator
                    version: 303.0.2+up1.5.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Una vez creado el archivo uniendo los bloques anteriores, se debe ejecutar
el siguiente comando en el clúster de gestión para iniciar el
aprovisionamiento del nuevo clúster descendente utilizando las funciones de
telecomunicaciones:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>Registro privado</title>
<para>Es posible configurar un registro privado como duplicado de las imágenes
usadas por las cargas de trabajo.</para>
<para>Para ello, creamos el secreto que contiene la información sobre el registro
privado que usará el clúster descendente.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para>Los archivos <literal>tls.crt</literal>, <literal>tls.key</literal> y
<literal>ca.crt</literal> son los certificados que se usarán para autenticar
el registro privado. <literal>username</literal> y
<literal>password</literal> son las credenciales que se usarán para
autenticar el registro privado.</para>
<note>
<para>Los campos <literal>tls.crt</literal>, <literal>tls.key</literal>,
<literal>ca.crt</literal>, <literal>username</literal> y
<literal>password</literal> deben cifrarse en formato base64 antes de
utilizarse en el secreto.</para>
</note>
<para>Con todos estos cambios mencionados, el bloque
<literal>RKE2ControlPlane</literal> del archivo
<literal>capi-provisioning-example.yaml</literal> tendrá el siguiente
aspecto:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>Donde <literal>registry.example.com</literal> es el nombre de ejemplo del
registro privado que usará el clúster descendente, y debe sustituirse por
los valores reales.</para>
</section>
<section xml:id="airgap-deployment">
<title>Aprovisionamiento de clústeres descendentes en entornos aislados</title>
<para>El flujo de trabajo de aprovisionamiento de red dirigida permite automatizar
el aprovisionamiento de clústeres descendentes en entornos aislados.</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>Requisitos para entornos aislados</title>
<orderedlist numeration="arabic">
<listitem>
<para>La imagen <literal>raw</literal> generada con <literal>EIB</literal> debe
incluir las imágenes de contenedor específicas (OCI de chart de Helm e
imágenes de contenedor) necesarias para ejecutar el clúster descendente en
un entorno aislado. Para obtener más información, consulte la <xref
linkend="eib-edge-image-airgap"/>.</para>
</listitem>
<listitem>
<para>En caso de usar SR-IOV o cualquier otra carga de trabajo personalizada, las
imágenes necesarias para ejecutar las cargas de trabajo deben cargarse
previamente en su registro privado siguiendo las instrucciones de la <xref
linkend="preload-private-registry"/>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>Inscripción de los hosts bare metal en entornos aislados</title>
<para>El proceso para inscribir los hosts bare metal en el clúster de gestión es
el mismo que se describe en la <xref linkend="enroll-bare-metal-host"/>.</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>Aprovisionamiento del clúster descendente en entornos aislados</title>
<para>Hay algunos cambios importantes que se deben realizar para aprovisionar el
clúster descendente en entornos aislados:</para>
<orderedlist numeration="arabic">
<listitem>
<para>El bloque <literal>RKE2ControlPlane</literal> del archivo
<literal>capi-provisioning-example.yaml</literal> debe incluir la directiva
<literal>spec.agentConfig.airGapped: true</literal>.</para>
</listitem>
<listitem>
<para>La configuración del registro privado debe incluirse en el bloque
<literal>RKE2ControlPlane</literal> del archivo
<literal>capi-provisioning-airgap-example.yaml</literal>, después de la
sección del registro privado (<xref linkend="atip-private-registry"/>).</para>
</listitem>
<listitem>
<para>Si usa SR-IOV o cualquier otra configuración
<literal>AdditionalUserData</literal> (guion de Combustion) que requiera la
instalación del chart de Helm, debe modificar el contenido para que haga
referencia al registro privado en lugar de utilizar el registro público.</para>
</listitem>
</orderedlist>
<para>El siguiente ejemplo muestra la configuración SR-IOV del bloque
<literal>AdditionalUserData</literal> del archivo
<literal>capi-provisioning-airgap-example.yaml</literal> con las
modificaciones necesarias para hacer referencia al registro privado.</para>
<itemizedlist>
<listitem>
<para>Referencias de secretos del registro privado</para>
</listitem>
<listitem>
<para>Definición del chart de Helm que usa el registro privado en lugar de las
imágenes OCI públicas</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 303.0.2+up1.5.0
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 303.0.2+up1.5.0
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=1800
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --timeout=30m --all ; do sleep 10 ; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>Acciones del ciclo de vida</title>
<para>Esta sección describe las acciones de gestión del ciclo de vida de los
clústeres desplegados con SUSE Edge for Telco.</para>
<section xml:id="id-management-cluster-upgrades">
<title>Actualizaciones del clúster de gestión</title>
<para>La actualización del clúster de gestión implica varios componentes. Para
obtener una lista de los componentes generales que requieren una
actualización, consulte la documentación sobre el clúster de gestión de
<literal>día 2</literal> (<xref linkend="day2-mgmt-cluster"/>).</para>
<para>El procedimiento de actualización de los componentes específicos de esta
configuración se indica a continuación.</para>
<para><emphasis role="strong">Actualización de
Metal<superscript>3</superscript></emphasis></para>
<para>Para actualizar <literal>Metal<superscript>3</superscript></literal>, use el
comando siguiente para actualizar la caché del repositorio de Helm y obtener
el chart más reciente para instalar
<literal>Metal<superscript>3</superscript></literal> desde el repositorio de
charts de Helm:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>Después, la forma más fácil de actualizar es exportar las configuraciones
actuales a un archivo y, a continuación, actualizar la versión de
<literal>Metal<superscript>3</superscript></literal> utilizando ese archivo
anterior. Si es necesario realizar algún cambio en la nueva versión, el
archivo se puede editar antes de la actualización.</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=303.0.7+up0.11.5</screen>
</section>
<section xml:id="atip-lifecycle-downstream">
<title>Actualización de clústeres descendentes</title>
<para>Para actualizar los clústeres descendentes hay que actualizar varios
componentes. Las siguientes secciones describen el proceso de actualización
de cada uno de los componentes.</para>
<para><emphasis role="strong">Actualización del sistema operativo</emphasis></para>
<para>Para este proceso, consulte la siguiente referencia (<xref
linkend="eib-edge-image-connected"/>) para crear la nueva imagen con una
nueva versión del sistema operativo. Con esta nueva imagen generada por
<literal>EIB</literal>, la siguiente fase de aprovisionamiento utiliza la
nueva versión operativa proporcionada. En el siguiente paso, la nueva imagen
se utiliza para actualizar los nodos.</para>
<para><emphasis role="strong">Actualización del clúster RKE2</emphasis></para>
<para>Los cambios necesarios para actualizar el clúster <literal>RKE2</literal>
utilizando el flujo de trabajo automatizado son los siguientes:</para>
<itemizedlist>
<listitem>
<para>Cambie el bloque <literal>RKE2ControlPlane</literal> en el archivo
<literal>capi-provisioning-example.yaml</literal> que se muestra en la
siguiente sección (<xref linkend="single-node-provision"/>):</para>
<itemizedlist>
<listitem>
<para>Especifique la estrategia <literal>rolloutStrategy</literal> deseada.</para>
</listitem>
<listitem>
<para>Cambie la versión del clúster <literal>RKE2</literal> a la nueva versión
sustituyendo <literal>${RKE2_NEW_VERSION}</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  version: ${RKE2_NEW_VERSION}
  replicas: 1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>Cambie el bloque <literal>Metal3MachineTemplate</literal> del archivo
<literal>capi-provisioning-example.yaml</literal> que se muestra en la
siguiente sección (<xref linkend="single-node-provision"/>):</para>
<itemizedlist>
<listitem>
<para>Cambie el nombre de la imagen y la suma de comprobación a la nueva versión
generada en el paso anterior.</para>
</listitem>
<listitem>
<para>En la directiva <literal>nodeReuse</literal> defina el valor
<literal>true</literal> para evitar crear un nuevo nodo.</para>
</listitem>
<listitem>
<para>En la directiva <literal>automatedCleaningMode</literal> defina
<literal>metadata</literal> para habilitar la limpieza automática del nodo.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>Después de realizar estos cambios, el archivo
<literal>capi-provisioning-example.yaml</literal> se puede aplicar al
clúster con este comando:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-troubleshooting-3">
<title>Solución de problemas</title>
<partintro>
<para>En esta sección se proporcionan instrucciones para diagnosticar y resolver
problemas comunes relacionados con los despliegues y operaciones de SUSE
Edge. Abarca diversos temas y ofrece pasos específicos para la resolución de
problemas de componentes, herramientas clave y la ubicación de los registros
relevantes.</para>
</partintro>
<chapter xml:id="general-troubleshooting-principles">
<title>Principios generales para resolver problemas</title>
<para>Antes de profundizar en cuestiones específicas de los componentes, tenga en
cuenta estos principios generales:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Consulte los registros</emphasis>: los registros son
la principal fuente de información. La mayoría de las veces, los errores se
explican por sí mismos y contienen pistas sobre lo que ha fallado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Compruebe los relojes</emphasis>: la diferencia
horaria entre sistemas pueden provocar todo tipo de errores. Asegúrese de
que los relojes estén sincronizados. Se puede indicar a EIB que fuerce la
sincronización del reloj al arrancar el sistema. Consulte la sección sobre
cómo configurar la hora del sistema operativo (<xref
linkend="quickstart-eib"/>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas de arranque</emphasis>: si el sistema se
bloquea durante el arranque, anote los últimos mensajes que se
muestran. Acceda al panel de control (físico o a través de BMC) para leer
los mensajes de arranque.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas de red</emphasis>: verifique la
configuración de la interfaz de red (<literal>ip a</literal>), la tabla de
enrutamiento (<literal>ip route</literal>), pruebe la conectividad desde y
hacia otros nodos y servicios externos (<literal>ping</literal>,
<literal>nc</literal>). Asegúrese de que las reglas del cortafuegos no estén
bloqueando los puertos necesarios.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique el estado de los componentes</emphasis>:
use <literal>kubectl get</literal> y <literal>kubectl describe</literal>
para los recursos de Kubernetes. Use <literal>kubectl get events
--sort-by='.lastTimestamp' -n &lt;espaciodenombres&gt;</literal> para ver
los eventos de un espacio de nombres concreto de Kubernetes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique el estado de los servicios</emphasis>: use
<literal>systemctl status &lt;servicio&gt;</literal> para los servicios
systemd.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Compruebe la sintaxis</emphasis>: el software espera
una estructura y sintaxis determinadas en los archivos de
configuración. Para los archivos yaml, por ejemplo, utilice
<literal>yamllint</literal> o herramientas similares para verificar que la
sintaxis sea correcta.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Aísle el problema</emphasis>: intente delimitar el
problema a un componente o capa específicos (por ejemplo, la red, el
almacenamiento, el sistema operativo, Kubernetes,
Metal<superscript>3</superscript>, Ironic, etc.​).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Documentación</emphasis>: consulte siempre la <link
xl:href="https://documentation.suse.com/suse-edge/">documentación oficial de
SUSE Edge</link> y la documentación original para obtener información
detallada.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Versiones</emphasis>: SUSE Edge es una versión
rigurosa y exhaustivamente probada de diferentes componentes de SUSE. Las
versiones de cada componente en cada versión de SUSE Edge se pueden
consultar en la <link
xl:href="https://documentation.suse.com/suse-edge/support-matrix/html/support-matrix/index.html">matriz
de asistencia de SUSE Edge</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas conocidos</emphasis>: en las notas de cada
versión de SUSE Edge hay una sección titulada "Problemas conocidos" que
contiene información sobre problemas que se solucionarán en futuras
versiones, pero que pueden afectar a la actual.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="troubleshooting-kiwi">
<title>Resolución de problemas de Kiwi</title>
<para>Kiwi se usa para generar imágenes actualizadas de SUSE Linux Micro que se
utilizarán con Edge Image Builder.</para>
<itemizedlist>
<title>Problemas comunes</title>
<listitem>
<para><emphasis role="strong">La versión de SL Micro no coincide</emphasis>: la
versión del sistema operativo del host debe coincidir con la del sistema
operativo que se está creando (host SL Micro 6.0 → imagen SL Micro 6.0).</para>
</listitem>
<listitem>
<para><emphasis role="strong">SELinux en estado forzado</emphasis>: debido a
ciertas limitaciones, es necesario inhabilitar SELinux temporalmente para
poder crear imágenes con Kiwi. Compruebe el estado de SELinux con
<literal>getenforce</literal> e inhabilítelo antes de ejecutar el proceso de
creación con <literal>setenforce 0</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Host de creación no registrado</emphasis>: el
proceso de creación utiliza las suscripciones del host de creación para
poder extraer paquetes del Centro de servicios al cliente de SUSE. Si el
host no está registrado, se produce un error.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Error en la prueba del dispositivo de
bucle</emphasis>: la primera vez que se ejecuta el proceso de creación de
Kiwi, falla poco después de iniciarse con el mensaje "ERROR: Early loop
device test failed, please retry the container run" (Error: la prueba
inicial del dispositivo de bucle ha fallado. Intente ejecutar el contenedor
de nuevo.). Esto se debe a que se están creando dispositivos de bucle en el
sistema host subyacente que no son visibles inmediatamente dentro de la
imagen del contenedor. Vuelva a ejecutar el proceso de creación de Kiwi y
debería continuar sin problemas.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Faltan permisos</emphasis>: se espera que sea el
usuario root quien ejecute el proceso de creación (o que se haga mediante
sudo).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Privilegios incorrectos</emphasis>: el proceso de
creación espera el indicador <literal>--privileged</literal> al ejecutar el
contenedor. Compruebe que está presente.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros del contenedor de creación</emphasis>:
compruebe los registros del contenedor de creación. Los registros se generan
en el directorio que se utilizó para almacenar los artefactos. Compruebe
también los registros de Docker o Podman para obtener la información
necesaria.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Directorios temporales de creación</emphasis>: Kiwi
crea directorios temporales durante el proceso de creación. Compruebe esos
directorios en busca de registros intermedios o artefactos si el resultado
principal es insuficiente.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Pasos para resolver problemas</title>
<listitem>
<para><emphasis role="strong">Revise el resultado de
<literal>build-image</literal></emphasis>: el mensaje de error que aparece
en el panel de control suele ser muy explicativo.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Compruebe el entorno de creación</emphasis>:
asegúrese de que se cumplen todos los requisitos previos para Kiwi (por
ejemplo, docker/podman, SElinux, espacio suficiente en disco) en el equipo
donde se ejecuta.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Inspeccione los registros del contenedor de
creación</emphasis>: revise los registros del contenedor que ha fallado para
obtener información más detallada sobre los errores (véase más arriba).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique el archivo de definición</emphasis>: si
usa un archivo de definición de imagen de Kiwi personalizado, compruebe que
no haya errores ortográficos ni sintácticos en el archivo.</para>
</listitem>
</orderedlist>
<note>
<para>Consulte la <link
xl:href="https://documentation.suse.com/appliance/kiwi-9/html/kiwi/troubleshooting.html">guía
de resolución de problemas de Kiwi</link>.</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-image-builder">
<title>Resolución de problemas de Edge Image Builder (EIB)</title>
<para>EIB se usa para crear imágenes personalizadas de SUSE Edge.</para>
<itemizedlist>
<title>Problemas comunes</title>
<listitem>
<para><emphasis role="strong">Código erróneo del SCC</emphasis>: asegúrese de que
el código del Centro de servicios al cliente de SUSE utilizado en el archivo
de definición de EIB coincida con la versión y la arquitectura de SL Micro.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Faltan dependencias</emphasis>: asegúrese de que no
falte ningún paquete o herramienta en el entorno de creación.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Tamaño de imagen incorrecto</emphasis>: para las
imágenes raw, se requiere el parámetro <literal>diskSize</literal>, que
depende en gran medida de las imágenes, los RPM y otros artefactos incluidos
en la imagen.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Permisos</emphasis>: si almacena un guion en el
directorio custom/files, asegúrese de que tenga permisos de ejecución, ya
que esos archivos solo están disponibles cuando se ejecuta Combustion, pero
EIB no realiza ningún cambio.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Dependencias del grupo del sistema
operativo</emphasis>: al crear una imagen con usuarios y grupos
personalizados, los grupos que se definan como
<literal>primaryGroup</literal> deben crearse explícitamente.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Las claves SSH de los usuarios del sistema operativo
requieren una carpeta de inicio</emphasis>: al crear una imagen con usuarios
con claves SSH, también es necesario crear la carpeta de inicio con
<literal>createHomeDir=true</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas de Combustion</emphasis>: EIB utiliza
Combustion para la personalización del sistema operativo y el despliegue de
todos los demás componentes de SUSE Edge. Esto también incluye guiones
personalizados que se colocan en la carpeta custom/scripts. Tenga en cuenta
que el proceso Combustion se ejecuta en el momento de
<literal>initrd</literal>, por lo que el sistema no está completamente
arrancado cuando se ejecutan los guiones.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Tamaño de la máquina de Podman</emphasis>: como se
explica en la sección de consejos y trucos de EIB (<xref
linkend="tips-and-tricks"/>), compruebe que la máquina de Podman tenga
suficiente CPU/memoria para ejecutar el contenedor de EIB en sistemas
operativos que no sean Linux.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Salida de EIB</emphasis>: el resultado del panel de
control del comando <literal>eib build</literal> es fundamental.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros del contenedor de creación</emphasis>:
compruebe los registros del contenedor de creación. Los registros se generan
en el directorio que se utilizó para almacenar los artefactos. Compruebe
también los registros de <literal>Docker</literal> o
<literal>Podman</literal> para obtener la información necesaria.</para>
<note>
<para>Para obtener más información, consulte la sección <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Debugging</link>
(Depuración).</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Directorios temporales de creación</emphasis>: EIB
crea directorios temporales durante el proceso de creación. Compruebe si hay
registros intermedios o artefactos en ellos si el resultado principal es
insuficiente.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros de Combustion</emphasis>: si la imagen que
se está creando con EIB no arranca por cualquier motivo, hay disponible una
shell raíz. Conéctese al panel de control del host (ya sea físicamente, a
través de BMC, etc.) y compruebe los registros de Combustion con
<literal>journalctl -u combustion</literal> y, en general, todos los
registros del sistema operativo con <literal>journalctl</literal> para
encontrar la causa raíz del fallo.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Pasos para resolver problemas</title>
<listitem>
<para><emphasis role="strong">Revise el resultado de
<literal>eib-build</literal></emphasis>: el mensaje de error que aparece en
el panel de control suele ser muy explicativo.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Compruebe el entorno de creación</emphasis>:
asegúrese de que se cumplen todos los requisitos previos de EIB (por
ejemplo, docker/podman, espacio suficiente en disco) en el equipo donde se
ejecuta EIB.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Inspeccione los registros del contenedor de
creación</emphasis>: revise los registros del contenedor que ha fallado para
obtener información más detallada sobre los errores (véase más arriba).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique la configuración de
<literal>eib</literal></emphasis>: compruebe bien el archivo de
configuración <literal>eib</literal> para detectar posibles errores
ortográficos o rutas incorrectas a los archivos de origen o a los guiones de
creación.</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Pruebe los componentes individualmente</emphasis>:
si su versión EIB incluye guiones o etapas personalizados, ejecútelos de
forma independiente para aislar los fallos.</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>Consulte la sección <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Edge
Image Builder Debugging</link> (Depuración de Edge Image Builder).</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-networking">
<title>Resolución de problemas de redes periféricas (NMC)</title>
<para>NMC se inyecta en las imágenes de EIB de SL Micro para configurar la red de
los hosts de Edge en el momento del arranque mediante Combustion. También se
ejecuta en el flujo de trabajo Metal3 como parte del proceso de
inspección. Pueden surgir problemas cuando se arranca el host por primera
vez o durante el proceso de inspección de Metal3.</para>
<itemizedlist>
<title>Problemas comunes</title>
<listitem>
<para><emphasis role="strong">El host no puede arrancar correctamente la primera
vez</emphasis>: los archivos de definición de red que no están bien formados
pueden provocar que la fase de Combustion falle y que el host pierda el
control de la shell raíz.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Los archivos no se han generado
correctamente</emphasis>: asegúrese de que los archivos de red cumplan el
formato <link xl:href="https://nmstate.io/examples.html">NMState</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Las interfaces de red no están configuradas
correctamente</emphasis>: asegúrese de que las direcciones MAC coincidan con
las interfaces que se utilizan en el host.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Los nombres no coinciden entre las
interfaces</emphasis>: el argumento del kernel
<literal>net.ifnames=1</literal> permite un <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">esquema
de nomenclatura predecible para interfaces de red</link>, por lo que ya no
existe <literal>eth0</literal>, sino otros esquemas de nomenclatura como
<literal>enp2s0</literal>.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros de Combustion</emphasis>: dado que nmc se
utiliza al mismo tiempo que Combustion, compruebe los registros de
Combustion con <literal>journalctl -u combustion</literal> en el host que se
está aprovisionando.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros de NetworkManager</emphasis>: en el flujo
de trabajo de despliegue de Metal<superscript>3</superscript>, nmc forma
parte de la ejecución de IPA y se ejecuta como dependencia del servicio
NetworkManager usando la funcionalidad ExecStartPre de systemd. Compruebe
los registros de NetworkManager en el host de IPA con <literal>journalctl -u
NetworkManager</literal> (consulte la sección de resolución de problemas de
aprovisionamiento de red dirigida (<xref
linkend="troubleshooting-directed-network-provisioning"/>) para comprender
cómo acceder al host cuando se inicia con IPA).</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Pasos para resolver problemas</title>
<listitem>
<para><emphasis role="strong">Verifique la sintaxis de YAML</emphasis>: los
archivos de configuración nmc son archivos YAML. Compruebe que la sintaxis
sea correcta con <literal>yamllint</literal> o herramientas similares.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Ejecute nmc manualmente</emphasis>: dado que nmc
forma parte del contenedor EIB, para depurar cualquier problema se puede
utilizar un comando de Podman local.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Cree una carpeta temporal para almacenar los archivos de nmc.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p ${HOME}/tmp/foo</screen>
</listitem>
<listitem>
<para>Guarde los archivos de nmc en esa ubicación.</para>
<screen language="shell" linenumbering="unnumbered">❯ tree --noreport ${HOME}/tmp/foo
/Users/johndoe/tmp/foo
├── host1.example.com.yaml
└── host2.example.com.yaml</screen>
</listitem>
<listitem>
<para>Ejecute el contenedor de EIB con nmc como punto de entrada y el comando
generate para realizar las mismas tareas que nmc haría al mismo tiempo que
Combustion:</para>
<screen language="shell" linenumbering="unnumbered">podman run -it --rm -v ${HOME}/tmp/foo:/tmp/foo:Z --entrypoint=/usr/bin/nmc registry.suse.com/edge/3.3/edge-image-builder:1.2.0 generate --config-dir /tmp/foo --output-dir /tmp/foo/

[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host2.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host1.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc] Successfully generated and stored network config</screen>
</listitem>
<listitem>
<para>Observe los registros y archivos que se generan en la carpeta temporal.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-phone-home-scenarios">
<title>Resolución de problemas en escenarios "phone home"</title>
<para>En los escenarios "phone home" se usa Elemental para volver a conectarse al
clúster de gestión y EIB a fin de crear una imagen del sistema operativo que
incluya los bits de registro de Elemental. Pueden surgir problemas cuando se
arranca el host por primera vez, durante el proceso de creación de EIB o al
intentar registrarse en el clúster de gestión.</para>
<itemizedlist>
<title>Problemas comunes</title>
<listitem>
<para><emphasis role="strong">El sistema no se registra</emphasis>: el nodo no se
registra en la interfaz del usuario. Asegúrese de que el host se haya
arrancado correctamente y pueda comunicarse con Rancher, que el reloj esté
sincronizado y que los servicios de Elemental funcionen correctamente.</para>
</listitem>
<listitem>
<para><emphasis role="strong">El sistema no se puede aprovisionar</emphasis>: el
nodo está registrado, pero no se puede aprovisionar. Asegúrese de que el
host pueda comunicarse con Rancher, que el reloj esté sincronizado y que los
servicios de Elemental funcionen correctamente.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros del sistema</emphasis>:
<literal>journalctl</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros de elemental-system-agent</emphasis>:
<literal>journalctl -u elemental-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros de K3s/RKE2</emphasis>:
<literal>journalctl -u k3s o journalctl -u rke2-server</literal> (o
<literal>rke2-agent</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">Pod del operador de Elemental</emphasis>:
<literal>kubectl logs -n cattle-elemental-system -l
app=elemental-operator</literal></para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Pasos para resolver problemas</title>
<listitem>
<para><emphasis role="strong">Consulte los registros</emphasis>: compruebe los
registros del pod del operador de Elemental para ver si hay algún
problema. Compruebe los registros del host si el nodo se ha arrancado.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Compruebe MachineRegistration y TPM</emphasis>: de
forma predeterminada, TPM se utiliza para la <link
xl:href="https://elemental.docs.rancher.com/authentication/">autenticación</link>,
pero existen alternativas para hosts sin TPM.</para>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-directed-network-provisioning">
<title>Resolución de problemas de aprovisionamiento de red dirigida</title>
<para>En el aprovisionamiento de red dirigida se usan
Metal<superscript>3</superscript> y CAPI para aprovisionar el clúster
descendente. También se usa EIB para crear una imagen del sistema
operativo. Pueden surgir problemas cuando se arranca el host por primera vez
o durante los procesos de inspección o aprovisionamiento.</para>
<itemizedlist>
<title>Problemas comunes</title>
<listitem>
<para><emphasis role="strong">Firmware antiguo</emphasis>: compruebe que todo el
firmware de los hosts físicos que se utilizan esté actualizado. Esto incluye
el firmware de BMC, ya que en ocasiones Metal<superscript>3</superscript>
<link
xl:href="https://book.metal3.io/bmo/supported_hardware#redfish-and-its-variants">requiere
un firmware específico/actualizado</link>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">El aprovisionamiento falla con errores de
SSL</emphasis>: si el servidor Web que proporciona las imágenes utiliza
https, Metal<superscript>3</superscript> debe configurarse para inyectar y
confiar en el certificado de la imagen IPA. Consulte la carpeta de
Kubernetes (<xref linkend="mgmt-cluster-kubernetes-folder"/>) para obtener
información sobre cómo incluir un archivo
<literal>ca-additional.crt</literal> en el chart de
Metal<superscript>3</superscript>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas con los certificados al arrancar los hosts
con IPA</emphasis>: algunos proveedores de servidores verifican la conexión
SSL al conectar imágenes ISO de medios virtuales al BMC, lo que puede causar
un problema porque los certificados generados para el despliegue de Metal3
son autofirmados. Puede ocurrir que el host se esté arrancando, pero caiga a
una shell UEFI. Consulte la sección sobre cómo inhabilitar TLS para la
conexión de ISO de medios virtuales (<xref
linkend="disabling-tls-for-virtualmedia-iso-attachment"/>) para averiguar
cómo solucionar este problema.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Referencia de nombre o etiqueta
incorrecta</emphasis>: si el clúster hace referencia a un nodo con un nombre
o etiqueta incorrectos, el clúster se despliega correctamente, pero el BMH
permanece con el estado "Available" (Disponible). Compruebe bien las
referencias de los objetos involucrados para los BMH.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas de comunicación de BMC</emphasis>:
asegúrese de que los pods de Metal<superscript>3</superscript> que se
ejecutan en el clúster de gestión puedan acceder al BMC de los hosts que se
están aprovisionando (normalmente, la red de BMC está muy restringida).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Estado incorrecto del host bare metal</emphasis>: el
objeto BMH pasa por diferentes estados (inspección, preparación,
aprovisionamiento, etc.) durante su ciclo de vida <link
xl:href="https://book.metal3.io/bmo/state_machine">Lifestyle of State
machine</link> (Ciclo de vida de la máquina de estados). Si se detecta un
estado incorrecto, compruebe el campo <literal>status</literal> del objeto
BMH, ya que contiene más información, como <literal>kubectl get bmh
&lt;nombre&gt; -o jsonpath=’{.status}’| jq</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">El host no se ha desaprovisionado</emphasis>: en
caso de que falle el desaprovisionamiento de un host, se puede intentar
eliminarlo después de añadir la anotación "detached" al objeto BMH de la
siguiente manera: <literal>kubectl annotate bmh/&lt;BMH&gt;
baremetalhost.metal3.io/detached=””</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Errores de imagen</emphasis>: compruebe que la
imagen que se está creando con EIB para el clúster descendente esté
disponible, tenga una suma de comprobación correcta y no sea demasiado
grande para descomprimirla ni para el disco.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Discrepancia en el tamaño del disco</emphasis>: de
forma predeterminada, el disco no se expandirá hasta ocupar todo el espacio
disponible. Como se explica en la sección del guion Growfs (<xref
linkend="growfs-script"/>), es necesario incluir un guion growfs en la
imagen que se está creando con EIB para los hosts del clúster descendente.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Proceso de limpieza bloqueado</emphasis>: el proceso
de limpieza se reintenta varias veces. Si debido a un problema con el host
ya no es posible realizar la limpieza, inhabilite primero la limpieza
definiendo en el campo <literal>automatedCleanMode</literal> el valor
<literal>disabled</literal> en el objeto BMH.</para>
<warning>
<para>No se recomienda eliminar manualmente el finalizador cuando el proceso de
limpieza tarda más de lo deseado o falla. Si se hiciera, se eliminaría el
registro del host de Kubernetes, pero se dejaría en Ironic. La acción que se
está ejecutando actualmente continuaría en segundo plano, y es posible que
el intento de volver a añadir el host fallara debido al conflicto.</para>
</warning>
</listitem>
<listitem>
<para><emphasis role="strong">Problemas en los pods de Metal3/Rancher
Turtles/CAPI</emphasis>: el flujo de despliegue para todos los componentes
necesarios es:</para>
<itemizedlist>
<listitem>
<para>El controlador de Rancher Turtles despliega el controlador del operador de
CAPI.</para>
</listitem>
<listitem>
<para>A continuación, el controlador del operador de CAPI despliega los
controladores del proveedor (plano de control/arranque del núcleo de CAPI,
CAPM3 y RKE2).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Compruebe que todos los pods funcionen correctamente y, si no fuera así,
revise los registros.</para>
<itemizedlist>
<title>Registros</title>
<listitem>
<para><emphasis role="strong">Registros de
Metal<superscript>3</superscript></emphasis>: consulte los registros de los
distintos pods.</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n metal3-system -l app.kubernetes.io/component=baremetal-operator
kubectl logs -n metal3-system -l app.kubernetes.io/component=ironic</screen>
<note>
<para>El pod metal3-ironic contiene al menos 4 contenedores distintos
(<literal>ironic-httpd</literal>,` ironic-log-watch`,
<literal>ironic</literal> e <literal>ironic-ipa-downloader</literal> (init))
en el mismo pod. Use el indicador <literal>-c</literal> cuando utilice
<literal>kubectl logs</literal> para verificar los registros de cada
contenedor.</para>
</note>
<note>
<para>El contenedor <literal>ironic-log-watch</literal> expone los registros del
panel de control de los hosts tras la inspección o el aprovisionamiento,
siempre que la conectividad de red permita enviar estos registros al clúster
de gestión. Esto puede resultar útil en casos en los que se producen errores
de aprovisionamiento, pero no se tiene acceso directo a los registros del
panel de control de BMC.</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Registros de Rancher Turtles</emphasis>: consulte
los registros de los diferentes pods.</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n rancher-turtles-system -l control-plane=controller-manager
kubectl logs -n rancher-turtles-system -l app.kubernetes.io/name=cluster-api-operator
kubectl logs -n rke2-bootstrap-system -l cluster.x-k8s.io/provider=bootstrap-rke2
kubectl logs -n rke2-control-plane-system -l cluster.x-k8s.io/provider=control-plane-rke2
kubectl logs -n capi-system -l cluster.x-k8s.io/provider=cluster-api
kubectl logs -n capm3-system -l cluster.x-k8s.io/provider=infrastructure-metal3</screen>
</listitem>
<listitem>
<para><emphasis role="strong">Registros de BMC</emphasis>: por lo general, los BMC
tienen una interfaz de usuario en la que se puede realizar la mayor parte de
la interacción. Suele haber una sección de "registros" en la que se pueden
observar posibles problemas (imposibilidad de acceder a la imagen, fallos de
hardware, etc.).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros del panel de control</emphasis>: conéctese
al panel de control de BMC (a través de la interfaz Web de BMC, en serie,
etc.) y compruebe si hay errores en los registros que se están escribiendo.</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Pasos para resolver problemas</title>
<listitem>
<para><emphasis role="strong">Compruebe el estado de
<literal>BareMetalHost</literal></emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>kubectl get bmh -A</literal> muestra el estado actual, que puede
ser <literal>provisioning</literal>, <literal>ready</literal>,
<literal>error</literal> o <literal>registering</literal>.</para>
</listitem>
<listitem>
<para><literal>kubectl describe bmh -n &lt;espaciodenombres&gt;
&lt;nombre_bmh&gt;</literal> proporciona información detallada sobre los
eventos y condiciones que explican por qué un BMH podría estar bloqueado.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Pruebe la conectividad de Redfish</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Use <literal>curl</literal> desde el plano de control de
Metal<superscript>3</superscript> para comprobar la conectividad con los BMC
a través de Redfish.</para>
</listitem>
<listitem>
<para>Asegúrese de que se proporcionan las credenciales de BMC correctas en la
definición de <literal>BareMetalHost-Secret</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique el estado del pod de
turtles/CAPI/metal3</emphasis>: asegúrese de que los contenedores del
clúster de gestión estén activos y en ejecución: <literal>kubectl get pods
-n metal3-system</literal> y <literal>kubectl get pods -n
rancher-turtles-system</literal> (consulte también
<literal>capi-system</literal>, <literal>capm3-system</literal>,
<literal>rke2-bootstrap-system</literal> y
<literal>rke2-control-plane-system</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique que se pueda acceder al punto final de
Ironic desde el host que se está aprovisionando</emphasis>: el host que se
está aprovisionando debe poder acceder al punto final de Ironic para
informar a Metal<superscript>3</superscript>. Compruebe la IP con
<literal>kubectl get svc -n metal3-system metal3-metal3-ironic</literal> e
intente acceder a ella mediante <literal>curl/nc</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique que se pueda acceder a la imagen IPA desde
el BMC</emphasis>: el punto final Ironic proporciona la imagen IPA, a la que
se debe poder acceder desde el BMC, ya que se utiliza como un CD virtual.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Verifique que se pueda acceder a la imagen del
sistema operativo desde el host que se está aprovisionando</emphasis>: debe
poder accederse a la imagen que se utiliza para aprovisionar el host desde
el propio host (cuando se ejecuta IPA), ya que se descargará temporalmente y
se escribirá en el disco.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Examine los registros de componentes de
Metal<superscript>3</superscript></emphasis>: consulte las secciones
anteriores.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Vuelva a lanzar la inspección de BMH</emphasis>: si
una inspección falla o cambia el hardware de un host disponible, se puede
iniciar un nuevo proceso de inspección anotando el objeto BMH con
<literal>inspect.metal3.io: ""</literal>. Consulte la guía de <link
xl:href="https://book.metal3.io/bmo/inspect_annotation">inspección de
control de Metal<superscript>3</superscript></link> para obtener más
información.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Panel de control IPA bare metal</emphasis>: para
solucionar problemas relacionados con IPA, existen varias alternativas:</para>
<itemizedlist>
<listitem>
<para>Habilite el "inicio de sesión automático". Esto permite que el usuario root
inicie sesión automáticamente al conectarse al panel de control de IPA.</para>
<warning>
<para>Esto es solo para fines de depuración, ya que proporciona acceso completo al
host.</para>
</warning>
<para>Para habilitar el inicio de sesión automático, el valor de helm
<literal>global.ironicKernelParams</literal> de Metal3 debe tener el
siguiente aspecto: <literal>console=ttyS0 suse.autologin=ttyS0</literal>
(dependiendo del panel de control, <literal>ttyS0</literal> puede
cambiarse). A continuación, se debe realizar un nuevo despliegue del chart
de Metal<superscript>3</superscript>. (Nota: <literal>ttyS0</literal> es un
ejemplo, debe coincidir con el terminal real, por ejemplo, puede ser
<literal>tty1</literal> en muchos casos en bare metal. Esto se puede
verificar mirando la salida del panel de control desde el ramdisk de IPA al
arrancar, donde <literal>/etc/issue</literal> produce el nombre del panel de
control).</para>
<para>Otra forma de hacerlo es cambiando el parámetro
<literal>IRONIC_KERNEL_PARAMS</literal> en el mapa de configuración
<literal>ironic-bmo</literal> del espacio de nombres
<literal>metal3-system</literal>. Esto puede resultar más sencillo, ya que
se puede hacer editando <literal>kubectl</literal>, pero se sobrescribirá al
actualizar el chart. A continuación, es necesario reiniciar el pod de
Metal<superscript>3</superscript> con <literal>kubectl delete pod -n
metal3-system -l app.kubernetes.io/component=ironic</literal>.</para>
</listitem>
<listitem>
<para>Inyecte una clave ssh para el usuario root en el IPA.</para>
<warning>
<para>Esto es solo para fines de depuración, ya que proporciona acceso completo al
host.</para>
</warning>
<para>Para inyectar la clave ssh para el usuario root, se debe utilizar el valor
de helm <literal>debug.ironicRamdiskSshKey</literal> de
Metal<superscript>3</superscript>. A continuación, se debe realizar un nuevo
despliegue del chart de Metal<superscript>3</superscript>.</para>
<para>Otra forma de hacerlo es cambiando el parámetro
<literal>IRONIC_RAMDISK_SSH_KEY</literal> en el mapa de configuración
<literal>ironic-bmo</literal> del espacio de nombres
<literal>metal3-system</literal>. Esto puede resultar más sencillo, ya que
se puede hacer editando <literal>kubectl</literal>, pero se sobrescribirá al
actualizar el chart. A continuación, es necesario reiniciar el pod de
Metal<superscript>3</superscript> con <literal>kubectl delete pod -n
metal3-system -l app.kubernetes.io/component=ironic</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>Consulte las guías de <link
xl:href="https://cluster-api.sigs.k8s.io/user/troubleshooting">resolución de
problemas de CAPI</link> y de <link
xl:href="https://book.metal3.io/troubleshooting">resolución de problemas de
Metal<superscript>3</superscript></link>.</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-other-components">
<title>Resolución de problemas de otros componentes</title>
<para>Es posible consultar otras guías de resolución de problemas de componentes
de SUSE Edge en su documentación oficial respectiva:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://documentation.suse.com/smart/micro-clouds/html/SLE-Micro-5.5-admin/index.html#id-1.10">Resolución
de problemas de SUSE Linux Micro</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.rke2.io/known_issues">Problemas conocidos de
RKE2</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.k3s.io/known-issues">Problemas conocidos de
K3s</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/troubleshooting/general-troubleshooting">Resolución
de problemas generales de Rancher</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://documentation.suse.com/multi-linux-manager/5.1/en/docs/administration/troubleshooting/tshoot-intro.html">Resolución
de problemas de SUSE Multi-Linux Manager</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://elemental.docs.rancher.com/troubleshooting-support/">Asistencia
de Elemental</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://turtles.docs.rancher.com/turtles/stable/en/troubleshooting/troubleshooting.html">Resolución
de problemas de Rancher Turtles</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://longhorn.io/docs/1.8.1/troubleshoot/troubleshooting/">Resolución
de problemas de Longhorn</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://open-docs.neuvector.com/next/troubleshooting/troubleshooting/">Resolución
de problemas de Neuvector</link></para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/troubleshooting">Resolución de
problemas de Fleet</link></para>
</listitem>
</itemizedlist>
<para>También puede consultar la <link
xl:href="https://www.suse.com/support/kb/">Base de conocimientos de
SUSE</link>.</para>
</chapter>
<chapter xml:id="collecting-diagnostics-for-support">
<title>Recopilación de diagnóstico para la asistencia</title>
<para>Al ponerse en contacto con el servicio de asistencia técnica de SUSE, es
fundamental proporcionar información de diagnóstico completa.</para>
<itemizedlist>
<title>Información esencial que se debe recopilar</title>
<listitem>
<para><emphasis role="strong">Descripción detallada del problema</emphasis>: ¿Qué
ha ocurrido?, ¿cuándo?, ¿qué estaba haciendo?, ¿cuál es el comportamiento
esperado?, y ¿cuál es el comportamiento real?</para>
</listitem>
<listitem>
<para><emphasis role="strong">Pasos para reproducir el problema</emphasis>: ¿Puede
reproducir el problema de forma fiable? Si es así, indique los pasos
exactos.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Versiones de los componentes</emphasis>: Versión de
SUSE Edge, versiones de los componentes (RKE2/K3, EIB,
Metal<superscript>3</superscript>, Elemental, etc.).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Registros relevantes</emphasis>:</para>
<itemizedlist>
<listitem>
<para>Resultado de <literal>journalctl</literal> (filtrado por servicio, si es
posible, o registros de arranque completos).</para>
</listitem>
<listitem>
<para>Registros de pod de Kubernetes (registros kubectl).</para>
</listitem>
<listitem>
<para>Registros de componente de Metal³/Elemental.</para>
</listitem>
<listitem>
<para>Registros de creación de EIB y otros registros</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Información del sistema</emphasis>:</para>
<itemizedlist>
<listitem>
<para><literal>uname -a</literal></para>
</listitem>
<listitem>
<para><literal>df -h</literal></para>
</listitem>
<listitem>
<para><literal>ip a</literal></para>
</listitem>
<listitem>
<para><literal>/etc/os-release</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Archivos de configuración</emphasis>: archivos de
configuración relevantes para Elemental, Metal<superscript>3</superscript>,
EIB, tales como valores de charts de helm, mapas de configuración, etc.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Información de Kubernetes</emphasis>: nodos,
servicios, despliegues, etc.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Objetos de Kubernetes afectados</emphasis>: BMH,
MachineRegistration, etc.</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Cómo recopilar la información</title>
<listitem>
<para><emphasis role="strong">Registros</emphasis>: redirija la salida del comando
a archivos (por ejemplo, <literal>journalctl -u k3s &gt;
k3s_logs.txt</literal>).</para>
</listitem>
<listitem>
<para><emphasis role="strong">Recursos de Kubernetes</emphasis>: use
<literal>kubectl get &lt;recurso&gt; -o yaml &gt;
&lt;nombre_de_recurso&gt;.yaml</literal> para obtener definiciones YAML
detalladas.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Información del sistema</emphasis>: recopile los
resultados de los comandos indicados anteriormente.</para>
</listitem>
<listitem>
<para><emphasis role="strong">SL Micro</emphasis>: consulte en la <link
xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-adm-support-slemicro.html">guía
de resolución de problemas de SUSE Linux Micro</link> cómo recopilar
información del sistema para obtener asistencia con
<literal>supportconfig</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">RKE2/Rancher</emphasis>: consulte el artículo <link
xl:href="https://www.suse.com/support/kb/doc/?id=000020191">The Rancher v2.x
Linux log collector script</link> (Guion de recopilación de registros de
Linux de Rancher v2.x) para ejecutar dicho guion.</para>
</listitem>
</itemizedlist>
<formalpara>
<title>Póngase en contacto con el servicio de asistencia</title>
<para>Consulte el artículo <link
xl:href="https://www.suse.com/support/kb/doc/?id=000019452">How-to
effectively work with SUSE Technical Support</link> (Cómo trabajar
eficazmente con el servicio de asistencia técnica de SUSE) y el manual de
asistencia técnica disponible en <link
xl:href="https://www.suse.com/support/handbook/">SUSE Technical Support
Handbook</link> para obtener más detalles sobre cómo ponerse en contacto con
el servicio de asistencia técnica de SUSE.</para>
</formalpara>
</chapter>
</part>
<part xml:id="id-appendix">
<title>Apéndice</title>
<chapter xml:id="id-release-notes">
<title>Notas de la versión</title>
<section xml:id="release-notes">
<title>Resumen</title>
<para>SUSE Edge 3.3 es una solución integral, estrechamente integrada y validada
de forma exhaustiva para abordar los retos únicos que plantea el despliegue
de infraestructura y aplicaciones nativas de la nube en el perímetro. Su
objetivo principal es proporcionar una plataforma propia, pero altamente
flexible, escalable y segura que abarque la creación de imágenes de
distribución inicial, el aprovisionamiento y la incorporación de nodos, el
despliegue de aplicaciones, la observabilidad y las operaciones completas
del ciclo de vida.</para>
<para>La solución se ha diseñado partiendo de la idea de que no existe una
plataforma periférica única "válida para todo", debido a la gran variedad de
requisitos y expectativas de los clientes. Los despliegues periféricos nos
obligan a evolucionar para resolver continuamente problemas difíciles, como
la escalabilidad masiva, la disponibilidad limitada de la red, las
restricciones de espacio físico, las nuevas amenazas de seguridad y vectores
de ataque, las variaciones en la arquitectura del hardware y los recursos
del sistema, la necesidad de implantar e interactuar con infraestructuras y
aplicaciones heredadas, y las soluciones de los clientes que tienen una vida
útil prolongada.</para>
<para>La plataforma se basa por completo en el mejor software de código abierto,
en consonancia con nuestros más de 30 años de historia proporcionando
plataformas SUSE Linux seguras, estables y certificadas, y en nuestra
experiencia en la gestión de Kubernetes altamente escalable y rica en
funciones con nuestra cartera de productos Rancher. SUSE Edge se basa en
estas capacidades para ofrecer funcionalidades que pueden aplicarse a
numerosos segmentos de mercado, incluyendo el comercio minorista, la
medicina, el transporte, la logística, las telecomunicaciones, la
fabricación inteligente y el IoT industrial.</para>
<note>
<para>SUSE Edge for Telco (anteriormente conocido como Adaptive Telco
Infrastructure Platform/ATIP) es un producto derivado de SUSE Edge, con
optimizaciones y componentes adicionales que permiten a la plataforma
adaptarse al uso práctico en el sector de las telecomunicaciones. A menos
que se indique explícitamente lo contrario, todas las notas de la versión
son aplicables tanto a SUSE Edge 3.3 como a SUSE Edge for Telco 3.3.</para>
</note>
</section>
<section xml:id="id-about">
<title>Acerca de</title>
<para>Estas notas de la versión son, salvo que se especifique y explique
explícitamente lo contrario, idénticas en todas las arquitecturas, y la
versión más reciente, junto con las notas de la versión de todos los demás
productos SUSE, están siempre disponibles en línea en <link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>.</para>
<para>Las entradas solo se muestran una vez, pero pueden aparecer referenciadas en
varios lugares si son importantes y pertenecen a más de una sección. Las
notas de la versión suelen mostrar solo los cambios que se han producido
entre dos versiones consecutivas. Es posible que se repitan algunas entradas
importantes de las notas de la versión de versiones anteriores del
producto. Para facilitar la identificación de estas entradas, se incluye una
nota al respecto.</para>
<para>Sin embargo, las entradas repetidas se proporcionan únicamente como
cortesía. Por lo tanto, si se salta una o más versiones, compruebe también
las notas de las versiones omitidas. Si solo lee las notas de la versión
actual, podría perderse cambios importantes que pueden afectar al
comportamiento del sistema. Las versiones de SUSE Edge se definen como
x.y.z, donde "x" denota la versión principal; "y", la secundaria y "z", la
versión del parche, también conocida como "z-stream". Los ciclos de vida de
los productos SUSE Edge se definen en función de una versión secundaria
determinada, por ejemplo, "3.3", pero se envían con actualizaciones de
parches posteriores a lo largo de su ciclo de vida, por ejemplo, "3.3.1".</para>
<note>
<para>Las versiones z-stream de SUSE Edge están estrechamente integradas y se han
probado exhaustivamente como una pila de versión. La actualización de
cualquier componente individual a una versión diferente a las indicadas
anteriormente probablemente provocará una interrupción del sistema. Aunque
es posible ejecutar clústeres de Edge en configuraciones no probadas, no es
recomendable y es posible que la resolución a través de los canales de
asistencia técnica lleve más tiempo.</para>
</note>
</section>
<section xml:id="release-notes-3-3-1">
<title>Versión 3.3.1</title>
<para>Fecha de lanzamiento: 13 de junio de 2025</para>
<para>Resumen: SUSE Edge 3.3.1 es la primera versión z-stream de SUSE Edge 3.3.</para>
<section xml:id="id-new-features">
<title>Funciones nuevas</title>
<itemizedlist>
<listitem>
<para>Se ha actualizado a Kubernetes 1.32.4 y Rancher Prime 2.11.2: <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.11.2">Notas de
la versión</link></para>
</listitem>
<listitem>
<para>Se ha actualizado a SUSE Security (Neuvector) 5.4.4: <link
xl:href="https://open-docs.neuvector.com/releasenotes/5x#544-may-2025">Notas
de la versión</link></para>
</listitem>
<listitem>
<para>Se ha actualizado a Rancher Turtles 0.20.0: <link
xl:href="https://turtles.docs.rancher.com/turtles/stable/en/changelogs/changelogs/v0.20.0.html">Notas
de la versión</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>Soluciones de seguridad y errores</title>
<itemizedlist>
<listitem>
<para>En algunos casos, al usar un clúster de gestión que se ha actualizado de
Edge 3.2 a 3.3.0, realizar actualizaciones progresivas descendentes a través
de CAPI podría provocar que los equipos se quedaran bloqueados en el estado
"Deleting" (Eliminando). Esto se ha resuelto mediante una actualización del
proveedor CAPI de RKE2: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/661">Problema
661 de versión superior del proveedor de RKE2</link></para>
</listitem>
<listitem>
<para>Al configurar la red mediante nm-configurator, algunas configuraciones que
identifican interfaces por MAC en la versión 3.3.0 no funcionaban. Esto se
ha resuelto actualizando NMC a la versión 0.3.3 con las correspondientes
actualizaciones de las imágenes de contenedor de descarga de IPA de EIB y
Metal<superscript>3</superscript>: <link
xl:href="https://github.com/suse-edge/nm-configurator/issues/163">Problema
de versión superior de NM Configurator</link></para>
</listitem>
<listitem>
<para>En los clústeres de gestión de Metal<superscript>3</superscript> de larga
duración de la versión 3.3.0, era posible que la caducidad del certificado
provocara un fallo en la conexión del operador bare metal con Ironic, lo que
requería una solución alternativa de reinicio manual del pod. Esto se ha
resuelto mediante actualizaciones del chart de
Metal<superscript>3</superscript>: <link
xl:href="https://github.com/suse-edge/charts/issues/178">Problema con los
charts de SUSE Edge</link></para>
</listitem>
<listitem>
<para>Anteriormente, la interfaz de usuario de Rancher no podría mostrar los
charts de SUSE Edge desde el registro de OCI del catálogo de
aplicaciones. Este problema se ha resuelto con la actualización a Rancher
2.11.2: <link
xl:href="https://github.com/rancher/rancher/issues/48746">Problema de
Rancher</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-known-issues-8">
<title>Problemas conocidos</title>
<warning>
<para>Si va a desplegar nuevos clústeres, siga las instrucciones del <xref
linkend="guides-kiwi-builder-images"/> para crear primero imágenes nuevas,
ya que ahora este es el primer paso necesario para crear clústeres tanto
para las arquitecturas AMD64/Intel 64 y AArch64 como para los clústeres de
gestión y los clústeres descendentes.</para>
</warning>
<itemizedlist>
<listitem>
<para>Cuando se usa <literal>toolbox</literal> en SUSE Linux Micro 6.1, la imagen
de contenedor predeterminada no contiene algunas herramientas que se
incluían en la versión 5.5 anterior. La solución alternativas es configurar
toolbox para que use la imagen de contenedor
<literal>suse/sle-micro/5.5/toolbox</literal> anterior. Consulte
<literal>toolbox --help</literal> para ver las opciones de configuración de
la imagen.</para>
</listitem>
<listitem>
<para>En algunos casos, las actualizaciones progresivas a través de CAPI pueden
provocar que las máquinas se queden bloqueadas en el estado "Deleting"
(Eliminando). Esto se resolverá mediante una futura actualización: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/655">Problema
655 de versión superior del proveedor de RKE2</link></para>
</listitem>
<listitem>
<para>Debido a las correcciones relacionadas con la <link
xl:href="https://nvd.nist.gov/vuln/detail/CVE-2025-1974">CVE-2025-1974</link>
mencionadas en la versión 3.3.0, SUSE Linux Micro 6.1 <emphasis
role="strong">debe</emphasis> actualizarse para incluir el kernel
<literal>&gt;=6.4.0-26-default</literal> o
<literal>&gt;=6.4.0-30-rt</literal> (kernel en tiempo real) debido a los
parches necesarios para el kernel SELinux. Si no se aplican, el pod
ingress-nginx permanecerá en estado
<literal>CrashLoopBackOff</literal>. Para aplicar la actualización del
kernel, ejecute <literal>transactional-update</literal> en el propio host
(para actualizar todos los paquetes) o <literal>transactional-update pkg
update kernel-default</literal> (o el kernel en tiempo real) para actualizar
solo el kernel y, a continuación, rearranque el host. Si va a desplegar
nuevos clústeres, siga el <xref linkend="guides-kiwi-builder-images"/> para
crear imágenes nuevas que contengan el kernel más reciente.</para>
</listitem>
<listitem>
<para>Se ha identificado un error en el controlador de trabajos de Kubernetes que,
en determinadas condiciones, puede provocar que los nodos RKE2/K3s
permanezcan en estado <literal>NotReady</literal> (consulte el <link
xl:href="https://github.com/rancher/rke2/issues/8357">problema n.º 8357 de
RKE2</link>). Los errores pueden tener el siguiente aspecto:</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">E0605 23:11:18.489721   »···1 job_controller.go:631] "Unhandled Error" err="syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch \"helm-install-rke2-ingress-nginx\": StorageError: invalid object, Code: 4, Key: /registry/jobs/kube-system/helm-install-rke2-ingress-nginx, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 0aa6a781-7757-4c61-881a-cb1a4e47802c, UID in object meta: 6a320146-16b8-4f83-88c5-fc8b5a59a581" logger="UnhandledError"</screen>
<para>Como solución, el pod <literal>kube-controller-manager</literal> se puede
reiniciar con <literal>crictl</literal> como:</para>
<screen language="bash" linenumbering="unnumbered">export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
export KUBEMANAGER_POD=$(/var/lib/rancher/rke2/bin/crictl ps --label io.kubernetes.container.name=kube-controller-manager --quiet)
/var/lib/rancher/rke2/bin/crictl stop ${KUBEMANAGER_POD} &amp;&amp; \
/var/lib/rancher/rke2/bin/crictl rm ${KUBEMANAGER_POD}</screen>
<itemizedlist>
<listitem>
<para>En las versiones 1.31 y 1.32 de RKE2/K3s, es posible que el directorio
<literal>/etc/cni</literal> utilizado para almacenar las configuraciones de
CNI no active una notificación de los archivos que se escriben allí en
<literal>containerd</literal> debido a ciertas condiciones relacionadas con
<literal>overlayfs</literal> (consulte el <link
xl:href="https://github.com/rancher/rke2/issues/8356">problema n.º 8356 de
RKE2</link>). Esto, a su vez, provoca que el despliegue de RKE2/K3s se quede
bloqueado a la espera de que se inicie la CNI y que los nodos RKE2/K3s
permanecen en estado <literal>NotReady</literal>. Puede observar este
problema en cada nodo con <literal>kubectl describe node
&lt;nodo_afectado&gt;</literal>:</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">&lt;200b&gt;&lt;200b&gt;Conditions:
  Type         »Status  LastHeartbeatTime             »·LastTransitionTime            »·Reason                   »··Message
  ----         »------  -----------------             »·------------------            »·------                   »··-------
  Ready        »False   Thu, 05 Jun 2025 17:41:28 +0000   Thu, 05 Jun 2025 14:38:16 +0000   KubeletNotReady          »··container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized</screen>
<para>Como solución alternativa, se puede montar un volumen tmpfs en el directorio
<literal>/etc/cni</literal> antes de que se inicie RKE2. Esto evita el uso
de overlayfs, lo que provoca que containerd pierda notificaciones y que las
configuraciones deban reescribirse cada vez que se reinicia el nodo y se
vuelven a ejecutar los contenedores de inicio de los pods. Si se utiliza
EIB, esto puede hacerse con un guion <literal>04-tmpfs-cni.sh</literal> en
el directorio <literal>custom/scripts</literal> (como se explica aquí [<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom">https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom</link>),
que tiene el siguiente aspecto:</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash
mkdir -p /etc/cni
mount -t tmpfs -o mode=0700,size=5M tmpfs /etc/cni
echo "tmpfs /etc/cni tmpfs defaults,size=5M,mode=0700 0 0" &gt;&gt; /etc/fstab</screen>
</section>
<section xml:id="id-component-versions">
<title>Versiones de componentes</title>
<para>La siguiente tabla describe los componentes individuales que conforman la
versión 3.3.1, incluyendo la versión, la versión del chart de Helm (si
procede) y desde donde se puede obtener el artefacto publicado en formato
binario. Consulte la documentación asociada para ver ejemplos de uso y
despliegue.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Nombre</para></entry>
<entry align="left" valign="top"><para>Versión</para></entry>
<entry align="left" valign="top"><para>Versión del chart de Helm</para></entry>
<entry align="left" valign="top"><para>Ubicación del artefacto (URL/Imagen)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Linux Micro</para></entry>
<entry align="left" valign="top"><para>6.1 (más reciente)</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">Página de descarga
de SUSE Linux Micro</link><?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso (sha256
70b9be28f2d92bc3b228412e4fc2b1d5026e691874b728e530b8063522158854)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-SelfInstall-GM.install.iso (sha256
9ce83e4545d4b36c7c6a44f7841dc3d9c6926fe32dbff694832e0fbd7c496e9d)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-GM.raw.xz (sha256
36e3efa55822113840dd76fdf6914e933a7b7e88a1dce5cb20c424ccf2fb4430)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-GM.raw.xz (sha256
2ee66735da3e1da107b4878e73ae68f5fb7309f5ec02b5dfdb94e254fda8415e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Multi-Linux Manager</para></entry>
<entry align="left" valign="top"><para>5.0.3</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">Página de
descarga de SUSE Multi-Linux Manager</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">K3s</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.32.4</emphasis></para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.32.4%2Bk3s1">Versión
superior de K3s</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">RKE2</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.32.4</emphasis></para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.32.4%2Brke2r1">Versión
superior de RKE2</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">SUSE Rancher Prime</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.11.2</emphasis></para></entry>
<entry align="left" valign="top"><para>2.11.2</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://charts.rancher.com/server-charts/prime/index.yaml">Repositorio
de Helm de Rancher Prime</link><?asciidoc-br?> <link
xl:href="https://github.com/rancher/rancher/releases/download/v2.11.1/rancher-images.txt">Imágenes
de contenedor de Rancher 2.11.1</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Storage</para></entry>
<entry align="left" valign="top"><para>1.8.1</para></entry>
<entry align="left" valign="top"><para>106.2.0+up1.8.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositorio de charts
de Helm de Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.2.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.13.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.2.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.13.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.15.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.8.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.52<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.8.1<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">SUSE Security</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">5.4.4</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">106.0.1+up2.8.6</emphasis></para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositorio de charts
de Helm de Rancher</link><?asciidoc-br?> <emphasis
role="strong">registry.suse.com/rancher/neuvector-controller:5.4.4</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/neuvector-enforcer:5.4.4</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/neuvector-manager:5.4.4</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/neuvector-compliance-config:1.0.5</emphasis><?asciidoc-br?>
registry.suse.com/rancher/neuvector-registry-adapter:0.1.6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-scanner:6<?asciidoc-br?> <emphasis
role="strong">registry.suse.com/rancher/neuvector-updater:0.0.3</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Rancher Turtles (CAPI)</emphasis></para></entry>
<entry align="left" valign="top"><para>0.20.0</para></entry>
<entry align="left" valign="top"><para>303.0.4+up0.20.0</para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/charts/rancher-turtles:303.0.3_up0.20.0</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.rancher.com/rancher/rancher/turtles:v0.20.0</emphasis><?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-operator:v0.17.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-controller:v1.9.3<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-ipam-controller:v1.9.4<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-controller:v1.9.5<?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.16.1</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.16.1</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Recursos de Rancher Turtles para entornos
aislados</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.20.0</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">303.0.4+up0.20.0</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/charts/rancher-turtles-airgap-resources:303.0.3_up0.20.0</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Metal<superscript>3</superscript></emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.11.5</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">303.0.7+up0.11.5</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/charts/metal3:303.0.7_up0.11.5</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/edge/3.3/baremetal-operator:0.9.1.1</emphasis><?asciidoc-br?>
registry.suse.com/edge/3.3/ironic:26.1.2.4<?asciidoc-br?> <emphasis
role="strong">registry.suse.com/edge/3.3/ironic-ipa-downloader:3.0.7</emphasis><?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metallb:303.0.0_up0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.3/metallb-controller:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.3/metallb-speaker:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.3/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.3/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.6.8</para></entry>
<entry align="left" valign="top"><para>1.6.8</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.6.8<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.6.8<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.6.8</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Extensión de panel de control de Elemental</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/4.0.0/charts/elemental/3.0.1">Chart
de Helm de extensión de Elemental</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Edge Image Builder</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.2.1</emphasis></para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/3.3/edge-image-builder:1.2.1</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">NM Configurator</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.3.3</emphasis></para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.3">Versión
superior de NM Configurator</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.4.0</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt:303.0.0_up0.5.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-operator:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-api:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-controller:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportproxy:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportserver:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-handler:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-launcher:1.4.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Extensión de panel de control KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.3.2</para></entry>
<entry align="left" valign="top"><para>303.0.2+up1.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt-dashboard-extension:303.0.2_up1.3.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.61.0</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/cdi:303.0.0_up0.5.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-operator:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-controller:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-importer:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-cloner:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-apiserver:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadserver:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadproxy:1.61.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.2.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/endpoint-copier-operator:303.0.0_up0.2.1<?asciidoc-br?>
registry.suse.com/edge/3.3/endpoint-copier-operator:0.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (tecnología en fase preliminar)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/akri:303.0.0_up0.12.20<?asciidoc-br?>
registry.suse.com/edge/charts/akri-dashboard-extension:303.0.0_up1.3.1<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Operador de red SR-IOV</para></entry>
<entry align="left" valign="top"><para>1.5.0</para></entry>
<entry align="left" valign="top"><para>303.0.2+up1.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/sriov-network-operator:303.0.2_up1.5.0<?asciidoc-br?>
registry.suse.com/edge/charts/sriov-crd:303.0.2_up1.5.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.15.2</para></entry>
<entry align="left" valign="top"><para>106.0.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositorio de charts
de Helm de Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.15.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Upgrade Controller</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.1.1</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">303.0.1+up0.1.1</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis
role="strong">registry.suse.com/edge/charts/upgrade-controller:303.0.1_up0.1.1</emphasis><?asciidoc-br?>
registry.suse.com/edge/3.3/upgrade-controller:0.1.1<?asciidoc-br?> <emphasis
role="strong">registry.suse.com/edge/3.3/kubectl:1.32.4</emphasis><?asciidoc-br?>
<emphasis
role="strong">registry.suse.com/edge/3.3/release-manifest:3.3.1</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Kiwi Builder</para></entry>
<entry align="left" valign="top"><para>10.2.12.0</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="release-notes-3-3-0">
<title>Versión 3.3.0</title>
<para>Fecha de lanzamiento: 20 de mayo de 2025</para>
<para>Resumen: SUSE Edge 3.3.0 es la primera versión de SUSE Edge 3.3.</para>
<section xml:id="id-new-features-2">
<title>Funciones nuevas</title>
<itemizedlist>
<listitem>
<para>Se ha actualizado a Kubernetes 1.32 y Rancher Prime 2.11</para>
</listitem>
<listitem>
<para>El sistema operativo se ha actualizado a <link
xl:href="https://documentation.suse.com/sle-micro/6.1">SUSE Linux Micro
6.1</link></para>
</listitem>
<listitem>
<para>Se han actualizado las versiones de Rancher Turtles, Cluster API y
Metal3/Ironic</para>
</listitem>
<listitem>
<para>Ahora se proporciona una imagen de contenedor que permite crear imágenes de
SUSE Linux Micro actualizadas. Consulte el <xref
linkend="guides-kiwi-builder-images"/> para obtener más detalles.</para>
</listitem>
<listitem>
<para>El despliegue de clústeres descendentes AArch64 ahora es posible a través
del flujo de aprovisionamiento de red dirigida. Consulte el <xref
linkend="atip-automated-provisioning"/> para obtener más detalles.</para>
</listitem>
<listitem>
<para>Ahora es posible desplegar clústeres descendentes de doble pila a través del
flujo de aprovisionamiento de red dirigida como tecnología en fase
preliminar.</para>
</listitem>
<listitem>
<para>La configuración del protocolo de tiempo de precisión (PTP) ahora es posible
como tecnología en fase preliminar. Consulte la <xref
linkend="tech-previews"/> para obtener más detalles.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes-2">
<title>Soluciones de seguridad y errores</title>
<itemizedlist>
<listitem>
<para>Soluciona la <link
xl:href="https://nvd.nist.gov/vuln/detail/CVE-2025-1974">CVE-2025-1974</link>
con parches para ingress-nginx en RKE2. Encontrará más información <link
xl:href="https://kubernetes.io/blog/2025/03/24/ingress-nginx-cve-2025-1974/">aquí</link>.</para>
</listitem>
<listitem>
<para>SUSE Storage (Longhorn) 1.8.1 contiene varias correcciones de errores, como:</para>
<itemizedlist>
<listitem>
<para>Solución para el problema del volumen FailedMount que puede causar que falle
la conexión del volumen: <link
xl:href="https://github.com/longhorn/longhorn/issues/9939">Problema de
versión superior de Longhorn</link>.</para>
</listitem>
<listitem>
<para>Solución para el problema del motor atascado en estado detenido que puede
impedir la conexión del volumen: <link
xl:href="https://github.com/longhorn/longhorn/issues/9938">Problema de
versión superior de Longhorn</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>La actualización del chart de Metal<superscript>3</superscript> contiene
varias correcciones de errores, como:</para>
<itemizedlist>
<listitem>
<para>Se ha solucionado un error que se producía al desplegar clústeres con
direcciones IP estáticas en redes con servidores DHCP: <link
xl:href="https://github.com/suse-edge/charts/pull/196">Problema de versión
superior</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>El chart de MetalLB contiene una corrección para garantizar que se usen
imágenes descendentes si frr-k8s está habilitado.</para>
</listitem>
<listitem>
<para>Kiwi Builder se ha actualizado a la versión 10.2.12 para adaptarse a los
recientes cambios de seguridad en Kiwi, pasando de <link
xl:href="https://github.com/OSInside/kiwi/commit/d4d39e481aaff8be28337a9c76c3913a8a482628">los
métodos de suma de verificación md5 a sha256</link> para la verificación de
imágenes.</para>
</listitem>
<listitem>
<para>La imagen de Edge Image Builder se ha reconstruido para incluir la versión
actualizada de MetalLB y abordar los cambios de Kiwi, ambos mencionados
anteriormente.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-known-issues-9">
<title>Problemas conocidos</title>
<warning>
<para>Si va a desplegar nuevos clústeres, siga las instrucciones del <xref
linkend="guides-kiwi-builder-images"/> para crear primero imágenes nuevas,
ya que ahora este es el primer paso necesario para crear clústeres tanto
para las arquitecturas AMD64/Intel 64 y AArch64 como para los clústeres de
gestión y los clústeres descendentes.</para>
</warning>
<itemizedlist>
<listitem>
<para>Cuando se usa <literal>toolbox</literal> en SUSE Linux Micro 6.1, la imagen
de contenedor predeterminada no contiene algunas herramientas que se
incluían en la versión 5.5 anterior. La solución alternativas es configurar
toolbox para que use la imagen de contenedor
<literal>suse/sle-micro/5.5/toolbox</literal> anterior. Consulte
<literal>toolbox --help</literal> para ver las opciones de configuración de
la imagen.</para>
</listitem>
<listitem>
<para>En algunos casos, las actualizaciones progresivas a través de CAPI pueden
provocar que las máquinas se queden bloqueadas en el estado "Deleting"
(Eliminando). Esto se resolverá mediante una futura actualización: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/655">Problema
655 de versión superior del proveedor de RKE2</link></para>
</listitem>
<listitem>
<para>En algunos casos, al usar un clúster de gestión que se ha actualizado desde
Edge 3.2, las actualizaciones progresivas descendentes a través de CAPI
pueden provocar que los equipos se queden bloqueados en el estado "Deleting"
(Eliminando). Esto se resolverá en una futura actualización: <link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/661">Problema
661 de versión superior del proveedor de RKE2</link></para>
</listitem>
<listitem>
<para>Al usar la versión 1.32.3 de RKE2, que resuelve la <link
xl:href="https://nvd.nist.gov/vuln/detail/CVE-2025-1974">CVE-2025-1974</link>,
SUSE Linux Micro 6.1 <emphasis role="strong">debe</emphasis> actualizarse
para incluir el kernel <literal>&gt;=6.4.0-26-default</literal> o
<literal>&gt;=6.4.0-30-rt</literal> (kernel en tiempo real) debido a los
parches necesarios para el kernel de SELinux. Si no se aplica, el pod
ingress-nginx permanecerá en estado
<literal>CrashLoopBackOff</literal>. Para aplicar la actualización del
kernel, ejecute <literal>transactional-update</literal> en el propio host
(para actualizar todos los paquetes) o <literal>transactional-update pkg
update kernel-default</literal> (o el kernel en tiempo real) para actualizar
solo el kernel y, a continuación, rearranque el host. Si va a desplegar
nuevos clústeres, siga el <xref linkend="guides-kiwi-builder-images"/> para
crear imágenes nuevas que contengan el kernel más reciente.</para>
</listitem>
<listitem>
<para>Al configurar la red mediante nm-configurator, algunas configuraciones que
identifican interfaces por su MAC no funcionan actualmente, esto se
resolverá en una futura actualización: <link
xl:href="https://github.com/suse-edge/nm-configurator/issues/163">Problema
de versión superior de NM Configurator</link></para>
</listitem>
<listitem>
<para>En clústeres de gestión de Metal<superscript>3</superscript> que llevan
mucho tiempo en funcionamiento, es posible que la caducidad del certificado
provoque un fallo en la conexión del operador bare metal con Ironic, lo que
requiere una solución alternativa consistente en reiniciar manualmente el
pod: <link xl:href="https://github.com/suse-edge/charts/issues/178">Problema
de charts de SUSE Edge</link></para>
</listitem>
<listitem>
<para>Se ha identificado un error en el controlador de trabajos de Kubernetes que,
en determinadas condiciones, puede provocar que los nodos RKE2/K3s
permanezcan en estado <literal>NotReady</literal> (consulte el <link
xl:href="https://github.com/rancher/rke2/issues/8357">problema n.º 8357 de
RKE2</link>). Los errores pueden tener el siguiente aspecto:</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">E0605 23:11:18.489721   	1 job_controller.go:631] "Unhandled Error" err="syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch \"helm-install-rke2-ingress-nginx\": StorageError: invalid object, Code: 4, Key: /registry/jobs/kube-system/helm-install-rke2-ingress-nginx, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 0aa6a781-7757-4c61-881a-cb1a4e47802c, UID in object meta: 6a320146-16b8-4f83-88c5-fc8b5a59a581" logger="UnhandledError"</screen>
<para>Como solución, el pod <literal>kube-controller-manager</literal> se puede
reiniciar con <literal>crictl</literal> como:</para>
<screen language="bash" linenumbering="unnumbered">export CONTAINER_RUNTIME_ENDPOINT=unix:///run/k3s/containerd/containerd.sock
export KUBEMANAGER_POD=$(/var/lib/rancher/rke2/bin/crictl ps --label io.kubernetes.container.name=kube-controller-manager --quiet)
/var/lib/rancher/rke2/bin/crictl stop ${KUBEMANAGER_POD} &amp;&amp; \
/var/lib/rancher/rke2/bin/crictl rm ${KUBEMANAGER_POD}</screen>
<itemizedlist>
<listitem>
<para>En las versiones 1.31 y 1.32 de RKE2/K3s, es posible que el directorio
<literal>/etc/cni</literal> utilizado para almacenar las configuraciones de
CNI no active una notificación de los archivos que se escriben allí en
<literal>containerd</literal> debido a ciertas condiciones relacionadas con
<literal>overlayfs</literal> (consulte el <link
xl:href="https://github.com/rancher/rke2/issues/8356">problema n.º 8356 de
RKE2</link>). Esto, a su vez, provoca que el despliegue de RKE2/K3s se quede
bloqueado a la espera de que se inicie la CNI y que los nodos RKE2/K3s
permanecen en estado <literal>NotReady</literal>. Puede observar este
problema en cada nodo con <literal>kubectl describe node
&lt;nodo_afectado&gt;</literal>:</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">​​Conditions:
  Type         	Status  LastHeartbeatTime             	LastTransitionTime            	Reason                   	Message
  ----         	------  -----------------             	------------------            	------                   	-------
  Ready        	False   Thu, 05 Jun 2025 17:41:28 +0000   Thu, 05 Jun 2025 14:38:16 +0000   KubeletNotReady          	container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized</screen>
<para>Como solución alternativa, se puede montar un volumen tmpfs en el directorio
<literal>/etc/cni</literal> antes de que se inicie RKE2. Esto evita el uso
de overlayfs, lo que provoca que containerd pierda notificaciones y que las
configuraciones deban reescribirse cada vez que se reinicia el nodo y se
vuelven a ejecutar los contenedores de inicio de los pods. Si se utiliza
EIB, esto puede hacerse con un guion <literal>04-tmpfs-cni.sh</literal> en
el directorio <literal>custom/scripts</literal> (como se explica aquí [<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom">https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom</link>),
que tiene el siguiente aspecto:</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash
mkdir -p /etc/cni
mount -t tmpfs -o mode=0700,size=5M tmpfs /etc/cni
echo "tmpfs /etc/cni tmpfs defaults,size=5M,mode=0700 0 0" &gt;&gt; /etc/fstab</screen>
</section>
<section xml:id="id-component-versions-2">
<title>Versiones de componentes</title>
<para>La siguiente tabla describe los componentes individuales que conforman la
versión 3.3.0, incluyendo la versión, la versión del chart de Helm (si
procede) y desde dónde se puede obtener el artefacto publicado en formato
binario. Siga la documentación asociada para ver ejemplos de uso y
distribución.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Nombre</para></entry>
<entry align="left" valign="top"><para>Versión</para></entry>
<entry align="left" valign="top"><para>Versión del chart de Helm</para></entry>
<entry align="left" valign="top"><para>Ubicación del artefacto (URL/Imagen)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Linux Micro</para></entry>
<entry align="left" valign="top"><para>6.1 (más reciente)</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">Página de descarga
de SUSE Linux Micro</link><?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso (sha256
70b9be28f2d92bc3b228412e4fc2b1d5026e691874b728e530b8063522158854)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-SelfInstall-GM.install.iso (sha256
9ce83e4545d4b36c7c6a44f7841dc3d9c6926fe32dbff694832e0fbd7c496e9d)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-GM.raw.xz (sha256
36e3efa55822113840dd76fdf6914e933a7b7e88a1dce5cb20c424ccf2fb4430)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-GM.raw.xz (sha256
2ee66735da3e1da107b4878e73ae68f5fb7309f5ec02b5dfdb94e254fda8415e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Multi-Linux Manager</para></entry>
<entry align="left" valign="top"><para>5.0.3</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">Página de
descarga de SUSE Multi-Linux Manager</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.32.3</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.32.3%2Bk3s1">Versión
superior de K3s</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.32.3</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.32.3%2Brke2r1">Versión
superior de RKE2</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.11.1</para></entry>
<entry align="left" valign="top"><para>2.11.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://charts.rancher.com/server-charts/prime/index.yaml">Repositorio
de Helm de Rancher Prime</link><?asciidoc-br?> <link
xl:href="https://github.com/rancher/rancher/releases/download/v2.11.1/rancher-images.txt">Imágenes
de contenedor de Rancher 2.11.1</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Storage</para></entry>
<entry align="left" valign="top"><para>1.8.1</para></entry>
<entry align="left" valign="top"><para>106.2.0+up1.8.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositorio de charts
de Helm de Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.7.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v4.0.1-20241007<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.12.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v7.0.2-20241007<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.12.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.14.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.7.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.7.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.7.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.7.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.7.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.7.2<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.45<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.7.2<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Security</para></entry>
<entry align="left" valign="top"><para>5.4.3</para></entry>
<entry align="left" valign="top"><para>106.0.0+up2.8.5</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositorio de charts
de Helm de Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/neuvector-controller:5.4.3<?asciidoc-br?>
registry.suse.com/rancher/neuvector-enforcer:5.4.3<?asciidoc-br?>
registry.suse.com/rancher/neuvector-manager:5.4.3<?asciidoc-br?>
registry.suse.com/rancher/neuvector-compliance-config:1.0.4<?asciidoc-br?>
registry.suse.com/rancher/neuvector-registry-adapter:0.1.6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-scanner:6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-updater:0.0.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles (CAPI)</para></entry>
<entry align="left" valign="top"><para>0.19.0</para></entry>
<entry align="left" valign="top"><para>303.0.2+up0.19.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles:303.0.2_up0.19.0<?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.19.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-operator:v0.17.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-controller:v1.9.3<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-ipam-controller:v1.9.4<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-controller:v1.9.5<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.15.1<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Recursos de Rancher Turtles para entornos aislados</para></entry>
<entry align="left" valign="top"><para>0.19.0</para></entry>
<entry align="left" valign="top"><para>303.0.2+up0.19.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles-airgap-resources:303.0.2_up0.19.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>0.11.3</para></entry>
<entry align="left" valign="top"><para>303.0.5+up0.11.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metal3:303.0.5_up0.11.3<?asciidoc-br?>
registry.suse.com/edge/3.3/baremetal-operator:0.9.1<?asciidoc-br?>
registry.suse.com/edge/3.3/ironic:26.1.2.4<?asciidoc-br?>
registry.suse.com/edge/3.3/ironic-ipa-downloader:3.0.6<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metallb:303.0.0_up0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.3/metallb-controller:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.3/metallb-speaker:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.3/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.3/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.6.8</para></entry>
<entry align="left" valign="top"><para>1.6.8</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.6.8<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.6.8<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.6.8</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Extensión de panel de control de Elemental</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/4.0.0/charts/elemental/3.0.1">Chart
de Helm de extensión de Elemental</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.2.0</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.3/edge-image-builder:1.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.2</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.2">Versión
superior de NM Configurator</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.4.0</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt:303.0.0_up0.5.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-operator:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-api:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-controller:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportproxy:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-exportserver:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-handler:1.4.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/virt-launcher:1.4.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Extensión de panel de control KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.3.2</para></entry>
<entry align="left" valign="top"><para>303.0.2+up1.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt-dashboard-extension:303.0.2_up1.3.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.61.0</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/cdi:303.0.0_up0.5.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-operator:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-controller:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-importer:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-cloner:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-apiserver:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadserver:1.61.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.6/cdi-uploadproxy:1.61.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.2.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/endpoint-copier-operator:303.0.0_up0.2.1<?asciidoc-br?>
registry.suse.com/edge/3.3/endpoint-copier-operator:0.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (tecnología en fase preliminar)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/akri:303.0.0_up0.12.20<?asciidoc-br?>
registry.suse.com/edge/charts/akri-dashboard-extension:303.0.0_up1.3.1<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.3/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Operador de red SR-IOV</para></entry>
<entry align="left" valign="top"><para>1.5.0</para></entry>
<entry align="left" valign="top"><para>303.0.2+up1.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/sriov-network-operator:303.0.2_up1.5.0<?asciidoc-br?>
registry.suse.com/edge/charts/sriov-crd:303.0.2_up1.5.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>System Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.15.2</para></entry>
<entry align="left" valign="top"><para>106.0.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Repositorio de charts
de Helm de Rancher</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.15.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Upgrade Controller</para></entry>
<entry align="left" valign="top"><para>0.1.1</para></entry>
<entry align="left" valign="top"><para>303.0.0+up0.1.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/upgrade-controller:303.0.0_up0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.3/upgrade-controller:0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.3/kubectl:1.30.3<?asciidoc-br?>
registry.suse.com/edge/3.3/release-manifest:3.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Kiwi Builder</para></entry>
<entry align="left" valign="top"><para>10.2.12.0</para></entry>
<entry align="left" valign="top"><para>N/D</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.3/kiwi-builder:10.2.12.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="tech-previews">
<title>Tecnologías en fase preliminar</title>
<para>A menos que se indique lo contrario, esto se aplica a la versión 3.3.0 y a
todas las versiones z-stream posteriores.</para>
<itemizedlist>
<listitem>
<para>Akri se ofrece como tecnología en fase preliminar y no está sujeta al
alcance estándar de la asistencia técnica.</para>
</listitem>
<listitem>
<para>Los despliegues de IPv6 y de doble pila descendentes se ofrecen como
tecnologías en fase preliminar y no están sujetas al alcance estándar de la
asistencia técnica.</para>
</listitem>
<listitem>
<para>El protocolo de tiempo de precisión (PTP) en despliegues descendentes se
ofrece como tecnología en fase preliminar y no está sujeto al alcance
estándar de la asistencia técnica.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-component-verification">
<title>Verificación de componentes</title>
<para>Los componentes mencionados anteriormente pueden verificarse utilizando los
datos de la lista de materiales de software (SBOM), por ejemplo, utilizando
<literal>cosign</literal> como se describe a continuación:</para>
<para>Descargue la clave pública del contenedor de SUSE Edge desde la <link
xl:href="https://www.suse.com/support/security/keys/">fuente de claves de
firma de SUSE</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>Verifique el hash de la imagen del contenedor; por ejemplo, usando
<literal>crane</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/3.3/baremetal-operator:0.9.1 --platform linux/amd64
sha256:02c5590cd51b1a1ea02f9908f2184ef4fbc856eb0197e804a7d57566d9278ddd</screen>
<note>
<para>Para imágenes de varias arquitecturas, también es necesario especificar una
plataforma al obtener el resumen; por ejemplo, <literal>--platform
linux/amd64</literal> o <literal>--platform linux/arm64</literal>. Si no se
hace, en el siguiente paso se producirá el error <literal>Error: no matching
attestations</literal> (Error: no hay certificaciones que coincidan).</para>
</note>
<para>Verifíquelo con <literal>cosign</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.3/baremetal-operator@sha256:02c5590cd51b1a1ea02f9908f2184ef4fbc856eb0197e804a7d57566d9278ddd &gt; /dev/null
#
Verification for registry.suse.com/edge/3.3/baremetal-operator@sha256:02c5590cd51b1a1ea02f9908f2184ef4fbc856eb0197e804a7d57566d9278ddd --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The claims were present in the transparency log
  - The signatures were integrated into the transparency log when the certificate was valid
  - The signatures were verified against the specified public key</screen>
<para>Extraiga los datos SBOM como se describe en la <link
xl:href="https://www.suse.com/support/security/sbom/">documentación de SBOM
de SUSE</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.2/baremetal-operator@sha256:d85c1bcd286dec81a3806a8fb8b66c0e0741797f23174f5f6f41281b1e27c52f | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>Pasos para la actualización</title>
<para>Consulte la <xref linkend="day-2-operations"/> para obtener información
sobre cómo actualizar a una versión nueva.</para>
</section>
<section xml:id="id-product-support-lifecycle">
<title>Ciclo de vida de la asistencia técnica del producto</title>
<para>SUSE Edge cuenta con el galardonado servicio de asistencia técnica de SUSE,
un líder tecnológico consolidado con una trayectoria probada en la
prestación de servicios de asistencia técnica de calidad empresarial. Para
obtener más información, consulte <link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>
y la página de la política de asistencia técnica en <link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>.
Si tiene alguna pregunta sobre cómo abrir un caso de asistencia, cómo
clasifica SUSE los niveles de gravedad o el alcance de la asistencia,
consulte el Manual de asistencia técnica en <link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>.</para>
<para>SUSE Edge "3.3" cuenta con 18 meses de asistencia para producción, con 6
meses iniciales de "asistencia completa", seguidos de 12 meses de
"asistencia de mantenimiento". Tras estas fases, el producto alcanza el "fin
de vida útil" y deja de recibir asistencia. En la siguiente tabla se ofrece
más información sobre las fases del ciclo de vida:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Asistencia completa (6 meses)</emphasis></para></entry>
<entry align="left" valign="top"><para>Durante el periodo de asistencia completa, se publicarán correcciones de
errores urgentes y seleccionadas de alta prioridad. El resto de parches (no
urgentes, mejoras, nuevas capacidades) se publicarán según el calendario de
lanzamientos habitual.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Asistencia de mantenimiento (12 meses)</emphasis></para></entry>
<entry align="left" valign="top"><para>Durante este periodo, solo se publicarán correcciones críticas mediante
parches. Otras correcciones de errores podrán publicarse a discreción de
SUSE, pero no deben esperarse.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Fin de vida útil</emphasis></para></entry>
<entry align="left" valign="top"><para>Una vez que un producto alcanza su fecha de fin de vida útil, el cliente
puede seguir utilizándolo dentro de los términos del contrato de licencia
del producto. Los planes de asistencia de SUSE no se aplican a los productos
que hayan superado su fecha de fin de vida útil.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>A menos que se indique explícitamente lo contrario, todos los componentes
mostrados se consideran de disponibilidad general y están cubiertos por el
alcance estándar de la asistencia técnica de SUSE. Algunos componentes
pueden aparecer como "tecnología en fase preliminar", lo que significa que
SUSE proporciona a los clientes acceso a características y funcionalidades
antes de que estén disponibles de forma generalizada para su evaluación,
pero no están sujetos a las políticas de asistencia técnica estándar y no se
recomiendan para casos de uso en producción. SUSE agradece enormemente los
comentarios y sugerencias sobre las mejoras que se pueden realizar en los
componentes de tecnología en fase preliminar, pero se reserva el derecho de
descartar una función de tecnología en fase preliminar antes de que pase a
estar disponible de forma generalizada si no satisface las necesidades de
nuestros clientes o no alcanza el estado de madurez que requerimos.</para>
<para>Tenga en cuenta que SUSE debe, ocasionalmente, dejar de utilizar funciones o
cambiar las especificaciones de la API. Las razones para dejar de utilizar
una función o cambiar la API pueden incluir la actualización o sustitución
de una función por una nueva implementación, un nuevo conjunto de funciones,
la indisponibilidad de la tecnología original o la introducción de cambios
incompatibles por parte de la comunidad original. No se pretende que esto
ocurra nunca dentro de una versión menor determinada (x.z), por lo que todas
las versiones z-stream mantendrán la compatibilidad de la API y la
funcionalidad de las funciones. SUSE se esforzará por proporcionar avisos de
descatalogación con suficiente antelación en las notas de la versión, junto
con soluciones alternativas, sugerencias y medidas paliativas para minimizar
la interrupción del servicio.</para>
<para>El equipo de SUSE Edge también agradece los comentarios de la comunidad, que
pueden plantear sus cuestiones dentro del repositorio de código
correspondiente en <link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>.</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>Obtención del código fuente</title>
<para>Este producto de SUSE incluye materiales licenciados a SUSE bajo la Licencia
Pública General de GNU (GPL) y otras licencias de código abierto. La GPL
exige a SUSE que proporcione el código fuente correspondiente al material
licenciado bajo la GPL, y SUSE cumple con todos los demás requisitos de las
licencias de código abierto. Por lo tanto, SUSE pone a disposición todo el
código fuente, que generalmente se puede encontrar en el repositorio GitHub
de SUSE Edge (<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>),
el repositorio GitHub de SUSE Rancher (<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)
para los componentes dependientes y, específicamente para SUSE Linux Micro,
el código fuente está disponible para su descarga en <link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>
en "Medium 2".</para>
</section>
<section xml:id="id-legal-notices">
<title>Información legal</title>
<para>SUSE no ofrece ninguna garantía con respecto al contenido o al uso de esta
documentación, y renuncia específicamente a cualquier garantía expresa o
implícita de comerciabilidad o idoneidad para un fin determinado. Además,
SUSE se reserva el derecho de revisar esta publicación y de realizar cambios
en su contenido en cualquier momento, sin la obligación de notificar a
ninguna persona o entidad dichas revisiones o cambios.</para>
<para>Además, SUSE no ofrece ninguna garantía con respecto a ningún software y, en
concreto, rechaza cualquier garantía expresa o implícita de comerciabilidad
o idoneidad para un fin determinado. Asimismo, SUSE se reserva el derecho a
realizar cambios en cualquier parte del software de SUSE, en cualquier
momento, sin obligación alguna de notificar dichos cambios a ninguna persona
o entidad.</para>
<para>Cualquier producto o información técnica proporcionada en virtud del
presente Acuerdo puede estar sujeto a los controles de exportación de los
Estados Unidos y a las leyes comerciales de otros países. Usted se
compromete a cumplir con todas las regulaciones de control de exportaciones
y a obtener las licencias o clasificaciones necesarias para exportar,
reexportar o importar los productos que se entregan. Usted se compromete a
no exportar ni reexportar a entidades que figuren en las listas actuales de
exclusión de exportaciones de los Estados Unidos ni a ningún país sujeto a
embargo o considerado terrorista, tal y como se especifica en las leyes de
exportación de los Estados Unidos. Usted acepta no utilizar los productos
que se entregan para usos finales prohibidos relacionados con armas
nucleares, misiles o armas químicas/biológicas. Consulte <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>
para obtener más información sobre la exportación de software de SUSE. SUSE
no asume ninguna responsabilidad por su incumplimiento en la obtención de
las autorizaciones de exportación necesarias.</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>Este documento de notas de la versión está protegido por una licencia
Creative Commons Attribution-NoDerivatives 4.0 International License
(CC-BY-ND-4.0). Debería haber recibido una copia de la licencia junto con
este documento. Si no es así, consulte <link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>.</para>
<para>SUSE posee los derechos de propiedad intelectual relacionados con la
tecnología incorporada en el producto que se describe en este documento. En
particular, y sin limitación, estos derechos de propiedad intelectual pueden
incluir una o más de las patentes estadounidenses listadas en <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>
y una o más patentes adicionales o solicitudes de patente pendientes en los
Estados Unidos y otros países.</para>
<para>Para obtener información sobre las marcas comerciales de SUSE, consulte la
lista de marcas comerciales y marcas de servicio de SUSE (<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>).
Todas las marcas comerciales de terceros son propiedad de sus respectivos
propietarios. Para obtener información sobre la marca SUSE y los requisitos
de uso, consulte las directrices publicadas en <link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link>.</para>
</section>
</chapter>
</part>
</book>
