<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book
xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink"
xmlns:its="http://www.w3.org/2005/11/its"
xmlns:xlink="http://www.w3.org/1999/xlink"
xmlns:xi="http://www.w3.org/2001/XInclude" xml:lang="zh-cn">
<info>
<title>SUSE Edge 文档</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>2025 年 9 月 26 日</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="suse-edge-documentation">
<title>SUSE Edge 3.4 文档</title>
<para>欢迎阅读 SUSE Edge
文档，在这里可以找到总体体系结构概述、快速入门指南、经过验证的设计、组件用法指南、第三方集成，以及有关管理边缘计算基础架构和工作负载的最佳实践。</para>
<section xml:id="id-what-is-suse-edge">
<title>什么是 SUSE Edge？</title>
<para>SUSE Edge
是有针对性的、紧密集成且经过全面验证的端到端解决方案，用于解决在边缘处部署基础架构和云原生应用程序时存在的独特挑战。其核心作用是提供一个有主见但高度灵活、高度可缩放且安全的平台，涵盖初始部署映像的构建、节点置备和初始配置、应用程序部署、可观测性和完整生命周期操作。该平台从一开始就构建于同类最佳的开源软件基础之上，传承了我们
30 年来提供安全、稳定且经认证的 SUSE Linux 平台的历史，继续通过 Rancher 产品组合提供高度可缩放且功能丰富的 Kubernetes
管理。SUSE Edge 基于这些功能构建，可以提供满足众多细分市场需求的功能，包括零售、医疗、交通、物流、电信、智能制造和工业物联网 (IoT)。</para>
</section>
<section xml:id="id-design-philosophy">
<title>设计理念</title>
<para>该解决方案的设计考虑到了客户的需求和期望千差万别，因此不存在“以一应百”的边缘平台。边缘部署促使我们解决并不断设想出一些极具挑战性的问题，包括大规模可伸缩性、网络受限情况下的可用性、物理空间限制、新的安全威胁和攻击途径、硬件体系结构和系统资源的差异、部署旧式基础架构和应用程序并与之连接的要求，以及使用寿命较长的客户解决方案。由于其中的许多挑战与传统思维方式不同（例如在数据中心或公有云中部署基础架构和应用程序），我们必须更周密地审视设计，并反复思考许多常见假设条件。</para>
<para>例如，我们发现极简主义、模块化和易操作性具有重要价值。极简主义对于边缘环境非常重要，因为系统越复杂，就越容易出现故障。在分析数百乃至数十万个位置后，我们发现复杂的系统会出现纷繁复杂的故障。解决方案中的模块化允许用户做出更多选择，同时消除部署的平台中不必要的复杂性。我们还需要在这些方面与易操作性之间取得平衡。人类用户在重复某个流程数千次后可能会出现失误，因此平台应确保任何潜在失误都可恢复，从而消除技术人员亲临现场解决问题的需要，同时尽力实现一致性和标准化。</para>
</section>
<section xml:id="id-high-level-architecture">
<title>总体体系结构</title>
<para>SUSE Edge
的总体系统体系结构分为两个核心类别，即“管理”群集和“下游”群集。管理群集负责一个或多个下游群集的远程管理。不过，在某些情况下，下游群集需要在没有远程管理的情况下运行，例如在边缘站点没有外部连接并且需要独立运行的情况下。在SUSE
Edge
中，用于管理群集和下游群集运维的技术组件在很大程度上是相同的，只是在系统规范和系统使用的应用程序方面可能有所不同：管理群集运行能够实现系统管理和生命周期操作的应用程序，而下游群集则是满足各项要求以支持用户应用程序的运行。</para>
<section xml:id="id-components-used-in-suse-edge">
<title>SUSE Edge 中使用的组件</title>
<para>SUSE Edge 由现有的 SUSE 和 Rancher 组件以及 Edge
团队构建的其他功能和组件组成，能帮助我们解决边缘计算中存在的限制和复杂性。以下是对管理群集和下游群集中所用组件的说明，并附有总体体系结构概要图（请注意，其中并未列举出所有组件）：</para>
<section xml:id="id-management-cluster">
<title>管理群集</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-management-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>Suse Edge 管理群集</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理</emphasis>：SUSE Edge
的中心部分，用于管理所连下游群集的置备和生命周期。管理群集通常包括以下组件：</para>
<itemizedlist>
<listitem>
<para>通过 Rancher Prime（<xref
linkend="components-rancher"/>）实现的多群集管理，为下游群集的初始配置以及基础架构和应用程序的持续生命周期管理提供了一个通用仪表板，还提供了全面的租户隔离和
<literal>IDP</literal>（身份提供程序）集成、第三方集成和扩展的巨大市场，以及不限供应商的 API。</para>
</listitem>
<listitem>
<para>通过 SUSE Multi-Linux Manager 实现的 Linux 系统管理，支持对下游群集上运行的底层 Linux 操作系统 *SUSE
Linux Micro（<xref linkend="components-slmicro"/>）执行自动化的 Linux
补丁和配置管理。请注意，虽然此组件已容器化，但它目前需要在与其他管理组件分离的单独系统上运行，因此在上图中标记为“Linux 管理”。</para>
</listitem>
<listitem>
<para>专用生命周期管理控制器（<xref
linkend="components-upgrade-controller"/>），用于处理将管理群集组件升级到指定 SUSE Edge
版本的相关事务。</para>
</listitem>
<listitem>
<para>借助 Elemental（<xref linkend="components-elemental"/>）实现的远程系统接入 Rancher
Prime，支持将连接的边缘节点与所需的 Kubernetes 群集和应用程序部署进行后期绑定（例如通过 GitOps）。</para>
</listitem>
<listitem>
<para>可选的完整裸机生命周期与管理支持，通过 Metal3（<xref
linkend="components-metal3"/>）、MetalLB（<xref
linkend="components-metallb"/>）和 <literal>CAPI</literal> (Cluster API)
基础架构提供程序实现，支持对具有远程管理功能的裸机系统执行完整的端到端置备。</para>
</listitem>
<listitem>
<para>名为 Fleet（<xref linkend="components-fleet"/>）的可选 GitOps
引擎，用于管理下游群集和其上安装的应用程序的置备和生命周期。</para>
</listitem>
<listitem>
<para>支撑管理群集本身的是作为基础操作系统的 SUSE Linux Micro（<xref
linkend="components-slmicro"/>），以及作为支持管理群集应用程序的 Kubernetes 发行版的 RKE2（<xref
linkend="components-rke2"/>）。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-downstream-clusters">
<title>下游群集</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-downstream-cluster.svg"
width="100%"/> </imageobject>
<textobject><phrase>Suse Edge 下游群集</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para><emphasis role="strong">下游</emphasis>：SUSE Edge
的分布式组件，用于在边缘运行用户工作负载，即边缘位置运行的软件，通常由以下组件组成：</para>
<itemizedlist>
<listitem>
<para>多种可供选择的 Kubernetes 发行版，包含 K3s（<xref linkend="components-k3s"/>）和 RKE2（<xref
linkend="components-rke2"/>）等安全的轻量级发行版（<literal>RKE2</literal>
已针对政府用途和受监管行业进行强化、认证和优化）。</para>
</listitem>
<listitem>
<para>SUSE Security（<xref
linkend="components-suse-security"/>），提供映像漏洞扫描、深度数据包检测、实时威胁和漏洞防护等安全功能。</para>
</listitem>
<listitem>
<para>借助 SUSE Storage（<xref
linkend="components-suse-storage"/>）实现的软件块存储，提供轻量级、持久化、弹性佳且可扩缩的块存储服务。</para>
</listitem>
<listitem>
<para>基于 SUSE Linux Micro（<xref linkend="components-slmicro"/>）打造的轻量级、容器优化且安全强化的
Linux 操作系统，可为在边缘运行容器和虚拟机提供不可变且高弹性的操作系统环境。SUSE Linux Micro 适用于 AArch64 和
AMD64/Intel 64 两种体系结构，还支持<literal>实时内核</literal>，可满足对延迟敏感的应用需求（如电信领域的使用场景）。</para>
</listitem>
<listitem>
<para>对于连接的群集（即与管理群集连接的群集），会部署两个代理。一个是 Rancher System Agent，用于管理与 Rancher Prime
的连接；另一个是 venv-salt-minion，用于接收 SUSE Multi-Linux Manager 的指令，以执行 Linux
软件更新。这些代理对于离线群集的管理而言并非必需组件。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-connectivity">
<title>连接</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="suse-edge-connected-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>Suse Edge 连接的体系结构</phrase></textobject>
</mediaobject>
</informalfigure>
<para>上图提供了<emphasis
role="strong">连接</emphasis>的下游群集及其与管理群集的连接的总体体系结构概览。管理群集可以部署在各种底层基础架构平台上，包括本地和云平台，具体取决于下游群集和目标管理群集之间的网络连接情况。这项功能的唯一要求是
API 和回调 URL 可以通过连接下游群集节点和管理基础架构的网络进行访问。</para>
<para>必须注意，建立这种连接的机制不同于下游群集部署的机制。下一节将对此进行更深入的介绍，不过先建立一个基本概念，连接的下游群集成为“受管理”群集主要有三种机制：</para>
<orderedlist numeration="arabic">
<listitem>
<para>下游群集先以“断开连接”的方式部署（例如通过 Edge Image Builder（<xref
linkend="components-eib"/>）），然后在连接允许的情况下导入管理群集。</para>
</listitem>
<listitem>
<para>下游群集会被配置为使用内置的初始配置机制（例如通过 Elemental（<xref
linkend="components-elemental"/>）），并且它们在首次引导时会自动注册到管理群集，以允许群集配置的后期绑定。</para>
</listitem>
<listitem>
<para>下游群集已置备裸机管理功能 (CAPI + Metal<superscript>3</superscript>)，群集一旦部署和配置（通过
Rancher Turtles 操作器），它们就会自动导入管理群集。</para>
</listitem>
</orderedlist>
<note>
<para>建议实施多个管理群集，以满足大规模部署需求，优化地理分散环境中的带宽和延迟问题，并在发生故障或管理群集升级时最大限度减少中断影响。您可在<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements">此处</link>查看当前管理群集的可伸缩性限制和系统要求。</para>
</note>
</section>
</section>
<section xml:id="id-common-edge-deployment-patterns">
<title>常见的 Edge 部署模式</title>
<para>由于操作环境和生命周期要求各不相同，我们已针对一些不同的部署模式实施了支持，这些模式与 SUSE Edge
适用的细分市场和使用场景大体一致。我们为其中的每种部署模式编写了一份快速入门指南，以帮助您根据自己的需求熟悉 SUSE Edge
平台。下面介绍了我们目前支持的三种部署模式，并附有相关快速入门页面的链接。</para>
<section xml:id="id-directed-network-provisioning">
<title>定向网络置备</title>
<para>定向网络置备适用于您知道要部署到的硬件的细节，并可以直接访问带外管理界面来编排和自动化整个置备过程的情况。在这种情况下，客户预期解决方案能够从一个中心位置全自动地置备边缘站点，并可以最大限度地减少边缘位置的手动操作，也就是说，解决方案的作用远不止是创建引导映像；您只需将物理硬件装入机架、通电并为其连接所需的网络，自动化过程就会通过带外管理（例如通过
Redfish
API）启动计算机并处理基础架构的置备、初始配置和部署，全程无需用户的干预。做到这一点的关键在于管理员了解系统；他们知道哪个硬件在哪个位置，并且部署预期可以得到集中处理。</para>
<para>此解决方案最为稳健，因为您可以直接与硬件的管理界面交互和处理已知硬件，并且很少会遇到网络可用性方面的限制。在功能上，此解决方案广泛使用 Cluster
API 和 Metal<superscript>3</superscript> 来自动完成从裸机到操作系统、Kubernetes
和分层应用程序的置备，并能够在部署后与 SUSE Edge 的其他常规生命周期管理功能相衔接。此解决方案的快速入门可在<xref
linkend="quickstart-metal3"/>中找到。</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>“自主回连”网络置备</title>
<para>有时，在您的操作环境中，中心管理群集无法直接管理硬件（例如，您的远程网络位于防火墙后面，或者没有带外管理界面；这种情况在边缘处经常使用的“PC”型硬件中很常见）。对于这种情况，我们提供了相应的工具来远程置备群集及其工作负载，而无需知道硬件在引导时位于何处。这就是大多数人对边缘计算的看法；有成千上万种不太常见的系统在边缘位置引导、安全地自主回连、验证它们的身份，并接收有关要执行哪种操作的指令。为此，我们希望除了出厂前预先构建计算机映像，或者简单挂接引导映像（例如通过
USB）来打开系统外，置备和生命周期管理工作几乎不需要用户干预即可完成。在此方面存在的主要挑战是如何解决这些设备在各种环境下的缩放性、一致性、安全性和生命周期管理问题。</para>
<para>此解决方案在系统置备和初始配置方式上提供了高度的灵活性和一致性，不受系统位置、类型或规格以及首次开机时间的影响。在 SUSE Edge 中，可以通过
Edge Image Builder 十分灵活地对系统进行自定义，同时可以利用 Rancher Elemental 提供的注册功能来初始配置节点和置备
Kubernetes，并利用 SUSE Multi-Linux 来修补操作系统。此解决方案的快速入门可在<xref
linkend="quickstart-elemental"/>中找到。</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>基于映像的置备</title>
<para>对于需要在独立、隔离或网络受限的环境中操作的客户，SUSE Edge
提供了一种解决方案供客户生成完全自定义的安装媒体，其中包含所有必要的部署制品，用于在边缘启用单节点和多节点高可用性 Kubernetes
群集，包括任何所需的工作负载或其他分层组件。全程无需与外界建立任何网络连接，且无需集中式管理平台的干预。用户体验与“自主回连”解决方案非常相似，安装媒体同样会提供给目标系统，但该解决方案会“就地引导”。在这种情况下，可将创建的群集挂接到
Rancher
进行持续管理（即从“断开连接”转换为“已连接”操作模式，无需经过大范围的重新配置或重新部署），或者群集可以继续独立运行。请注意，在这两种情况下，都可以采用相同的一致性机制来自动执行生命周期操作。</para>
<para>此外，使用此解决方案可以快速创建管理群集，这些管理群集可以托管同时支持“定向网络置备”和“自主回连网络置备”模型的集中式基础架构，因为它可能是置备各种
Edge 基础架构最快速、最简单的方式。此解决方案充分利用 SUSE Edge Image Builder
的功能，生成完全自定义的无人照管安装媒体；快速入门可在<xref linkend="quickstart-eib"/>中找到。</para>
</section>
</section>
<section xml:id="id-suse-edge-stack-validation">
<title>SUSE Edge 堆栈验证</title>
<para>所有 SUSE Edge 版本均由紧密集成且经过全面验证的组件组成，这些组件作为一个整体进行版本控制。SUSE Edge
团队在集成和堆栈验证方面不断努力，不仅测试了组件之间的集成，还确保系统在因外界因素影响而发生故障的情况下仍能按预期运行。所有测试运行和结果均已向公众发布，结果和所有输入参数都可以在
<link xl:href="https://ci.edge.suse.com">ci.edge.suse.com</link> 上找到。</para>
</section>
<section xml:id="id-full-component-list">
<title>完整组件列表</title>
<para>以下是组件的完整列表，以及对每种组件的概要说明及其在 SUSE Edge 中用法的相关链接：</para>
<itemizedlist>
<listitem>
<para>Rancher（<xref linkend="components-rancher"/>）</para>
</listitem>
<listitem>
<para>Rancher 仪表板扩展（<xref linkend="components-rancher-dashboard-extensions"/>）</para>
</listitem>
<listitem>
<para>Rancher Turtles（<xref linkend="components-rancher-turtles"/>）</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux Manager</para>
</listitem>
<listitem>
<para>Fleet（<xref linkend="components-fleet"/>）</para>
</listitem>
<listitem>
<para>SUSE Linux Micro（<xref linkend="components-slmicro"/>）</para>
</listitem>
<listitem>
<para>Metal³（<xref linkend="components-metal3"/>）</para>
</listitem>
<listitem>
<para>Edge Image Builder（<xref linkend="components-eib"/>）</para>
</listitem>
<listitem>
<para>NetworkManager Configurator（<xref linkend="components-nmc"/>）</para>
</listitem>
<listitem>
<para>Elemental（<xref linkend="components-elemental"/>）</para>
</listitem>
<listitem>
<para>Akri（<xref linkend="components-akri"/>）</para>
</listitem>
<listitem>
<para>K3s（<xref linkend="components-k3s"/>）</para>
</listitem>
<listitem>
<para>RKE2（<xref linkend="components-rke2"/>）</para>
</listitem>
<listitem>
<para>SUSE Storage（<xref linkend="components-suse-storage"/>）</para>
</listitem>
<listitem>
<para>SUSE Security（<xref linkend="components-suse-security"/>）</para>
</listitem>
<listitem>
<para>MetalLB（<xref linkend="components-metallb"/>）</para>
</listitem>
<listitem>
<para>KubeVirt（<xref linkend="components-kubevirt"/>）</para>
</listitem>
<listitem>
<para>系统升级控制器（<xref linkend="components-system-upgrade-controller"/>）</para>
</listitem>
<listitem>
<para>升级控制器（<xref linkend="components-upgrade-controller"/>）</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>快速入门</title>
<partintro>
<para>从这里快速入门</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>使用 Metal<superscript>3</superscript> 实现 BMC 自动化部署</title>
<para>Metal<superscript>3</superscript> 是一个 <link
xl:href="https://metal3.io/">CNCF 项目</link>，它为 Kubernetes 提供裸机基础架构管理功能。</para>
<para>Metal<superscript>3</superscript> 提供 Kubernetes 原生资源来管理裸机服务器的生命周期，支持通过 <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link> 等带外协议进行管理。</para>
<para>它还为 <link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link> 提供成熟的支持，允许通过广泛采用的不限供应商的 API 来管理跨多个基础架构提供商的基础架构资源。</para>
<section xml:id="id-why-use-this-method">
<title>为何使用此方法</title>
<para>此方法非常适合用于目标硬件支持带外管理，并且需要全自动化基础架构管理流程的场景。</para>
<para>管理群集配置为提供声明性 API 来对下游群集裸机服务器进行清单和状态管理，包括自动检查、清理和置备/取消置备。</para>
</section>
<section xml:id="id-high-level-architecture-2">
<title>总体体系结构</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>metal3 体系结构快速入门</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>先决条件</title>
<para>下游群集服务器硬件和网络相关的限制具体如下所述：</para>
<itemizedlist>
<listitem>
<para>管理群集</para>
<itemizedlist>
<listitem>
<para>必须与目标服务器管理/BMC API 建立网络连接</para>
</listitem>
<listitem>
<para>必须与目标服务器控制平面网络建立网络连接</para>
</listitem>
<listitem>
<para>对于多节点管理群集，需要配置额外的预留 IP 地址</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>要控制的主机</para>
<itemizedlist>
<listitem>
<para>必须支持通过 Redfish、iDRAC 或 iLO 接口进行带外管理</para>
</listitem>
<listitem>
<para>必须支持通过虚拟媒体进行部署（目前不支持 PXE）</para>
</listitem>
<listitem>
<para>必须与管理群集建立网络连接，以便能够访问 Metal<superscript>3</superscript> 置备 API</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>需要安装一些工具，可以安装在管理群集上，也可以安装在能够访问管理群集的主机上。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://helm.sh">Helm</link> 和 <link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para><link xl:href="https://podman.io">Podman</link> 或 <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link> 等容器运行时</para>
</listitem>
</itemizedlist>
<para><literal>SL-Micro.x86_64-6.1-Base-GM.raw</literal> 操作系统映像文件必须从 <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link> 或 <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE 下载页面</link>下载。</para>
</section>
<section xml:id="id-deployment">
<title>部署</title>
<section xml:id="id-setup-management-cluster">
<title>设置管理群集</title>
<para>安装管理群集和使用 Metal<superscript>3</superscript> 的基本步骤如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para>安装 RKE2 管理群集</para>
</listitem>
<listitem>
<para>安装 Rancher</para>
</listitem>
<listitem>
<para>安装存储服务提供程序（可选）</para>
</listitem>
<listitem>
<para>安装 Metal<superscript>3</superscript> 依赖项</para>
</listitem>
<listitem>
<para>通过 Rancher Turtles 安装 CAPI 依赖项</para>
</listitem>
<listitem>
<para>为下游群集主机构建 SLEMicro 操作系统映像</para>
</listitem>
<listitem>
<para>注册 BareMetalHost CR 以定义裸机清单</para>
</listitem>
<listitem>
<para>通过定义 CAPI 资源创建下游群集</para>
</listitem>
</orderedlist>
<para>本指南假设已安装现有的 RKE2 群集和 Rancher（包括 cert-manager），例如已使用 Edge Image Builder
安装（<xref linkend="components-eib"/>）。</para>
<tip>
<para>也可以按照管理群集文档（<xref linkend="atip-management-cluster"/>）中所述，将此处所述的步骤完全自动化。</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>安装 Metal<superscript>3</superscript> 依赖项</title>
<para>必须安装并运行 cert-manager（如果在安装 Rancher 的过程中未安装）。</para>
<para>必须安装持久性存储提供程序。建议使用 SUSE Storage，但对于开发/PoC 环境，也可以使用
<literal>local-path-provisioner</literal>。以下说明假设已将 StorageClass <link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">标记为默认值</link>，否则需要对
Metal<superscript>3</superscript> chart 进行额外的配置。</para>
<para>需要提供一个额外的 IP，该 IP 由 <link
xl:href="https://metallb.universe.tf/">MetalLB</link> 管理，用于为
Metal<superscript>3</superscript> 管理服务提供一致的端点。此 IP
必须是控制平面子网的一部分，并且为静态配置预留（不是任何 DHCP 池的一部分）。</para>
<tip>
<para>如果管理群集仅包含单个节点，则可以避免通过 MetalLB 管理额外的浮动 IP，具体参见<xref
linkend="id-single-node-configuration"/></para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>首先，我们需要安装 MetalLB：</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>然后使用预留 IP 来定义 <literal>IPAddressPool</literal> 和
<literal>L2Advertisement</literal>（如下方的 <literal>STATIC_IRONIC_IP</literal>
所定义）：</para>
<screen language="bash" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>现在可以安装 Metal<superscript>3</superscript>：</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/charts/metal3 \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="$STATIC_IRONIC_IP"</screen>
</listitem>
<listitem>
<para>大约要经过两分钟，init 容器才会在此部署中运行，因此请确保所有 Pod 都已运行再继续操作：</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para>请在 <literal>metal3-system</literal> 名称空间中的所有 Pod 都已运行后再继续执行以下步骤。</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>安装 Cluster API 依赖项</title>
<para>Cluster API 依赖项通过 Rancher Turtles Helm chart 管理：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/charts/rancher-turtles \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</screen>
<para>一段时间后，控制器 Pod 应会在
<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>
和 <literal>rke2-control-plane-system</literal> 名称空间中运行。</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>准备下游群集映像</title>
<para>Kiwi（<xref linkend="guides-kiwi-builder-images"/>）和 Edge Image Builder（<xref
linkend="components-eib"/>）用于准备将在下游群集主机上置备的经过修改的 SLEMicro 基础映像。</para>
<para>本指南介绍部署下游群集所需的最低限度配置。</para>
<section xml:id="id-image-configuration">
<title>映像配置</title>
<note>
<para>首先，请先按照<xref linkend="guides-kiwi-builder-images"/>中的说明构建一个创建群集所需的全新映像。</para>
</note>
<para>运行 Edge Image Builder 时，将从主机挂载一个目录，因此需要创建一个目录结构来存储用于定义目标映像的配置文件。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> 是映像定义文件，有关详细信息，请参见<xref
linkend="quickstart-eib"/>。</para>
</listitem>
<listitem>
<para>下载的基础映像已经过 <literal>xz</literal> 压缩，必须使用 <literal>unxz</literal>
将其解压缩，并将其复制/移动到 <literal>base-images</literal> 文件夹中。</para>
</listitem>
<listitem>
<para><literal>network</literal> 文件夹是可选的，有关详细信息，请参见<xref
linkend="metal3-add-network-eib"/>。</para>
</listitem>
<listitem>
<para>custom/scripts 目录包含首次引导时运行的脚本；目前需要使用 <literal>01-fix-growfs.sh</literal>
脚本来调整部署中的操作系统根分区的大小</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>下游群集映像定义文件</title>
<para><literal>downstream-cluster-config.yaml</literal> 文件是下游群集映像的主配置文件。下面是通过
Metal<superscript>3</superscript> 进行部署的极简示例：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
  sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>其中 <literal>$SCC_REGISTRATION_CODE</literal> 是从 <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>
中复制的注册代码，并且软件包列表包含必需的 <literal>jq</literal>。</para>
<para><literal>$ROOT_PASSWORD</literal> 是 root 用户的已加密口令，可用于测试/调试目的。可以使用
<literal>openssl passwd-6 PASSWORD</literal> 命令生成此口令</para>
<para>对于生产环境，建议使用可添加到 users 块的 SSH 密钥（需将 <literal>$USERKEY1</literal> 替换为实际 SSH
密钥）。</para>
<note>
<para><literal>net.ifnames=1</literal> 会启用<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">可预测网络接口命名</link></para>
<para>这与 Metal<superscript>3</superscript> chart 的默认配置相匹配，但设置必须与配置的 chart
<literal>predictableNicNames</literal> 值相匹配。</para>
<para>另请注意，<literal>ignition.platform.id=openstack</literal> 是必需的，如果不指定此参数，在
Metal<superscript>3</superscript> 自动化流程中通过 ignition 进行 SUSE Linux Micro
配置将会失败。</para>
<para><literal>time</literal>
部分为可选配置，但强烈建议您进行设置，以避免证书和时钟偏差可能引发的问题。本示例中提供的值仅作说明之用，请根据您的具体要求相应调整。</para>
</note>
</section>
<section xml:id="growfs-script">
<title>Growfs 脚本</title>
<para>目前，在置备后首次引导时，需要使用一个自定义脚本
(<literal>custom/scripts/01-fix-growfs.sh</literal>)
来增大文件系统，使之与磁盘大小匹配。<literal>01-fix-growfs.sh</literal> 脚本包含以下信息：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>使用相同的方法添加要在置备过程中执行的您自己的自定义脚本。有关详细信息，请参见<xref linkend="quickstart-eib"/>。</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>映像创建</title>
<para>按照前面的章节准备好目录结构后，运行以下命令来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>这会根据上述定义创建名为 <literal>SLE-Micro-eib-output.raw</literal> 的输出映像文件。</para>
<para>然后必须通过 Web 服务器提供输出映像，该服务器可以是通过 Metal3 chart 启用的媒体服务器容器（<xref
linkend="metal3-media-server"/>），也可以是其他某个本地可访问的服务器。在下面的示例中，此服务器是
<literal>imagecache.local:8080</literal></para>
<note>
<para>将 EIB 映像部署到下游群集时，还需要在 <literal>Metal3MachineTemplate</literal> 对象中包含该映像的
sha256 校验和。可通过以下方式生成该校验和：</para>
<screen language="shell" linenumbering="unnumbered">sha256sum &lt;image_file&gt; &gt; &lt;image_file&gt;.sha256
# On this example:
sha256sum SLE-Micro-eib-output.raw &gt; SLE-Micro-eib-output.raw.sha256</screen>
</note>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>添加 BareMetalHost 清单</title>
<para>注册自动部署的裸机服务器需要创建两个资源：一个用于存储 BMC 访问身份凭证的机密，以及一个用于定义 BMC 连接和其他细节的
Metal<superscript>3</superscript> BareMetalHost 资源：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>请注意以下几点：</para>
<itemizedlist>
<listitem>
<para>机密用户名/口令必须采用 base64 编码。请注意，此值不能包含任何尾部换行符（例如，应使用
<literal>echo-n</literal>，而不是简单地使用 <literal>echo</literal>！）</para>
</listitem>
<listitem>
<para>可以现在设置 <literal>cluster-role</literal> 标签，也可以稍后在创建群集时设置。在以下示例中，应该设置
<literal>control-plane</literal> 或 <literal>worker</literal></para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal> 必须是与主机的控制平面 NIC 匹配的有效 MAC</para>
</listitem>
<listitem>
<para><literal>bmc</literal> 地址用于连接 BMC 管理 API，支持以下类型：</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;IP 地址&gt;/redfish/v1/Systems/&lt;系统
ID&gt;</literal>：Redfish 虚拟媒体，例如 SuperMicro</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;IP
地址&gt;/redfish/v1/Systems/System.Embedded.1</literal>：Dell iDRAC</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>有关 BareMetalHost API 的详细信息，请参见<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">上游
API 文档</link></para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>配置静态 IP</title>
<para>上面的 BareMetalHost 示例假设 DHCP 提供控制平面网络配置，但对于需要手动配置（例如静态 IP）的情况，可以提供附加配置，如下所述。</para>
<section xml:id="metal3-add-network-eib">
<title>用于静态网络配置的附加脚本</title>
<para>使用 Edge Image Builder 创建基础映像时，请在 <literal>network</literal> 文件夹中创建以下
<literal>configure-network.sh</literal> 文件。</para>
<para>这会在首次引导时使用配置驱动器数据，并使用 <link
xl:href="https://github.com/suse-edge/nm-configurator">NM Configurator
工具</link>来配置主机网络。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>包含主机网络配置的附加机密</title>
<para>可为每个主机定义一个附加机密，其中包含采用 NM Configurator（<xref linkend="components-nmc"/>）所支持的
<link xl:href="https://nmstate.io/">nmstate</link> 格式的数据。</para>
<para>然后通过 <literal>preprovisioningNetworkDataName</literal> 规范字段在
<literal>BareMetalHost</literal> 资源中引用该机密。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>在某些情况下，可以省略 MAC 地址。请参见<xref linkend="networking-unified"/>了解更多信息。</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>BareMetalHost 准备</title>
<para>按照上述步骤创建 BareMetalHost 资源和关联的机密后，将触发主机准备工作流程：</para>
<itemizedlist>
<listitem>
<para>通过将虚拟媒体挂接到目标主机 BMC，引导内存盘映像</para>
</listitem>
<listitem>
<para>内存盘会检查硬件细节，并为主机做好置备准备（例如，清理磁盘中的旧数据）</para>
</listitem>
<listitem>
<para>完成此过程后，BareMetalHost <literal>status.hardware</literal> 字段中的硬件细节将会更新并可供验证</para>
</listitem>
</itemizedlist>
<para>此过程可能需要几分钟时间，但完成后，您应该会看到 BareMetalHost 状态变为 <literal>available</literal>：</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>创建下游群集</title>
<para>现在创建用于定义下游群集的 Cluster API 资源，以及创建计算机资源，后者会导致置备 BareMetalHost 资源，然后引导这些资源以形成
RKE2 群集。</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>控制平面部署</title>
<para>为了部署控制平面，我们需要定义一个如下所示的 yaml 清单，其中包含以下资源：</para>
<itemizedlist>
<listitem>
<para>群集资源定义群集名称、网络和控制平面/基础架构提供程序的类型（在本例中为 RKE2/Metal3）</para>
</listitem>
<listitem>
<para>Metal3Cluster 定义控制平面端点（单节点群集的主机 IP，多节点群集的负载平衡器端点，本示例假设使用单节点群集）</para>
</listitem>
<listitem>
<para>RKE2ControlPlane 定义 RKE2 版本，以及群集引导期间所需的任何其他配置</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate 定义要应用于 BareMetalHost 资源的操作系统映像，hostSelector 定义要使用的
BareMetalHost</para>
</listitem>
<listitem>
<para>Metal3DataTemplate 定义要传递给 BareMetalHost 的其他元数据（请注意，Edge 解决方案目前不支持
networkData）</para>
</listitem>
</itemizedlist>
<note>
<para>为简单起见，本示例假设使用单节点控制平面，其中 BareMetalHost 配置了 IP
<literal>192.168.125.200</literal>。有关更高级的多节点示例，请参见<xref
linkend="atip-automated-provisioning"/>。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>根据您的环境对该示例清单进行调整后，便可通过 <literal>kubectl</literal> 应用该清单，然后通过
<literal>clusterctl</literal> 监控群集状态。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>工作/计算节点部署</title>
<para>与部署控制平面时一样，我们需要定义一个 YAML 清单，其中包含以下资源：</para>
<itemizedlist>
<listitem>
<para>MachineDeployment 定义复本（主机）数量和引导/基础架构提供程序（在本例中为 RKE2/Metal3）</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplate 描述代理主机引导的 RKE2 版本和首次引导配置</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate 定义要应用于 BareMetalHost 资源的操作系统映像，主机选择器定义要使用的
BareMetalHost</para>
</listitem>
<listitem>
<para>Metal3DataTemplate 定义要传递给 BareMetalHost 的其他元数据（请注意，目前不支持
<literal>networkData</literal>）</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.33.3+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.33.3+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>复制以上示例并根据您的环境进行修改后，可以通过 <literal>kubectl</literal> 应用该示例，然后使用
<literal>clusterctl</literal> 监控群集状态</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait for the worker nodes to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>群集取消置备</title>
<para>可以通过删除上述创建步骤中应用的资源来取消置备下游群集：</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>这会触发 BareMetalHost 资源的取消置备，此过程可能需要几分钟，之后，这些资源将再次处于可用状态：</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>已知问题</title>
<itemizedlist>
<listitem>
<para>目前不支持上游 <link xl:href="https://github.com/metal3-io/ip-address-manager">IP
地址管理控制器</link>，因为它与我们在 SLEMicro 中选择的网络配置工具和首次引导工具链尚不兼容。</para>
</listitem>
<listitem>
<para>此外，也不支持相关的 IPAM 资源和 Metal3DataTemplate networkData 字段。</para>
</listitem>
<listitem>
<para>目前仅支持通过 redfish-virtualmedia 进行部署。</para>
</listitem>
<listitem>
<para>在 Ironic Python Agent (IPA) 与目标操作系统 (SL Micro 6.0/6.1)
之间，可能会出现网络设备名称不匹配的情况，尤其是在尝试为设备配置可预测名称时。</para>
</listitem>
</itemizedlist>
<para>出现此问题的原因是，当前 Ironic Python Agent (IPA) 的内核与目标操作系统 (SL Micro 6.0/6.1)
的内核未保持一致，导致网络驱动程序不匹配，使得 IPA 发现网络设备时采用的命名规则与 SL Micro 的预期不一致。</para>
<para>目前可采用以下两种方法作为临时解决方案：* 创建两个不同的网络配置机密，一个供 IPA 使用，其设备名称需与 IPA 发现的名称一致，并在
<literal>BareMetalHost</literal> 定义中用作
<literal>preprovisioningNetworkDataName</literal>；另一个机密的设备名称需与 SL Micro
发现的名称一致，并在 <literal>BareMetalHost</literal> 定义中用作
<literal>networkData.name</literal>。* 改用 UUID 在生成的 nmconnection
文件中引用其他接口。更多详细信息请参见相关<link xl:href="..tips/metal3.adoc">提示与技巧</link>。</para>
</section>
<section xml:id="id-planned-changes">
<title>计划的更改</title>
<itemizedlist>
<listitem>
<para>通过 networkData 字段启用 IPAM 资源和配置支持</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>其他资源</title>
<para>SUSE Telco Cloud 文档（<xref linkend="atip"/>）中提供了针对电信领域使用场景的
Metal<superscript>3</superscript> 高级用法示例。</para>
<section xml:id="id-single-node-configuration">
<title>单节点配置</title>
<para>对于管理群集仅包含单个节点的测试/PoC 环境，可能不需要通过 MetalLB 管理额外的浮动 IP。</para>
<para>在此模式下，管理群集 API 的端点是管理群集的 IP，因此在使用 DHCP 时应预留该 IP，或者配置静态 IP，以确保管理群集 IP 不会变化 -
在下面显示为 <literal>MANAGEMENT_CLUSTER_IP</literal>。</para>
<para>为了实现此方案，必须如下所示指定 Metal<superscript>3</superscript> chart 值：</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="disabling-tls-for-virtualmedia-iso-attachment">
<title>对虚拟媒体 ISO 挂接禁用 TLS</title>
<para>某些服务器供应商在将虚拟媒体 ISO 映像挂接到 BMC 时会验证 SSL 连接，这可能会导致出现问题，因为针对
Metal<superscript>3</superscript> 部署生成的证书是自我签名证书。要解决此问题，可以使用如下所示的
Metal<superscript>3</superscript> chart 值仅对虚拟媒体磁盘挂接禁用 TLS：</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>另一种解决方法是使用 CA 证书配置 BMC - 在这种情况下，可以使用 <literal>kubectl</literal> 从群集读取证书：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>然后可以在服务器 BMC 控制台上配置证书，不过，配置过程因供应商而异（并且不一定适用于所有供应商，如果不适用，可能需要指定
<literal>enable_vmedia_tls</literal> 标志）。</para>
</section>
<section xml:id="id-storage-configuration">
<title>存储配置</title>
<para>在管理群集仅包含一个节点的测试/PoC 环境中，不需要持久性存储机制；但对于生产环境，建议在管理群集上安装 SUSE Storage
(Longhorn)，以便与 Metal<superscript>3</superscript> 相关的映像在 Pod 重启或重新调度时能够保持持久性。</para>
<para>为了实现此持久性存储机制，必须如下所示指定 Metal<superscript>3</superscript> chart 值：</para>
<screen language="yaml" linenumbering="unnumbered">metal3-ironic:
  persistence:
    ironic:
      size: "5Gi"</screen>
<para>SUSE Telco Cloud 管理群集文档（<xref
linkend="atip-management-cluster"/>）详细介绍了如何配置使用持久性存储机制的管理群集。</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>使用 Elemental 进行远程主机接入</title>
<para>本章介绍 SUSE Edge 中的“自主回连网络置备”解决方案。我们将使用 Elemental 来协助完成节点接入。Elemental
是一个软件堆栈，可用于注册远程主机和通过 Kubernetes 实现集中式云原生操作系统全面管理。在 SUSE Edge 堆栈中，我们将使用
Elemental 的注册功能将远程主机接入 Rancher，以便将主机集成到集中式管理平台，然后从该平台部署和管理 Kubernetes
群集以及分层组件、应用程序及其生命周期，所有这些操作都在一个中心位置完成。</para>
<para>此方法在以下情况下可能很有用：您要控制的设备与管理群集不在同一网络中，或没有带外管理控制器接入功能可以进行更直接的控制；您要在边缘处引导许多不同的“未知”系统，并且需要安全地大规模接入和管理这些系统。这种情况在以下领域的使用场景中很常见：零售、工业物联网，或几乎无法通过设备要安装到的网络进行控制的其他领域。</para>
<section xml:id="id-high-level-architecture-3">
<title>总体体系结构</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.svg"
width="100%"/> </imageobject>
<textobject><phrase>Elemental 体系结构快速入门</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>所需资源</title>
<para>下面说明了学习本快速入门所要满足的最低系统和环境要求：</para>
<itemizedlist>
<listitem>
<para>集中式管理群集的主机（托管 Rancher 和 Elemental 的主机）：</para>
<itemizedlist>
<listitem>
<para>开发或测试环境至少需要 8 GB 内存和 20 GB 磁盘空间（生产环境要求请参见<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/installation-requirements#hardware-requirements">此处</link>）</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>要置备的目标节点，即边缘设备（可以使用虚拟机进行演示或测试）</para>
<itemizedlist>
<listitem>
<para>至少 4GB RAM、2 个 CPU 核心和 20 GB 磁盘空间</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>管理群集的可解析主机名，或用于 sslip.io 等服务的静态 IP 地址</para>
</listitem>
<listitem>
<para>用于通过 Edge Image Builder 构建安装媒体的主机</para>
<itemizedlist>
<listitem>
<para>运行 SLES 15 SP6、openSUSE Leap 15.6 或其他支持 Podman 的兼容操作系统。</para>
</listitem>
<listitem>
<para>已安装 <link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://podman.io">Podman</link> 和 <link
xl:href="https://helm.sh">Helm</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>用于引导的 USB 闪存盘（如果使用物理硬件）</para>
</listitem>
<listitem>
<para>下载的最新 SUSE Linux Micro 6.1 自安装 ISO 映像，该映像可在<link
xl:href="https://www.suse.com/download/sle-micro/">此处</link>获取。</para>
</listitem>
</itemizedlist>
<note>
<para>在初始配置过程中会重写目标计算机上的现有数据，因此请务必备份挂接到目标部署节点的所有 USB 存储设备和磁盘上的所有数据。</para>
</note>
<para>本指南是使用托管上游群集的 Digital Ocean droplet 以及用作下游设备的 Intel NUC 制作的。为了构建安装媒体，我们使用了
SUSE Linux Enterprise Server。</para>
</section>
<section xml:id="build-bootstrap-cluster">
<title>构建引导群集</title>
<para>首先创建一个能够托管 Rancher 和 Elemental 的群集。必须可以从下游节点所连接到的网络路由此群集。</para>
<section xml:id="id-create-kubernetes-cluster">
<title>创建 Kubernetes 群集</title>
<para>如果您使用的是超大规模云（例如 Azure、AWS 或 Google
Cloud），那么，设置群集的最简单方法是使用这些云的内置工具。为简洁起见，本指南不会详细介绍使用每种选项的操作过程。</para>
<para>如果您要安装到裸机或其他宿主服务，同时需要提供 Kubernetes 发行版本身，我们建议您使用 <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>。</para>
</section>
<section xml:id="id-set-up-dns">
<title>设置 DNS</title>
<para>在继续之前，需要设置群集访问权限。与群集本身的设置一样，DNS 的配置方式因群集的托管位置而异。</para>
<tip>
<para>如果您不想处理 DNS 记录的设置（例如，这只是一个临时使用的测试服务器），可以改用 <link
xl:href="https://sslip.io">sslip.io</link> 之类的服务。通过此服务，可以使用
<literal>&lt;address&gt;.sslip.io</literal> 解析任何 IP 地址。</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>安装 Rancher</title>
<para>要安装 Rancher，需要访问刚刚创建的群集的 Kubernetes API。具体方式因使用的 Kubernetes 发行版而异。</para>
<para>对于 RKE2，kubeconfig 文件会写入
<literal>/etc/rancher/rke2/rke2.yaml</literal>。将此文件作为
<literal>~/.kube/config</literal> 保存在本地系统上。可能需要编辑该文件，以包含正确的外部可路由 IP 地址或主机名。</para>
<para>您可通过 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Rancher
文档</link>中的命令轻松安装 Rancher：</para>
<para>安装 <link xl:href="https://cert-manager.io">cert-manager</link>：</para>
<screen language="bash" linenumbering="unnumbered">helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace \
 --set crds.enabled=true</screen>
<para>然后安装 Rancher 本身：</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo update
helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt; \
  --version 2.12.1</screen>
<note>
<para>如果目标系统是生产系统，请使用 cert-manager 配置实际证书（例如 Let's Encrypt 提供的证书）。</para>
</note>
<para>浏览到您设置的主机名，然后使用您的 <literal>bootstrapPassword</literal> 登录到
Rancher。系统将指导您完成一个简短的设置过程。</para>
</section>
<section xml:id="install-elemental">
<title>安装 Elemental</title>
<para>安装 Rancher 后，接下来可以安装 Elemental Operator 和所需的 CRD。Elemental 的 Helm chart 作为
OCI 制品发布，因此其安装过程比其他 chart 略简单一些。可以通过您用来安装 Rancher 的同一外壳安装 Helm
chart，也可以在浏览器中通过 Rancher 的外壳安装。</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.7.3

helm install -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.7.3</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>（可选）安装 Elemental UI 扩展</title>
<orderedlist numeration="arabic">
<listitem>
<para>要使用 Elemental UI，请登录到您的 Rancher 实例，单击左上角的三线菜单：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width="85%"/> </imageobject>
<textobject><phrase>安装 Elemental 扩展 1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>在此页面上的“Available”（可用）选项卡中，单击“Elemental”卡片上的“Install”（安装）：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width="85%"/> </imageobject>
<textobject><phrase>安装 Elemental 扩展 2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>确认您要安装该扩展：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width="100%"/> </imageobject>
<textobject><phrase>安装 Elemental 扩展 3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>安装后，系统将提示您重新加载页面。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width="100%"/> </imageobject>
<textobject><phrase>安装 Elemental 扩展 4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>重新加载后，可以通过“OS Management”（操作系统管理）全局应用程序访问 Elemental 扩展。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width="100%"/> </imageobject>
<textobject><phrase>访问 Elemental 扩展</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configure-elemental">
<title>配置 Elemental</title>
<para>为方便起见，我们建议将变量 <literal>$ELEM</literal> 设置为配置目录所在的完整路径：</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>为了能够将计算机注册到 Elemental，我们需要在 <literal>fleet-default</literal> 名称空间中创建
<literal>MachineRegistration</literal> 对象。</para>
<para>我们来创建该对象的基本版本：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para><literal>cat</literal> 命令使用反斜杠 (<literal>\</literal>) 将每个
<literal>$</literal> 符号转义，以免 Bash 将其模板化。如果手动复制命令，请去除反斜杠。</para>
</note>
<para>创建该对象后，找到并记下分配的端点：</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>或者，可以在 UI 中执行此操作。</para>
<variablelist>
<varlistentry>
<term>UI 扩展</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>在“OS Management”（操作系统管理）扩展中，单击“Create Registration Endpoint”（创建注册端点）：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png"
width="100%"/> </imageobject>
<textobject><phrase>单击“Create Registration”（创建注册）</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>为此配置命名。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png"
width="100%"/> </imageobject>
<textobject><phrase>添加名称</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>您可以忽略“Cloud Configuration”（云配置）字段，因为此处的数据将被 Edge Image Builder 中的后续步骤覆盖。</para>
</note>
</listitem>
<listitem>
<para>接下来，向下滚动并单击“Add Label”（添加标签），为注册计算机时创建的每个资源添加标签。标签可用于区分计算机。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png"
width="100%"/> </imageobject>
<textobject><phrase>添加标签</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>单击“Create”（创建）以保存配置。</para>
</listitem>
<listitem>
<para>创建注册后，您应该就会看到列出的注册 URL，此时可以单击“Copy”（复制）来复制该网址：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width="100%"/>
</imageobject>
<textobject><phrase>复制 URL</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>如果您退出了该屏幕，可以单击左侧菜单中的“Registration Endpoints”（注册端点），然后单击刚刚创建的端点的名称。</para>
</tip>
<para>此 URL 将在下一步骤中使用。</para>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="build-installation-media">
<title>构建映像</title>
<para>尽管当前版本的 Elemental 支持构建自身的安装媒体，但在 SUSE Edge 3.4 中，我们改用 Kiwi 和 Edge Image
Builder 来完成此项工作，因此最终生成的系统将以 <link
xl:href="https://www.suse.com/products/micro/">SUSE Linux Micro</link>
作为基础操作系统。</para>
<tip>
<para>有关使用 Kiwi 的详细信息，请先按照 Kiwi 映像构建器流程（<xref
linkend="guides-kiwi-builder-images"/>）构建全新映像；如要使用 Edge Image Builder，请参见
Edge Image Builder 入门指南（<xref linkend="quickstart-eib"/>）和组件文档（<xref
linkend="components-eib"/>）。</para>
</tip>
<para>在安装了 Podman 的 Linux 系统中，创建相应目录并将 Kiwi 所构建的基础映像存放到其中：</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
cp /path/to/{micro-base-image-iso} $ELEM/eib_quickstart/base-images/
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.3
image:
    imageType: iso
    arch: x86_64
    baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  isoConfiguration:
    installDevice: /dev/vda
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: XXX
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para><literal>time</literal>
部分为可选配置，但强烈建议您进行设置，以避免证书和时钟偏差可能引发的问题。本示例中提供的值仅作说明之用，请根据您的具体要求相应调整。</para>
</listitem>
<listitem>
<para>未编码的口令是 <literal>eib</literal>。</para>
</listitem>
<listitem>
<para>要从官方来源下载和安装必要的 RPM，则需要配置 <literal>sccRegistrationCode</literal>（或者，也可以手动侧载
<literal>elemental-register</literal> 和
<literal>elemental-system-agent</literal>）</para>
</listitem>
<listitem>
<para><literal>cat</literal> 命令使用反斜杠 (<literal>\</literal>) 将每个
<literal>$</literal> 符号转义，以免 Bash 将其模板化。如果手动复制命令，请去除反斜杠。</para>
</listitem>
<listitem>
<para>安装期间将会擦除安装设备。</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file eib-config.yaml</screen>
<para>如果要引导物理设备，我们需要将映像刻录到 USB 闪存盘中。为此请使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>引导下游节点</title>
<para>现在我们已创建安装媒体，接下来可以用它来引导下游节点。</para>
<para>对于您要使用 Elemental 控制的每个系统，请添加安装媒体并引导设备。安装后，系统会重引导并自行注册。</para>
<para>如果您正在使用 UI 扩展，则应会看到您的节点出现在“Inventory of Machines”（计算机清单）中。</para>
<note>
<para>在出现登录提示之前，请勿移除安装媒体；在首次引导期间，仍需访问 USB 记忆棒上的文件。</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>创建下游群集</title>
<para>使用 Elemental 置备新群集时，需要创建两个对象。</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>第一个对象是
<literal>MachineInventorySelectorTemplate</literal>。此对象用于指定群集与清单中计算机之间的映射。</para>
<orderedlist numeration="arabic">
<listitem>
<para>创建一个选择器，用于根据标签匹配清单中的任何计算机：</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>将资源应用于群集：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>获取计算机名称并添加匹配标签：</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>创建简单的单节点 K3s 群集资源，并将其应用于群集：</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.33.3+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI 扩展</term>
<listitem>
<para>通过 UI 扩展可以采取几种简便做法。请注意，管理多个位置可能涉及大量的手动工作。</para>
<orderedlist numeration="arabic">
<listitem>
<para>如前所述，打开左侧的三线菜单并选择“OS Management”（操作系统管理）。您随即会转到管理 Elemental 系统的主屏幕。</para>
</listitem>
<listitem>
<para>在左侧边栏上，单击“Inventory of Machines”（计算机清单）。这会打开已注册计算机的清单。</para>
</listitem>
<listitem>
<para>要基于这些计算机创建群集，请选择所需的系统，单击“Actions”（操作）下拉列表，然后单击“Create Elemental Cluster”（创建
Elemental 群集）。这会打开“Cluster Creation”（创建群集）对话框，同时还会创建要在后台使用的
MachineSelectorTemplate。</para>
</listitem>
<listitem>
<para>在此屏幕上，配置您要构建的群集。本快速入门选择了“K3s v1.30.5+k3s1”，其余选项保持原样。</para>
<tip>
<para>可能需要向下滚动才能看到更多选项。</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>创建这些对象后，您应会看到一个新的 Kubernetes 群集使用刚刚安装的新节点运行。</para>
</section>
<section xml:id="id-node-reset-optional">
<title>节点重置（可选）</title>
<para>SUSE Rancher Elemental 支持执行“节点重置”，从 Rancher
中删除整个群集、从群集中删除单个节点或者从计算机清单中手动删除某个节点时，可以选择性地触发该操作。当您想要重置和清理任何孤立资源，并希望自动将清理的节点放回计算机清单以便重复使用时，此功能非常有用。默认未启用此功能，因此不会清理任何已去除的系统（即，不会去除数据，任何
Kubernetes 群集资源将继续在下游群集上运行），并且需要手动干预才能擦除数据，并通过 Elemental 将计算机重新注册到 Rancher。</para>
<para>如果您希望默认启用此功能，则需要通过添加 <literal>config.elemental.reset.enabled:
true</literal>，来确保 <literal>MachineRegistration</literal> 明确启用此功能，例如：</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>然后，所有使用此 <literal>MachineRegistration</literal> 注册的系统将自动在其配置中收到
<literal>elemental.cattle.io/resettable: 'true'</literal>
注解。如果您希望在各个节点上手动执行此操作（例如，您的现有 <literal>MachineInventory</literal>
没有此注解，或者您已部署节点），可以修改 <literal>MachineInventory</literal> 并添加
<literal>resettable</literal> 配置，例如：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>在 SUSE Edge 3.1 中，Elemental Operator 会在操作系统上设置一个标记，该标记将自动触发清理过程；它会停止所有
Kubernetes 服务、去除所有持久性数据、卸装所有 Kubernetes 服务、清理所有剩余 Kubernetes/Rancher
目录，并通过原始 Elemental <literal>MachineRegistration</literal> 配置强制重新注册到
Rancher。此过程会自动发生，无需任何手动干预。调用的脚本存放在
<literal>/opt/edge/elemental_node_cleanup.sh</literal> 中。一旦设置了标记，脚本便会通过
<literal>systemd.path</literal> 触发，因此会立即执行。</para>
<warning>
<para>使用 <literal>resettable</literal> 功能的假设条件是，预期在从 Rancher
去除节点/群集时执行的行为是擦除数据并强制重新注册。在这种情况下，必然会丢失数据，因此，请仅在您确定要执行自动重置时才使用此功能。</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>后续步骤</title>
<para>下面是使用本指南后建议阅读的一些资源：</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-fleet"/>中的端到端自动化</para>
</listitem>
<listitem>
<para><xref linkend="components-nmc"/>中的其他网络配置选项</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>使用 Edge Image Builder 配置独立群集</title>
<para>Edge Image Builder (EIB) 工具可以简化为引导计算机生成随时可引导 (CRB)
的自定义磁盘映像的过程，即使在完全隔离场景中也能做到这一点。EIB 用于创建所有三个 SUSE Edge
部署空间中使用的部署映像，因为它足够灵活，可以提供最简单的自定义过程，例如通过提供全面配置的映像来添加用户或设置时区。例如，该映像可以设置复杂的网络配置、部署多节点
Kubernetes 群集、部署客户工作负载，并通过 Rancher/Elemental 和 SUSE Multi-Linux Manager
注册到集中式管理平台。EIB
以容器映像的形式运行，因此具有极高的跨平台可移植性，可确保所有必需的依赖项都是独立的，对系统中所安装用于操作该工具的软件包的影响极小。</para>
<note>
<para>对于多节点场景，EIB 会自动部署 MetalLB 和 Endpoint Copier Operator，以便让使用构建的同一映像置备的主机自动加入
Kubernetes 群集。</para>
</note>
<para>有关详细信息，请阅读 Edge Image Builder 简介（<xref linkend="components-eib"/>）。</para>
<warning>
<para>Edge Image Builder 1.3.0 支持自定义 SUSE Linux Micro 6.1 映像，但不支持 SUSE Linux
Enterprise Micro 5.5 或 6.0 等旧版本。</para>
</warning>
<section xml:id="id-prerequisites-2">
<title>先决条件</title>
<itemizedlist>
<listitem>
<para>运行 SLES 15 SP6 的 AMD64/Intel 64 构建主机（物理机或虚拟机）。</para>
</listitem>
<listitem>
<para>Podman 容器引擎</para>
</listitem>
<listitem>
<para>通过 Kiwi 构建器过程（<xref linkend="guides-kiwi-builder-images"/>）创建的 SUSE Linux
Micro 6.1 自安装 ISO 映像</para>
</listitem>
</itemizedlist>
<note>
<para>对于非生产用途，可将 openSUSE Leap 15.6 或 openSUSE Tumbleweed
用作构建主机。其他操作系统也可能正常运行，前提是存在兼容的容器运行时。</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>获取 EIB 映像</title>
<para>EIB 容器映像已公开提供，可以通过在映像构建主机上运行以下命令从 SUSE Edge 仓库下载：</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>创建映像配置目录</title>
<para>由于 EIB 在容器中运行，我们需要从主机挂载一个配置目录，以便能够指定所需的配置，并使 EIB
能够在构建过程中访问任何所需的输入文件和支持制品。此目录必须遵循特定的结构。我们在主目录中创建此目录，并将其命名为“eib”：</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>在上一步骤中，我们创建了“base-images”目录，用于托管 SUSE Linux Micro 6.1
输入映像，现在我们确保将该映像复制到配置目录：</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>在 EIB 运行过程中，<emphasis role="strong">不会</emphasis>修改原始基础映像；在 EIB
配置目录的根目录中，会使用所需的配置创建新的自定义版本。</para>
</note>
<para>此时，配置目录应如下所示：</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>创建映像定义文件</title>
<para>定义文件包含了 Edge Image Builder 支持的大部分可配置选项，完整的选项示例可在<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/pkg/image/testdata/full-valid-example.yaml">此处</link>查看，建议您参考<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">上游构建映像指南</link>，获取比下文所述更全面的示例。以下是我们为操作系统映像准备的一个非常基础的定义文件：</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>此定义指定我们要为基于 AMD64/Intel 64 的系统生成输出映像。用作基础需要进一步修改的映像是名为
<literal>slemicro.iso</literal> 的 <literal>iso</literal> 映像，应该位于
<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>。此定义还概述了在 EIB
修改完映像后，输出映像将命名为 <literal>eib-image.iso</literal>，默认情况下，该输出映像驻留在
<literal>$CONFIG_DIR</literal> 中。</para>
<para>现在，我们的目录结构应如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>下面的章节将介绍几个常见操作的示例：</para>
<section xml:id="id-configuring-os-users">
<title>配置操作系统用户</title>
<para>EIB 允许您为用户预先配置登录信息（例如口令或 SSH 密钥），包括设置固定的 root 口令。在此示例中，我们将修复 root 口令，而第一步是使用
<literal>OpenSSL</literal> 创建一次性的已加密口令：</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>此命令将输出如下所示的内容：</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>然后，我们可以在定义文件中添加一个名为 <literal>operatingSystem</literal> 的部分，其中包含
<literal>users</literal> 数组。最终的文件应如下所示：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>您还可以添加其他用户、创建主目录、设置用户 ID、添加 SSH 密钥身份验证，以及修改组信息。请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">上游构建映像指南</link>获取更多示例。</para>
</note>
</section>
<section xml:id="configuring-os-time">
<title>配置操作系统时间</title>
<para>虽然 <literal>time</literal> 部分是可选的，但强烈建议配置该部分，以免出现证书和时钟偏差方面的潜在问题。EIB
将根据此处的参数配置 chronyd 和 <literal>/etc/localtime</literal>。</para>
<screen language="console" linenumbering="unnumbered">operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2</screen>
<itemizedlist>
<listitem>
<para><literal>timezone</literal> 以“区域/地点”格式指定时区（例如“欧洲/伦敦”）。在 Linux 系统上运行
<literal>timedatectl list-timezones</literal> 命令即可查看完整列表。</para>
</listitem>
<listitem>
<para>ntp：定义与（使用 chronyd）配置 NTP 相关的属性：</para>
</listitem>
<listitem>
<para>forceWait：要求 chronyd 在启动其他服务前尝试同步时间源，超时时间为 180 秒。</para>
</listitem>
<listitem>
<para>pools：指定 chronyd 将用作数据源的池列表（使用 <literal>iburst</literal> 缩短初始同步耗时）。</para>
</listitem>
<listitem>
<para>servers：指定 chronyd 将用作数据源的服务器列表（使用 <literal>iburst</literal> 缩短初始同步耗时）。</para>
</listitem>
</itemizedlist>
<note>
<para>本示例中提供的值仅作说明之用，请根据您的具体要求相应调整。</para>
</note>
</section>
<section xml:id="adding-certificates">
<title>添加证书</title>
<para><literal>certificates</literal>
目录中存储的扩展名为“.pem”或“.crt”的证书文件将安装在节点系统范围的证书存储区内：</para>
<screen language="console" linenumbering="unnumbered">.
├── definition.yaml
└── certificates
    ├── my-ca.pem
    └── my-ca.crt</screen>
<para>有关详细信息，请参见<link
xl:href="https://documentation.suse.com/smart/security/html/tls-certificates/index.html#tls-adding-new-certificates">《Securing
Communication with TLS Certificate》指南</link>。</para>
</section>
<section xml:id="eib-configuring-rpm-packages">
<title>配置 RPM 软件包</title>
<para>EIB 的主要功能之一是提供相关机制来向映像添加更多软件包，以便在安装完成时，系统能够立即利用已安装的软件包。EIB 允许用户指定以下信息：</para>
<itemizedlist>
<listitem>
<para>映像定义中按名称列出的软件包</para>
</listitem>
<listitem>
<para>要从中搜索这些软件包的网络储存库</para>
</listitem>
<listitem>
<para>SUSE Customer Center (SCC) 身份凭证，用于在官方 SUSE 储存库中搜索列出的软件包</para>
</listitem>
<listitem>
<para>通过 <literal>$CONFIG_DIR/rpms</literal> 目录侧载网络储存库中不存在的自定义 RPM</para>
</listitem>
<listitem>
<para>通过同一目录 (<literal>$CONFIG_DIR/rpms/gpg-keys</literal>) 指定 GPG 密钥，以便可以验证第三方软件包</para>
</listitem>
</itemizedlist>
<para>然后，EIB 将在映像构建时运行软件包解析过程，将基础映像用作输入，并尝试提取和安装所有提供的软件包（通过列表指定或在本地提供）。EIB
将所有软件包（包括所有依赖项）下载到输出映像中存在的储存库，并指示系统在首次引导过程中安装这些软件包。在映像构建期间执行此过程可确保在首次引导期间成功将软件包安装在所需平台（例如边缘节点）上。这对于需要将其他软件包嵌入映像，而非在运行时通过网络提取的环境（例如隔离环境或网络受限环境）而言，同样具有优势。</para>
<para>我们通过一个简单的示例来演示这一点：我们将安装由第三方供应商提供支持的 NVIDIA 储存库中的
<literal>nvidia-container-toolkit</literal> RPM 软件包：</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>最终的定义文件如下所示：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>上面是一个简单的示例，但为了完整性，请在运行映像生成过程之前先下载 NVIDIA 软件包签名密钥：</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; $CONFIG_DIR/rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>通过此方法添加其他 RPM 的目的是添加受支持的第三方组件或用户提供（和维护）的软件包；请不要使用此机制来添加通常不受 SUSE Linux Micro
支持的软件包。如果使用此机制从 openSUSE 储存库（不受支持）添加组件，包括来自更新版本或服务包的组件，可能会导致配置不受支持 -
尤其是当依赖项解析导致操作系统核心部分被替换时，即便最终系统看似能正常运行，也可能存在问题。如有不确定之处，请联系 SUSE
代表，以协助确定您所需配置的可支持性。</para>
</warning>
<note>
<para>在<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">上游安装软件包指南</link>中可以找到更详细的指南和更多示例。</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>配置 Kubernetes 群集和用户工作负载</title>
<para>EIB 的另一个特点是，可以使用它来自动部署可“就地引导”（即不需要任何形式的集中式管理基础架构来进行协调）的单节点和多节点高可用性
Kubernetes 群集。这种方式的主要应用场景是隔离式部署或网络受限环境，但即便在可完全无限制访问网络的情况下，它也可用于快速引导独立群集。</para>
<para>此方法不仅支持部署自定义操作系统，还能够指定 Kubernetes 配置、通过 Helm chart 添加任何额外的分层组件，以及通过提供的
Kubernetes
清单文件部署任何用户工作负载。不过，此方法的设计原则默认假设用户需要在隔离式环境下操作。因此，映像定义中指定的所有项目（包括用户提供的工作负载）都将被纳入映像。EIB
会确保将定义所需的所有已发现映像复制到本地，并由最终部署的系统中的嵌入式映像仓库提供服务。</para>
<para>在下一个示例中，我们将采用现有的映像定义并指定 Kubernetes 配置（在本示例中，未列出系统及其角色，因此我们默认假设使用单节点群集），这会指示
EIB 置备单节点 RKE2 Kubernetes 群集。为了展示用户提供的工作负载（通过清单）和分层组件（通过 Helm）部署的自动化，我们将通过
SUSE Edge Helm chart 安装 KubeVirt，并通过 Kubernetes 清单安装
NGINX。需要追加到现有映像定义的附加配置如下：</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<para>最终的完整定义文件现在应如下所示：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.33.3+k3s1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt
        version: 304.0.1+up0.6.0
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts</screen>
<note>
<para>有关多节点部署、自定义网络和 Helm chart 选项/值等更多选项示例，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">上游文档</link>。</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>配置网络</title>
<para>在本快速入门的最后一个示例中，我们来配置在使用 EIB
生成的映像置备系统时启动的网络。请务必知道，除非提供网络配置，否则默认模式是在引导时对所有发现的接口使用
DHCP。但这并非在所有情况下都是理想配置，尤其是在 DHCP 不可用而需要提供静态配置时，或者需要设置更复杂的网络结构（例如绑定、LACP 和
VLAN）时，又或者需要覆盖某些参数（例如主机名、DNS 服务器和路由）时。</para>
<para>EIB 能够提供每个节点的配置（其中的相关系统由其 MAC 地址唯一标识），或者为每台计算机提供相同配置的覆盖值，这在系统 MAC
地址未知时更有用。EIB 使用一个称为 Network Manager Configurator（简称为
<literal>nmc</literal>）的附加工具，这是 SUSE Edge 团队构建的工具，用于基于 <link
xl:href="https://nmstate.io/">nmstate.io</link>
声明式网络纲要应用自定义网络配置，在引导时识别其正在引导的节点，并在任何服务启动之前应用所需的网络配置。</para>
<para>我们现在通过在所需 <literal>network</literal>
目录中特定于节点的文件中（基于所需主机名）描述期望的网络状态，为使用单一接口的系统应用静态网络配置：</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>上述示例是针对默认 <literal>192.168.122.0/24</literal>
子网设置的，假设测试在虚拟机上执行，请根据您的环境进行调整，不要忘记设置 MAC 地址。由于可以使用同一映像来置备多个节点，EIB（通过
<literal>nmc</literal>）配置的网络取决于它是否能够根据节点 MAC 地址唯一标识节点，因此在引导期间
<literal>nmc</literal> 将向每台计算机应用正确的网络配置。这意味着，您需要知道所要安装到的系统的 MAC
地址。或者，默认行为是依赖 DHCP，但您可以利用 <literal>configure-network.sh</literal>
钩子将通用配置应用于所有节点 - 有关详细信息，请参见网络指南（<xref linkend="components-nmc"/>）。</para>
</warning>
<para>最终的文件结构应如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>我们刚刚创建的网络配置将被分析，必要的 NetworkManager 连接文件将自动生成并插入到 EIB
要创建的新安装映像中。这些文件将在主机置备期间应用，从而生成完整的网络配置。</para>
<note>
<para>有关上述配置的更详细解释以及此功能的示例，请参见“边缘网络”组件（<xref linkend="components-nmc"/>）。</para>
</note>
</section>
</section>
<section xml:id="eib-how-to-build-image">
<title>构建映像</title>
<para>获得基础映像和可供 EIB 使用的映像定义后，接下来我们需要构建映像。为此，只需使用 <literal>podman</literal>
结合“build”命令调用 EIB 容器，并指定定义文件：</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>该命令应输出如下所示的内容：</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Downloading file: dl-manifest-1.yaml 100% (498/498 B, 9.5 MB/s)
Pulling selected Helm charts... 100% (1/1, 43 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (3/3, 10 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (657/657 MB, 48 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (368/368 MB, 48 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (35/35 MB, 50 MB/s)
Downloading file: sha256sum-amd64.txt 100% (4.3/4.3 kB, 6.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>构建的 ISO 映像存储在 <literal>$CONFIG_DIR/eib-image.iso</literal> 中：</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>每次构建时都会在 <literal>$CONFIG_DIR/_build/</literal>
中创建一个带时间戳的文件夹，其中包含构建日志、构建期间使用的制品以及 <literal>combustion</literal> 和
<literal>artefacts</literal> 目录，这两个目录包含添加到 CRB 映像的所有脚本和制品。</para>
<para>此目录的内容如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:&lt;version&gt;-registry.tar.zst
│   │   │   ├── rancher_kubectl:&lt;version&gt;-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.6_virt-operator:&lt;version&gt;-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   ├── nvidia-container-toolkit-&lt;version&gt;.rpm
│   │   │       │   ├── nvidia-container-toolkit-base-&lt;version&gt;.rpm
│   │   │       │   ├── libnvidia-container1-&lt;version&gt;.rpm
│   │   │       │   └── libnvidia-container-tools-&lt;version&gt;.rpm
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt
│   │       └── kubevirt-0.4.0.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>如果构建失败，首先会在 <literal>eib-build.log</literal> 中记录相关信息。该日志会指出哪个组件构建失败，方便您进行调试。</para>
<para>此时，您应该有了一个随时可用的映像，它可以：</para>
<orderedlist numeration="arabic">
<listitem>
<para>部署 SUSE Linux Micro 6.1</para>
</listitem>
<listitem>
<para>配置 root 口令</para>
</listitem>
<listitem>
<para>安装 <literal>nvidia-container-toolkit</literal> 软件包</para>
</listitem>
<listitem>
<para>配置嵌入的容器仓库以在本地处理内容</para>
</listitem>
<listitem>
<para>安装单节点 RKE2</para>
</listitem>
<listitem>
<para>配置静态网络</para>
</listitem>
<listitem>
<para>安装 KubeVirt</para>
</listitem>
<listitem>
<para>部署用户提供的清单</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>调试映像构建过程</title>
<para>如果映像构建过程失败，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/debugging.md">上游调试指南</link>。</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>测试新构建的映像</title>
<para>有关如何测试新构建的 CRB 映像的说明，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/testing-guide.md">上游映像测试指南</link>。</para>
</section>
</chapter>
<chapter xml:id="quickstart-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Edge 中包含的 SUSE Multi-Linux Manager 可提供自动化管理功能，确保边缘部署的所有节点上作为底层操作系统的
SUSE Linux Micro 始终保持最新状态。它还可用于管理边缘节点上的 Kubernetes 以及在 Kubernetes 上部署的应用程序。</para>
<para>本快速入门指南旨在帮助您尽快熟悉 SUSE Multi-Linux
Manager，目标是为您的边缘节点提供操作系统更新。本快速入门指南不会涉及诸如调整存储容量、创建和管理用于过渡的额外软件通道，或是针对大规模部署场景管理用户、系统组及组织等主题。对于生产用途，我们强烈建议您熟悉内容全面的
<link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUSE
Multi-Linux Manager 文档</link>。</para>
<para>要使 SUSE Edge 能够有效使用 SUSE Multi-Linux Manager，需执行以下步骤：</para>
<itemizedlist>
<listitem>
<para>部署并配置 SUSE Multi-Linux Manager 服务器。</para>
</listitem>
<listitem>
<para>同步 SUSE Linux Micro 软件包储存库。</para>
</listitem>
<listitem>
<para>创建系统组。</para>
</listitem>
<listitem>
<para>创建激活密钥。</para>
</listitem>
<listitem>
<para>使用 Edge Image Builder 准备用于 SUSE Multi-Linux Manager 注册的安装媒体。</para>
</listitem>
</itemizedlist>
<section xml:id="id-deploy-suse-multi-linux-manager-server">
<title>部署 SUSE Multi-Linux Manager 服务器</title>
<para>如果您已在运行 SUSE Multi-Linux Manager 5.0.5 实例，则可跳过此步骤。</para>
<para>您可以在专用物理服务器、自有硬件上的虚拟机或云中运行 SUSE Multi-Linux Manager 服务器。针对受支持的公有云，我们提供了预配置的
SUSE Multi-Linux 服务器虚拟机映像。</para>
<para>在本快速入门中，我们针对 AMD64/Intel 64 体系结构使用“qcow2”映像
<literal>SUSE-Manager-Server.x86_64-5.0.4-Qcow-5.0-2025-04.qcow2</literal>，您可通过
<link
xl:href="https://www.suse.com/download/suse-manager/">https://www.suse.com/download/suse-manager/</link>
或 SUSE Customer Center 获取该映像。此映像可在 KVM
等超级管理程序上作为虚拟机运行。请务必检查是否有可用的最新版映像，并在执行全新安装时使用最新版本。</para>
<para>您也可以在其他任何受支持的硬件体系结构上安装 SUSE Multi-Linux Manager
服务器。在这种情况下，请选择与您的硬件体系结构相匹配的映像。</para>
<para>下载映像后，请创建一台至少满足以下最低硬件规格的虚拟机：</para>
<itemizedlist>
<listitem>
<para>16 GB RAM</para>
</listitem>
<listitem>
<para>4 个物理或虚拟核心</para>
</listitem>
<listitem>
<para>一个至少 100 GB 的额外块设备</para>
</listitem>
</itemizedlist>
<para>使用 qcow2 映像时，无需安装操作系统。您可以直接将该映像挂接为根分区。</para>
<para>您需要设置网络，以便您的边缘节点日后能够通过包含完全限定域名（简称“FQDN”）的主机名访问 SUSE Multi-Linux Manager 服务器！</para>
<para>首次引导 SUSE Multi-Linux Manager 时，需要执行一些初始配置：</para>
<itemizedlist>
<listitem>
<para>选择键盘布局</para>
</listitem>
<listitem>
<para>接受许可协议</para>
</listitem>
<listitem>
<para>选择您所在的时区</para>
</listitem>
<listitem>
<para>输入操作系统的 root 口令</para>
</listitem>
</itemizedlist>
<para>后续步骤需要以“root”用户身份执行：</para>
<para>下一步操作需要使用 SUSE Multi-Linux Manager 扩展的注册代码，您可在 SUSE Customer Center
中找到该注册代码。此注册代码可同时用于注册 SUSE Linux Micro 和 SUSE Multi-Linux Manager：</para>
<para>注册 SUSE Linux Micro：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -r &lt;REGCODE&gt; -e &lt;your_email&gt;</screen>
<para>注册 SUSE Multi-Linux Manager：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update register -p SUSE-Manager-Server/5.0/x86_64 -r &lt;REGCODE&gt;</screen>
<para>产品字符串取决于您的硬件体系结构！例如，如果您在 64 位 Arm 系统上使用 SUSE Multi-Linux
Manager，则该字符串为“SUSE-Manager-Server/5.0/aarch64”。</para>
<para>重引导</para>
<para>更新系统：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update</screen>
<para>除非未发生任何更改，否则请重引导系统以应用更新。</para>
<para>SUSE Multi-Linux Manager 通过由 Podman 管理的容器提供。<literal>mgradm</literal>
命令会为您处理设置和配置工作。</para>
<warning>
<para>请务必为您的 SUSE Multi-Linux Manager
服务器配置包含完全限定域名（简称“FQDN”）的主机名，并确保您要管理的边缘节点能够在网络中正确解析该主机名！</para>
</warning>
<para>在安装和配置 SUSE Multi-Linux Manager Server
容器之前，您需要准备好之前添加的额外块设备。为此，您需要知道虚拟机为该设备分配的名称。例如，如果块设备为
<literal>/dev/vdb</literal>，可使用以下命令将其配置为供 SUSE Multi-Linux Manager 使用：</para>
<screen language="shell" linenumbering="unnumbered">mgr-storage-server /dev/vdb</screen>
<para>部署 SUSE Multi-Linux Manager：</para>
<screen language="shell" linenumbering="unnumbered">mgradm install podman &lt;FQDN&gt;</screen>
<para>提供 CA 证书的口令。该口令应与您的登录口令不同。通常情况下，您之后无需输入此口令，但应将其记录下来。</para>
<para>提供“admin”用户的口令。这是登录 SUSE Multi-Linux Manager 的初始用户。您之后可以创建具有完整权限或受限权限的其他用户。</para>
</section>
<section xml:id="id-configure-suse-multi-linux-manager">
<title>配置 SUSE Multi-Linux Manager</title>
<para>部署完成后，您可以使用之前提供的主机名登录 SUSE Multi-Linux Manager Web
用户界面。初始用户为“admin”，请使用上一步设置的口令登录。</para>
<para>下一步需要您的组织身份凭证，这些凭证可在 SUSE Customer Center
中您所在组织的“用户”选项卡的第二个子选项卡中找到。通过这些身份凭证，SUSE Multi-Linux Manager 能够同步您已订阅的所有产品。</para>
<para>选择<literal>管理 &gt; 安装向导</literal>。</para>
<para>在<literal>组织身份凭证</literal>选项卡中，使用您在 SUSE Customer Center
中找到的<literal>用户名</literal>和<literal>口令</literal>创建新身份凭证。</para>
<para>前往下一个选项卡 <literal>SUSE 产品</literal>。您需要等待与 SUSE Customer Center 的首次数据同步完成。</para>
<para>列表加载完毕后，使用过滤器仅显示“Micro 6.1”。选中与边缘节点运行的硬件体系结构（<literal>x86_64</literal> 或
<literal>aarch64</literal>）对应的 SUSE Linux Micro 6.1 复选框。</para>
<para>单击<literal>添加产品</literal>。此操作将会添加 SUSE Linux Micro 的主软件包储存库（即“通道”），并会自动添加
SUSE Manager 客户端工具的通道作为子通道。</para>
<para>首次同步可能需要一段时间才能完成，具体时长取决于您的互联网连接情况。您可以在此期间开始执行后续步骤：</para>
<para>在<literal>“系统”>“系统组”</literal>下，至少创建一个组，以便系统在初始配置时自动加入该组。组是对系统进行分类的重要方式，通过组可以一次性对整组系统应用配置或操作。其概念类似于
Kubernetes 中的标签。</para>
<para>单击 <literal>+ 创建组</literal></para>
<para>提供一个简短名称（例如“边缘节点”）和详细说明。</para>
<para>在<literal>“系统”>“激活密钥”</literal>下，至少创建一个激活密钥。激活密钥可视为一种配置文件，为系统进行 SUSE
Multi-Linux Manager
初始配置时会自动应用该配置。如果您希望特定边缘节点加入不同的组或使用不同的配置，可以为它们创建单独的激活密钥，之后在 Edge Image
Builder 中使用这些密钥来创建自定义安装媒体。</para>
<para>激活密钥的一个典型高级使用场景是：将测试群集分配到包含最新更新的软件通道，而生产群集则分配到仅会获取已在测试群集中验证过的那些最新更新的软件通道。</para>
<para>单击 <literal>+ 创建密钥</literal></para>
<para>输入简短说明（例如“边缘节点”），并提供一个唯一名称来标识该密钥（例如，对于硬件体系结构为 AMD64/Intel 64
的边缘节点，可命名为“edge-x86_64”）。密钥前面会自动添加一个数字，默认组织的前缀始终为“1”；如果在 SUSE Multi-Linux
Manager 中创建其他组织并为其创建密钥，前缀数字可能会不同。</para>
<para>如果尚未创建任何克隆的软件通道，可将“基础通道”的设置保留为“SUSE Manager Default”，这会自动为边缘节点分配正确的 SUSE
更新储存库。</para>
<para>在“子通道”中，针对激活密钥适用的硬件体系结构，选择“包括建议项”滑块，这会添加“SUSE-Manager-Tools-For-SL-Micro-6.1”通道。</para>
<para>在“组”选项卡中，添加之前创建的组。所有使用该激活密钥进行初始配置的节点都将自动添加到该组中。</para>
</section>
<section xml:id="id-create-a-customized-installation-image-with-edge-image-builder">
<title>使用 Edge Image Builder 创建自定义安装映像</title>
<para>要使用 Edge Image Builder，您只需要拥有一个可通过 Podman 启动基于 Linux 的容器的环境。</para>
<para>在极简实验设置中，我们实际上可以使用运行 SUSE Multi-Linux Manager
服务器的同一台虚拟机。请确保该虚拟机有充足的磁盘空间！但不建议在生产环境中采用这种设置。有关我们已测试过与 Edge Image Builder
兼容的主机操作系统，请参见<xref linkend="id-prerequisites-2"/>。</para>
<para>以 root 身份登录 SUSE Multi-Linux Manager 服务器主机。</para>
<para>提取 Edge Image Builder 容器：</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
<para>创建目录 <literal>/opt/eib</literal> 及其子目录 <literal>base-images</literal>：</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p /opt/eib/base-images</screen>
<para>在本快速入门中，我们使用的是 SUSE Linux Micro 映像的“自安装”版本。该映像之后可写入物理 USB
闪存盘，用于在物理服务器上安装系统。如果服务器支持通过 BMC（基板管理控制器）远程挂接安装
ISO，也可采用这种方式。此外，该映像还可与大多数虚拟化工具结合使用。</para>
<para>如果需要将映像直接预加载到物理节点，或直接从虚拟机 (VM) 启动，也可使用“原始”版本的映像。</para>
<para>您可以在 SUSE Customer Center 或 <link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro/</link>
上找到这些映像。</para>
<para>将映像
<literal>SL-Micro.x86_64-6.1-Default-SelfInstall-GM.install.iso</literal>
下载或复制到 <literal>base-images</literal> 目录，并将其命名为“slemicro.iso”。</para>
<para>在基于 Arm 的构建主机上构建 AArch64 映像属于 SUSE Edge 3.4
的技术预览功能。该功能很可能可以正常运行，但目前尚未提供支持。如果您想尝试该功能，则需要在 64 位 Arm 计算机上运行
Podman，并且需要将所有示例和代码段中的“x86_64”替换为“aarch64”。</para>
<para>在 <literal>/opt/eib</literal> 中，创建一个名为
<literal>iso-definition.yaml</literal> 的文件。这是用于 Edge Image Builder 的构建定义文件。</para>
<para>下面的简单示例将会安装 SL Micro 6.1、设置 root 口令和键盘映射、启动 Cockpit 图形界面，并将节点注册到 SUSE
Multi-Linux Manager：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
  - username: root
    createHomeDir: true
    encryptedPassword: $6$aaBTHyqDRUMY1HAp$pmBY7.qLtoVlCGj32XR/Ogei4cngc3f4OX7fwBD/gw7HWyuNBOKYbBWnJ4pvrYwH2WUtJLKMbinVtBhMDHQIY0
  keymap: de
  systemd:
    enable:
      - cockpit.socket
  packages:
    noGPGCheck: true
  suma:
    host: ${fully qualified hostname of your SUSE Multi-Linux Manager Server}
    activationKey: 1-edge-x86_64</screen>
<para>Edge Image Builder 还可以配置网络、在节点上自动安装 Kubernetes，甚至通过 Helm chart
部署应用程序。如需更详尽的示例，请参见<xref linkend="quickstart-eib"/>。</para>
<para>对于 <literal>baseImage</literal>，请指定要使用的 <literal>base-images</literal> 目录中
ISO 的实际名称。</para>
<para>在此示例中，root 口令为“root”。请参见<xref
linkend="id-configuring-os-users"/>了解如何为要使用的安全口令创建口令哈希。</para>
<para>将键盘映射设置为安装系统后所需的实际键盘布局。</para>
<note>
<para>我们使用 <literal>noGPGCheck: true</literal> 选项，因为我们不打算提供用于检查 RPM 软件包的 GPG
密钥。请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/installing-packages.md">上游软件包安装指南</link>，获取我们建议用于生产环境的更安全设置的综合指南。</para>
</note>
<para>如前文所述， SUSE Multi-Linux Manager 主机需要一个完全限定的主机名，且在边缘节点引导至的网络中能够解析该主机名。</para>
<para><literal>activationKey</literal> 的值应与您在 SUSE Multi-Linux Manager 中创建的密钥一致。</para>
<para>要构建在安装后能自动将边缘节点注册到 SUSE Multi-Linux Manager 的安装映像，您还需要准备两个制品：</para>
<itemizedlist>
<listitem>
<para>用于安装 SUSE Multi-Linux Manager 管理代理的 Salt 受控端软件包</para>
</listitem>
<listitem>
<para>SUSE Multi-Linux Manager 服务器的 CA 证书</para>
</listitem>
</itemizedlist>
<section xml:id="id-download-the-venv-salt-minion-package">
<title>下载 venv-salt-minion 软件包</title>
<para>在 <literal>/opt/eib</literal> 中创建子目录 <literal>rpms</literal>。</para>
<para>从 SUSE Multi-Linux Manager 服务器将 <literal>venv-salt-minion</literal>
软件包下载到该目录。可以通过 Web UI 获取（在<literal>“软件”>“通道列表”</literal>中找到该软件包，并从
SUSE-Manager-Tools … 通道下载），或者使用 curl 等工具从 SUSE Multi-Linux Manager
的“引导储存库”下载：</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/repositories/slmicro/6/1/bootstrap/x86_64/venv-salt-minion-3006.0-8.1.x86_64.rpm</screen>
<para>如果已有更新的版本发布，实际软件包名称可能会有所不同。如果有多个软件包可供选择，请始终选择最新版本。</para>
<para>为解决 SUSE Multi-Linux Manager <link
xl:href="https://www.suse.com/releasenotes/x86_64/multi-linux-manager/5.1/index.html#_bootstrapping_sl_micro_6_1_clients">发行说明</link>中记录的问题，您还需要将最新版构建密钥软件包（本文档创建时为
<literal>suse-build-key-12.0-slfo.1.1_3.1.noarch.rpm</literal>）放入
<literal>rpms</literal> 目录。您可通过 SL Micro 池通道的
<literal>Packages</literal>（软件包）选项卡，在 SUSE Multi-Linux Manager 的
<literal>Software</literal>（软件）部分找到该软件包。<literal>Details</literal>（详细信息）视图中提供了
<literal>Download</literal>（下载）按钮。</para>
</section>
</section>
<section xml:id="id-download-the-suse-multi-linux-manager-ca-certificate">
<title>下载 SUSE Multi-Linux Manager CA 证书</title>
<para>在 <literal>/opt/eib</literal> 中创建子目录 <literal>certificates</literal></para>
<para>从 SUSE Multi-Linux Manager 将 CA 证书下载到该目录：</para>
<screen language="shell" linenumbering="unnumbered">curl -O http://${HOSTNAME_OF_SUSE_MANAGER}/pub/RHN-ORG-TRUSTED-SSL-CERT</screen>
<warning>
<para>必须将证书重命名为 <literal>RHN-ORG-TRUSTED-SSL-CERT.crt</literal>。这样，Edge Image
Builder 将在安装过程中确保在边缘节点上安装并激活该证书。</para>
</warning>
<para>现在，您可以运行 Edge Image Builder：</para>
<screen language="bash" linenumbering="unnumbered">cd /opt/eib
podman run --rm -it --privileged -v /opt/eib:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file iso-definition.yaml</screen>
<para>如果您为 YAML 定义文件使用了不同的名称，或者想使用其他版本的 Edge Image Builder，则需要相应地调整命令。</para>
<para>构建完成后，您可在 <literal>/opt/eib</literal> 目录中找到名为
<literal>eib-image.iso</literal> 的安装 ISO。</para>
<para>现在，您便可使用该映像部署节点，这些节点将尝试向 SUSE Multi-Linux Manager 注册。</para>
<para>节点完全安装后，您将在 SUSE Multi-Linux Manager 的 <literal>Salt/Keys</literal>
部分看到其密钥状态显示为 <literal>pending</literal>（待处理）。接受该密钥后，节点将自动加入 SUSE Multi-Linux
Manager，并会在该过程完成后显示在 <literal>Systems</literal>（系统）列表中。它将被分配到您在激活密钥中提供的系统组。</para>
<para>在应用任何额外配置之前，建议您安排一次重引导。</para>
<para>请注意，可按照<link
xl:href="https://docs.saltproject.io/en/latest/topics/tutorials/autoaccept_grains.html">此处</link>所述，使用白名单实现密整个密钥接受过程的自动化。</para>
</section>
</chapter>
</part>
<part xml:id="id-components">
<title>组件</title>
<partintro>
<para>Edge 的组件列表</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para>请参见 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12">https://ranchermanager.docs.rancher.com/v2.12</link>
上的 Rancher 文档。</para>
<blockquote>
<para>Rancher 是一个功能强大的开源 Kubernetes 管理平台，可以简化跨多个环境的 Kubernetes
群集的部署、操作和监控。无论您是在本地、云中还是边缘管理群集，Rancher 都能提供统一且集中的平台来满足您的所有 Kubernetes 需求。</para>
</blockquote>
<section xml:id="id-key-features-of-rancher">
<title>Rancher 的主要功能</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">多群集管理：</emphasis>Rancher
的直观界面让您可以从任何位置（公有云、专用数据中心和边缘位置）管理 Kubernetes 群集。</para>
</listitem>
<listitem>
<para><emphasis role="strong">安全性与合规性：</emphasis>Rancher 在您的 Kubernetes
环境中实施安全策略、基于角色的访问控制 (RBAC) 与合规性标准。</para>
</listitem>
<listitem>
<para><emphasis role="strong">简化的群集操作：</emphasis>Rancher
可自动执行群集置备、升级和查错，并为各种规模的团队简化 Kubernetes 操作。</para>
</listitem>
<listitem>
<para><emphasis role="strong">集中式应用程序目录：</emphasis>Rancher 应用程序目录提供多种 Helm chart 和
Kubernetes 操作器，使容器化应用程序的部署和管理变得简单。</para>
</listitem>
<listitem>
<para><emphasis role="strong">持续交付：</emphasis>Rancher 支持 GitOps 和 CI/CD
管道，可以自动化和简化应用程序交付过程。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>Rancher 在 SUSE Edge 中的使用</title>
<para>Rancher 为 SUSE Edge 堆栈提供多项核心功能：</para>
<section xml:id="id-centralized-kubernetes-management">
<title>集中式 Kubernetes 管理</title>
<para>在采用大量分布式群集的典型边缘部署中，Rancher 充当中心控制平面来管理这些 Kubernetes
群集。它提供统一的界面用于置备、升级、监控和查错、简化操作并确保一致性。</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>简化的群集部署</title>
<para>Rancher 简化了轻量级 SUSE Linux Micro 操作系统上的 Kubernetes 群集创建，并通过稳健的 Kubernetes
功能简化了边缘基础架构的部署。</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>应用程序部署和管理</title>
<para>集成的 Rancher 应用程序目录可以简化跨 SUSE Edge 群集的容器化应用程序的部署和管理，实现无缝的边缘工作负载部署。</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>安全性和策略实施</title>
<para>Rancher 提供基于策略的治理工具、基于角色的访问控制 (RBAC)，以及与外部身份验证提供程序的集成。这有助于 SUSE Edge
部署保持安全且合规，在分布式环境中，这一点至关重要。</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>最佳实践</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>Rancher 包含内置组件 Fleet。使用 Fleet 可以通过 git 中存储的代码管理群集配置和应用程序部署。</para>
</section>
<section xml:id="id-observability">
<title>可观测性</title>
<para>Rancher 包含 Prometheus 和 Grafana 等内置监控和日志记录工具，可提供群集健康状况和性能的综合深入信息。</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>使用 Edge Image Builder 进行安装</title>
<para>SUSE Edge 使用<xref linkend="components-eib"/>来自定义基础 SUSE Linux Micro
操作系统映像。请按照<xref linkend="rancher-install"/>中所述，在 EIB 置备的 Kubernetes 群集上进行
Rancher 隔离式安装。</para>
</section>
<section xml:id="id-additional-resources-2">
<title>其他资源</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancher 文档</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher 学院</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher 社区</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Helm Chart</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Kubernetes 操作器</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Rancher 仪表板扩展</title>
<para>用户、开发人员、合作伙伴及客户可以使用扩展来扩展和增强 Rancher UI。SUSE Edge 提供 KubeVirt 和 Akri 仪表板扩展。</para>
<para>有关 Rancher 仪表板扩展的一般信息，请参见 <literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions">Rancher
文档</link></literal>。</para>
<section xml:id="id-installation">
<title>安装</title>
<para>SUSE Edge 3.4 的所有组件（包括仪表板扩展）均以 OCI 制品形式分发。您可通过 Rancher 仪表板 UI、Helm 或 Fleet
安装 SUSE Edge 扩展：</para>
<section xml:id="id-installing-with-rancher-dashboard-ui">
<title>通过 Rancher 仪表板 UI 安装</title>
<orderedlist numeration="arabic">
<listitem>
<para>单击导航侧边栏 <emphasis role="strong">Configuration</emphasis>（配置）部分的 <emphasis
role="strong">Extensions</emphasis>（扩展）。</para>
</listitem>
<listitem>
<para>在“Extensions”（扩展）页面上，单击右上角的三点菜单，然后选择 <emphasis role="strong">Manage
Repositories</emphasis>（管理储存库）。</para>
<para>每个扩展都通过各自的 OCI 制品分发，可通过 SUSE Edge Helm chart 储存库获取。</para>
</listitem>
<listitem>
<para>在 <emphasis role="strong">Repositories</emphasis>（储存库）页面上，单击
<literal>Create</literal>（创建）。</para>
</listitem>
<listitem>
<para>在表单中指定储存库名称和 URL，然后单击 <literal>Create</literal>（创建）。</para>
<para>SUSE Edge Helm chart 储存库
URL：<literal>oci://registry.suse.com/edge/charts</literal></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-create-oci-repository.png" width="100%"/>
</imageobject>
<textobject><phrase>仪表板扩展创建 OCI 储存库</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>您会看到扩展储存库已添加到列表中，并处于 <literal>Active</literal>（活动）状态。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-repositories-list.png" width="100%"/>
</imageobject>
<textobject><phrase>仪表板扩展 - 储存库列表</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>导航回导航侧边栏 <emphasis role="strong">Configuration</emphasis>（配置）部分的 <emphasis
role="strong">Extensions</emphasis>（扩展）。</para>
<para>在 <emphasis role="strong">Available</emphasis>（可用）选项卡中，可以看到可供安装的扩展。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="dashboard-extensions-available-extensions.png" width="100%"/>
</imageobject>
<textobject><phrase>仪表板扩展 - 可用扩展</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>在扩展卡片上单击 <literal>Install</literal>（安装）并确认安装。</para>
<para>扩展安装完毕后，Rancher UI 会提示重新加载页面，具体说明请参见 <literal><link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/rancher-extensions#installing-extensions">Rancher
的安装扩展文档页面</link></literal>。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-helm">
<title>使用 Helm 进行安装</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension --version 304.0.2+up1.3.2 --namespace cattle-ui-plugin-system

# Akri extension
helm install akri-dashboard-extension oci://registry.suse.com/edge/charts/akri-dashboard-extension --version 304.0.2+up1.3.1 --namespace cattle-ui-plugin-system</screen>
<note>
<para>扩展需安装在 <literal>cattle-ui-plugin-system</literal> 名称空间中。</para>
</note>
<note>
<para>安装扩展后，需要重新加载 Rancher 仪表板 UI。</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>使用 Fleet 进行安装</title>
<para>使用 Fleet 安装仪表板扩展需要定义一个 <literal>gitRepo</literal> 资源，该资源指向包含自定义
<literal>fleet.yaml</literal> 捆绑包配置文件的 Git 储存库。</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/kubevirt-dashboard-extension
  version: "304.0.2+up1.3.2"</screen>
<screen language="yaml" linenumbering="unnumbered"># Akri extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: akri-dashboard-extension
  chart: oci://registry.suse.com/edge/charts/akri-dashboard-extension
  version: "304.0.2+up1.3.1"</screen>
<note>
<para>必须指定 <literal>releaseName</literal> 属性，而且它需要与扩展名称匹配，才能正确安装扩展。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>有关详细信息，请参见<xref linkend="components-fleet"/>和 <literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>
储存库。</para>
<para>安装扩展后，它们将列在 <emphasis role="strong">Installed</emphasis>（已安装）选项卡下的 <emphasis
role="strong">Extensions</emphasis>（扩展）部分中。由于它们不是通过 Apps/Marketplace
安装的，因此带有 <literal>Third-Party</literal>（第三方）标签。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width="100%"/> </imageobject>
<textobject><phrase>已安装的仪表板扩展</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>KubeVirt 仪表板扩展</title>
<para>KubeVirt 扩展为 Rancher 仪表板 UI 提供基本虚拟机管理。<xref
linkend="kubevirt-dashboard-extension-usage"/>中介绍了其功能。</para>
</section>
<section xml:id="id-akri-dashboard-extension">
<title>Akri 仪表板扩展</title>
<para>Akri 是一个 Kubernetes 资源接口，让您可以轻松地将异构叶设备（例如 IP 摄像头和 USB 设备）公开为 Kubernetes
群集中的资源，同时还支持公开 GPU 和 FPGA 等嵌入式硬件资源。Akri 会持续检测有权访问这些设备的节点，并根据这些节点调度工作负载。</para>
<para>Akri 仪表板扩展允许您使用 Rancher 仪表板用户界面来管理和监控叶设备，并在发现这些设备后运行工作负载。</para>
<para><xref linkend="akri-dashboard-extension-usage"/>中详细介绍了扩展功能。</para>
</section>
</chapter>
<chapter xml:id="components-rancher-turtles">
<title>Rancher Turtles</title>
<para>请参见 Rancher Turtles 文档，网址为：<link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/">https://documentation.suse.com/cloudnative/cluster-api/</link></para>
<blockquote>
<para>Rancher Turtles 是一个 Kubernetes 操作器，它提供 Rancher Manager 与 Cluster API (CAPI)
之间的集成，旨在为 Rancher 添加全面的 CAPI 支持</para>
</blockquote>
<section xml:id="id-key-features-of-rancher-turtles">
<title>Rancher Turtles 的主要功能</title>
<itemizedlist>
<listitem>
<para>通过在 CAPI 置备的群集中安装 Rancher 群集代理，自动将 CAPI 群集导入 Rancher。</para>
</listitem>
<listitem>
<para>通过 <link xl:href="https://cluster-api-operator.sigs.k8s.io/">CAPI
操作器</link>安装并配置 CAPI 控制器依赖项。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-rancher-turtles-use-in-suse-edge">
<title>Rancher Turtles 在 SUSE Edge 中的使用</title>
<para>SUSE Edge 堆栈提供了一个 Helm 封装 chart，该 chart 可使用特定配置安装 Rancher Turtles，以启用：</para>
<itemizedlist>
<listitem>
<para>核心 CAPI 控制器组件</para>
</listitem>
<listitem>
<para>RKE2 控制平面和引导提供程序组件</para>
</listitem>
<listitem>
<para>Metal3（<xref linkend="components-metal3"/>）基础架构提供程序组件</para>
</listitem>
</itemizedlist>
<para>仅支持通过封装 chart 安装的默认提供程序 - 目前不支持替代的控制平面、引导和基础架构提供程序作为 SUSE Edge 堆栈的一部分。</para>
</section>
<section xml:id="id-installing-rancher-turtles">
<title>安装 Rancher Turtles</title>
<para>可按照 Metal3 快速入门（<xref linkend="quickstart-metal3"/>）指南或管理群集（<xref
linkend="atip-management-cluster"/>）文档所述安装 Rancher Turtles。</para>
</section>
<section xml:id="id-additional-resources-3">
<title>其他资源</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancher 文档</link></para>
</listitem>
<listitem>
<para><link xl:href="https://cluster-api.sigs.k8s.io/">《Cluster API Book》</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link xl:href="https://fleet.rancher.io">Fleet</link>
是一个容器管理和部署引擎，旨在让用户更好地控制本地群集，并通过 GitOps 进行持续监控。Fleet
不仅注重缩放能力，而且还为用户提供很高的控制度和可见性，以准确监控群集上安装的组件。</para>
<para>Fleet 可以管理通过原始 Kubernetes YAML、Helm chart、Kustomize 的 Git
软件包或这三者的任意组合完成的部署。无论来源是什么，所有资源都会动态转换为 Helm chart，并使用 Helm
作为引擎在群集中部署所有资源。因此，用户可以获享很高的群集控制度、一致性和可审计性。</para>
<para>有关 Fleet 工作原理的信息，请参见 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/architecture">Fleet
Architecture</link>。</para>
<section xml:id="id-installing-fleet-with-helm">
<title>使用 Helm 安装 Fleet</title>
<para>Fleet 已内置于 Rancher 中，但您也可以使用 Helm 将 Fleet <link
xl:href="https://fleet.rancher.io/installation">安装</link>为任何 Kubernetes
群集上的独立应用程序。</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>使用 Rancher 中的 Fleet</title>
<para>Rancher 使用 Fleet 在受管群集中部署应用程序。Fleet 的持续交付功能引入了大规模 GitOps，旨在管理在大量群集上运行的应用程序。</para>
<para>Fleet 作为 Rancher 的集成部分优势显著：使用 Rancher 管理的群集会在安装/导入过程中自动由 Fleet
代理部署，并且之后群集立即便能由 Fleet 管理。</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>在 Rancher UI 中访问 Fleet</title>
<para>Rancher 中预安装了 Fleet，可通过 Rancher UI 中的 <emphasis role="strong">Continuous
Delivery</emphasis>（持续交付）选项进行管理。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet 仪表板</phrase></textobject>
</mediaobject>
</informalfigure>
<para>“Continuous Delivery”（持续交付）部分包括以下项目：</para>
<section xml:id="id-dashboard">
<title>Dashboard（仪表板）</title>
<para>所有工作空间中所有 GitOps 储存库的概览页面。仅显示包含储存库的工作空间。</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos（Git 储存库）</title>
<para>所选工作空间中的 GitOps 储存库列表。使用页面顶部的下拉列表选择活动工作空间。</para>
</section>
<section xml:id="id-clusters">
<title>Clusters（群集）</title>
<para>受管群集列表。默认情况下，Rancher 管理的所有群集都会添加到 <literal>fleet-default</literal>
工作空间。<literal>fleet-local</literal>
工作空间包含本地（管理）群集。在此处可以<literal>暂停</literal>或<literal>强制更新</literal>群集，或者将群集移动到另一个工作空间。可以通过编辑群集来更新用于群集分组的标签和注解。</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups（群集组）</title>
<para>在此部分，可以使用选择器对工作空间内的群集进行自定义分组。</para>
</section>
<section xml:id="id-advanced">
<title>Advanced（高级）</title>
<para>在“Advanced”（高级）部分可以管理工作空间和其他相关 Fleet 资源。</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>使用 Rancher 仪表板通过 Rancher 和 Fleet 安装 KubeVirt 的示例</title>
<orderedlist numeration="arabic">
<listitem>
<para>创建包含 <literal>fleet.yaml</literal> 文件的 Git 储存库：</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/charts/kubevirt"
  version: "304.0.1+up0.6.0"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>在 Rancher 仪表板中，导航到 <emphasis role="strong">☰ &gt; Continuous
Delivery（持续交付）&gt; Git repos（Git 储存库）</emphasis>，然后单击 <literal>Add
Repository</literal>（添加储存库）。</para>
</listitem>
<listitem>
<para>储存库创建向导将指导您完成 Git 储存库创建步骤。提供<emphasis role="strong">名称</emphasis>、<emphasis
role="strong">储存库 URL</emphasis>（引用上一步骤中创建的 Git
储存库），并选择适当的分支或修订版。对于较复杂的储存库，请指定<emphasis
role="strong">路径</emphasis>以便在单个储存库中使用多个目录。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet - 创建储存库 1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>单击 <literal>Next</literal>（下一步）。</para>
</listitem>
<listitem>
<para>在下一步骤中，可以定义工作负载的部署位置。在群集选择方面，可以使用多个基本选项：可以不选择任何群集、选择所有群集，或者直接选择特定的受管群集或群集组（如果已定义）。“Advanced”（高级）选项允许通过
YAML 直接编辑选择器。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet - 创建储存库 2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>单击 <literal>Create</literal>（创建）。系统即会创建储存库。从现在起，工作负载将在与储存库定义匹配的群集上安装并保持同步。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>调试和查错</title>
<para>“Advanced”（高级）导航部分提供了低层级 Fleet 资源的概览。<link
xl:href="https://fleet.rancher.io/ref-bundle-stages">捆绑包</link>是一个内部资源，用于编排
Git 中的资源。扫描 Git 储存库时，会生成一个或多个捆绑包。</para>
<para>要查找与特定储存库相关的捆绑包，请转到 Git 储存库细节页面，并单击 <literal>Bundles</literal>（捆绑包）选项卡。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet 储存库捆绑包</phrase></textobject>
</mediaobject>
</informalfigure>
<para>对于每个群集，捆绑包将应用于创建的 BundleDeployment 资源。要查看 BundleDeployment 细节，请单击 Git
储存库细节页面右上角的 <literal>Graph</literal>（图表）按钮。系统随即会加载 <emphasis
role="strong">Repo（储存库）&gt; Bundles（捆绑包）&gt; BundleDeployment</emphasis>
图表。在图表中单击相应 BundleDeployment 可查看其细节，单击 <literal>ID</literal> 可查看
BundleDeployment YAML。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet 储存库图表</phrase></textobject>
</mediaobject>
</informalfigure>
<para>有关 Fleet 查错提示的更多信息，请参见<link
xl:href="https://fleet.rancher.io/troubleshooting">此处</link>。</para>
</section>
<section xml:id="id-fleet-examples">
<title>Fleet 示例</title>
<para>Edge 团队维护的一个<link
xl:href="https://github.com/suse-edge/fleet-examples">储存库</link>包含了有关使用
Fleet 安装 Edge 项目的示例。</para>
<para>Fleet 项目包含 <link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>
储存库，其中涵盖了 <link xl:href="https://fleet.rancher.io/gitrepo-content">Git
储存库结构</link>的所有使用场景。</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SUSE Linux Micro</title>
<para>请参见 <link xl:href="https://documentation.suse.com/sle-micro/6.1/">SUSE Linux
Micro 官方文档</link></para>
<blockquote>
<para>SUSE Linux Micro 是一个安全的轻量级边缘操作系统。它将 SUSE Linux Enterprise
强化组件与开发人员需要的各种功能融入一套不可变的新式操作系统，从而形成一个可靠的基础架构平台，不仅具有同类最佳的合规性，而且易于使用。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-suse-linux-micro">
<title>SUSE Edge 如何使用 SUSE Linux Micro？</title>
<para>我们使用 SUSE Linux Micro 作为平台堆栈的基础操作系统。它为我们提供了安全、稳定且精简的构建基础。</para>
<para>SUSE Linux Micro 的独特之处在于它使用了文件系统 (Btrfs)
快照，一旦在升级期间出错，我们就可以轻松回滚。这样，在出现问题时，即使无法进行物理访问，也可以对整个平台安全地进行远程升级。</para>
</section>
<section xml:id="id-best-practices-2">
<title>最佳实践</title>
<section xml:id="id-installation-media">
<title>安装媒体</title>
<para>SUSE Edge 使用 Edge Image Builder（<xref linkend="components-eib"/>）来预配置 SUSE
Linux Micro 自安装映像。</para>
</section>
<section xml:id="id-local-administration">
<title>本地管理</title>
<para>SUSE Linux Micro 附带 Cockpit，让您可以通过 Web 应用程序对主机进行本地管理。</para>
<para>此服务默认已禁用，但可以通过启用 systemd 服务 <literal>cockpit.socket</literal> 来启动。</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>已知问题</title>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro 目前不提供桌面环境，但我们正在开发容器化解决方案。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>
是一个 CNCF 项目，它为 Kubernetes 提供裸机基础架构管理功能。</para>
<para>Metal<superscript>3</superscript> 提供 Kubernetes 原生资源来管理裸机服务器的生命周期，支持通过 <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link> 等带外协议进行管理。</para>
<para>它还为 <link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link> 提供成熟的支持，允许通过广泛采用的不限供应商的 API 来管理跨多个基础架构提供商的基础架构资源。</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>SUSE Edge 如何使用 Metal<superscript>3</superscript>？</title>
<para>此方法非常适合用于目标硬件支持带外管理，并且需要全自动化基础架构管理流程的场景。</para>
<para>此方法提供声明性 API 来对裸机服务器进行清单和状态管理，包括自动检查、清理和置备/取消置备。</para>
</section>
<section xml:id="id-known-issues-3">
<title>已知问题</title>
<itemizedlist>
<listitem>
<para>上游 <link xl:href="https://github.com/metal3-io/ip-address-manager">IP
地址管理控制器</link>目前不受支持，因为它与我们选择的网络配置工具尚不兼容。</para>
</listitem>
<listitem>
<para>相关的 IPAM 资源和 Metal3DataTemplate networkData 字段也不受支持。</para>
</listitem>
<listitem>
<para>目前仅支持通过 redfish-virtualmedia 进行部署。</para>
</listitem>
<listitem>
<para>在 Ironic Python Agent (IPA) 与目标操作系统 (SL Micro 6.0/6.1)
之间，可能会出现网络设备名称不匹配的情况，尤其是在尝试为设备配置可预测名称时。</para>
</listitem>
</itemizedlist>
<para>出现此问题的原因是，当前 Ironic Python Agent (IPA) 的内核与目标操作系统 (SL Micro 6.0/6.1)
的内核未保持一致，导致网络驱动程序不匹配，使得 IPA 发现网络设备时采用的命名规则与 SL Micro 的预期不一致。</para>
<para>目前可采用以下两种方法作为临时解决方案：* 创建两个不同的网络配置机密，一个供 IPA 使用，其设备名称需与 IPA 发现的名称一致，并在
<literal>BareMetalHost</literal> 定义中用作
<literal>preprovisioningNetworkDataName</literal>；另一个机密的设备名称需与 SL Micro
发现的名称一致，并在 <literal>BareMetalHost</literal> 定义中用作
<literal>networkData.name</literal>。* 改用 UUID 在生成的 nmconnection
文件中引用其他接口。更多详细信息请参见相关<link xl:href="..tips/metal3.adoc">提示与技巧</link>。</para>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para>请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder">官方储存库</link>。</para>
<para>Edge Image Builder (EIB) 工具可以简化为引导计算机生成自定义的随时可引导 (CRB)
磁盘映像的过程。使用一个这样的映像就能实现整个 SUSE 软件堆栈的端到端部署。</para>
<para>虽然 EIB 能够为所有置备场景创建 CRB 映像，但 EIB 在网络受限或完全隔离的隔离式部署中展现出极大价值。</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>SUSE Edge 如何使用 Edge Image Builder？</title>
<para>SUSE Edge 使用 EIB 来简化和快速配置适用于多种场景的自定义 SUSE Linux Micro
映像。这些场景包括为虚拟机和裸机引导部署，具体如下：</para>
<itemizedlist>
<listitem>
<para>K3s/RKE2 Kubernetes 的完全隔离式部署（单节点和多节点）</para>
</listitem>
<listitem>
<para>完全隔离式 Helm chart和 Kubernetes 清单部署</para>
</listitem>
<listitem>
<para>通过 Elemental API 注册到 Rancher</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>自定义网络（例如静态 IP、主机名、VLAN、绑定等）</para>
</listitem>
<listitem>
<para>自定义操作系统配置（例如用户、组、口令、SSH 密钥、代理、NTP、自定义 SSL 证书等）</para>
</listitem>
<listitem>
<para>主机级和侧载 RPM 软件包的隔离式安装（包括依赖项解析）</para>
</listitem>
<listitem>
<para>注册到 SUSE Multi-Linux Manager 以进行操作系统管理</para>
</listitem>
<listitem>
<para>嵌入式容器映像</para>
</listitem>
<listitem>
<para>内核命令行参数</para>
</listitem>
<listitem>
<para>引导时启用/禁用的 Systemd 单元</para>
</listitem>
<listitem>
<para>用于任何手动任务的自定义脚本和文件</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>入门</title>
<para>有关 Edge Image Builder 用法和测试的综合文档，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.3/docs">此处</link>。</para>
<para>另外请参见<xref linkend="quickstart-eib"/>，其中介绍了一种基本部署场景。</para>
<para>当您熟悉此工具后，可在我们的 EIB 提示和技巧（<xref linkend="tips-and-tricks"/>）页面中查找一些更有用的信息。</para>
</section>
<section xml:id="id-known-issues-4">
<title>已知问题</title>
<itemizedlist>
<listitem>
<para>EIB 通过模板化 Helm chart 并分析模板中的所有映像来隔离 Helm chart。如果 Helm chart
未将其所有映像保留在模板中，而是侧载映像，则 EIB 将无法自动隔离这些映像。可通过手动将任何未检测到的映像添加到定义文件的
<literal>embeddedArtifactRegistry</literal> 部分，来解决此问题。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>边缘网络</title>
<para>本章介绍 SUSE Edge 解决方案中的网络配置方法。我们将展示如何以声明方式在 SUSE Linux Micro 上配置
NetworkManager，并说明如何集成相关的工具。</para>
<section xml:id="id-overview-of-networkmanager">
<title>NetworkManager 概述</title>
<para>NetworkManager 是用于管理主网络连接和其他连接接口的工具。</para>
<para>NetworkManager 将网络配置存储为包含期望状态的连接文件。这些连接以文件的形式存储在
<literal>/etc/NetworkManager/system-connections/</literal> 目录中。</para>
<para>有关 NetworkManager 的详细信息，请参见 <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-network-configuration/index.html">SUSE
Linux Micro 文档</link>。</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>nmstate 概述</title>
<para>nmstate 是广泛采用的库（附带 CLI 工具），它提供一个声明性 API，可用于通过预定义的纲要进行网络配置。</para>
<para>有关 nmstate 的详细信息，请参见<link xl:href="https://nmstate.io/">上游文档</link>。</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>NetworkManager Configurator (nmc) 概述</title>
<para>SUSE Edge 中提供的网络自定义选项通过一个称为 NetworkManager Configurator（简称为
<emphasis>nmc</emphasis>）的 CLI 工具实现。此工具利用 nmstate 库提供的功能，因此完全能够配置静态 IP
地址、DNS 服务器、VLAN、绑定、网桥等。我们可以使用此工具根据预定义的期望状态生成网络配置，并自动将这些配置应用于许多不同的节点。</para>
<para>有关 NetworkManager Configurator (nmc) 的详细信息，请参见<link
xl:href="https://github.com/suse-edge/nm-configurator">上游储存库</link>。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>SUSE Edge 如何使用 NetworkManager Configurator？</title>
<para>SUSE Edge 利用 <emphasis>nmc</emphasis> 在各种不同的置备模型中进行网络自定义：</para>
<itemizedlist>
<listitem>
<para>定向网络置备场景中的自定义网络配置（<xref linkend="quickstart-metal3"/>）</para>
</listitem>
<listitem>
<para>基于映像的置备场景中的声明性静态配置（<xref linkend="quickstart-eib"/>）</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>使用 Edge Image Builder 进行配置</title>
<para>Edge Image Builder (EIB)
是可用于通过单个操作系统映像配置多个主机的工具。本节将介绍如何使用声明式方法来描述期望的网络状态，如何将这些状态转换为相应的
NetworkManager 连接，然后在置备过程中应用这些连接。</para>
<section xml:id="id-prerequisites-3">
<title>先决条件</title>
<para>如果您要学习本指南，事先需要做好以下准备：</para>
<itemizedlist>
<listitem>
<para>一台运行 SLES 15 SP6 或 openSUSE Leap 15.6 的 AMD64/Intel 64 物理主机（或虚拟机）</para>
</listitem>
<listitem>
<para>一个可用的容器运行时（例如 Podman）</para>
</listitem>
<listitem>
<para>SUSE Linux Micro 6.1 原始映像，可在<link
xl:href="https://www.suse.com/download/sle-micro/">此处</link>获取</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>获取 Edge Image Builder 容器映像</title>
<para>EIB 容器映像已公开提供，可以通过运行以下命令从 SUSE Edge 仓库下载：</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/3.4/edge-image-builder:1.3.0</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>创建映像配置目录</title>
<para>我们来开始创建配置目录：</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>确保下载的基础映像已移动到配置目录：</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SL-Micro.x86_64-6.1-Base-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIB 永远不会修改基础映像输入。它会创建一个包含所需修改内容的新映像。</para>
</note>
</blockquote>
<para>此时，配置目录应如下所示：</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>创建映像定义文件</title>
<para>定义文件描述了 Edge Image Builder 支持的大多数可配置选项。</para>
<para>首先，我们为操作系统映像创建一个非常简单的定义文件：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.3
image:
  arch: x86_64
  imageType: raw
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para>必须包含 <literal>image</literal>
部分，它指定输入映像、输入映像的体系结构和类型，以及输出映像的名称。<literal>operatingSystem</literal>
是可选部分，其中包含的配置允许用户通过 <literal>root/eib</literal> 用户名/口令登录到置备的系统。</para>
<blockquote>
<note>
<para>您可以运行 <literal>openssl passwd -6 &lt;password&gt;</literal> 来使用自己的已加密口令。</para>
</note>
</blockquote>
<para>此时，配置目录应如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>定义网络配置</title>
<para>期望的网络配置不是我们刚刚创建的映像定义文件的一部分。现在我们在特殊的 <literal>network/</literal>
目录下填充这些配置。我们来创建该目录：</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>如前所述，NetworkManager Configurator (<emphasis>nmc</emphasis>)
工具要求提供预定义纲要形式的输入。您可以参见<link xl:href="https://nmstate.io/examples.html">上游
NMState 示例文档</link>，了解如何设置各种不同的网络选项。</para>
<para>本指南将介绍如何在三个不同的节点上配置网络：</para>
<itemizedlist>
<listitem>
<para>使用两个以太网接口的节点</para>
</listitem>
<listitem>
<para>使用网络绑定的节点</para>
</listitem>
<listitem>
<para>使用网桥的节点</para>
</listitem>
</itemizedlist>
<warning>
<para>不建议在生产构建中使用完全不同的网络设置，尤其是在配置 Kubernetes
群集时。网络配置通常应在节点之间或至少在给定群集内的角色之间保持同质。本指南包含的各种不同选项仅供参考。</para>
</warning>
<blockquote>
<note>
<para>以下示例采用 IP 地址范围为 <literal>192.168.122.1/24</literal> 的默认
<literal>libvirt</literal> 网络。如果您的环境与此不同，请相应地进行调整。</para>
</note>
</blockquote>
<para>我们来为第一个节点（名为 <literal>node1.suse.com</literal>）创建期望的状态：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>在此示例中，我们将定义两个以太网接口（eth0 和 eth3）的期望状态、其请求 IP 地址、路由和 DNS 解析。</para>
<warning>
<para>必须确保列出了所有以太网接口的 MAC 地址。这些地址在置备过程中用作节点的标识符，用于确定要应用哪些配置。这就是我们使用单个 ISO 或 RAW
映像配置多个节点的方式。</para>
</warning>
<para>接下来对第二个节点（名为 <literal>node2.suse.com</literal>）进行操作，该节点使用网络绑定：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>在此示例中，我们将定义两个未启用 IP 寻址的以太网接口（eth0 和 eth1）的期望状态，以及采用轮替策略的绑定及其用于转发网络流量的相应地址。</para>
<para>最后，我们将创建第三个（也是最后一个）期望状态文件，该文件利用网桥，我们将其命名为 <literal>node3.suse.com</literal>：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>此时，配置目录应如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<blockquote>
<note>
<para><literal>network/</literal> 目录下的文件名是特意设定的，它们与置备过程中将要设置的主机名相对应。</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>构建操作系统映像</title>
<para>完成所有必要配置后，接下来我们只需运行以下命令来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>输出应如下所示：</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>以上输出片段告诉我们<literal>网络</literal>组件已成功配置，我们可以继续置备边缘节点。</para>
<blockquote>
<note>
<para>可以在所运行映像的带时间戳目录下生成的 <literal>_build</literal> 目录中，检查日志文件
(<literal>network-config.log</literal>) 和相应的 NetworkManager 连接文件。</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>置备边缘节点</title>
<para>我们来复制生成的原始映像：</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>您会发现，我们复制了构建的映像四次，但仅指定了三个节点的网络配置。这是因为，我们还想展示当置备的节点与任何期望的配置都不匹配时会发生什么。</para>
<blockquote>
<note>
<para>本指南将为节点置备示例使用虚拟化。请确保在 BIOS 中启用必要的扩展（有关详细信息，请参见<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">此处</link>）。</para>
</note>
</blockquote>
<para>我们将运行 <literal>virt-install</literal> 并使用复制的原始磁盘来创建虚拟机。每个虚拟机使用 10 GB RAM 和 6
个 vCPU。</para>
<section xml:id="id-provisioning-the-first-node">
<title>置备第一个节点</title>
<para>我们来创建虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>要创建的网络接口的 MAC 地址必须与上述期望的状态中使用的 MAC 地址相同。</para>
</note>
</blockquote>
<para>操作完成后，我们将看到如下所示的输出：</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Micro 6.0 (x86_64) - Kernel 6.4.0-18-default (tty1).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>现在我们可以使用 <literal>root:eib</literal> 身份凭证对登录。如果需要，我们也可以通过 SSH
连接到主机，而不是如此处所示使用 <literal>virsh 控制台</literal>进行连接。</para>
<para>登录后，我们需要确认是否正确完成了所有设置。</para>
<para>校验是否正确设置了主机名：</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>校验是否正确配置了路由：</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>校验互联网连接是否可用：</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>校验是否刚好配置了两个以太网接口，并且只有其中的一个接口处于活动状态：</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>您会发现，在期望的网络状态中，第二个接口是 <literal>eth1</literal>，而不是预定义的
<literal>eth3</literal>。之所以出现这种情况，是因为 NetworkManager Configurator
(<emphasis>nmc</emphasis>) 能够检测到操作系统为 MAC 地址为
<literal>34:8a:b1:4b:16:e2</literal> 的 NIC 指定了不同的名称，并相应地调整了其设置。</para>
<para>通过检查置备的 Combustion 阶段来校验是否确实发生了这种情况：</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>我们现在将置备其余节点，但只会显示最终配置中的差异。您可以对即将置备的所有节点应用上述任意检查或所有检查。</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>置备第二个节点</title>
<para>我们来创建虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>虚拟机启动并运行后，我们可以确认此节点是否正在使用绑定的接口：</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>确认路由是否正在使用绑定：</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>确保正确利用静态连接文件：</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>置备第三个节点</title>
<para>我们来创建虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>虚拟机启动并运行后，我们可以确认此节点是否正在使用网桥：</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>确认路由是否正在使用网桥：</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>确保正确利用静态连接文件：</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>置备第四个节点</title>
<para>最后，我们将置备 MAC 地址与任何预定义配置都不匹配的节点。在这种情况下，我们将默认使用 DHCP 来配置网络接口。</para>
<para>我们来创建虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>虚拟机启动并运行后，我们可以确认此节点是否正在为其网络接口使用随机 IP 地址：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>校验 nmc 是否无法为此节点应用静态配置：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>校验是否通过 DHCP 配置了以太网接口：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="networking-unified">
<title>统一节点配置</title>
<para>有时我们无法依赖已知的 MAC 地址。在这种情况下，我们可以选择所谓的<emphasis>统一配置</emphasis>，这样就可以在
<literal>_all.yaml</literal> 文件中指定设置，然后将这些设置应用于所有已置备的节点。</para>
<para>我们将使用不同的配置结构来构建和置备边缘节点。请按照从<xref linkend="image-config-dir-creation"/>到<xref
linkend="default-network-definition"/>的所有步骤进行操作。</para>
<para>在此示例中，我们将定义两个以太网接口（eth0 和 eth1）的期望状态 - 一个接口使用 DHCP，另一个接口使用静态 IP 地址。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>成功构建映像后，我们将使用它来创建虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>置备过程可能需要几分钟时间。完成后，使用提供的身份凭证登录到系统。</para>
<para>校验是否正确配置了路由：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>校验互联网连接是否可用：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>校验以太网接口是否已配置并处于活动状态：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>自定义网络配置</title>
<para>我们已介绍 Edge Image Builder 的默认网络配置，这种配置依赖于 NetworkManager
Configurator。不过，我们还可以选择通过自定义脚本来修改这种配置。虽然这种方法非常灵活而且不依赖于 MAC
地址，但它的局限性在于，在使用单个映像引导多个节点时，这种方法不太方便。</para>
<blockquote>
<note>
<para>建议通过 <literal>/network</literal>
目录下描述期望的网络状态的文件来使用默认网络配置。请仅在该行为不适用于您的使用场景时，才选择自定义脚本。</para>
</note>
</blockquote>
<para>我们将使用不同的配置结构来构建和置备边缘节点。请按照从<xref linkend="image-config-dir-creation"/>到<xref
linkend="default-network-definition"/>的所有步骤进行操作。</para>
<para>在此示例中，我们将创建一个自定义脚本来对所有已置备节点上的 <literal>eth0</literal> 接口应用静态配置，同时去除并禁用
NetworkManager
自动创建的有线连接。如果您想要确保群集中每个节点都采用相同的网络配置，则这种做法非常有利，这样，您就不需要在创建映像之前考虑每个节点的 MAC 地址。</para>
<para>首先，我们将连接文件存储在 <literal>/custom/files</literal> 目录中：</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>创建静态配置后，我们再创建自定义网络脚本：</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>默认情况下仍会包含 nmc 二进制文件，因此如果需要，也可以在 <literal>configure-network.sh</literal>
脚本中使用它。</para>
</note>
</blockquote>
<warning>
<para>必须始终在配置目录中的 <literal>/network/configure-network.sh</literal>
下提供自定义脚本。系统将忽略存在的所有其他文件。因为无法同时使用 YAML 格式的静态配置和自定义脚本来配置网络。</para>
</warning>
<para>此时，配置目录应如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SL-Micro.x86_64-6.1-Base-GM.raw</screen>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file definition.yaml</screen>
<para>成功构建映像后，我们将使用它来创建虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>置备过程可能需要几分钟时间。完成后，使用提供的身份凭证登录到系统。</para>
<para>校验是否正确配置了路由：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>校验互联网连接是否可用：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>校验是否使用连接文件静态配置了以太网接口，并且该接口处于活动状态：</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elemental 是一个软件堆栈，可用于通过 Kubernetes 实现集中式云原生操作系统全面管理。Elemental 堆栈由驻留在 Rancher
本身或边缘节点上的许多组件构成。核心组件包括：</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> - 驻留在 Rancher
上的核心操作器，用于处理来自客户端的注册请求。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis> - 在边缘节点上运行的客户端，支持通过
<literal>elemental-operator</literal> 进行注册。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis> - 驻留在边缘节点上的代理；其配置由
<literal>elemental-register</literal> 馈送，接收用于配置
<literal>rancher-system-agent</literal> 的<literal>计划</literal>。</para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis> - 完全注册边缘节点后，此组件将接管
<literal>elemental-system-agent</literal>，并等待 Rancher Manager
接下来的<literal>计划</literal>（例如 Kubernetes 安装）。</para>
</listitem>
</itemizedlist>
<para>有关 Elemental 及其与 Rancher 的关系的完整信息，请参见 <link
xl:href="https://elemental.docs.rancher.com/">Elemental 上游文档</link>。</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>SUSE Edge 如何使用 Elemental？</title>
<para>我们将使用 Elemental 的部分功能来管理无法使用 Metal<superscript>3</superscript> 的远程设备（例如，没有
BMC 的设备，或者 NAT 网关后的设备）。在知道设备何时运送或运送到何处之前，操作员可以使用此工具在实验室中引导其设备。也就是说，我们利用
<literal>elemental-register</literal> 和
<literal>elemental-system-agent</literal> 组件将 SUSE Linux Micro 主机接入
Rancher，以支持“自主回连”网络置备使用场景。使用 Edge Image Builder (EIB) 创建部署映像时，可以通过在 EIB
的配置目录中指定注册配置，来使用 Elemental 通过 Rancher 完成自动注册。</para>
<note>
<para>在 SUSE Edge 3.4 中，我们<emphasis role="strong">不会</emphasis>利用 Elemental
的操作系统管理功能，因此无法通过 Rancher 管理操作系统修补。SUSE Edge 不会使用 Elemental 工具来构建部署映像，而是使用
Edge Image Builder 工具，后者利用注册配置。</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>最佳实践</title>
<section xml:id="id-installation-media-2">
<title>安装媒体</title>
<para>SUSE Edge 建议的、可以在“自主回连网络置备”部署空间中利用 Elemental 注册到 Rancher 的部署映像构建方法是，遵循有关使用
Elemental 进行远程主机初始配置（<xref linkend="quickstart-elemental"/>）快速入门中详述的说明。</para>
</section>
<section xml:id="id-labels">
<title>标签</title>
<para>Elemental 使用 <literal>MachineInventory</literal> CRD
跟踪其清单，并提供选择清单的方法，例如，根据标签选择要将 Kubernetes
群集部署到的计算机。这样，用户就可以在购买硬件之前，预定义其大部分（甚至所有）基础架构需求。另外，由于节点可以在其相应清单对象上添加/去除标签（结合附加标志
<literal>--label "FOO=BAR"</literal> 重新运行
<literal>elemental-register</literal>），我们可以编写脚本来发现节点的引导位置并告诉 Rancher。</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>已知问题</title>
<itemizedlist>
<listitem>
<para>Elemental UI 目前不知道如何构建安装媒体或更新非“Elemental Teal”操作系统。此问题在将来的版本中应会得到解决。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-akri">
<title>Akri</title>
<para>Akri 是一个 CNCF 沙箱项目，旨在发现叶设备并将其呈现为 Kubernetes 原生资源。它还允许为每个发现的设备调度一个 Pod
或作业。设备可以在节点本地，也可以联网，并可以使用多种协议。</para>
<para>有关 Akri 的上游文档，请访问 <link
xl:href="https://docs.akri.sh">https://docs.akri.sh</link></para>
<section xml:id="id-how-does-suse-edge-use-akri">
<title>SUSE Edge 如何使用 Akri？</title>
<warning>
<para>Akri 目前在 SUSE Edge 堆栈中以技术预览的形式提供。</para>
</warning>
<para>每当需要发现叶设备以及针对叶设备调度工作负载时，都可以使用 Edge 堆栈中包含的 Akri。</para>
</section>
<section xml:id="id-installing-akri">
<title>安装 Akri</title>
<para>Akri 在 Edge Helm 储存库中作为 Helm chart 提供。建议的 Akri 配置方法是使用给定的 Helm chart
部署不同的组件（代理、控制器、发现处理程序），然后使用您偏好的部署机制部署 Akri 的配置 CRD。</para>
</section>
<section xml:id="id-configuring-akri">
<title>配置 Akri</title>
<para>使用 <literal>akri.sh/Configuration</literal> 对象来配置
Akri，该对象包含有关如何发现设备，以及发现了匹配的设备时应执行什么操作的所有信息。</para>
<para>下面列出了示例配置的明细，其中解释了所有字段：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: akri.sh/v0
kind: Configuration
metadata:
  name: sample-configuration
spec:</screen>
<para>此部分描述发现处理程序的配置，您必须指定处理程序的名称（作为 Akri chart 一部分提供的处理程序包括
<literal>udev</literal>、<literal>opcua</literal>、<literal>onvif</literal>）。<literal>discoveryDetails</literal>
与特定的处理程序相关，有关其配置方法，请参见处理程序的文档。</para>
<screen language="yaml" linenumbering="unnumbered">  discoveryHandler:
    name: debugEcho
    discoveryDetails: |+
      descriptions:
        - "foo"
        - "bar"</screen>
<para>此部分定义要为每个已发现设备部署的工作负载。该示例显示了 <literal>brokerPodSpec</literal> 中
<literal>Pod</literal> 配置的最低版本，在此处可以使用 Pod 规范的所有常规字段。其中还显示了
<literal>resources</literal> 部分中用于请求设备的 Akri 特定语法。</para>
<para>您也可以使用作业来代替 Pod，方法是改用 <literal>brokerJobSpec</literal> 键，并在其中提供作业的规范部分。</para>
<screen language="yaml" linenumbering="unnumbered">  brokerSpec:
    brokerPodSpec:
      containers:
      - name: broker-container
        image: rancher/hello-world
        resources:
          requests:
            "{{PLACEHOLDER}}" : "1"
          limits:
            "{{PLACEHOLDER}}" : "1"</screen>
<para>这两个部分显示如何配置 Akri 以便为每个中介程序部署一个服务
(<literal>instanceService</literal>)，或指向所有中介程序
(<literal>configurationService</literal>)。这些部分包含与常规服务相关的所有元素。</para>
<screen language="yaml" linenumbering="unnumbered">  instanceServiceSpec:
    type: ClusterIp
    ports:
    - name: http
      port: 80
      protocol: tcp
      targetPort: 80
  configurationServiceSpec:
    type: ClusterIp
    ports:
    - name: https
      port: 443
      protocol: tcp
      targetPort: 443</screen>
<para><literal>brokerProperties</literal> 字段是一个键/值存储区，它将作为附加环境变量公开给请求已发现设备的任何 Pod。</para>
<para>capacity 是已发现设备的并发用户的允许数量。</para>
<screen language="yaml" linenumbering="unnumbered">  brokerProperties:
    key: value
  capacity: 1</screen>
</section>
<section xml:id="id-writing-and-deploying-additional-discovery-handlers">
<title>编写和部署更多发现处理程序</title>
<para>如果现有的发现处理程序无法涵盖您的设备使用的协议，您可以遵循<link
xl:href="https://docs.akri.sh/development/handler-development">处理程序开发指南</link>编写自己的发现处理程序。</para>
</section>
<section xml:id="akri-dashboard-extension-usage">
<title>Akri Rancher 仪表板扩展</title>
<para>Akri 仪表板扩展允许您使用 Rancher 仪表板用户界面来管理和监控叶设备，并在发现这些设备后运行工作负载。</para>
<para>请参见<xref linkend="components-rancher-dashboard-extensions"/>获取安装指导。</para>
<para>安装扩展后，您可以使用群集资源管理器导航到任何已启用 Akri 的受管群集。在 <emphasis
role="strong">Akri</emphasis> 导航组下，可以看到 <emphasis
role="strong">Configurations</emphasis>（配置）和 <emphasis
role="strong">Instances</emphasis>（实例）部分。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configurations.png"
width="100%"/> </imageobject>
<textobject><phrase>Akri 扩展配置</phrase></textobject>
</mediaobject>
</informalfigure>
<para>配置列表提供了有关<literal>配置发现处理程序</literal>和实例数量的信息。单击名称会打开配置细节页面。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-detail.png"
width="100%"/> </imageobject>
<textobject><phrase>Akri 扩展配置细节</phrase></textobject>
</mediaobject>
</informalfigure>
<para>您还可以编辑或创建新的<emphasis role="strong">配置</emphasis>。扩展允许您选择发现处理程序、设置中介程序 Pod
或作业、自定义配置和实例服务，以及设置配置容量。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-edit.png"
width="100%"/> </imageobject>
<textobject><phrase>Akri 扩展配置编辑</phrase></textobject>
</mediaobject>
</informalfigure>
<para><emphasis role="strong">Instances</emphasis>（实例）列表中会列出已发现的设备。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instances-list.png"
width="100%"/> </imageobject>
<textobject><phrase>Akri 扩展实例列表</phrase></textobject>
</mediaobject>
</informalfigure>
<para>单击<emphasis role="strong">实例名称</emphasis>会打开细节页面，在其中可以查看工作负载和实例服务。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instance-detail.png"
width="100%"/> </imageobject>
<textobject><phrase>Akri 扩展实例细节</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link xl:href="https://k3s.io/">K3s</link> 是经过认证的高可用性 Kubernetes
发行版，专为无人照管、资源受限的远程位置或物联网设备内的生产工作负载而设计。</para>
<para>它封装为单个较小二进制文件，因此可以快速轻松地安装和更新。</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>SUSE Edge 如何使用 K3s</title>
<para>K3s 可用作支持 SUSE Edge 堆栈的 Kubernetes 发行版。它适合安装在 SUSE Linux Micro 操作系统上。</para>
<para>仅当用作后端的 etcd 不能满足您的约束条件时，才建议使用 K3s 作为 SUSE Edge 堆栈 Kubernetes 发行版。如果 etcd
可用作后端，则最好使用 RKE2（<xref linkend="components-rke2"/>）。</para>
</section>
<section xml:id="id-best-practices-4">
<title>最佳实践</title>
<section xml:id="id-installation-2">
<title>安装</title>
<para>将 K3s 安装为 SUSE Edge 堆栈一部分的建议方法是使用 Edge Image Builder (EIB)。有关如何配置 EIB 来部署
K3s 的详细信息，请参见其文档（<xref linkend="components-eib"/>）。</para>
<para>K3s 原生支持 HA 设置以及 Elemental 设置。</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>用于 GitOps 工作流程的 Fleet</title>
<para>SUSE Edge 堆栈使用 Fleet 作为其首选 GitOps 工具。有关 Fleet 安装和用法的详细信息，请参见本文档中的<xref
linkend="components-fleet"/>。</para>
</section>
<section xml:id="id-storage-management">
<title>存储管理</title>
<para>K3s 预配置了本地路径存储服务，这种存储服务适用于单节点群集。对于跨多个节点的群集，我们建议使用 SUSE Storage（<xref
linkend="components-suse-storage"/>）。</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>负载平衡和 HA</title>
<para>如果您是使用 EIB 安装的 K3s，请参见 EIB 文档中的“HA”一章，其中已介绍本节所述的内容。</para>
<para>否则，您需要按照 MetalLB 文档（<xref linkend="guides-metallb-k3s"/>）安装和配置 MetalLB。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para>请参见 <link xl:href="https://docs.rke2.io/">RKE2 官方文档</link>。</para>
<para>RKE2 是完全符合规范且注重安全性与合规性的 Kubernetes 发行版，因为它：</para>
<itemizedlist>
<listitem>
<para>提供默认设置和配置选项，使群集能够在最低限度的运维干预下通过 CIS Kubernetes 基准 v1.6 或 v1.23</para>
</listitem>
<listitem>
<para>实现 FIPS 140-2 合规</para>
</listitem>
<listitem>
<para>在 RKE2 构建管道中使用 <link xl:href="https://trivy.dev">trivy</link> 定期扫描组件中存在的 CVE</para>
</listitem>
</itemizedlist>
<para>RKE2 将控制平面组件作为 kubelet 管理的静态 Pod 进行启动。嵌入式容器运行时为 containerd。</para>
<para>注意：RKE2 也称为 RKE 政府版，这个名称旨在体现它目前面向的另一类使用场景和行业领域。</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2 与 K3s 的比较</title>
<para>K3s 是完全合规的轻量级 Kubernetes 发行版，主要用于 Edge、IoT 和 ARM，已针对易用性和资源受限的环境进行优化。</para>
<para>RKE2 结合了 1.x 版 RKE（后文称为 RKE1）和 K3s 的最大优点。</para>
<para>RKE2 承袭了 K3s 的易用性、易操作性和部署模式。</para>
<para>从 RKE1 继承了与上游 Kubernetes 的紧密一致性。在某些方面，K3s 为了优化边缘部署而与上游 Kubernetes 产生了差异，但
RKE1 和 RKE2 能够保持与上游的紧密一致。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>SUSE Edge 如何使用 RKE2？</title>
<para>RKE2 是 SUSE Edge 堆栈的基本组成部分。它位于 SUSE Linux Micro（<xref
linkend="components-slmicro"/>）的顶层，提供部署 Edge 工作负载所需的标准 Kubernetes 接口。</para>
</section>
<section xml:id="id-best-practices-5">
<title>最佳实践</title>
<section xml:id="id-installation-3">
<title>安装</title>
<para>将 RKE2 安装为 SUSE Edge 堆栈一部分的建议方法是使用 Edge Image Builder (EIB)。有关如何配置 EIB 来部署
RKE2 的详细信息，请参见 EIB 文档（<xref linkend="components-eib"/>）。</para>
<para>EIB 足够灵活，支持 RKE2 所需的任何参数（例如指定 RKE2 版本、<link
xl:href="https://docs.rke2.io/reference/server_config">服务器</link>或<link
xl:href="https://docs.rke2.io/reference/linux_agent_config">代理</link>配置），适用于所有
Edge 使用场景。</para>
<para>对于涉及 Metal<superscript>3</superscript> 的其他使用场景，也可以使用和安装 RKE2。在这种特殊情况下，<link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">Cluster
API 提供程序 RKE2</link> 会自动在使用 Edge Stack 通过 Metal<superscript>3</superscript>
置备的群集上部署 RKE2。</para>
<para>在这种情况下，必须在涉及的不同 CRD 上应用 RKE2 配置。以下示例说明如何使用
<literal>RKE2ControlPlane</literal> CRD 提供不同的 CNI：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>有关 Metal<superscript>3</superscript> 使用场景的详细信息，请参见<xref
linkend="components-metal3"/>。</para>
</section>
<section xml:id="id-high-availability">
<title>高可用性</title>
<para>对于高可用性 (HA) 部署，EIB 会自动部署并配置 MetalLB（<xref linkend="components-metallb"/>）和
Endpoint Copier Operator（<xref linkend="components-eco"/>），以向外部公开 RKE2 API
端点。</para>
</section>
<section xml:id="id-networking">
<title>网络</title>
<para>SUSE Edge 堆栈支持 <link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link>、<link
xl:href="https://docs.tigera.io/calico/latest/about/">Calico</link>，并使用
Cilium 作为其默认 CNI。如果 Pod 需要多个网络接口，也可使用 <link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>
元插件。RKE2 独立版本支持<link
xl:href="https://docs.rke2.io/install/network_options">更广泛的 CNI 选项</link>。</para>
</section>
<section xml:id="id-storage">
<title>存储</title>
<para>RKE2 不提供任何类型的持久性存储类或操作器。对于跨多个节点的群集，建议使用 SUSE Storage（<xref
linkend="components-suse-storage"/>）。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-suse-storage">
<title><link xl:href="https://www.suse.com/products/rancher/storage/">SUSE
Storage</link></title>
<para>SUSE Storage 是一款可靠、易用的轻量级分布式块存储系统，专为 Kubernetes 设计。它是基于 Longhorn 的一款产品，而
Longhorn 是一个开源项目，最初由 Rancher Labs 开发，目前在 CNCF 旗下孵化。</para>
<section xml:id="id-prerequisites-4">
<title>先决条件</title>
<para>如果您要学习本指南，事先需要做好以下准备：</para>
<itemizedlist>
<listitem>
<para>至少一台装有 SUSE Linux Micro 6.1 的主机，可以是物理主机，也可以是虚拟主机</para>
</listitem>
<listitem>
<para>已安装一个 Kubernetes 群集，可以是 K3s 或 RKE2</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-suse-storage">
<title>手动安装 SUSE Storage</title>
<section xml:id="id-installing-open-iscsi">
<title>安装 Open-iSCSI</title>
<para>要部署并使用 SUSE Storage，需满足的一项核心要求是在所有 Kubernetes 节点上安装
<literal>open-iscsi</literal> 软件包并运行 <literal>iscsid</literal>
守护程序。之所以有此要求，是因为 Longhorn 依赖于主机上的 <literal>iscsiadm</literal> 来为 Kubernetes
提供持久卷。</para>
<para>我们来安装此软件包：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>请务必注意，在操作完成后，该软件包只会安装到新快照中，因为 SUSE Linux Micro 是不可变的操作系统。要加载该软件包并让
<literal>iscsid</literal> 守护程序开始运行，我们必须重引导至刚刚创建的新快照。准备就绪后，发出 reboot 命令：</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>如需更多有关安装 open-iscsi 的帮助，请参见<link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/#installing-open-iscsi">官方
Longhorn 文档</link>。</para>
</tip>
</section>
<section xml:id="id-installing-suse-storage">
<title>安装 SUSE Storage</title>
<para>可通过多种方式在 Kubernetes 群集上安装 SUSE Storage。本指南将介绍如何通过 Helm
进行安装，但如果您想要采用其他方法，请按照<link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/">官方文档</link>中的说明操作。</para>
<orderedlist numeration="arabic">
<listitem>
<para>添加 Rancher Chart Helm 储存库：</para>
<screen language="shell" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>从储存库提取最新的 chart：</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para>在 <literal>longhorn-system</literal> 名称空间中安装 SUSE Storage：</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn-crd rancher-charts/longhorn-crd --namespace longhorn-system --create-namespace --version 107.0.0+up1.9.1
helm install longhorn rancher-charts/longhorn --namespace longhorn-system --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>确认部署是否成功：</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-suse-storage-volumes">
<title>创建 SUSE Storage 卷</title>
<para>SUSE Storage 利用名为 <literal>StorageClass</literal> 的 Kubernetes 资源来自动为 Pod 置备
<literal>PersistentVolume</literal> 对象。可以将 <literal>StorageClass</literal>
视为管理员描述其提供的存储<emphasis>类</emphasis>或<emphasis>配置文件</emphasis>的一种方式。</para>
<para>我们来创建一个采用默认选项的 <literal>StorageClass</literal>：</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para>创建 <literal>StorageClass</literal> 后，我们需要提供一个
<literal>PersistentVolumeClaim</literal>
来引用它。<literal>PersistentVolumeClaim</literal> (PVC) 是用户发出的存储请求。PVC 使用
<literal>PersistentVolume</literal>
资源。声明可以请求特定的大小和访问模式（例如，可以以读/写模式挂载声明一次，或以只读模式挂载声明多次）。</para>
<para>我们来创建 <literal>PersistentVolumeClaim</literal>：</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>大功告成！创建 <literal>PersistentVolumeClaim</literal> 后，我们可以继续将其挂接到
<literal>Pod</literal>。部署 <literal>Pod</literal> 时，如果有可用存储空间，Kubernetes 会创建
Longhorn 卷并将其绑定到 <literal>Pod</literal>。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>Kubernetes 中的存储概念复杂但重要。我们简要介绍了一些最常见的 Kubernetes 资源，不过建议您熟悉 Longhorn 提供的<link
xl:href="https://longhorn.io/docs/1.9.1/terminology/">术语文档</link>。</para>
</tip>
<para>对于此示例，结果应如下所示：</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>访问 UI</title>
<para>如果您使用 kubectl 或 Helm 安装了 Longhorn，则需要设置入口控制器，使外部流量能够进入群集。默认不会启用身份验证。如果使用了
Rancher 目录应用程序，Rancher 已自动创建一个提供访问控制的入口控制器 (rancher-proxy)。</para>
<orderedlist numeration="arabic">
<listitem>
<para>获取 Longhorn 的外部服务 IP 地址：</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para>检索到 <literal>longhorn-frontend</literal> IP 地址后，您可以通过在浏览器中导航到该前端来开始使用 UI。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>使用 Edge Image Builder 进行安装</title>
<para>SUSE Edge 使用<xref linkend="components-eib"/>来自定义基础 SUSE Linux Micro
操作系统映像。本节将展示如何执行自定义操作，以置备 RKE2 群集并在其上安装 Longhorn。</para>
<para>我们来创建定义文件：</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.3
image:
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>可以通过 <literal>helm.charts[].valuesFile</literal> 下提供的独立文件自定义任何 Helm chart
值。有关详细信息，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md#kubernetes">上游文档</link>。</para>
</note>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/3.4/edge-image-builder:1.3.0 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>构建映像后，可以使用它在物理主机或虚拟主机上安装操作系统。置备完成后，可以使用 <literal>root:eib</literal>
身份凭证对登录到系统。</para>
<para>确保 Longhorn 已成功部署：</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>此安装不适用于完全隔离的环境。对于这种情况，请参见<xref linkend="suse-storage-install"/>。</para>
</note>
</section>
</chapter>
<chapter xml:id="components-suse-security">
<title><link xl:href="https://www.suse.com/products/rancher/security/">SUSE
Security</link></title>
<para>SUSE Security 是适用于 Kubernetes 的安全解决方案，它在统一的软件包中提供 L7
网络安全性、运行时安全性、供应链安全性与合规性检查。</para>
<para>SUSE Security 是一款作为多容器平台部署的产品，各个容器通过不同端口和接口进行通信。其底层采用 NeuVector 作为容器安全组件。构成
SUSE Security 平台的容器如下：</para>
<itemizedlist>
<listitem>
<para>管理器：提供基于 Web
的控制台的无状态容器。通常只需一个，可在任何位置运行。管理器发生故障不会影响控制器或执行器的任何操作。但是，某些通知（事件）和最近的连接数据将由管理器缓存在内存中，因此这些信息的查看操作会受到影响。</para>
</listitem>
<listitem>
<para>控制器：SUSE Security 的“控制平面”必须部署在 HA
配置中，这样，配置就不会在节点发生故障时丢失。这些容器可在任何位置运行，不过，由于它们的重要性，客户通常会将它们放在“管理”节点、主节点或基础架构节点上。</para>
</listitem>
<listitem>
<para>执行器：此容器部署为
DaemonSet，因此每个要保护的节点上都有一个执行器。通常会部署到每个工作节点，但可为主节点和基础架构节点启用调度，以便将这些容器同时部署到这些节点。注意：如果执行器不在群集节点上，并且连接来自该节点上的
Pod，则 SUSE Security 会将这些容器标记为“不受管”工作负载。</para>
</listitem>
<listitem>
<para>扫描器：根据控制器的指示，使用内置 CVE
数据库执行漏洞扫描。可以部署多个扫描器来提高扫描能力。扫描器可在任何位置运行，但通常在运行控制器的节点上运行。请参见下文了解扫描器节点的大小调整注意事项。在用于构建阶段的扫描时，还可以独立调用扫描器，例如，在触发扫描、检索结果和停止扫描器的管道中。扫描器包含最新的
CVE 数据库，因此应每日更新。</para>
</listitem>
<listitem>
<para>更新器：需要更新 CVE 数据库时，更新器会通过 Kubernetes cron 作业触发扫描器的更新。请务必根据您的环境配置此设置。</para>
</listitem>
</itemizedlist>
<para>在<link xl:href="https://open-docs.neuvector.com/">此处</link>可以找到更深入的 SUSE
Security 初始配置信息和最佳实践文档。</para>
<section xml:id="id-how-does-suse-edge-use-suse-security">
<title>SUSE Edge 如何使用 SUSE Security？</title>
<para>SUSE Edge 提供了更为精简的 SUSE Security 配置，方便您着手进行边缘部署。</para>
</section>
<section xml:id="id-important-notes">
<title>重要注意事项</title>
<itemizedlist>
<listitem>
<para><literal>扫描器</literal>容器必须有充足的内存，以便能够将要扫描的映像提取到内存并对其进行扩展。要扫描 1 GB
以上的映像，请将扫描器的内存增加至略高于最大预期映像大小。</para>
</listitem>
<listitem>
<para>在保护模式下需要高速网络连接。处于保护（内联防火墙阻止）模式的<literal>执行器</literal>需要占用 CPU
和内存来保持和检查连接以及可能的有效负载 (DLP)。增加内存并专门分配一个 CPU
核心供<literal>执行器</literal>使用可确保拥有充足的数据包过滤能力。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>使用 Edge Image Builder 进行安装</title>
<para>SUSE Edge 使用<xref linkend="components-eib"/>来自定义基础 SUSE Linux Micro
操作系统映像。请按照<xref linkend="suse-security-install"/>中所述，在 EIB 置备的 Kubernetes
群集上进行 SUSE Security 隔离式安装。</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para>请参见 <link xl:href="https://metallb.universe.tf/">MetalLB 官方文档</link>。</para>
<blockquote>
<para>MetalLB 是使用标准路由协议的裸机 Kubernetes 群集的负载平衡器实现。</para>
<para>在裸机环境中，设置网络负载平衡器比在云环境中要复杂得多。与云设置中的直接 API 调用不同，裸机需要通过专用网络设备或者负载平衡器和虚拟 IP
(VIP) 配置的组合，来管理高可用性 (HA) 或解决单节点负载平衡器固有的潜在单一故障点 (SPOF)。这些配置不容易实现自动化，在组件会动态扩缩的
Kubernetes 部署中带来了挑战。</para>
<para>MetalLB 可以解决这些挑战，因为它利用 Kubernetes 模型创建 LoadBalancer
类型的服务，就如同这些服务是在云环境中运行一样，即使在裸机设置中，也能做到这一点。</para>
<para>为此可以采用两种不同的方法：通过 <link
xl:href="https://metallb.universe.tf/concepts/layer2/">L2 模式</link>（使用 ARP
<emphasis>技巧</emphasis>）或通过 <link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>。大体而言，L2
不需要任何特殊网络设备，但 BGP 通常效果更好。使用哪种方法取决于使用场景。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>SUSE Edge 如何使用 MetalLB？</title>
<para>SUSE Edge 主要通过三种方式使用 MetalLB：</para>
<itemizedlist>
<listitem>
<para>作为负载平衡器解决方案：MetalLB 充当裸机的负载平衡器解决方案。</para>
</listitem>
<listitem>
<para>对于 HA K3s/RKE2 设置：MetalLB 允许使用虚拟 IP 地址对 Kubernetes API 进行负载平衡。</para>
</listitem>
<listitem>
<para>作为 L3 BGP 解决方案，MetalLB 会向附近的路由器通告指向服务 IP 的路由。</para>
</listitem>
</itemizedlist>
<note>
<para>为了能够公开 API，会使用 Endpoint Copier Operator（<xref linkend="components-eco"/>）将
K8s API 端点从 <literal>kubernetes</literal> 服务同步到
<literal>kubernetes-vip</literal> LoadBalancer 服务。</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>最佳实践</title>
<para>有关 L2 模式下 MetalLB 的安装说明，请参见<xref linkend="guides-metallb-k3s"/>；有关 L3
模式下的安装说明，请参见<xref linkend="guides-metallb-k3s-l3"/>。</para>
<para>有关在 <literal>kube-api-server</literal> 前端安装 MetalLB 以实现高可用性拓扑的指南，可参见<xref
linkend="guides-metallb-kubernetes"/>。</para>
</section>
<section xml:id="id-known-issues-6">
<title>已知问题</title>
<itemizedlist>
<listitem>
<para>K3s 附带负载平衡器解决方案 <literal>Klipper</literal>。要使用 MetalLB，必须禁用
<literal>Klipper</literal>。为此，可以按照 <link
xl:href="https://docs.k3s.io/networking">K3s 文档</link>中所述，使用
<literal>--disable servicelb</literal> 选项启动 K3s 服务器。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eco">
<title>Endpoint Copier Operator</title>
<para><link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link> 是一个 Kubernetes 操作器，其用途是创建 Kubernetes
服务和端点的副本，并将它们保持同步。</para>
<section xml:id="id-how-does-suse-edge-use-endpoint-copier-operator">
<title>SUSE Edge 如何使用 Endpoint Copier Operator？</title>
<para>在 SUSE Edge 中，Endpoint Copier Operator 在实现 K3s/RKE2 群集的高可用性 (HA)
设置方面发挥着关键作用。此目标通过创建一个类型为 <literal>LoadBalancer</literal> 的
<literal>kubernetes-vip</literal> 服务并确保其端点与 kubernetes 端点始终保持同步来实现。系统会使用
MetalLB（<xref linkend="components-metallb"/>）来管理
<literal>kubernetes-vip</literal> 服务，因为其他节点会使用其公开 IP 地址加入群集。</para>
</section>
<section xml:id="id-best-practices-7">
<title>最佳实践</title>
<para>有关使用 Endpoint Copier Operator 的完整文档，请参见<link
xl:href="https://github.com/suse-edge/endpoint-copier-operator/blob/main/README.md">此处</link>。</para>
<para>此外，可参考我们的指南（<xref linkend="guides-metallb-k3s"/>），了解如何使用 Endpoint Copier
Operator 和 MetalLB 实现 K3s/RKE2 高可用性设置。</para>
</section>
<section xml:id="id-known-issues-7">
<title>已知问题</title>
<para>目前，Endpoint Copier Operator 仅能用于单个服务/端点。未来计划增强其功能，以支持多个服务/端点。</para>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>本节介绍如何使用 Edge Virtualization 在边缘节点上运行虚拟机。Edge Virtualization
为轻量级虚拟化使用场景而设计，这些使用场景预期会使用一个通用工作流程来部署和管理虚拟化和容器化的应用程序。</para>
<para>SUSE Edge Virtualization 支持两种虚拟机运行方法：</para>
<orderedlist numeration="arabic">
<listitem>
<para>在主机级别通过 libvirt+qemu-kvm 手动部署虚拟机（不涉及 Kubernetes）</para>
</listitem>
<listitem>
<para>部署 KubeVirt 操作器来实现基于 Kubernetes 的虚拟机管理</para>
</listitem>
</orderedlist>
<para>这两种方法都有效，但下面仅介绍第二种方法。如果您要使用 SUSE Linux Micro 提供的现成可用的标准虚拟化机制，可在<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/chap-virtualization-introduction.html">此处</link>找到详细的指南，尽管该指南主要是针对
SUSE Linux Enterprise Server 编写的，但概念几乎相同。</para>
<para>本指南首先介绍如何将其他虚拟化组件部署到已预先部署的系统，然后会在一个章节中介绍如何通过 Edge Image Builder
将此配置嵌入到初始部署中。如果您不想了解基础知识并想要手动完成设置，请直接跳到该章节。</para>
<section xml:id="id-kubevirt-overview">
<title>KubeVirt 概述</title>
<para>KubeVirt 让您可以通过 Kubernetes 管理虚拟机及其他容器化工作负载。它通过在容器中运行 Linux
虚拟化堆栈的用户空间部分来实现此目的。这样可以最大程度地降低对主机系统的要求，从而简化设置和管理。</para>
<informalexample>
<para>有关 KubeVirt 体系结构的详细信息，请参见<link
xl:href="https://kubevirt.io/user-guide/architecture/">上游文档</link>。</para>
</informalexample>
</section>
<section xml:id="id-prerequisites-5">
<title>先决条件</title>
<para>如果您要学习本指南，事先需要做好以下准备：</para>
<itemizedlist>
<listitem>
<para>至少有一台装有 SUSE Linux Micro 6.1 的物理主机，并且在 BIOS 中启用了虚拟化扩展（有关详细信息，请参见<link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">此处</link>）。</para>
</listitem>
<listitem>
<para>已在您的节点中部署了 K3s/RKE2 Kubernetes 群集，并提供了相应的
<literal>kubeconfig</literal>，使超级用户能够访问该群集。</para>
</listitem>
<listitem>
<para>root 用户访问权限 — 本章中的说明假设您是 root 用户，而<emphasis>未</emphasis>通过
<literal>sudo</literal> 提升特权。</para>
</listitem>
<listitem>
<para>已在本地安装 <link xl:href="https://helm.sh/docs/intro/install/">Helm</link>
并建立了速度够快的网络连接，以便可以将配置推送到 Kubernetes 群集和下载所需的映像。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>手动安装 Edge Virtualization</title>
<para>本指南不会指导您完成 Kubernetes 的部署过程，而是假设您已安装适用于 SUSE Edge 的 <link
xl:href="https://k3s.io/">K3s</link> 或 <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> 版本，并已相应地配置了
kubeconfig，以便能够以超级用户身份执行标准的 <literal>kubectl</literal>
命令。假设您的节点构成了单节点群集，不过，此过程与多节点部署预期不会有太大的差异。</para>
<para>具体而言，将通过以下三个独立的 Helm chart 来部署 SUSE Edge Virtualization：</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>：核心虚拟化组件，即 Kubernetes CRD、操作器，以及使
Kubernetes 能够部署和管理虚拟机的其他组件。</para>
</listitem>
<listitem>
<para><emphasis role="strong">KubeVirt 仪表板扩展</emphasis>：可选的 Rancher UI
扩展，用于实现基本的虚拟机管理，例如启动/停止虚拟机以及访问控制台。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer
(CDI)</emphasis>：一个附加组件，可为 KubeVirt 实现持久性存储集成，使虚拟机能够使用现有 Kubernetes
存储后端来存储数据，同时使用户能够导入或克隆虚拟机的数据卷。</para>
</listitem>
</itemizedlist>
<para>其中的每个 Helm chart 将根据您当前使用的 SUSE Edge 版本进行版本控制。对于生产/支持用途，请采用 SUSE 仓库中提供的制品。</para>
<para>首先，请确保可以正常进行 <literal>kubectl</literal> 访问：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>此命令应会显示如下所示的输出：</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.30.5+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.30.5+rke2r1</screen>
<para>现在您可以继续安装 <emphasis role="strong">KubeVirt</emphasis> 和 <emphasis
role="strong">Containerized Data Importer (CDI)</emphasis> Helm chart：</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/charts/kubevirt --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/charts/cdi --namespace cdi-system --create-namespace</screen>
<para>几分钟后，所有 KubeVirt 和 CDI 组件应会部署完成。您可以通过检查 <literal>kubevirt-system</literal> 和
<literal>cdi-system</literal> 名称空间中部署的所有资源进行验证。</para>
<para>校验 KubeVirt 资源：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>此命令应会显示如下所示的输出：</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>校验 CDI 资源：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>此命令应会显示如下所示的输出：</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para>要校验是否已部署 <literal>VirtualMachine</literal> 自定义资源定义 (CRD)，请使用以下命令：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para>此命令应会列显 <literal>VirtualMachine</literal> 对象的定义，如下所示：</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>部署虚拟机</title>
<para>部署 KubeVirt 和 CDI 后，我们需要基于 <link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE Tumbleweed</link>
定义一个简单的虚拟机。此虚拟机采用最简单的配置，与任何其他 Pod 一样使用标准的“Pod 网络”进行网络配置。与任何没有 <link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>
的容器一样，它也采用非持久性存储空间，因此可确保存储空间是临时性的。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>此命令列显的输出应会指出已创建 <literal>VirtualMachine</literal>：</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>此 <literal>VirtualMachine</literal>
定义极其简洁，几乎未指定配置信息。它只是概括性地指明，此计算机的类型为“<link
xl:href="https://wiki.qemu.org/Features/Q35">q35</link>”，具有 2 GB 内存，使用基于临时
<literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>
的磁盘映像（即，存储在远程映像储存库中某个容器映像内的磁盘映像）。此定义还指定了一个 base64 编码的 cloudInit
磁盘，该磁盘仅用于在引导时创建用户和执行口令（可使用 <literal>base64-d</literal> 对其进行解码）。</para>
<blockquote>
<note>
<para>此虚拟机映像仅用于测试。该映像不受官方支持，仅用作文档示例。</para>
</note>
</blockquote>
<para>此虚拟机需要几分钟时间才能完成引导，因为它需要下载 openSUSE Tumbleweed
磁盘映像，但一旦完成此过程，您可以通过检查虚拟机信息来查看有关该虚拟机的更多细节：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>此命令应会列显启动虚拟机的节点以及虚拟机的 IP 地址。请记住，由于它使用 Pod 网络，因此报告的 IP 地址与任何其他 Pod 一样并且可路由：</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>在使用 CNI（例如 Cilium）将流量直接路由到 Pod 的情况下，在 Kubernetes 群集节点本身上运行这些命令时，您应该可以通过
<literal>ssh</literal> 直接连接到该虚拟机本身。请将下面的 IP 地址替换为分配给您的虚拟机的 IP 地址：</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>进入此虚拟机后，可对其进行任意操作，但请记住，它的资源有限，磁盘空间只有 1 GB。完成后，请按 <literal>Ctrl-D</literal>
或输入 <literal>exit</literal> 与 SSH 会话断开连接。</para>
<para>虚拟机进程仍封装在标准 Kubernetes Pod 中。<literal>VirtualMachine</literal> CRD
代表期望的虚拟机，但与任何其他应用程序一样，实际启动虚拟机的过程是通过 <literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>
Pod（标准 Kubernetes Pod）进行的。启动的每个虚拟机都有一个对应的 <literal>virt-launcher</literal>
Pod：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>此命令应会针对我们定义的 Tumbleweed 虚拟机显示一个 <literal>virt-launcher</literal> Pod：</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>如果深入查看这个 <literal>virt-launcher</literal> Pod，您会看到它正在执行
<literal>libvirt</literal> 和 <literal>qemu-kvm</literal> 进程。我们可以进入该 Pod
本身并查看其内部工作。请注意您需要根据自己的 Pod 名称修改以下命令：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>进入 Pod 后，尝试运行 <literal>virsh</literal> 命令并查看进程。您会看到
<literal>qemu-system-x86_64</literal>
二进制文件正在运行，还会看到用于监控虚拟机的某些进程，以及磁盘映像的位置及网络（作为 tap 设备）的插接方式：</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>最后，我们需要删除此虚拟机以清理资源：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>使用 virtctl</title>
<para>除了标准的 Kubernetes 命令行工具（即 <literal>kubectl</literal>）之外，KubeVirt
还附带了一个配套的命令行实用程序，它能让你与群集进行交互，从而填补虚拟化领域与 Kubernetes
设计初衷所面向的领域之间的一些空白。例如，<literal>virtctl</literal>
工具提供了管理虚拟机生命周期（启动、停止、重启等）、访问虚拟控制台、上载虚拟机映像以及与 Kubernetes 结构（如服务）交互的功能，且无需直接使用
API 或 CRD。</para>
<para>我们来下载最新的稳定版 <literal>virtctl</literal> 工具：</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.5.2
$ wget https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64</screen>
<para>如果您使用的是其他体系结构或非 Linux 计算机，可在<link
xl:href="https://github.com/kubevirt/kubevirt/releases">此处</link>找到其他版本。需要先将其转换为可执行文件才能继续，将其移动到
<literal>$PATH</literal> 中的某个位置可能会有帮助：</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-$VERSION-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>然后，可以使用 <literal>virtctl</literal>
命令行工具创建虚拟机。我们来复制前面创建的虚拟机，请注意我们会通过管道将输出直接传入 <literal>kubectl apply</literal>：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>此命令应会显示虚拟机正在运行（由于容器映像将被缓存，因此这一次虚拟机的启动速度更快一些）：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>现在我们可以使用 <literal>virtctl</literal> 直接连接到该虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para><literal>virtctl</literal> 还可以使用其他许多命令。例如，如果网络出现故障，您可以使用 <literal>virtctl
console</literal> 访问串行控制台；可以使用 <literal>virtctl guestosinfo</literal>
获取详细的操作系统信息，前提是已在 Guest 上安装并运行 <literal>qemu-guest-agent</literal>。</para>
<para>最后，我们来暂停再恢复该虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para>您会发现，<literal>VirtualMachine</literal> 对象显示为 <emphasis
role="strong">Paused</emphasis>，而 <literal>VirtualMachineInstance</literal>
对象则显示为 <emphasis role="strong">Running</emphasis>，但同时显示了 <emphasis
role="strong">READY=False</emphasis>：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>您还会发现不再可以连接到该虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>我们来恢复该虚拟机并重试：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>现在我们应该可以重新建立连接：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>最后，我们将该虚拟机去除：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>简单入口网络</title>
<para>本节介绍如何将虚拟机公开为标准 Kubernetes 服务，并通过 Kubernetes 入口服务（例如 <link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">RKE2
中的 NGINX</link> 或 <link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">K3s
中的 Traefik</link>）来提供这些虚拟机。本文档假设已正确配置这些组件，并且有一个适当的 DNS 指针指向 Kubernetes
服务器节点或入口虚拟 IP（例如通过通配符来指向），以正确解析入口。</para>
<blockquote>
<note>
<para>在 SUSE Edge 3.1+ 中，如果您在多服务器节点配置中使用 K3s，则可能需要为入口配置基于 MetalLB 的 VIP；对于 RKE2
则不需要这样做。</para>
</note>
</blockquote>
<para>在示例环境中，部署了另一个 openSUSE Tumbleweed 虚拟机，cloud-init 用于在引导时将 NGINX 安装为简单 Web
服务器，并且配置了一条简单的返回消息，用于在调用时验证其是否按预期工作。要了解如何执行此操作，只需对以下输出中的 cloud-init 部分运行
<literal>base64 -d</literal> 即可。</para>
<para>现在我们来创建此虚拟机：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>此虚拟机成功启动后，我们可以使用 <literal>virtctl</literal> 命令公开
<literal>VirtualMachineInstance</literal>，其外部端口为
<literal>8080</literal>，目标端口为 <literal>80</literal>（NGINX 默认侦听此端口）。此处我们之所以使用
<literal>virtctl</literal> 命令，是因为它能够识别虚拟机对象与 Pod 之间的映射。这为我们创建了新服务：</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>然后会自动创建一个适当的服务：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>接下来，如果您使用 <literal>kubectl create ingress</literal>，则可以创建一个指向此服务的 ingress
对象。此处请根据您的 DNS 配置修改 URL（在 <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link>
对象中称为“host”），并确保将其指向端口 <literal>8080</literal>：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>正确配置 DNS 后，可以立即对 URL 运行 curl 命令：</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>我们来通过去除此虚拟机及其服务和入口资源进行清理：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>使用 Rancher UI 扩展</title>
<para>SUSE Edge Virtualization 为 Rancher Manager 提供了 UI 扩展，让您可以使用 Rancher 仪表板 UI
进行基本的虚拟机管理。</para>
<section xml:id="id-installation-4">
<title>安装</title>
<para>请参见<xref linkend="components-rancher-dashboard-extensions"/>获取安装指导。</para>
</section>
<section xml:id="kubevirt-dashboard-extension-usage">
<title>使用 KubeVirt Rancher 仪表板扩展</title>
<para>该扩展在群集资源管理器中引入了新的 <emphasis role="strong">KubeVirt</emphasis> 部分。此部分已添加到装有
KubeVirt 的任何受管群集。</para>
<para>使用该扩展，您可以直接与 KubeVirt 虚拟机资源进行交互，以管理虚拟机生命周期。</para>
<section xml:id="id-creating-a-virtual-machine">
<title>创建虚拟机</title>
<orderedlist numeration="arabic">
<listitem>
<para>单击左侧导航栏中已启用 KubeVirt 的受管群集，导航到 <emphasis role="strong">Cluster
Explorer</emphasis>（群集资源管理器）。</para>
</listitem>
<listitem>
<para>导航到 <emphasis role="strong">KubeVirt &gt; Virtual
Machines（虚拟机）</emphasis>页面，然后单击屏幕右上角的 <literal>Create from YAML</literal>（基于
YAML 创建）。</para>
</listitem>
<listitem>
<para>填写或粘贴虚拟机定义，然后单击 <literal>Create</literal>（创建）。使用“部署虚拟机”一节中创建的虚拟机定义作为灵感来源。</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width="100%"/>
</imageobject>
<textobject><phrase>虚拟机页面</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-virtual-machine-actions">
<title>虚拟机操作</title>
<para>使用操作菜单（可通过每个虚拟机右侧的 <emphasis role="strong">⋮</emphasis>
下拉列表访问）来执行启动、停止、暂停或软重引导操作。或者，您也可以选中多个虚拟机并使用列表顶部的组操作来对它们执行操作。</para>
<para>执行这些操作可能会对虚拟机运行策略产生影响。有关详细信息，<link
xl:href="https://kubevirt.io/user-guide/compute/run_strategies/#virtctl">请参见
KubeVirt 文档中的表格</link>。</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>访问虚拟机控制台</title>
<para>“Virtual machines”（虚拟机）列表提供了<literal>控制台</literal>下拉列表，用于通过 <emphasis
role="strong">VNC 或串行控制台</emphasis>连接到虚拟机。此操作仅适用于正在运行的虚拟机。</para>
<para>在某些情况下，需要等待一段时间才能在全新启动的虚拟机上访问控制台。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width="100%"/>
</imageobject>
<textobject><phrase>VNC 控制台 UI</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>使用 Edge Image Builder 进行安装</title>
<para>SUSE Edge 使用<xref linkend="components-eib"/>来自定义基础 SUSE Linux Micro
操作系统映像。请按照<xref linkend="kubevirt-install"/>中所述，在 EIB 置备的 Kubernetes 群集上进行
KubeVirt 和 CDI 隔离式安装。</para>
</section>
</chapter>
<chapter xml:id="components-system-upgrade-controller">
<title>系统升级控制器</title>
<para>请参见<link
xl:href="https://github.com/rancher/system-upgrade-controller">系统升级控制器文档</link>。</para>
<blockquote>
<para>系统升级控制器 (SUC) 旨在提供一个针对节点的 Kubernetes 原生通用升级控制器。它引入了一个新的
CRD，即计划，用于定义您的各种升级策略/要求。计划的主要意图是改变群集中的节点。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-system-upgrade-controller">
<title>SUSE Edge 如何使用系统升级控制器？</title>
<para>SUSE Edge 使用 <literal>SUC</literal> 来协助完成管理群集和下游群集中与操作系统及 Kubernetes
版本升级相关的各类“Day 2”操作。</para>
<para>“Day 2”操作通过 <literal>SUC 计划</literal>来定义。<literal>SUC</literal>
会根据这些计划在每个节点上部署工作负载，以执行相应的“Day 2”操作。</para>
<para><xref linkend="components-upgrade-controller"/>中也会使用
<literal>SUC</literal>。要了解 SUC 与升级控制器之间的主要区别，请参见<xref
linkend="components-upgrade-controller-uc-vs-suc"/>。</para>
</section>
<section xml:id="components-system-upgrade-controller-install">
<title>安装系统升级控制器</title>
<important>
<para>从 Rancher <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.10.0">2.10.0</link>
版本开始，会自动安装<literal>系统升级控制器</literal>。</para>
<para><emphasis role="strong">仅当</emphasis>您的环境<emphasis
role="strong">未</emphasis>由 Rancher 管理，或者您的 Rancher 版本低于
<literal>2.10.0</literal> 时，才需要执行以下步骤。</para>
</important>
<para>建议您通过位于 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库中的 Fleet（<xref linkend="components-fleet"/>）安装 SUC。</para>
<note>
<para><literal>suse-edge/fleet-examples</literal> 储存库提供的资源<emphasis
role="strong">必须</emphasis>始终在有效的 <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">fleet-examples
版本</link>中使用。要确定需使用哪个版本，请参见发行说明（<xref linkend="release-notes"/>）。</para>
</note>
<para>如果无法使用 Fleet 安装 SUC，可以通过 Rancher 的 Helm chart 储存库进行安装，或者将 Rancher 的 Helm
chart 整合到您自己的第三方 GitOps 工作流程中。</para>
<para>本节介绍如下内容：</para>
<itemizedlist>
<listitem>
<para>Fleet 安装（<xref linkend="components-system-upgrade-controller-fleet"/>）</para>
</listitem>
<listitem>
<para>Helm 安装（<xref linkend="components-system-upgrade-controller-helm"/>）</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet">
<title>系统升级控制器 Fleet 安装</title>
<para>使用 Fleet 时，可以通过两种资源来部署 SUC：</para>
<itemizedlist>
<listitem>
<para><link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link> 资源 -
适用于有外部/本地 Git 服务器可用的使用场景。有关安装说明，请参见<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>。</para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源 - 适用于不支持本地
Git 服务器选项的隔离使用场景。有关安装说明，请参见<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>。</para>
</listitem>
</itemizedlist>
<section xml:id="components-system-upgrade-controller-fleet-gitrepo">
<title>系统升级控制器安装 - GitRepo</title>
<note>
<para>如果有可用的 Rancher UI，也可通过该界面完成此过程。有关详细信息，请参见<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">在
Rancher UI 中访问 Fleet</link>。</para>
</note>
<para>在您的<emphasis role="strong">管理</emphasis>群集中：</para>
<orderedlist numeration="arabic">
<listitem>
<para>确定要在哪些群集上部署 SUC，方法是在<emphasis role="strong">管理</emphasis>群集内的适当 Fleet
工作空间中部署 SUC <literal>GitRepo</literal>。默认情况下，Fleet 有两个工作空间：</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - 用于需要部署在<emphasis
role="strong">管理</emphasis>群集上的资源。</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - 用于需要部署在<emphasis
role="strong">下游</emphasis>群集上的资源。</para>
<para>有关 Fleet 工作空间的详细信息，请参见<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">上游</link>文档。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>部署 <literal>GitRepo</literal> 资源：</para>
<itemizedlist>
<listitem>
<para>要在管理群集上部署 SUC，请使用：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-local -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
EOF</screen>
</listitem>
<listitem>
<para>要在下游群集上部署 SUC，请使用：</para>
<note>
<para>在部署下方资源之前，您<emphasis
role="strong">必须</emphasis>提供有效的<literal>目标</literal>配置，以便 Fleet
了解资源要部署至哪些下游群集。有关如何映射到下游群集的信息，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link>。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n fleet-default -f - &lt;&lt;EOF
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: system-upgrade-controller
spec:
  revision: release-3.4.0
  paths:
  - fleets/day2/system-upgrade-controller
  repo: https://github.com/suse-edge/fleet-examples.git
  targets:
  - clusterSelector: CHANGEME
  # Example matching all clusters:
  # targets:
  # - clusterSelector: {}
EOF</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>验证 <literal>GitRepo</literal> 资源是否已部署：</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get gitrepo system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.4.0   1/1</screen>
</listitem>
<listitem>
<para>验证系统升级控制器部署：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           2m20s</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-fleet-bundle">
<title>系统升级控制器安装 - 捆绑包</title>
<para>本节说明如何使用 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link> 在标准
Fleet 配置下构建并部署<literal>捆绑包</literal>资源。</para>
<orderedlist numeration="arabic">
<listitem>
<para>在连接网络的计算机上下载 <literal>fleet-cli</literal>：</para>
<note>
<para>确保您下载的 fleet-cli 版本与群集上部署的 Fleet 版本匹配。</para>
</note>
<itemizedlist>
<listitem>
<para>对于 Mac 用户，有一个 <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulae。</para>
</listitem>
<listitem>
<para>对于 Linux 和 Windows 用户，每个 Fleet <link
xl:href="https://github.com/rancher/fleet/releases">版本</link>都会有作为<emphasis
role="strong">资产</emphasis>存在的二进制文件。</para>
<itemizedlist>
<listitem>
<para>Linux AMD：</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-amd64</screen>
</listitem>
<listitem>
<para>Linux ARM：</para>
<screen language="bash" linenumbering="unnumbered">curl -L -o fleet-cli https://github.com/rancher/fleet/releases/download/v0.13.1/fleet-linux-arm64</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <literal>fleet-cli</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x fleet-cli</screen>
</listitem>
<listitem>
<para>克隆您要使用的 <literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>：</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>导航到 <literal>fleet-examples</literal> 储存库中的 SUC Fleet：</para>
<screen language="bash" linenumbering="unnumbered">cd fleet-examples/fleets/day2/system-upgrade-controller</screen>
</listitem>
<listitem>
<para>确定要在哪些群集上部署 SUC，方法是在管理群集内的适当 Fleet 工作空间中部署 SUC 捆绑包。默认情况下，Fleet 有两个工作空间：</para>
<itemizedlist>
<listitem>
<para><literal>fleet-local</literal> - 用于需要部署在<emphasis
role="strong">管理</emphasis>群集上的资源。</para>
</listitem>
<listitem>
<para><literal>fleet-default</literal> - 用于需要部署在<emphasis
role="strong">下游</emphasis>群集上的资源。</para>
<para>有关 Fleet 工作空间的详细信息，请参见<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">上游</link>文档。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>如果您只打算在下游群集上部署 SUC，请创建一个与特定群集匹配的 <literal>targets.yaml</literal> 文件：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterSelector: CHANGEME
EOF</screen>
<para>有关如何映射到下游群集的信息，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link></para>
</listitem>
<listitem>
<para>继续构建捆绑包：</para>
<note>
<para>请确保您<emphasis role="strong">没有</emphasis>下载
<literal>fleet-examples/fleets/day2/system-upgrade-controller</literal> 目录中的
fleet-cli，否则它将和捆绑包封装在一起，不建议这么做。</para>
</note>
<itemizedlist>
<listitem>
<para>要在管理群集上部署 SUC，请执行：</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress -n fleet-local -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>要在下游群集上部署 SUC，请执行：</para>
<screen language="bash" linenumbering="unnumbered">fleet-cli apply --compress --targets-file=targets.yaml -n fleet-default -o - system-upgrade-controller . &gt; system-upgrade-controller-bundle.yaml</screen>
<para>有关此过程的详细信息，请参见<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm Chart 转换为捆绑包</link>。</para>
<para>有关 <literal>fleet-cli apply</literal> 命令的详细信息，请参见 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <literal>system-upgrade-controller-bundle.yaml</literal> 捆绑包传输到您的管理群集计算机：</para>
<screen language="bash" linenumbering="unnumbered">scp system-upgrade-controller-bundle.yaml &lt;machine-address&gt;:&lt;filesystem-path&gt;</screen>
</listitem>
<listitem>
<para>在您的管理群集上，部署 <literal>system-upgrade-controller-bundle.yaml</literal> 捆绑包：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>在您的管理群集上，验证捆绑包是否已部署：</para>
<screen language="bash" linenumbering="unnumbered"># Namespace will vary based on where you want to deploy SUC
kubectl get bundle system-upgrade-controller -n &lt;fleet-local/fleet-default&gt;

NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   1/1</screen>
</listitem>
<listitem>
<para>根据捆绑包部署到的 Fleet 工作空间，导航到群集并验证 SUC 部署：</para>
<note>
<para>SUC 始终部署在 <emphasis role="strong">cattle-system</emphasis> 名称空间中。</para>
</note>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           111s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-helm">
<title>系统升级控制器 Helm 安装</title>
<orderedlist numeration="arabic">
<listitem>
<para>添加 Rancher chart 储存库：</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/</screen>
</listitem>
<listitem>
<para>部署 SUC chart：</para>
<screen language="bash" linenumbering="unnumbered">helm install system-upgrade-controller rancher-charts/system-upgrade-controller --version 107.0.0 --set global.cattle.psp.enabled=false -n cattle-system --create-namespace</screen>
<para>此命令将安装 Edge 3.4 平台所需的 SUC 0.16.0 版本。</para>
</listitem>
<listitem>
<para>验证 SUC 部署：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment system-upgrade-controller -n cattle-system
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
system-upgrade-controller   1/1     1            1           37s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans">
<title>监控系统升级控制器计划</title>
<para>可以通过以下方式查看 SUC 计划：</para>
<itemizedlist>
<listitem>
<para>通过 Rancher UI（<xref
linkend="components-system-upgrade-controller-monitor-plans-rancher"/>）。</para>
</listitem>
<listitem>
<para>通过群集内部的手动监控（<xref
linkend="components-system-upgrade-controller-monitor-plans-manual"/>）。</para>
</listitem>
</itemizedlist>
<important>
<para>为 SUC 计划部署的 Pod 在执行成功后会保留 <emphasis role="strong">15</emphasis>
分钟，之后会被创建它们的相应作业去除。如果您希望在此时间段后仍能访问 Pod 日志，需为群集启用日志记录。有关如何在 Rancher
中实现此操作的信息，请参见 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/logging">Rancher
与日志记录服务的集成</link>。</para>
</important>
<section xml:id="components-system-upgrade-controller-monitor-plans-rancher">
<title>监控系统升级控制器计划 - Rancher UI</title>
<para>要检查特定 SUC 计划的 Pod 日志，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>在左上角，选择 <emphasis role="strong">☰ → &lt;您的群集名称&gt;</emphasis></para>
</listitem>
<listitem>
<para>选择“Workloads”（工作负载）→“Pod”</para>
</listitem>
<listitem>
<para>选择 <literal>Only User Namespaces</literal>（仅用户名称空间）下拉菜单，并添加
<literal>cattle-system</literal> 名称空间。</para>
</listitem>
<listitem>
<para>在 Pod 过滤栏中输入 SUC 计划 Pod
的名称。该名称采用以下模板格式：<literal>apply-&lt;计划名称&gt;-on-&lt;节点名称&gt;</literal></para>
<note>
<para>对于特定的 SUC 计划，可能同时有<literal>已完成</literal>和<literal>未知</literal>的
Pod。这是正常的，原因要归咎于一些升级的性质。</para>
</note>
</listitem>
<listitem>
<para>选择您要查看其日志的 Pod，然后导航到 <emphasis role="strong">⋮ → View Logs（查看日志）</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="components-system-upgrade-controller-monitor-plans-manual">
<title>监控系统升级控制器计划 - 手动</title>
<note>
<para>以下步骤假设 <literal>kubectl</literal> 已配置为连接到已部署 <emphasis role="strong">SUC
计划</emphasis>的群集。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>列出部署的 <emphasis role="strong">SUC</emphasis> 计划：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get plans -n cattle-system</screen>
</listitem>
<listitem>
<para>获取 <emphasis role="strong">SUC</emphasis> 计划的 Pod：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l upgrade.cattle.io/plan=&lt;plan_name&gt; -n cattle-system</screen>
<note>
<para>对于特定的 SUC 计划，可能同时有<literal>已完成</literal>和<literal>未知</literal>的
Pod。这是正常的，原因要归咎于一些升级的性质。</para>
</note>
</listitem>
<listitem>
<para>获取 Pod 的日志：</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n cattle-system</screen>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="components-upgrade-controller">
<title>升级控制器</title>
<para>Kubernetes 控制器可以执行以下 SUSE Edge 平台组件的升级：</para>
<itemizedlist>
<listitem>
<para>操作系统 (SUSE Linux Micro)</para>
</listitem>
<listitem>
<para>Kubernetes（K3s 和 RKE2）</para>
</listitem>
<listitem>
<para>其他组件（Rancher、Elemental、SUSE Security 等）</para>
</listitem>
</itemizedlist>
<para><link
xl:href="https://github.com/suse-edge/upgrade-controller">升级控制器</link>通过将上述组件的复杂性封装在一个<literal>面向用户</literal>的资源（该资源可作为升级<emphasis
role="strong">触发器</emphasis>）中，来简化这些组件的升级过程。用户只需配置此资源，其余工作均由<literal>升级控制器</literal>处理。</para>
<note>
<para>目前，<literal>升级控制器</literal>仅支持为<emphasis role="strong">非隔离管理</emphasis>群集进行
SUSE Edge 平台升级。有关详细信息，请参见<xref
linkend="components-upgrade-controller-known-issues"/>。</para>
</note>
<section xml:id="id-how-does-suse-edge-use-upgrade-controller">
<title>SUSE Edge 如何使用升级控制器？</title>
<para><emphasis role="strong">升级控制器</emphasis>对于将管理群集从一个 SUSE Edge
版本升级到下一个版本时需自动执行（以前是手动执行）的“Day 2”操作至关重要。</para>
<para>为了实现这种自动化，升级控制器使用了系统升级控制器（<xref
linkend="components-system-upgrade-controller"/>）和 <link
xl:href="https://github.com/k3s-io/helm-controller/">Helm 控制器</link>等工具。</para>
<para>有关升级控制器工作原理的详细信息，请参见<xref linkend="components-upgrade-controller-how"/>。</para>
<para>有关升级控制器的已知限制，请参见<xref
linkend="components-upgrade-controller-known-issues"/>。</para>
<para>如需了解升级控制器与系统升级控制器的区别，请参见<xref
linkend="components-upgrade-controller-uc-vs-suc"/>。</para>
</section>
<section xml:id="components-upgrade-controller-uc-vs-suc">
<title>升级控制器与系统升级控制器</title>
<para>系统升级控制器 (SUC)（<xref
linkend="components-system-upgrade-controller"/>）是一款通用工具，负责将升级指令分发到特定的
Kubernetes 节点。</para>
<para>虽然它支持 SUSE Edge 平台的部分“Day 2”操作，但<emphasis
role="strong">并未</emphasis>涵盖所有此类操作。此外，即便是对于受支持的操作，用户也必须手动配置、维护和部署多个
<literal>SUC 计划</literal>，因为这一过程容易出错，可能导致意外问题。</para>
<para>这就催生了对下面这样的工具的需求：该工具能够<emphasis role="strong">自动化</emphasis>并<emphasis
role="strong">抽象化</emphasis> SUSE Edge 平台各类“Day
2”操作管理的复杂性。因此，<literal>升级控制器</literal>应运而生。它通过引入一个<literal>面向用户的单一资源</literal>来驱动升级，从而简化了升级过程。用户只需管理这一资源，其余工作均由<literal>升级控制器</literal>处理。</para>
</section>
<section xml:id="components-upgrade-controller-installation">
<title>安装升级控制器</title>
<section xml:id="id-prerequisites-6">
<title>先决条件</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://helm.sh/docs/intro/install/">Helm</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://cert-manager.io/v1.15-docs/installation/helm/#installing-with-helm">cert-manager</link></para>
</listitem>
<listitem>
<para>系统升级控制器（<xref linkend="components-system-upgrade-controller-install"/>）</para>
</listitem>
<listitem>
<para>一个 Kubernetes 群集，可以是 K3s 或 RKE2</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-steps">
<title>步骤</title>
<orderedlist numeration="arabic">
<listitem>
<para>在您的管理群集上安装升级控制器 Helm chart：</para>
<screen language="bash" linenumbering="unnumbered">helm install upgrade-controller oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1 --create-namespace --namespace upgrade-controller-system</screen>
</listitem>
<listitem>
<para>验证升级控制器部署：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get deployment -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>验证升级控制器 Pod：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n upgrade-controller-system</screen>
</listitem>
<listitem>
<para>验证升级控制器 Pod 日志：</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n upgrade-controller-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-how">
<title>升级控制器的工作原理</title>
<para>为了执行 Edge 版本升级，升级控制器引入了两个新的 Kubernetes <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>：</para>
<itemizedlist>
<listitem>
<para>UpgradePlan（<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>）-
由用户创建；保存有关 Edge 版本升级的配置。</para>
</listitem>
<listitem>
<para>ReleaseManifest（<xref
linkend="components-upgrade-controller-extensions-release-manifest"/>）-
由升级控制器创建；保存特定的 Edge 版本所对应的组件版本。<emphasis role="strong">用户不得编辑此文件。</emphasis></para>
</listitem>
</itemizedlist>
<para>升级控制器还会创建一个 <literal>ReleaseManifest</literal> 资源，该资源保存用户在
<literal>UpgradePlan</literal> 资源的 <literal>releaseVersion</literal> 属性下指定的
Edge 版本的组件数据。</para>
<para>之后，升级控制器会使用 <literal>ReleaseManifest</literal> 中的组件数据，按以下顺序升级 Edge 版本组件：</para>
<orderedlist numeration="arabic">
<listitem>
<para>操作系统 (OS)（<xref linkend="components-upgrade-controller-how-os"/>）。</para>
</listitem>
<listitem>
<para>Kubernetes（<xref linkend="components-upgrade-controller-how-k8s"/>）。</para>
</listitem>
<listitem>
<para>其他组件（<xref linkend="components-upgrade-controller-how-additional"/>）。</para>
</listitem>
</orderedlist>
<note>
<para>在升级过程中，升级控制器会持续向创建的 <literal>UpgradePlan</literal>
输出升级信息。有关如何跟踪升级过程的详细信息，请参见<xref
linkend="components-upgrade-controller-how-track"/>。</para>
</note>
<section xml:id="components-upgrade-controller-how-os">
<title>操作系统升级</title>
<para>升级操作系统时，升级控制器会创建命名模板如下的 SUC（<xref
linkend="components-system-upgrade-controller"/>）计划：</para>
<itemizedlist>
<listitem>
<para>对于与控制平面节点操作系统升级相关的 SUC 计划 -
<literal>control-plane-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
<listitem>
<para>对于与工作节点操作系统升级相关的 SUC 计划 -
<literal>workers-&lt;os-name&gt;-&lt;os-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
</itemizedlist>
<para>SUC 会根据这些计划，继续在执行实际操作系统升级的群集的每个节点上创建工作负载。</para>
<para>根据 <literal>ReleaseManifest</literal>，操作系统升级可能包括：</para>
<itemizedlist>
<listitem>
<para>仅软件包更新 - 适用于操作系统版本不随 Edge 版本变动的使用场景。</para>
</listitem>
<listitem>
<para>完整操作系统迁移 - 适用于操作系统版本随 Edge 版本变动的使用场景。</para>
</listitem>
</itemizedlist>
<para>升级首先从控制平面节点开始，一次执行<emphasis
role="strong">一个</emphasis>节点的升级。只有在控制平面节点的升级完成后，工作节点才会开始升级。</para>
<note>
<para>如果群集有<emphasis role="strong">多个</emphasis>指定类型的节点，升级控制器会配置操作系统 SUC 计划来<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">清空</link>群集节点。</para>
<para>对于控制平面节点<emphasis role="strong">有多个</emphasis>而工作节点<emphasis
role="strong">只有一个</emphasis>的群集，只会清空控制平面节点，反之亦然。</para>
<para>有关如何完全禁用节点清空的信息，请参见“UpgradePlan”（<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>）一节。</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-k8s">
<title>Kubernetes 升级</title>
<para>升级群集的 Kubernetes 发行版时，升级控制器会创建具有以下命名模板的 SUC（<xref
linkend="components-system-upgrade-controller"/>）计划：</para>
<itemizedlist>
<listitem>
<para>对于与控制平面节点 Kubernetes 升级相关的 SUC 计划 -
<literal>control-plane-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
<listitem>
<para>对于与工作节点 Kubernetes 升级相关的 SUC 计划 -
<literal>workers-&lt;k8s-version&gt;-&lt;suffix&gt;</literal>。</para>
</listitem>
</itemizedlist>
<para>SUC 会根据这些计划，继续在执行实际 Kubernetes 升级的群集的每个节点上创建工作负载。</para>
<para>Kubernetes 升级将从控制平面节点开始，一次升级<emphasis
role="strong">一个</emphasis>节点。只有在控制平面节点的升级完成后，工作节点才会开始升级。</para>
<note>
<para>如果群集有<emphasis role="strong">多个</emphasis>指定类型的节点，升级控制器会配置 Kubernetes SUC
计划来<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">清空</link>群集节点。</para>
<para>对于控制平面节点<emphasis role="strong">有多个</emphasis>而工作节点<emphasis
role="strong">只有一个</emphasis>的群集，只会清空控制平面节点，反之亦然。</para>
<para>有关如何完全禁用节点清空的信息，请参见<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>。</para>
</note>
</section>
<section xml:id="components-upgrade-controller-how-additional">
<title>其他组件的升级</title>
<para>目前，所有其他组件都是通过 Helm chart 安装的。有关特定版本各组件的完整列表，请参见发行说明（<xref
linkend="release-notes"/>）。</para>
<para>对于通过 EIB（<xref linkend="components-eib"/>）部署的 Helm chart，升级控制器会更新每个组件的现有
<link xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart
CR</link>。</para>
<para>对于在 EIB 之外部署的 Helm chart，升级控制器会为每个组件创建一个 <literal>HelmChart</literal> 资源。</para>
<para>在创建/更新 <literal>HelmChart</literal> 资源后，升级控制器会依赖 <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>
来拾取此更改并继续执行实际的组件升级。</para>
<para>Chart 将按照 <literal>ReleaseManifest</literal> 中的顺序依次升级。您还可通过
<literal>UpgradePlan</literal> 传递其他值。如果在新 SUSE Edge 版本中某个 chart
的版本保持不变，则不会将其升级。有关这方面的详细信息，请参见<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>。</para>
</section>
</section>
<section xml:id="components-upgrade-controller-extensions">
<title>Kubernetes API 扩展</title>
<para>升级控制器引入的 Kubernetes API 的扩展。</para>
<section xml:id="components-upgrade-controller-extensions-upgrade-plan">
<title>UpgradePlan</title>
<para>升级控制器引入了一个名为 <literal>UpgradePlan</literal> 的新 Kubernetes <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>。</para>
<para><literal>UpgradePlan</literal> 是升级控制器的指令机制，支持以下配置：</para>
<itemizedlist>
<listitem>
<para><literal>releaseVersion</literal> - 群集应升级到的 Edge 版本。版本必须遵循<link
xl:href="https://semver.org">语义</link>版本规范，并且应从发行说明（<xref
linkend="release-notes"/>）中检索。</para>
</listitem>
<listitem>
<para><literal>disableDrain</literal> - <emphasis
role="strong">可选</emphasis>；指示升级控制器是否禁用节点<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">清空</link>。当您有具有<link
xl:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">干扰预算</link>的工作负载时，可以用到。</para>
<itemizedlist>
<listitem>
<para>禁用控制平面节点清空的示例：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true</screen>
</listitem>
<listitem>
<para>禁用控制平面和工作节点清空的示例：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  disableDrain:
    controlPlane: true
    worker: true</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>helm</literal> - <emphasis role="strong">可选</emphasis>；为通过 Helm
安装的组件指定其他值。</para>
<warning>
<para>建议仅将此字段用于对升级至关重要的值。标准 chart 值的更新应在相应 chart 升级到下一版本后进行。</para>
</warning>
<itemizedlist>
<listitem>
<para>示例：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  helm:
  - chart: foo
    values:
      bar: baz</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="components-upgrade-controller-extensions-release-manifest">
<title>ReleaseManifest</title>
<para>升级控制器引入了一个名为 <literal>ReleaseManifest</literal> 的新 Kubernetes <link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>。</para>
<para><literal>ReleaseManifest</literal> 资源由升级控制器创建，保存<emphasis
role="strong">一个</emphasis>特定 Edge 版本的组件数据。这意味着每个 Edge 版本升级都将由不同的
<literal>ReleaseManifest</literal> 资源表示。</para>
<warning>
<para>ReleaseManifest 只能由升级控制器创建。</para>
<para>不建议手动创建或编辑 <literal>ReleaseManifest</literal> 资源，否则，用户需<emphasis
role="strong">自行承担风险</emphasis>。</para>
</warning>
<para>版本清单附带的组件数据包括但不限于：</para>
<itemizedlist>
<listitem>
<para>操作系统数据 - 版本、支持的体系结构、其他升级数据等</para>
</listitem>
<listitem>
<para>Kubernetes 发行版数据 - <link xl:href="https://docs.rke2.io">RKE2</link>/<link
xl:href="https://k3s.io">K3s</link> 支持的版本</para>
</listitem>
<listitem>
<para>其他组件数据 - SUSE Helm chart 数据（位置、版本、名称等）</para>
</listitem>
</itemizedlist>
<para>有关版本清单的示例，请参见<link
xl:href="https://github.com/suse-edge/upgrade-controller/blob/main/config/samples/lifecycle_v1alpha1_releasemanifest.yaml">上游</link>文档。<emphasis>请注意，这只是一个示例，不可用于创建为有效的
<literal>ReleaseManifest</literal> 资源。</emphasis></para>
</section>
</section>
<section xml:id="components-upgrade-controller-how-track">
<title>跟踪升级过程</title>
<para>本节介绍如何跟踪和调试升级控制器在用户创建 <literal>UpgradePlan</literal> 资源后启动的升级资源过程。</para>
<section xml:id="components-upgrade-controller-how-track-general">
<title>一般信息</title>
<para>有关升级过程状态的一般信息，可以在升级计划的状态条件中查看。</para>
<para>可以通过以下方式查看升级计划资源的状态：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get upgradeplan &lt;upgradeplan_name&gt; -n upgrade-controller-system -o yaml</screen>
<formalpara>
<title>升级计划运行示例：</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Control plane nodes are being upgraded
    reason: InProgress
    status: "False"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Kubernetes upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Rancher upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Longhorn upgrade is not yet started
    reason: Pending
    status: Unknown
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: MetalLB upgrade is not yet started
    reason: Pending
    status: Unknown
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: CDI upgrade is not yet started
    reason: Pending
    status: Unknown
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: KubeVirt upgrade is not yet started
    reason: Pending
    status: Unknown
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: NeuVector upgrade is not yet started
    reason: Pending
    status: Unknown
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: EndpointCopierOperator upgrade is not yet started
    reason: Pending
    status: Unknown
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Elemental upgrade is not yet started
    reason: Pending
    status: Unknown
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: SRIOV upgrade is not yet started
    reason: Pending
    status: Unknown
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Akri upgrade is not yet started
    reason: Pending
    status: Unknown
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: Metal3 upgrade is not yet started
    reason: Pending
    status: Unknown
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:26:27Z"
    message: RancherTurtles upgrade is not yet started
    reason: Pending
    status: Unknown
    type: RancherTurtlesUpgraded
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
<para>通过这种方式，您可以查看升级控制器将尝试安排升级的每个组件。每个条件都采用以下模板：</para>
<itemizedlist>
<listitem>
<para><literal>lastTransitionTime</literal> - 此组件条件上一次改变状态的时间。</para>
</listitem>
<listitem>
<para><literal>message</literal> - 指示特定组件条件当前升级状态的消息。</para>
</listitem>
<listitem>
<para><literal>reason</literal> - 特定组件条件的当前升级状态。可能的 <literal>reason</literal> 包括：</para>
<itemizedlist>
<listitem>
<para><literal>Succeeded</literal> - 特定组件升级成功。</para>
</listitem>
<listitem>
<para><literal>Failed</literal> - 特定组件升级失败。</para>
</listitem>
<listitem>
<para><literal>InProgress</literal> - 特定组件的升级当前正在进行中。</para>
</listitem>
<listitem>
<para><literal>Pending</literal> - 尚未安排特定组件的升级。</para>
</listitem>
<listitem>
<para><literal>Skipped</literal> - 在群集上找不到特定组件，因此将跳过其升级。</para>
</listitem>
<listitem>
<para><literal>Error</literal> - 特定组件发生了临时错误。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>status</literal> - 当前条件 <literal>type</literal> 的状态，可以是
<literal>True</literal>、<literal>False</literal> 或
<literal>Unknown</literal>。</para>
</listitem>
<listitem>
<para><literal>type</literal> - 当前已升级组件的指示器。</para>
</listitem>
</itemizedlist>
<para>升级控制器会为 <literal>OSUpgraded</literal> 和
<literal>KubernetesUpgraded</literal> 类型的组件条件创建 SUC 计划。要进一步跟踪为这些组件创建的 SUC
计划，请参见<xref linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<para>要进一步跟踪所有其他组件条件类型，可以查看 <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>
为它们创建的资源。有关详细信息，请参见<xref
linkend="components-upgrade-controller-how-track-helm"/>。</para>
<para>当满足以下条件时，升级控制器安排的升级计划可以标记为 <literal>successful</literal>：</para>
<orderedlist numeration="arabic">
<listitem>
<para>不存在 <literal>Pending</literal> 或 <literal>InProgress</literal> 的组件条件。</para>
</listitem>
<listitem>
<para><literal>lastSuccessfulReleaseVersion</literal> 属性指向升级计划配置中指定的
<literal>releaseVersion</literal>。<emphasis>升级过程成功后，升级控制器会将此属性添加至升级计划的状态。</emphasis></para>
</listitem>
</orderedlist>
<formalpara>
<title>成功的 <literal>UpgradePlan</literal> 示例：</title>
<para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  namespace: upgrade-controller-system
spec:
  releaseVersion: 3.4
status:
  conditions:
  - lastTransitionTime: "2024-10-01T06:26:48Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: OSUpgraded
  - lastTransitionTime: "2024-10-01T06:26:59Z"
    message: All cluster nodes are upgraded
    reason: Succeeded
    status: "True"
    type: KubernetesUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart rancher upgrade succeeded
    reason: Succeeded
    status: "True"
    type: RancherUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart longhorn is not installed
    reason: Skipped
    status: "False"
    type: LonghornUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Specified version of chart metallb is already installed
    reason: Skipped
    status: "False"
    type: MetalLBUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart cdi is not installed
    reason: Skipped
    status: "False"
    type: CDIUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart kubevirt is not installed
    reason: Skipped
    status: "False"
    type: KubeVirtUpgraded
  - lastTransitionTime: "2024-10-01T06:27:13Z"
    message: Chart neuvector-crd is not installed
    reason: Skipped
    status: "False"
    type: NeuVectorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Specified version of chart endpoint-copier-operator is already installed
    reason: Skipped
    status: "False"
    type: EndpointCopierOperatorUpgraded
  - lastTransitionTime: "2024-10-01T06:27:14Z"
    message: Chart elemental-operator upgrade succeeded
    reason: Succeeded
    status: "True"
    type: ElementalUpgraded
  - lastTransitionTime: "2024-10-01T06:27:15Z"
    message: Chart sriov-crd is not installed
    reason: Skipped
    status: "False"
    type: SRIOVUpgraded
  - lastTransitionTime: "2024-10-01T06:27:16Z"
    message: Chart akri is not installed
    reason: Skipped
    status: "False"
    type: AkriUpgraded
  - lastTransitionTime: "2024-10-01T06:27:19Z"
    message: Chart metal3 is not installed
    reason: Skipped
    status: "False"
    type: Metal3Upgraded
  - lastTransitionTime: "2024-10-01T06:27:27Z"
    message: Chart rancher-turtles is not installed
    reason: Skipped
    status: "False"
    type: RancherTurtlesUpgraded
  lastSuccessfulReleaseVersion: 3.4
  observedGeneration: 1
  sucNameSuffix: 90315a2b6d</screen>
</para>
</formalpara>
</section>
<section xml:id="components-upgrade-controller-how-track-helm">
<title>Helm 控制器</title>
<para>本节介绍如何跟踪 <link
xl:href="https://github.com/k3s-io/helm-controller/">helm-controller</link>
创建的资源。</para>
<note>
<para>以下步骤假设 <literal>kubectl</literal> 已配置为连接到已部署升级控制器的群集。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>找到特定组件的 <literal>HelmChart</literal> 资源：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmcharts -n kube-system</screen>
</listitem>
<listitem>
<para>使用 <literal>HelmChart</literal> 资源的名称找到由 <literal>helm-controller</literal>
创建的升级 Pod：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -l helmcharts.helm.cattle.io/chart=&lt;helmchart_name&gt; -n kube-system

# Example for Rancher
kubectl get pods -l helmcharts.helm.cattle.io/chart=rancher -n kube-system
NAME                         READY   STATUS      RESTARTS   AGE
helm-install-rancher-tv9wn   0/1     Completed   0          16m</screen>
</listitem>
<listitem>
<para>查看特定组件 Pod 的日志：</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;pod_name&gt; -n kube-system</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="components-upgrade-controller-known-issues">
<title>已知限制</title>
<itemizedlist>
<listitem>
<para>下游群集的升级目前还不受升级控制器管理。有关如何升级下游群集的信息，请参见<xref
linkend="day2-downstream-clusters"/>。</para>
</listitem>
<listitem>
<para>升级控制器希望通过 EIB（<xref linkend="components-eib"/>）部署的任何其他 SUSE Edge Helm chart
将其 <link xl:href="https://docs.rke2.io/helm#using-the-helm-crd">HelmChart
CR</link> 部署在 <literal>kube-system</literal> 名称空间中。为此，请在 EIB 定义文件中配置
<literal>installationNamespace</literal> 属性。有关详细信息，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">上游</link>文档。</para>
</listitem>
<listitem>
<para>目前，升级控制器无法确定管理群集上当前运行的 Edge 版本。请确保提供的 Edge 版本高于群集上当前运行的 Edge 版本。</para>
</listitem>
<listitem>
<para>目前，升级控制器仅支持<emphasis role="strong">非隔离</emphasis>环境升级，还不支持<emphasis
role="strong">隔离式</emphasis>升级。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-suma">
<title>SUSE Multi-Linux Manager</title>
<para>SUSE Edge 中包含 SUSE Multi-Linux Manager，其作用是提供自动化与控制能力，确保边缘部署的所有节点上作为底层操作系统的
SUSE Linux Micro 始终保持最新状态。</para>
<para>有关详细信息，请参见<xref linkend="quickstart-suma"/>和 <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUSE
Multi-Linux Manager 文档</link>。</para>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>操作指南</title>
<partintro>
<para>操作指南和最佳实践</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>K3s 上的 MetalLB（使用第 2 层模式）</title>
<para>MetalLB 是使用标准路由协议的裸机 Kubernetes 群集的负载平衡器实现。</para>
<para>本指南将展示如何以第 2 层 (L2) 模式部署 MetalLB。</para>
<section xml:id="id-why-use-metallb">
<title>为何使用 MetalLB</title>
<para>由于以下原因，MetalLB 成为了用来实现裸机 Kubernetes 群集负载平衡的较佳选择：</para>
<orderedlist numeration="arabic">
<listitem>
<para>与 Kubernetes 本机集成：MetalLB 可无缝与 Kubernetes 集成，让用户可使用熟悉的 Kubernetes
工具和方法轻松进行部署和管理。</para>
</listitem>
<listitem>
<para>裸机兼容性：与基于云的负载平衡器不同，MetalLB 专门用于本地部署，在这种部署中，可能无法使用传统负载平衡器或使用传统负载平衡器不现实。</para>
</listitem>
<listitem>
<para>支持多种协议：MetalLB 支持 Layer 2 和 BGP（边界网关协议）模式，提供了很大的灵活性，适合不同的网络架构并可满足不同的要求。</para>
</listitem>
<listitem>
<para>高可用性：MetalLB 可以将负载平衡负担分散在多个节点中，从而确保服务的高可用性和可靠性。</para>
</listitem>
<listitem>
<para>可伸缩性：MetalLB 可以处理大规模部署，并且能够将 Kubernetes 群集扩容来满足日益增长的需求。</para>
</listitem>
</orderedlist>
<para>在第 2 层模式下，一个节点负责向本地网络播发服务。从网络的角度看，似乎为该计算机的网络接口分配了多个 IP 地址。</para>
<para>第 2 层模式的主要优势在于其通用性：它可以在任何以太网上正常工作，不需要任何特殊硬件，甚至不需要各种形式的路由器。</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>K3s 上的 MetalLB（使用 L2）</title>
<para>本快速入门将使用 L2 模式。这意味着我们不需要任何特殊网络设备，只需使用网络范围内的三个空闲 IP。</para>
</section>
<section xml:id="id-prerequisites-7">
<title>先决条件</title>
<itemizedlist>
<listitem>
<para>要部署 MetalLB 的 K3s 群集。</para>
</listitem>
</itemizedlist>
<warning>
<para>K3S 自身附带服务负载平衡器（名为 Klipper）。需要<link
xl:href="https://metallb.universe.tf/configuration/k3s/">禁用 Klipper 才能运行
MetalLB</link>。要禁用 Klipper，需使用 <literal>--disable=servicelb</literal> 标志安装
K3s。</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>网络范围内的三个空闲 IP 地址。在本示例中为 <literal>192.168.122.10-192.168.122.12</literal></para>
</listitem>
</itemizedlist>
<important>
<para>必须确保这三个 IP 地址未被分配。在 DHCP 环境中，为了避免出现双重分配情况，不得将这些地址添加到 DHCP 池中。</para>
</important>
</section>
<section xml:id="id-deployment-2">
<title>部署</title>
<para>我们将使用作为 SUSE Edge 解决方案一部分发布的 MetalLB Helm chart：</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>配置</title>
<para>安装现已完成。接下来请使用示例值进行<link
xl:href="https://metallb.universe.tf/configuration/">配置</link>：</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>现在，MetalLB 可供您使用。可以自定义 L2 模式的许多设置，例如：</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">IPv6
和双栈服务</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">控制自动地址分配</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">将地址分配范围缩小为特定的名称空间和服务</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">限制可从中声明服务的节点集</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">指定可从中声明
LB IP 的网络接口</link></para>
</listitem>
</itemizedlist>
<para>还可以对 <link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>
进行其他许多自定义设置。</para>
<section xml:id="traefik-and-metallb">
<title>Traefik 和 MetalLB</title>
<para>默认情况下，Traefik 会随 K3s 一起部署（可以使用 <literal>--disable=traefik</literal> <link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">禁用
Traefik</link>），并作为 <literal>LoadBalancer</literal> 公开（与 Klipper
一起使用）。但是，由于需要禁用 Klipper，用于入口的 Traefik 服务仍是 <literal>LoadBalancer</literal>
类型。因此在部署 MetalLB 的那一刻，第一个 IP 将自动分配给 Traefik 入口。</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>我们将在稍后的过程（<xref linkend="ingress-with-metallb"/>）中应用此操作。</para>
</section>
</section>
<section xml:id="id-usage">
<title>用法</title>
<para>我们来创建示例部署：</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>最后创建服务：</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>我们来看看此示例的实际效果：</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<section xml:id="ingress-with-metallb">
<title>MetalLB 的入口</title>
<para>由于 Traefik 已用作入口控制器，我们可以通过 <literal>Ingress</literal> 对象公开任何 HTTP / HTTPS
流量，例如：</para>
<screen language="bash" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>然后运行：</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>校验 MetalLB 是否正常工作：</para>
<screen language="bash" linenumbering="unnumbered">% arping hellok3s.${IP}.sslip.io

ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>在以上示例中，流量的流动方式如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal> 解析为实际 IP。</para>
</listitem>
<listitem>
<para>然后，流量由 <literal>metallb-speaker</literal> Pod 处理。</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal> 将流量重定向到 <literal>traefik</literal> 控制器。</para>
</listitem>
<listitem>
<para>最后，Traefik 将请求转发到 <literal>hello-kubernetes</literal> 服务。</para>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="guides-metallb-k3s-l3">
<title>K3s 上的 MetalLB（使用第 3 层模式）</title>
<para>MetalLB 是使用标准路由协议的裸机 Kubernetes 群集的负载平衡器实现。</para>
<para>本指南将演示如何在第 3 层 (L3) BGP 模式下部署 MetalLB。</para>
<section xml:id="id-why-use-metallb-2">
<title>为何使用 MetalLB</title>
<para>由于以下原因，MetalLB 成为了用来实现裸机 Kubernetes 群集负载平衡的较佳选择：</para>
<orderedlist numeration="arabic">
<listitem>
<para>与 Kubernetes 本机集成：MetalLB 可无缝与 Kubernetes 集成，让用户可使用熟悉的 Kubernetes
工具和方法轻松进行部署和管理。</para>
</listitem>
<listitem>
<para>裸机兼容性：与基于云的负载平衡器不同，MetalLB 专门用于本地部署，在这种部署中，可能无法使用传统负载平衡器或使用传统负载平衡器不现实。</para>
</listitem>
<listitem>
<para>支持多种协议：MetalLB 支持第 2 层和第 3 层 BGP（边界网关协议）模式，提供了很大的灵活性，适合不同的网络架构并可满足不同的要求。</para>
</listitem>
<listitem>
<para>高可用性：MetalLB 可以将负载平衡负担分散在多个节点中，从而确保服务的高可用性和可靠性。</para>
</listitem>
<listitem>
<para>可伸缩性：MetalLB 可以处理大规模部署，并且能够将 Kubernetes 群集扩容来满足日益增长的需求。</para>
</listitem>
</orderedlist>
<para>在第 2 层模式下，一个节点负责向本地网络播发服务。从网络的角度看，似乎为该计算机的网络接口分配了多个 IP 地址。</para>
<para>第 2 层模式的主要优势在于其通用性：它可以在任何以太网上正常工作，不需要任何特殊硬件，甚至不需要各种形式的路由器。</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l3">
<title>K3s 上的 MetalLB（使用 L3）</title>
<para>本快速入门中使用 L3 模式，这意味着网络范围内需要有支持 BGP 功能的相邻路由器。</para>
</section>
<section xml:id="id-prerequisites-8">
<title>先决条件</title>
<itemizedlist>
<listitem>
<para>要部署 MetalLB 的 K3s 群集。</para>
</listitem>
<listitem>
<para>网络中支持 BGP 协议的路由器。</para>
</listitem>
<listitem>
<para>网络范围内用于服务的空闲 IP 地址，本示例中为 <literal>192.168.10.100</literal></para>
</listitem>
</itemizedlist>
<important>
<para>您必须确保此 IP 地址未被分配。在 DHCP 环境中，该地址不得属于 DHCP 池，以避免重复分配。</para>
</important>
</section>
<section xml:id="id-configuration-to-advertise-service-ip-addresses">
<title>配置服务 IP 地址通告</title>
<para>默认情况下，BGP 会向所有已配置的对等方通告服务 IP 地址。这些对等方（通常是路由器）会收到每个服务 IP 地址的路由，且网络掩码为 32
位。在本示例中，我们将使用与群集处于同一网络的基于 FRR 的路由器，然后利用 MetalLB 的 BGP 功能向该基于 FRR 的路由器通告服务。</para>
</section>
<section xml:id="id-deployment-3">
<title>部署</title>
<para>我们将使用作为 SUSE Edge 解决方案一部分发布的 MetalLB Helm chart：</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration-2">
<title>配置</title>
<orderedlist numeration="arabic">
<listitem>
<para>至此，安装已完成。接下来创建 <literal>IPAddressPool</literal>：</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: bgp-pool
  namespace: metallb-system
  labels:
    app: httpd
spec:
  addresses:
  - 192.168.10.100/32
  autoAssign: true
  avoidBuggyIPs: false
  serviceAllocation:
    namespaces:
    - metallb-system
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - key: serviceType
        operator: In
        values:
        - httpd
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>配置 <literal>BGPPeer</literal>。</para>
</listitem>
</orderedlist>
<note>
<para>FRR 路由器的 ASN 为 1000，而我们的 <literal>BGPPeer</literal> 的 ASN 为 1001。您还可看到 FRR
路由器的 IP 地址为 192.168.3.140。</para>
</note>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: mypeertest
spec:
  peerAddress: 192.168.3.140
  peerASN: 1000
  myASN: 1001
  routerID: 4.4.4.4
EOF</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>创建 BGPAdvertisement（L3 模式）：</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-test
  namespace: metallb-system
spec:
  ipAddressPools:
  - bgp-pool
EOF</screen>
</section>
<section xml:id="id-usage-2">
<title>用法</title>
<orderedlist numeration="arabic">
<listitem>
<para>创建包含服务的示例应用程序。在本示例中，该服务将使用来自 <literal>IPAddressPool</literal> 的 IP 地址
<literal>192.168.10.100</literal>。</para>
</listitem>
</orderedlist>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment
  namespace: metallb-system
  labels:
    app: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      pod-label: httpd
  template:
    metadata:
      labels:
        pod-label: httpd
    spec:
      containers:
      - name: httpdcontainer
        image: image: docker.io/library/httpd:2.4
        ports:
          - containerPort: 80
            protocol: TCP
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: http-service
  namespace: metallb-system
  labels:
    serviceType: httpd
spec:
  selector:
    pod-label: httpd
  type: LoadBalancer
  ports:
  - protocol: TCP
    port: 8080
    name: 8080-tcp
    targetPort: 80
EOF</screen>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<para>要进行验证，可登录 FRR 路由器查看 BGP 通告生成的路由。</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered">42178089cba5# show ip bgp all

For address family: IPv4 Unicast
BGP table version is 3, local router ID is 2.2.2.2, vrf id 0
Default local pref 100, local AS 1000
Status codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
* i172.16.0.0/24    1.1.1.1                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
* i172.17.0.0/24    3.3.3.3                  0    100      0 i
*&gt;                  0.0.0.0                  0         32768 i
*= 192.168.10.100/32
                    192.168.3.162                          0 1001 i
*=                  192.168.3.163                          0 1001 i
*&gt;                  192.168.3.161                          0 1001 i

Displayed  3 routes and 7 total paths
kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s</screen>
<orderedlist numeration="arabic" startingnumber="3">
<listitem>
<para>如果该路由器是您网络的默认网关，可在网络中的某个设备上运行 <literal>curl</literal> 命令，验证是否能访问 httpd
示例应用程序。</para>
</listitem>
</orderedlist>
<screen language="console" linenumbering="unnumbered"># curl http://192.168.10.100:8080
&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
#</screen>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>Kubernetes API 服务器前面的 MetalLB</title>
<para>本指南将演示如何在具有三个控制平面节点的 HA 群集上，使用 MetalLB 服务将 RKE2/K3s API
向外部公开。要实现此目的，需要手动创建类型为 <literal>LoadBalancer</literal> 的 Kubernetes
服务。系统随后会自动创建 <literal>EndpointSlices</literal> 对象，该对象会保留群集中所有可用控制平面节点的
IP。为确保 EndpointSlices 能与群集中发生的事件（添加/去除节点或节点下线）保持实时同步，需部署 Endpoint Copier
Operator（<xref linkend="components-eco"/>）。该操作器会监控默认
<literal>kubernetes</literal> EndpointSlices 中的事件，并自动更新受管理的
EndpointSlices，以使两者保持同步。由于受管理的服务类型为 <literal>LoadBalancer</literal>，MetalLB
会为其分配静态 <literal>ExternalIP</literal>。此 <literal>ExternalIP</literal> 将用于与
API 服务器通信。</para>
<section xml:id="id-prerequisites-9">
<title>先决条件</title>
<itemizedlist>
<listitem>
<para>要在其上部署 RKE2/K3s 的三台主机。</para>
<itemizedlist>
<listitem>
<para>请确保这些主机的主机名不同。</para>
</listitem>
<listitem>
<para>对于测试目的，这些主机可以是虚拟机</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>网络中至少有 2 个可用 IP（一个用于 Traefik/Nginx，另一个用于受管服务）。</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-rke2k3s">
<title>安装 RKE2/K3s</title>
<note>
<para>如果您不想使用新群集，而要使用现有群集，请跳过此步骤并执行下一步。</para>
</note>
<para>首先，必须在网络中预留一个可用 IP，该 IP 稍后将用作受管服务的 <literal>ExternalIP</literal>。</para>
<para>通过 SSH 连接到第一台主机并以群集模式安装所需的发行版。</para>
<para>对于 RKE2，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC="server \
 --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" sh -

systemctl enable rke2-server.service
systemctl start rke2-server.service

# Fetch the cluster token:
RKE2_TOKEN=$(tr -d '\n' &lt; /var/lib/rancher/rke2/server/node-token)</screen>
<para>对于 K3s，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>确保在 <literal>k3s server</literal> 命令中提供
<literal>--disable=servicelb</literal> 标志。</para>
</note>
<important>
<para>从现在开始，应在本地计算机上运行命令。</para>
</important>
<para>要从外部访问 API 服务器，需使用 RKE2/K3s VM 的 IP。</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
export KUBE_DISTRIBUTION=&lt;k3s/rke2&gt;

scp ${NODE_IP}:/etc/rancher/${KUBE_DISTRIBUTION}/${KUBE_DISTRIBUTION}.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-cluster">
<title>配置现有群集</title>
<note>
<para>仅当您要使用现有的 RKE2/K3s 群集时，此步骤才有效。</para>
</note>
<para>要使用现有群集，应修改 <literal>tls-san</literal> 标志，此外还应针对 K3s 禁用
<literal>servicelb</literal> LB。</para>
<para>要更改 RKE2 或 K3s 服务器的标志，需要修改群集所有 VM 上的
<literal>/etc/systemd/system/rke2.service</literal> 或
<literal>/etc/systemd/system/k3s.service</literal> 文件，具体取决于发行版。</para>
<para>应在 <literal>ExecStart</literal> 中插入标志。例如：</para>
<para>对于 RKE2，使用以下命令：</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/rke2 \
    server \
        '--write-kubeconfig-mode=644' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>对于 K3s，使用以下命令：</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>然后应执行以下命令来加载新配置：</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart ${KUBE_DISTRIBUTION}</screen>
</section>
<section xml:id="id-installing-metallb">
<title>安装 MetalLB</title>
<para>要部署 <literal>MetalLB</literal>，可以使用 K3s 上的 MetalLB（<xref
linkend="guides-metallb-k3s"/>）指南。</para>
<para><emphasis role="strong">注意：</emphasis>请确保 <literal>VIP_SERVICE_IP</literal>
IP 地址与群集中现有的 <literal>IPAddressPools</literal> 不重叠。</para>
<para>创建单独的 <literal>IpAddressPool</literal> 和
<literal>L2Advertisement</literal>，仅用于受管理的服务。</para>
<para><emphasis role="strong">注意：</emphasis>下面的 IPAddressPool 将分配给
<literal>default</literal> 命名空间中类型为 <literal>LoadBalancer</literal>
的服务。如果该命名空间中存在多个 <literal>LoadBalancer</literal> 服务，可能需要配置额外的 <link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">ServiceSelectors</link>，以明确匹配此
VIP 服务。</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: kubernetes-vip-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>安装 Endpoint Copier Operator</title>
<screen language="bash" linenumbering="unnumbered">helm install \
endpoint-copier-operator oci://registry.suse.com/edge/charts/endpoint-copier-operator \
--namespace endpoint-copier-operator \
--create-namespace</screen>
<para>以上命令会部署包含两个复本的 <literal>endpoint-copier-operator</literal>
操作器部署。其中一个复本是领导者，另一个复本在需要时接管领导者角色。</para>
<para>现在应部署 <literal>kubernetes-vip</literal> 服务，这将由操作器来协调，另外需创建具有所配置端口和 IP 的
EndpointSlices。</para>
<para>对于 RKE2，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  type: LoadBalancer
EOF</screen>
<para>对于 K3s，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: https
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer
EOF</screen>
<para>校验 <literal>kubernetes-vip</literal> 服务是否使用正确的 IP 地址：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para>确保 <literal>default</literal> 名称空间中的 <literal>kubernetes-vip</literal> 和
<literal>kubernetes</literal> EndpointSlices 资源指向相同的 IP。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpointslices | grep kubernetes</screen>
<para>如果所有设置正确，剩下的最后一项操作就是在 <literal>Kubeconfig</literal> 中使用
<literal>VIP_SERVICE_IP</literal>。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>从现在开始，所有 <literal>kubectl</literal> 命令都将通过 <literal>kubernetes-vip</literal>
服务运行。</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>添加控制平面节点</title>
<para>要监控整个过程，可以打开另外两个终端选项卡。</para>
<para>第一个终端：</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>第二个终端：</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpointslices</screen>
<para>现在，在第二和第三个节点上执行以下命令。</para>
<para>对于 RKE2，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
systemctl enable rke2-server.service


mkdir -p /etc/rancher/rke2/
cat &lt;&lt;EOF &gt; /etc/rancher/rke2/config.yaml
server: https://${VIP_SERVICE_IP}:9345
token: ${RKE2_TOKEN}
EOF

systemctl start rke2-server.service</screen>
<para>对于 K3s，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>使用 Edge Image Builder 进行隔离式部署</title>
<section xml:id="id-intro">
<title>简介</title>
<para>本指南将介绍如何使用 Edge Image Builder (EIB)（<xref linkend="components-eib"/>）在 SUSE
Linux Micro 6.1 上以完全隔离的方式部署多个 SUSE Edge 组件。使用此方法可以引导至 EIB 所创建的随时可引导 (CRB)
的自定义映像，并在 RKE2 或 K3s
群集上部署指定的组件，而无需连接到互联网，也无需执行任何手动步骤。对于想要将部署所需的所有制品预先嵌入其操作系统映像的客户而言，此配置非常理想，这样就可以在引导时立即使用这些制品。</para>
<para>本指南将介绍以下组件的隔离式安装：</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-security"/></para>
</listitem>
<listitem>
<para><xref linkend="components-suse-storage"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIB 将分析并预先下载提供的 Helm chart 和 Kubernetes
清单中引用的所有映像。但是，其中一些操作可能会尝试提取容器映像并在运行时基于这些映像创建 Kubernetes
资源。在这种情况下，如果我们想要设置完全隔离的环境，则必须在定义文件中手动指定所需的映像。</para>
</warning>
</section>
<section xml:id="id-prerequisites-10">
<title>先决条件</title>
<para>我们假设本指南的读者已事先熟悉 EIB（<xref linkend="components-eib"/>）。如果您不熟悉，请阅读快速入门指南（<xref
linkend="quickstart-eib"/>），以便更好地理解下面实际操作中涉及的概念。</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Libvirt 网络配置</title>
<note>
<para>为了演示隔离式部署，本指南将使用模拟的 <literal>libvirt</literal>
隔离网络，并根据该网络定制以下配置。对于您自己的部署，可能需要修改下一步骤中将介绍的
<literal>host1.local.yaml</literal> 配置。</para>
</note>
<para>如果您要使用相同的 <literal>libvirt</literal> 网络配置，请继续阅读。否则请跳到<xref
linkend="config-dir-creation"/>。</para>
<para>我们来为 DHCP 创建 IP 地址范围为 <literal>192.168.100.2/24</literal> 的隔离网络配置：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>现在，唯一剩下的操作就是创建并启动网络：</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>基础目录配置</title>
<para>基础目录配置在所有组件中都相同，现在我们就设置此配置。</para>
<para>首先创建所需的子目录：</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>请确保将您要使用的任何基础映像添加到 <literal>base-images</literal> 目录中。本指南将重点介绍<link
xl:href="https://www.suse.com/download/sle-micro/">此处</link>提供的自安装 ISO 映像。</para>
<para>我们来复制已下载的映像：</para>
<screen language="shell" linenumbering="unnumbered">cp SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIB 永远不会修改基础映像输入。</para>
</note>
<para>我们来创建一个包含所需网络配置的文件：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>此配置确保置备的系统上存在以下设置（使用指定的 MAC 地址）：</para>
<itemizedlist>
<listitem>
<para>采用静态 IP 地址的以太网接口</para>
</listitem>
<listitem>
<para>路由</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>主机名 (<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>最终的文件结构现在应如下所示：</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>基础定义文件</title>
<para>Edge Image Builder 使用<emphasis>定义文件</emphasis>来修改 SUSE Linux Micro
映像。这些文件包含大部分可配置选项。其中的许多选项将在不同的组件部分中重复出现，因此下面列出并解释了这些选项。</para>
<tip>
<para>定义文件中自定义选项的完整列表可以在<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.1/docs/building-images.md#image-definition-file">上游文档</link>中找到</para>
</tip>
<para>我们来看看所有定义文件中的以下字段：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para><literal>image</literal> 部分是必需的，用于指定输入映像、输入映像的体系结构和类型，以及输出映像的名称。</para>
<para><literal>operatingSystem</literal> 部分是可选的，其中包含的配置允许用户通过
<literal>root/eib</literal> 用户名/口令登录到置备的系统。</para>
<para><literal>kubernetes</literal> 部分为可选配置，用于定义 Kubernetes 的类型和版本。本指南中我们将使用 RKE2
发行版。如果需要使用 K3s，则将配置改为 <literal>kubernetes.version:
v1.33.3+k3s1</literal>。除非通过 <literal>kubernetes.nodes</literal>
字段明确配置，否则本指南中引导的所有群集均为单节点群集。</para>
<para><literal>embeddedArtifactRegistry</literal> 部分包含仅在运行时为特定组件引用和提取的所有映像。</para>
</section>
<section xml:id="rancher-install">
<title>Rancher 安装</title>
<note>
<para>为便于演示，我们将大幅精简演示用的 Rancher（<xref
linkend="components-rancher"/>）部署。对于实际部署，可能需要根据您的配置添加其他制品。</para>
</note>
<para><link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">Rancher
2.12.1</link> 发行版资产中包含 <literal>rancher-images.txt</literal>
文件，该文件列出了隔离式安装所需的所有映像。</para>
<para>总共有超过 600 个容器映像，这意味着生成的 CRB 映像的大小约为 30GB。对于我们的 Rancher
安装，我们将精简该列表，使之与最小有效配置相当。您可以在该列表中重新添加部署所需的任何映像。</para>
<para>创建定义文件并在其中包含精简的映像列表：</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.15.3/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.18.2
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v8.0.0
    - name: registry.rancher.com/rancher/compliance-operator:v1.1.0
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.23-build20250612
    - name: registry.rancher.com/rancher/hardened-calico:v3.30.2-build20250711
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.26.0-build20250611
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.21-k3s1-build20250612
    - name: registry.rancher.com/rancher/hardened-flannel:v0.27.1-build20250710
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.33.3-rke2r1-build20250716
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-multus-dynamic-networks-controller:v0.3.7-build20250711
    - name: registry.rancher.com/rancher/hardened-multus-thick:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.9.1-build20250704
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.13
    - name: registry.rancher.com/rancher/kubectl:v1.33.1
    - name: registry.rancher.com/rancher/kuberlr-kubectl:v5.0.0
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.31
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher131
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.10.2
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.rancher.com/rancher/prom-prometheus:v3.2.1
    - name: registry.rancher.com/rancher/prometheus-federator:v4.1.0
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.5-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.5-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.12.1
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v7.0.0
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/remotedialer-proxy:v0.5.0
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.33.1-0.20250516163953-99d91538b132-build20250612
    - name: registry.rancher.com/rancher/rke2-runtime:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/scc-operator:v0.1.1
    - name: registry.rancher.com/rancher/security-scan:v0.7.1
    - name: registry.rancher.com/rancher/shell:v0.5.0
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.33.3-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.33.3-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.13-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.rancher.com/rancher/ui-plugin-catalog:4.0.3
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.0
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.1
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.2
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.5.3
    - name: registry.rancher.com/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.6.0</screen>
<para>与包含 600 多个容器映像的完整列表相比，此精简版本仅包含约 60 个容器映像，因此新 CRB 映像的大小只有大约 7GB。</para>
<para>我们还需要为 Rancher 创建 Helm values 文件：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para>将 <literal>systemDefaultRegistry</literal> 设置为
<literal>registry.rancher.com</literal> 可让 Rancher 在引导时，在 CRB
映像内启动的嵌入式制品仓库中自动查找映像。省略此字段可能会导致无法在节点上找到容器映像。</para>
</warning>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>输出应如下所示：</para>
<screen language="console" linenumbering="unnumbered">Downloading file: dl-manifest-1.yaml 100% |██████████████████████████████████████████████████████████████████████████████| (583/583 kB, 12 MB/s)
Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████| (56/56, 8 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% |███████████████████████████████████████████████████████████| (644/644 MB, 29 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% |█████████████████████████████████████████████████████████| (400/400 MB, 29 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% |███████████████████████████████████████████████████████████████████████████| (36/36 MB, 30 MB/s)
Downloading file: sha256sum-amd64.txt 100% |█████████████████████████████████████████████████████████████████████████████| (4.3/4.3 kB, 29 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>置备使用构建映像的节点后，可以校验 Rancher 安装：</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cattle-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>输出应类似于以下内容，这表明已成功部署所有组件：</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/helm-operation-6l6ld                        0/2     Completed   0          107s
pod/helm-operation-8tk2v                        0/2     Completed   0          2m2s
pod/helm-operation-blnrr                        0/2     Completed   0          2m49s
pod/helm-operation-hdcmt                        0/2     Completed   0          3m19s
pod/helm-operation-m74c7                        0/2     Completed   0          97s
pod/helm-operation-qzzr4                        0/2     Completed   0          2m30s
pod/helm-operation-s9jh5                        0/2     Completed   0          3m
pod/helm-operation-tq7ts                        0/2     Completed   0          2m41s
pod/rancher-99d599967-ftjkk                     1/1     Running     0          4m15s
pod/rancher-webhook-79798674c5-6w28t            1/1     Running     0          2m27s
pod/system-upgrade-controller-56696956b-trq5c   1/1     Running     0          104s

NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/rancher           ClusterIP   10.43.255.80   &lt;none&gt;        80/TCP,443/TCP   4m15s
service/rancher-webhook   ClusterIP   10.43.7.238    &lt;none&gt;        443/TCP          2m27s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rancher                     1/1     1            1           4m15s
deployment.apps/rancher-webhook             1/1     1            1           2m27s
deployment.apps/system-upgrade-controller   1/1     1            1           104s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/rancher-99d599967                     1         1         1       4m15s
replicaset.apps/rancher-webhook-79798674c5            1         1         1       2m27s
replicaset.apps/system-upgrade-controller-56696956b   1         1         1       104s</screen>
<para>当我们访问 <literal>https://192.168.100.50.sslip.io</literal> 并使用先前设置的
<literal>adminadminadmin</literal> 口令登录后，Rancher 仪表板即会显示：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width="100%"/>
</imageobject>
<textobject><phrase>隔离式 Rancher</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="suse-security-install">
<title>SUSE Security 安装</title>
<para>与 Rancher 安装不同，SUSE Security 安装不需要在 EIB 中进行任何特殊处理。EIB 将自动隔离底层组件 NeuVector
所需的每个映像。</para>
<para>创建定义文件：</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>另外，为 NeuVector 创建 Helm 值文件：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>输出应如下所示：</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████| (2/2, 4 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████| (5/5, 13 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>置备使用构建映像的节点后，可以校验 SUSE Security 安装：</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>输出应类似于以下内容，这表明已成功部署所有组件：</para>
<screen language="console" linenumbering="unnumbered">NAME                                            READY   STATUS      RESTARTS   AGE
pod/neuvector-cert-upgrader-job-bxbnz           0/1     Completed   0          3m39s
pod/neuvector-controller-pod-7d854bfdc7-nhxjf   1/1     Running     0          3m44s
pod/neuvector-enforcer-pod-ct8jm                1/1     Running     0          3m44s

NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.234.241   &lt;none&gt;        443/TCP                         3m44s
service/neuvector-svc-controller          ClusterIP   None            &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   3m44s
service/neuvector-svc-crd-webhook         ClusterIP   10.43.50.190    &lt;none&gt;        443/TCP                         3m44s

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          3m44s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           3m44s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-7d854bfdc7   1         1         1       3m44s

NAME                                        SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-cert-upgrader-pod   0 0 1 1 *   &lt;none&gt;     True      0        &lt;none&gt;          3m44s
cronjob.batch/neuvector-updater-pod         0 0 * * *   &lt;none&gt;     False     0        &lt;none&gt;          3m44s

NAME                                    STATUS     COMPLETIONS   DURATION   AGE
job.batch/neuvector-cert-upgrader-job   Complete   1/1           7s         3m39s</screen>
</section>
<section xml:id="suse-storage-install">
<title>SUSE Storage 安装</title>
<para>Longhorn <link
xl:href="https://longhorn.io/docs/1.9.1/deploy/install/airgap/">官方文档</link>中包含
<literal>longhorn-images.txt</literal> 文件，该文件列出了隔离式安装所需的所有映像。我们将在定义文件中包含来自
Rancher 容器仓库的相应镜像副本。请按以下方式创建定义文件：</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
  packages:
    sccRegistrationCode: [reg-code]
    packageList:
      - open-iscsi
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 107.0.0+up1.9.1
      - name: longhorn-crd
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        version: 107.0.0+up1.9.1
    repositories:
      - name: longhorn
        url: https://charts.rancher.io
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.52
    - name: registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1</screen>
<note>
<para>您会注意到，定义文件列出了 <literal>open-iscsi</literal> 软件包。该软件包是必需的组件，因为 Longhorn
依赖于不同节点上运行的 <literal>iscsiadm</literal> 守护程序来为 Kubernetes 提供持久性卷。</para>
</note>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>输出应如下所示：</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Pulling selected Helm charts... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 3 it/s)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 20956 it/s)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>置备使用构建映像的节点后，可以校验 Longhorn 安装：</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>输出应类似于以下内容，这表明已成功部署所有组件：</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS   AGE
pod/csi-attacher-787fd9c6c8-sf42d                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-tb82p                       1/1     Running   0          2m28s
pod/csi-attacher-787fd9c6c8-zhc6s                       1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-b2v9s                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-hwllt                    1/1     Running   0          2m28s
pod/csi-provisioner-74486b95c6-mlrpk                    1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-t54zk                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-vdt5d                        1/1     Running   0          2m28s
pod/csi-resizer-859d4557fd-x9kh4                        1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-r62gr                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-vrwjn                    1/1     Running   0          2m28s
pod/csi-snapshotter-6f69c6c8cc-z65nb                    1/1     Running   0          2m28s
pod/engine-image-ei-4623b511-9vhkb                      1/1     Running   0          3m13s
pod/instance-manager-6f95fd57d4a4cd0459e469d75a300552   1/1     Running   0          2m43s
pod/longhorn-csi-plugin-gx98x                           3/3     Running   0          2m28s
pod/longhorn-driver-deployer-55f9c88499-fbm6q           1/1     Running   0          3m28s
pod/longhorn-manager-dpdp7                              2/2     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-gg5hq                        1/1     Running   0          3m28s
pod/longhorn-ui-59c85fcf94-s49jc                        1/1     Running   0          3m28s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.77.89    &lt;none&gt;        9502/TCP   3m28s
service/longhorn-backend              ClusterIP   10.43.56.17    &lt;none&gt;        9500/TCP   3m28s
service/longhorn-conversion-webhook   ClusterIP   10.43.54.73    &lt;none&gt;        9501/TCP   3m28s
service/longhorn-frontend             ClusterIP   10.43.22.82    &lt;none&gt;        80/TCP     3m28s
service/longhorn-recovery-backend     ClusterIP   10.43.45.143   &lt;none&gt;        9503/TCP   3m28s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-4623b511   1         1         1       1            1           &lt;none&gt;          3m13s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          2m28s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          3m28s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           2m28s
deployment.apps/csi-provisioner            3/3     3            3           2m28s
deployment.apps/csi-resizer                3/3     3            3           2m28s
deployment.apps/csi-snapshotter            3/3     3            3           2m28s
deployment.apps/longhorn-driver-deployer   1/1     1            1           3m28s
deployment.apps/longhorn-ui                2/2     2            2           3m28s

NAME                                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-787fd9c6c8               3         3         3       2m28s
replicaset.apps/csi-provisioner-74486b95c6            3         3         3       2m28s
replicaset.apps/csi-resizer-859d4557fd                3         3         3       2m28s
replicaset.apps/csi-snapshotter-6f69c6c8cc            3         3         3       2m28s
replicaset.apps/longhorn-driver-deployer-55f9c88499   1         1         1       3m28s
replicaset.apps/longhorn-ui-59c85fcf94                2         2         2       3m28s</screen>
</section>
<section xml:id="kubevirt-install">
<title>KubeVirt 和 CDI 安装</title>
<para>KubeVirt 和 CDI 的 Helm chart
只会安装各自的操作器。系统的其余组件将由操作器来部署，这意味着，我们必须在定义文件中包含所有必要的容器映像。我们来创建定义文件：</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: kubevirt
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi
        repositoryName: suse-edge
        version: 304.0.1+up0.6.0
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge/charts
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0-150700.9.3.1
    - name: registry.suse.com/suse/sles/15.7/virt-api:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-controller:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-handler:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2-150700.3.5.2
    - name: registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2-150700.3.5.2</screen>
<para>我们来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-iso-definition.yaml</screen>
<para>输出应如下所示：</para>
<screen language="console" linenumbering="unnumbered">Pulling selected Helm charts... 100% |███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| (2/2, 48 it/min)
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Os Files ..................... [SKIPPED]
Systemd ...................... [SKIPPED]
Fips ......................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% |██████████████████████████████████████████████████████████████████████████████████████████████████████████| (15/15, 4 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Cleanup ...................... [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Build complete, the image can be found at: eib-image.iso</screen>
<para>置备使用构建映像的节点后，可以校验 KubeVirt 和 CDI 的安装。</para>
<para>校验 KubeVirt：</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>输出应类似于以下内容，这表明已成功部署所有组件：</para>
<screen language="console" linenumbering="unnumbered">NAME                                  READY   STATUS    RESTARTS   AGE
pod/virt-api-59cb997648-mmt67         1/1     Running   0          2m34s
pod/virt-controller-69786b785-7cc96   1/1     Running   0          2m8s
pod/virt-controller-69786b785-wq2dz   1/1     Running   0          2m8s
pod/virt-handler-2l4dm                1/1     Running   0          2m8s
pod/virt-operator-7c444cff46-nps4l    1/1     Running   0          3m1s
pod/virt-operator-7c444cff46-r25xq    1/1     Running   0          3m1s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.167.109   &lt;none&gt;        443/TCP   2m36s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m36s
service/virt-api                      ClusterIP   10.43.18.202    &lt;none&gt;        443/TCP   2m36s
service/virt-exportproxy              ClusterIP   10.43.142.188   &lt;none&gt;        443/TCP   2m36s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   2m8s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m34s
deployment.apps/virt-controller   2/2     2            2           2m8s
deployment.apps/virt-operator     2/2     2            2           3m1s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-59cb997648         1         1         1       2m34s
replicaset.apps/virt-controller-69786b785   2         2         2       2m8s
replicaset.apps/virt-operator-7c444cff46    2         2         2       3m1s

NAME                            AGE    PHASE
kubevirt.kubevirt.io/kubevirt   3m1s   Deployed</screen>
<para>校验 CDI：</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>输出应类似于以下内容，这表明已成功部署所有组件：</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-5598c9bf47-pqfxw     1/1     Running   0          3m44s
pod/cdi-deployment-7cbc5db7f8-g46z7    1/1     Running   0          3m44s
pod/cdi-operator-777c865745-2qcnj      1/1     Running   0          3m48s
pod/cdi-uploadproxy-646f4cd7f7-fzkv7   1/1     Running   0          3m44s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.2.224    &lt;none&gt;        443/TCP    3m44s
service/cdi-prometheus-metrics   ClusterIP   10.43.237.13   &lt;none&gt;        8080/TCP   3m44s
service/cdi-uploadproxy          ClusterIP   10.43.114.91   &lt;none&gt;        443/TCP    3m44s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m44s
deployment.apps/cdi-deployment    1/1     1            1           3m44s
deployment.apps/cdi-operator      1/1     1            1           3m48s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m44s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-5598c9bf47     1         1         1       3m44s
replicaset.apps/cdi-deployment-7cbc5db7f8    1         1         1       3m44s
replicaset.apps/cdi-operator-777c865745      1         1         1       3m48s
replicaset.apps/cdi-uploadproxy-646f4cd7f7   1         1         1       3m44s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>查错</title>
<para>如果您在构建映像时遇到任何问题，或者想要进一步测试和调试该过程，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.1/docs">上游文档</link>。</para>
</section>
</chapter>
<chapter xml:id="guides-kiwi-builder-images">
<title>使用 Kiwi 构建更新的 SUSE Linux Micro 映像</title>
<para>本章将说明如何生成更新的 SUSE Linux Micro 映像，这些映像可用于 Edge Image Builder、Cluster API
(CAPI) +
Metal<superscript>3</superscript>，或直接将磁盘映像写入块设备。此流程适用于需要在初始系统引导映像中包含最新补丁（以减少安装后的补丁传输量）的场景，或使用
CAPI 的场景，在该场景中，更适合使用新映像重新安装操作系统，而非就地升级主机。</para>
<para>该流程借助 <link xl:href="https://osinside.github.io/kiwi/">Kiwi</link>
来执行映像构建。SUSE Edge 随附容器化版本，其中内置了实用辅助工具，可简化整体流程，并支持指定所需的目标<emphasis
role="strong">配置文件</emphasis>。配置文件定义了所需输出映像的类型，常见类型如下：</para>
<itemizedlist>
<listitem>
<para>“<emphasis role="strong">Base</emphasis>”- SUSE Linux Micro
磁盘映像，包含精简的软件包集合（含 podman）。</para>
</listitem>
<listitem>
<para>“<emphasis role="strong">Base-SelfInstall</emphasis>”- 基于上述“Base”的自安装映像。</para>
</listitem>
<listitem>
<para>“<emphasis role="strong">Base-RT</emphasis>”- 与上述“Base”相同，但使用实时 (rt) 内核。</para>
</listitem>
<listitem>
<para>“<emphasis role="strong">Base-RT-SelfInstall</emphasis>”-
基于上述“Base-RT”的自安装映像。</para>
</listitem>
<listitem>
<para>“<emphasis role="strong">Default</emphasis>”- 基于上述“Base”的 SUSE Linux Micro
磁盘映像，另外还包含其他工具，如虚拟化堆栈、Cockpit 和 salt-minion。</para>
</listitem>
<listitem>
<para>“<emphasis role="strong">Default-SelfInstall</emphasis>”-
基于上述“Default”的自安装映像。</para>
</listitem>
</itemizedlist>
<para>有关详细信息，请参见 <link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-deployment-images/index.html#alp-images-installer-type">SUSE
Linux Micro 6.1</link> 文档。</para>
<para>此过程适用于 AMD64/Intel 64 和 AArch64 两种体系结构，但并非所有映像配置文件都支持这两种体系结构。例如，在使用 SUSE
Linux Micro 6.1 的 SUSE Edge 3.4 中，目前暂不提供适用于 AArch64
体系结构的实时内核配置文件（即“Base-RT”或“Base-RT-SelfInstall”）。</para>
<note>
<para>构建主机的体系结构必须与待构建映像的体系结构一致。也就是说，要构建 AArch64 体系结构的映像，必须使用 AArch64
体系结构的构建主机；AMD64/Intel 64 体系结构同样如此 - 目前不支持跨体系结构构建。</para>
</note>
<section xml:id="id-prerequisites-11">
<title>先决条件</title>
<para>Kiwi 映像构建器的要求如下：</para>
<itemizedlist>
<listitem>
<para>一台体系结构与待构建映像体系结构相同的 SUSE Linux Micro 6.1 主机（即“构建系统”）。</para>
</listitem>
<listitem>
<para>构建系统需已通过 <literal>SUSEConnect</literal> 完成注册（注册是为了从 SUSE 储存库提取最新软件包）。</para>
</listitem>
<listitem>
<para>能够连接互联网以提取所需软件包；如果通过代理连接，需预先配置构建主机的代理设置。</para>
</listitem>
<listitem>
<para>构建主机上需禁用 SELinux（因为 SELinux 标签会在容器内生成，可能与主机策略冲突）。</para>
</listitem>
<listitem>
<para>至少需要 10GB 可用磁盘空间，以容纳容器映像、构建根目录及最终生成的输出映像。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started-2">
<title>入门指南</title>
<para>由于存在某些限制，目前需要禁用 SELinux。请连接到 SUSE Linux Micro 6.1 映像构建主机，并确保 SELinux 已禁用：</para>
<screen language="console" linenumbering="unnumbered"># setenforce 0</screen>
<para>创建一个将与 Kiwi 构建容器共享的输出目录，用于保存生成的映像：</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/output</screen>
<para>从 SUSE 仓库提取最新的 Kiwi 构建器映像：</para>
<screen language="console" linenumbering="unnumbered"># podman pull registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
(...)</screen>
</section>
<section xml:id="id-building-the-default-image">
<title>构建默认映像</title>
<para>如果运行容器映像时未提供任何参数，这就是 Kiwi 映像容器的默认行为。以下命令运行 <literal>podman</literal>
时会将两个目录映射到容器：</para>
<itemizedlist>
<listitem>
<para>底层主机的 <literal>/etc/zypp/repos.d</literal> SUSE Linux Micro 软件包储存库目录。</para>
</listitem>
<listitem>
<para>上文创建的输出目录 <literal>~/output</literal>。</para>
</listitem>
</itemizedlist>
<para>Kiwi 映像容器需要按如下方式运行 <literal>build-image</literal> 辅助脚本：</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<note>
<para>如果您是首次运行此脚本，脚本预计会在启动后不久<emphasis role="strong">失败</emphasis>，并显示“<emphasis
role="strong">ERROR: Early loop device test failed, please retry the
container
run.</emphasis>”，这是由于底层主机系统上创建的循环设备无法立即在容器映像内可见所致。只需重新运行该命令，即可顺利执行。</para>
</note>
<para>几分钟后，即可在本地输出目录中找到映像：</para>
<screen language="console" linenumbering="unnumbered">(...)
INFO: Image build successful, generated images are available in the 'output' directory.

# ls -1 output/
SLE-Micro.x86_64-6.1.changes
SLE-Micro.x86_64-6.1.packages
SLE-Micro.x86_64-6.1.raw
SLE-Micro.x86_64-6.1.verified
build
kiwi.result
kiwi.result.json</screen>
</section>
<section xml:id="id-building-images-with-other-profiles">
<title>使用其他配置文件构建映像</title>
<para>要构建不同的映像配置文件，需要使用 Kiwi 容器映像辅助脚本中的“<emphasis
role="strong">-p</emphasis>”命令选项。例如，要构建“<emphasis
role="strong">Default-SelfInstall</emphasis>”ISO 映像，请运行：</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall
(...)</screen>
<note>
<para>如果 <literal>output</literal> 目录中存在映像，Kiwi 将拒绝运行，以免数据丢失。继续操作前，需要使用
<literal>rm -f output/*</literal> 命令去除输出目录中的内容。</para>
</note>
<para>或者，要构建包含实时内核（“<emphasis role="strong">kernel-rt</emphasis>”）的自安装 ISO 映像，请运行：</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Base-RT-SelfInstall
(...)</screen>
</section>
<section xml:id="id-building-images-with-large-sector-sizes">
<title>构建大扇区大小的映像</title>
<para>有些硬件要求映像采用大扇区大小（即 <emphasis role="strong">4096 字节</emphasis>，而非标准的 512
字节）。容器化 Kiwi 构建器支持通过指定“<emphasis
role="strong">-b</emphasis>”参数生成大块大小的映像。例如，要构建大扇区大小的“<emphasis
role="strong">Default-SelfInstall</emphasis>”映像，请运行：</para>
<screen language="console" linenumbering="unnumbered"># podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image -p Default-SelfInstall -b
(...)</screen>
</section>
<section xml:id="id-using-a-custom-kiwi-image-definition-file">
<title>创建自定义 Kiwi 映像定义文件</title>
<para>对于高级使用场景，可以使用自定义 Kiwi 映像定义文件 (<literal>SL-Micro.kiwi</literal>)
以及任何必要的构建后脚本。这需要覆盖 SUSE Edge 团队预先打包的默认定义。</para>
<para>创建一个新目录，并将其映射到容器映像中辅助脚本将查找定义文件的目录 (<literal>/micro-sdk/defs</literal>)：</para>
<screen language="console" linenumbering="unnumbered"># mkdir ~/mydefs/
# cp /path/to/SL-Micro.kiwi ~/mydefs/
# cp /path/to/config.sh ~/mydefs/
# podman run --privileged -v /etc/zypp/repos.d:/micro-sdk/repos/ -v ~/output:/tmp/output -v ~/mydefs/:/micro-sdk/defs/ \
    -it registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0 build-image
(...)</screen>
<warning>
<para>此操作仅适用于高级使用场景，可能会导致可支持性问题。请联系您的 SUSE 代表以获取进一步建议和指导。</para>
</warning>
<para>要获取容器中包含的默认 Kiwi 映像定义文件，可使用以下命令：</para>
<screen language="console" linenumbering="unnumbered">$ podman create --name kiwi-builder registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi .
$ podman cp kiwi-builder:/micro-sdk/defs/SL-Micro.kiwi.4096 .
$ podman rm kiwi-builder
$ ls ./SL-Micro.*
(...)</screen>
</section>
</chapter>
<chapter xml:id="guides-clusterclass-example">
<title>使用 clusterclass 部署下游群集</title>
<section xml:id="id-introduction">
<title>简介</title>
<para>置备 Kubernetes
群集是一项复杂的任务，需要具备深入的群集组件配置专业知识。随着配置变得愈发复杂，或者不同提供商的需求引入了大量提供商专用的资源定义，群集创建可能会让人望而生畏。值得庆幸的是，Kubernetes
Cluster API (CAPI) 提供了一种更简洁的声明式方法，而 ClusterClass
进一步增强了这种方法。此功能引入了一种基于模板的模型，允许您定义可重用的群集类，该类封装了复杂性并提升了一致性。</para>
</section>
<section xml:id="id-what-is-clusterclass">
<title>什么是 ClusterClass？</title>
<para>CAPI 项目引入了 ClusterClass 功能，通过采用基于模板的群集实例化方法，实现了 Kubernetes
群集生命周期管理的范式转变。用户无需为每个群集独立定义资源，而是定义一个 ClusterClass，将它作为一个全面且可重用的蓝图。这种抽象表示封装了
Kubernetes
群集的期望状态和配置，让您能快速、一致地创建多个符合已定义规范的群集。这种抽象化减轻了配置负担，使部署清单更易于管理。这意味着工作负载群集的核心组件在类级别定义，这样用户便可将这些模板用作
Kubernetes 群集版本，可多次重用以置备群集。ClusterClass 的实现提供了多项关键优势，解决了传统 CAPI 大规模管理所固有的挑战：</para>
<itemizedlist>
<listitem>
<para>大幅降低复杂性和 YAML 冗余度</para>
</listitem>
<listitem>
<para>优化维护和更新过程</para>
</listitem>
<listitem>
<para>增强部署间的一致性和标准化</para>
</listitem>
<listitem>
<para>提高可扩展性和自动化能力</para>
</listitem>
<listitem>
<para>声明式管理和强大的版本控制</para>
</listitem>
</itemizedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="clusterclass.png" width="100%"/>
</imageobject>
<textobject><phrase>clusterclass</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-example-of-current-capi-provisioning-file">
<title>当前 CAPI 置备文件示例</title>
<para>利用 Cluster API (CAPI) 和 RKE2 提供程序部署 Kubernetes
群集需要定义多个自定义资源。这些资源定义了群集及其底层基础架构的期望状态，使 CAPI
能够编排置备和管理生命周期。下面的代码段说明了必须配置的资源类型：</para>
<itemizedlist>
<listitem>
<para><emphasis
role="strong">群集</emphasis>：此资源封装了总体配置，包括将管理节点间通信和服务发现的网络拓扑。此外，它还建立了与控制平面规范和指定的基础架构提供程序资源的必要关联，从而告知
CAPI 期望的群集体系结构及将用于置备群集的底层基础架构。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3Cluster</emphasis>：此资源定义 Metal3 特有的基础架构级属性，例如
Kubernetes API 服务器可通过其访问的外部端点。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">RKE2ControlPlane</emphasis>：此资源定义群集控制平面节点的特性和行为。此规范中配置了期望的控制平面复本数量（对确保高可用性和容错能力至关重要）、特定的
Kubernetes 发行版版本（与所选的 RKE2 版本一致），以及控制平面组件的滚动更新策略等参数。此外，此资源规定了将在群集中采用的容器网络接口
(CNI)，便于注入特定代理的配置，通常利用 Ignition 来无缝自动完成控制平面节点的 RKE2 代理置备。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3MachineTemplate</emphasis>：此资源充当将构成 Kubernetes
群集工作节点的各计算实例的创建蓝图，定义了要使用的映像。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3DataTemplate</emphasis>：此资源作为
Metal3MachineTemplate 的补充，可用于为新置备的计算机实例指定其他元数据。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: emea-spa-cluster-3
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: emea-spa-cluster-3
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  controlPlaneEndpoint:
    host: 192.168.122.203
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: emea-spa-cluster-3
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  registrationMethod: "control-plane-endpoint"
  registrationAddress: 192.168.122.203
  serverConfig:
    cni: cilium
    cniMultusEnable: true
    tlsSan:
      - 192.168.122.203
      - https://192.168.122.203.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - 192.168.122.203/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          node: group-3
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
</section>
<section xml:id="id-transforming-the-capi-provisioning-file-to-clusterclass">
<title>将 CAPI 置备文件转换为 ClusterClass</title>
<section xml:id="id-clusterclass-definition">
<title>ClusterClass 定义</title>
<para>以下代码定义了一个 ClusterClass 资源，这是一个用于采用一致方式部署特定 Kubernetes
群集类型的声明式模板。该规范包含通用的基础架构和控制平面配置，可实现群集 Fleet 的高效置备和统一生命周期管理。下面的 clusterclass
示例中包含一些变量，这些变量将在群集实例化过程中被实际值替换。示例中使用的变量如下：</para>
<itemizedlist>
<listitem>
<para><literal>controlPlaneMachineTemplate</literal>：这是用于定义要使用的控制平面计算机模板引用的名称</para>
</listitem>
<listitem>
<para><literal>controlPlaneEndpointHost</literal>：这是控制平面端点的主机名或 IP 地址</para>
</listitem>
<listitem>
<para><literal>tlsSan</literal>：这是控制平面端点的 TLS 主题备用名称</para>
</listitem>
</itemizedlist>
<para>clusterclass 定义文件基于以下 3 种资源定义：</para>
<itemizedlist>
<listitem>
<para><emphasis
role="strong">ClusterClass</emphasis>：此资源封装了整个群集类定义，包括控制平面和基础架构模板。此外，它还包含在实例化过程中将被替换的变量列表。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">RKE2ControlPlaneTemplate</emphasis>：此资源定义控制平面模板，指定期望的控制平面节点配置。它包含复本数量、Kubernetes
版本、要使用的 CNI 等参数。此外，一些参数在实例化过程中将被替换为适当的值。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">Metal3ClusterTemplate</emphasis>：此资源定义基础架构模板，指定期望的底层基础架构配置。它包含控制平面端点、noCloudProvider
标志等参数。此外，一些参数在实例化过程中将被替换为适当的值。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlaneTemplate
metadata:
  name: example-controlplane-type2
  namespace: emea-spa
spec:
  template:
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: example-controlplane    # This will be replaced by the patch applied in each cluster instances
        namespace: emea-spa
      replicas: 1
      version: v1.33.3+rke2r1
      rolloutStrategy:
        type: "RollingUpdate"
        rollingUpdate:
          maxSurge: 1
      registrationMethod: "control-plane-endpoint"
      registrationAddress: "default"  # This will be replaced by the patch applied in each cluster instances
      serverConfig:
        cni: cilium
        cniMultusEnable: true
        tlsSan:
          - "default"  # This will be replaced by the patch applied in each cluster instances
      agentConfig:
        format: ignition
        additionalUserData:
          config: |
            default
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "localhost.localdomain"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3ClusterTemplate
metadata:
  name: example-cluster-template-type2
  namespace: emea-spa
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: "default"  # This will be replaced by the patch applied in each cluster instances
        port: 6443
      noCloudProvider: true
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: example-clusterclass-type2
  namespace: emea-spa
spec:
  variables:
    - name: controlPlaneMachineTemplate
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: controlPlaneEndpointHost
      required: true
      schema:
        openAPIV3Schema:
          type: string
    - name: tlsSan
      required: true
      schema:
        openAPIV3Schema:
          type: array
          items:
            type: string
  infrastructure:
    ref:
      kind: Metal3ClusterTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: example-cluster-template-type2
  controlPlane:
    ref:
      kind: RKE2ControlPlaneTemplate
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      name: example-controlplane-type2
  patches:
    - name: setControlPlaneMachineTemplate
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/infrastructureRef/name"
              valueFrom:
                variable: controlPlaneMachineTemplate
    - name: setControlPlaneEndpoint
      definitions:
        - selector:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: Metal3ClusterTemplate
            matchResources:
              infrastructureCluster: true  # Added to select InfraCluster
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/controlPlaneEndpoint/host"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setRegistrationAddress
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/registrationAddress"
              valueFrom:
                variable: controlPlaneEndpointHost
    - name: setTlsSan
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true  # Added to select ControlPlane
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/serverConfig/tlsSan"
              valueFrom:
                variable: tlsSan
    - name: updateAdditionalUserData
      definitions:
        - selector:
            apiVersion: controlplane.cluster.x-k8s.io/v1beta1
            kind: RKE2ControlPlaneTemplate
            matchResources:
              controlPlane: true
          jsonPatches:
            - op: replace
              path: "/spec/template/spec/agentConfig/additionalUserData"
              valueFrom:
                template: |
                  config: |
                    variant: fcos
                    version: 1.4.0
                    storage:
                      files:
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: endpoint-copier-operator
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                                targetNamespace: endpoint-copier-operator
                                version: 304.0.1+up0.3.0
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: helm.cattle.io/v1
                              kind: HelmChart
                              metadata:
                                name: metallb
                                namespace: kube-system
                              spec:
                                chart: oci://registry.suse.com/edge/charts/metallb
                                targetNamespace: metallb-system
                                version: 304.0.0+up0.14.9
                                createNamespace: true
                        - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: metallb.io/v1beta1
                              kind: IPAddressPool
                              metadata:
                                name: kubernetes-vip-ip-pool
                                namespace: metallb-system
                              spec:
                                addresses:
                                  - {{ .controlPlaneEndpointHost }}/32
                                serviceAllocation:
                                  priority: 100
                                  namespaces:
                                    - default
                                  serviceSelectors:
                                    - matchExpressions:
                                      - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                              ---
                              apiVersion: metallb.io/v1beta1
                              kind: L2Advertisement
                              metadata:
                                name: ip-pool-l2-adv
                                namespace: metallb-system
                              spec:
                                ipAddressPools:
                                  - kubernetes-vip-ip-pool
                        - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
                          overwrite: true
                          contents:
                            inline: |
                              apiVersion: v1
                              kind: Service
                              metadata:
                                name: kubernetes-vip
                                namespace: default
                                labels:
                                  serviceType: kubernetes-vip
                              spec:
                                ports:
                                - name: rke2-api
                                  port: 9345
                                  protocol: TCP
                                  targetPort: 9345
                                - name: k8s-api
                                  port: 6443
                                  protocol: TCP
                                  targetPort: 6443
                                type: LoadBalancer
                    systemd:
                      units:
                        - name: rke2-preinstall.service
                          enabled: true
                          contents: |
                            [Unit]
                            Description=rke2-preinstall
                            Wants=network-online.target
                            Before=rke2-install.service
                            ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                            [Service]
                            Type=oneshot
                            User=root
                            ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                            ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                            ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                            ExecStartPost=/bin/sh -c "umount /mnt"
                            [Install]
                            WantedBy=multi-user.target</screen>
</section>
<section xml:id="id-cluster-instance-definition">
<title>群集实例定义</title>
<para>在 ClusterClass 的概念中，群集实例指的是基于定义的 ClusterClass 创建且已实例化的特定运行中群集。它代表一个具体的部署，基于
ClusterClass 中指定的蓝图直接衍生而成，具有独特的配置、资源和运行状态。这包括正在运行的一组特定计算机、网络配置以及相关的
Kubernetes 组件。要管理使用 ClusterClass
框架置备的特定已部署群集的生命周期、执行升级、进行扩容操作和实施监控，理解群集实例至关重要。</para>
<para>要定义一个群集实例，需要定义以下资源：</para>
<itemizedlist>
<listitem>
<para>群集</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate</para>
</listitem>
<listitem>
<para>Metal3DataTemplate</para>
</listitem>
</itemizedlist>
<para>之前在模板（clusterclass 定义文件）中定义的变量将被替换为该群集实例化后的最终值：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  topology:
    class: example-clusterclass-type2  # Correct way to reference ClusterClass
    version: v1.33.3+rke2r1
    controlPlane:
      replicas: 1
    variables:                         # Variables to be replaced for this cluster instance
      - name: controlPlaneMachineTemplate
        value: emea-spa-cluster-3-machinetemplate
      - name: controlPlaneEndpointHost
        value: 192.168.122.203
      - name: tlsSan
        value:
          - 192.168.122.203
          - https://192.168.122.203.sslip.io
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: emea-spa-cluster-3-machinetemplate
  namespace: emea-spa
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: emea-spa-cluster-3
      hostSelector:
        matchLabels:
          cluster-role: control-plane
          deploy-region: emea-spa
          cluster-type: type2
      image:
        checksum: http://fileserver.local:8080/eibimage-downstream-cluster.raw.sha256
        checksumType: sha256
        format: raw
        url: http://fileserver.local:8080/eibimage-downstream-cluster.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: emea-spa-cluster-3
  namespace: emea-spa
spec:
  clusterName: emea-spa-cluster-3
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine</screen>
<para>这种方法使整个过程更加精简，一旦定义了 clusterclass，只需定义 3 种资源即可部署一个群集。</para>
</section>
</section>
</chapter>
</part>
<part xml:id="tips-and-tricks">
<title>提示和技巧</title>
<partintro>
<para>关于 Edge 组件的提示和技巧</para>
</partintro>
<chapter xml:id="tips-edge-image-builder">
<title>Edge Image Builder</title>
<section xml:id="id-common">
<title>通用</title>
<itemizedlist>
<listitem>
<para>如果您处于非 Linux 环境中，并且按照这些说明来构建映像，那么您可能是通过虚拟机运行 <literal>Podman</literal>
的。默认情况下配置的此类虚拟机只会分配到少量系统资源，这可能会在 <literal>Edge Image Builder</literal>
执行资源密集型操作（如 RPM 解析过程）时产生不稳定因素。您需要调整 Podman 计算机的资源，可通过 Podman
Desktop（设置齿轮图标→Podman 计算机编辑图标）或直接使用 <literal>podman-machine-set</literal>
<link
xl:href="https://docs.podman.io/en/stable/markdown/podman-machine-set.1.html">命令</link>来完成。</para>
</listitem>
<listitem>
<para>目前，<literal>Edge Image Builder</literal>
无法在跨体系结构环境中构建映像，也就是说，您必须在适当系统中运行该工具来构建相应映像：</para>
<itemizedlist>
<listitem>
<para>在 AArch64 系统（如 Apple Silicon） 中构建 SL Micro <literal>aarch64</literal> 映像</para>
</listitem>
<listitem>
<para>在 AMD64/Intel 64 系统中构建 SL Micro <literal>x86_64</literal>映像。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-kubernetes">
<title>Kubernetes</title>
<itemizedlist>
<listitem>
<para>如要创建多节点 Kubernetes 群集，则需要对定义文件中的 <literal>kubernetes</literal> 部分进行以下调整：</para>
<itemizedlist>
<listitem>
<para>在 <literal>kubernetes.nodes</literal> 下列出所有服务器节点和代理节点</para>
</listitem>
<listitem>
<para>在 <literal>kubernetes.network.apiVIP</literal> 下设置一个虚拟 IP
地址，供所有非初始化节点加入群集时使用</para>
</listitem>
<listitem>
<para>（可选）在 <literal>kubernetes.network.apiHost</literal> 下设置一个 API
主机，以指定用于访问群集的域名地址。要了解有关此配置的详细信息，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">介绍
Kubernetes 各部分的文档</link>。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>Edge Image Builder</literal> 通过不同节点的主机名来确定它们的 Kubernetes
类型（<literal>服务器</literal>或<literal>代理</literal>）。虽然此配置在定义文件中进行管理，但对于计算机的一般网络设置，我们可以使用<xref
linkend="components-nmc"/>中所述的 DHCP 配置。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="tips-elemental">
<title>Elemental</title>
<section xml:id="id-common-2">
<title>通用</title>
<section xml:id="id-expose-rancher-service">
<title>公开 Rancher 服务</title>
<para>使用 RKE2 或 K3s 时，我们需要通过管理群集公开服务（此处指 Rancher），因为它们默认不会公开。RKE2 中通过 NGINX
入口控制器来实现此目的，K3s 使用的则是 Traefik。当前的工作流程建议使用 MetalLB 来公布服务（通过 L2 或 BGP
通告），并使用相应的入口控制器通过 <literal>HelmChartConfig</literal>
创建入口，因为创建新的入口对象时会覆盖现有的设置。</para>
<orderedlist numeration="arabic">
<listitem>
<para>安装 Rancher Prime（通过 Helm）并配置必要的值</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-192.168.64.101.sslip.io
replicas: 1
bootstrapPassword: Admin
global.cattle.psp.enabled: "false"</screen>
<tip>
<para>有关详细信息，请参见 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster">Rancher
安装</link>文档。</para>
</tip>
</listitem>
<listitem>
<para>创建 LoadBalancer 服务以公开 Rancher</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local
EOF</screen>
</listitem>
<listitem>
<para>使用我们之前在 Helm 值中设置的 IP 地址为该服务创建 IP 地址池</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.64.101/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}
EOF</screen>
</listitem>
<listitem>
<para>为该 IP 地址池创建 L2 通告</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ingress-ippool
EOF</screen>
</listitem>
<listitem>
<para>确保 Elemental 已正确安装</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>在管理节点上安装 Elemental Operator 和 Elemental UI</para>
</listitem>
<listitem>
<para>在下游节点上添加 Elemental 配置以及注册代码，这样 Edge Image Builder 将会为计算机提供远程注册选项。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<tip>
<para>有关更多信息和示例，请参见<xref linkend="install-elemental"/>和<xref
linkend="configure-elemental"/>。</para>
</tip>
</section>
</section>
<section xml:id="id-hardware-specific">
<title>硬件特定配置</title>
<section xml:id="id-trusted-platform-module">
<title>可信平台模块</title>
<para>必须妥善处理<link xl:href="https://elemental.docs.rancher.com/tpm/">可信平台模块</link>
(TPM) 配置，否则将导致如下所示的错误：</para>
<screen language="console" linenumbering="unnumbered">Nov 25 18:17:06 eled elemental-register[4038]: Error: registering machine: cannot generate authentication token: opening tpm for getting attestation data: TPM device not available</screen>
<para>可通过以下方法之一缓解此问题：</para>
<itemizedlist>
<listitem>
<para>在虚拟机设置中启用 TPM</para>
</listitem>
</itemizedlist>
<para><emphasis>MacOS 上的 UTM 示例</emphasis></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="tpm.png" width="100%"/> </imageobject>
<textobject><phrase>TPM</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<para>通过在 <literal>MachineRegistration</literal> 资源中为 TPM 种子使用负值来模拟 TPM</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: true
        emulated-tpm-seed: -1</screen>
<itemizedlist>
<listitem>
<para>在 <literal>MachineRegistration</literal> 资源中禁用 TPM</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ...
  namespace: ...
spec:
    ...
    elemental:
      ...
      registration:
        emulate-tpm: false</screen>
</section>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>第三方集成</title>
<partintro>
<para>如何集成第三方工具</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link xl:href="https://nats.io/">NATS</link>
是为日益发展的超级互联世界而开发的连接技术。仅凭这一项技术，应用程序就能在云供应商、本地、边缘、Web 和移动设备的任意组合之间安全地通信。NATS
由一系列开源产品组成，这些产品紧密集成，但可以轻松独立部署。NATS 已由全球数千家公司使用，涵盖微服务、边缘计算、移动通信和 IoT
等使用场景，并可用于增强或取代传统的消息传递方式。</para>
<section xml:id="id-architecture">
<title>体系结构</title>
<para>NATS 是能够在应用程序之间以消息形式实现数据交换的基础架构。</para>
<section xml:id="id-nats-client-applications">
<title>NATS 客户端应用程序</title>
<para>应用程序可以使用 NATS
客户端库在不同的实例之间发布和订阅消息，以及发出请求和做出答复。这些应用程序通常称作<literal>客户端应用程序</literal>。</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>NATS 服务基础架构</title>
<para>NATS 服务由一个或多个 NATS 服务器进程提供，这些进程配置为彼此互连，提供了 NATS 服务基础架构。NATS
服务基础架构可以从一个终端设备上运行的单个 NATS
服务器进程，扩展为由许多群集组成的全球公用超级群集，这些群集跨越所有主要云提供商服务和全球所有区域。</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>简单的消息传递设计</title>
<para>NATS
使应用程序能够通过发送和接收消息来轻松进行通信。这些消息按照主题字符串进行寻址和标识，并且不依赖于网络位置。数据经过编码，并构造为由发布者发送的消息。该消息由一个或多个订阅者接收、解码和处理。</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATS 内置了一套称为 JetStream 的分布式持久化系统，旨在解决当今流媒体技术存在的诸多问题 —
复杂性高、稳定性差以及可伸缩性不足。JetStream 还能解决发布者和订阅者之间的耦合问题（即订阅者必须处于正常运行状态，才能接收发布的消息）。有关
NATS JetStream 的详细信息，请参见<link
xl:href="https://docs.nats.io/nats-concepts/jetstream">此处</link>。</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>安装</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>在 K3s 上安装 NATS</title>
<para>NATS 是为多种体系结构构建的，因此可以在 K3s 上轻松安装。（<xref linkend="components-k3s"/>）</para>
<para>我们创建一个值文件来重写 NATS 的默认值。</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>现在我们需要通过 Helm 安装 NATS：</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>在上面创建的 <literal>values.yaml</literal> 文件中，需将以下组件放在 <literal>nats</literal>
名称空间中：</para>
<orderedlist numeration="arabic">
<listitem>
<para>HA 版本的 NATS 有状态副本集，其中包含三个容器：NATS 服务器、配置重载器和指标分支。</para>
</listitem>
<listitem>
<para>NATS 箱容器，其中附带一组可用于校验设置的 <literal>NATS</literal> 实用程序。</para>
</listitem>
<listitem>
<para>JetStream 还会利用其键值后端，该后端附带与 Pod 绑定的 <literal>PVC</literal>。</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>测试设置</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>为测试主题创建订阅：</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>向测试主题发送消息：</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>清理</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>NATS 用作 K3s 的后端</title>
<para>K3s 利用的一个组件是 <link
xl:href="https://github.com/k3s-io/kine">KINE</link>，它是一种适配层，能够用最初面向关系数据库的其他存储后端替代
etcd。由于 JetStream 提供了键值对 API，因此可以将 NATS 用作 K3s 群集的后端。</para>
<para>有一个已经合并的 PR 可以直接将内置的 NATS 包含在 K3s 中，但这项更改仍<link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">未包含</link>在
K3s 版本中。</para>
<para>出于此原因，应该手动构建 K3s 二进制文件。</para>
<section xml:id="id-building-k3s">
<title>构建 K3s</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>以下命令会在构建标记中添加 <literal>nats</literal>，以在 K3s 中启用 NATS 内置功能：</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>请将 &lt;node-ip&gt; 替换为启动 K3s 的节点的实际 IP：</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>在本地构建 K3s 需要 buildx Docker CLI 插件。如果 <literal>$ make local</literal>
失败，可以<link
xl:href="https://github.com/docker/buildx#manual-download">手动安装</link>该插件。</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>安装 NATS CLI</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>运行用作 K3s 后端的 NATS</title>
<para>我们需要在节点上通过 <literal>ssh</literal> 进行连接，并使用指向 <literal>nats</literal> 的
<literal>--datastore-endpoint</literal> 标志运行 K3s。</para>
<note>
<para>以下命令将 K3s 作为前台进程启动，因此您可以轻松地通过日志来查看是否出现了任何问题。为了不阻碍当前终端，可以在该命令的前面添加
<literal>&amp;</literal> 标志，以将其作为后台进程启动。</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>为了将使用 NATS 后端的 K3s 服务器永久保留在您的 <literal>slemicro</literal> VM
上，可以运行以下脚本，以创建包含所需配置的 <literal>systemd</literal> 服务。</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>查错</title>
<para>可以在节点上运行以下命令来校验流的所有功能是否正常运行：</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-suse-linux-micro">
<title>SUSE Linux Micro 上的 NVIDIA GPU</title>
<section xml:id="id-intro-2">
<title>简介</title>
<para>本指南将演示如何通过预构建的<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">开源驱动程序</link>在
SUSE Linux Micro 6.1 上实现主机级别的 NVIDIA GPU 支持。这些驱动程序是内置在操作系统中的，而非由 NVIDIA 的
<link xl:href="https://github.com/NVIDIA/gpu-operator">GPU Operator</link>
动态加载。对于希望将部署所需的所有制品预先嵌入映像，且不需要动态选择驱动程序版本（即用户无需通过 Kubernetes
选择驱动程序版本）的客户而言，这种配置极具吸引力。本指南首先介绍如何将其他组件部署到已预先部署的系统，然后在一个章节中介绍如何通过 Edge Image
Builder 将此配置嵌入到初始部署中。如果您不想了解基础知识并想要手动完成设置，请直接跳到该章节。</para>
<para>需要特别说明的是，这些驱动程序的支持服务由 SUSE 和 NVIDIA 紧密合作提供，SUSE
负责驱动程序的构建，并将其作为软件包储存库的一部分分发。不过，如果您在驱动程序的使用组合方面有任何疑问或顾虑，请咨询您的 SUSE 或 NVIDIA
客户经理以获得进一步的帮助。如果您打算使用 <link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE)，请确保使用的是<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">经过
NVAIE 认证的 GPU</link>，这<emphasis>可能</emphasis>需要搭配使用 NVIDIA
的专有驱动程序。如果对此不确定，请咨询您的 NVIDIA 代表。</para>
<para>本指南<emphasis>不会</emphasis>介绍有关 NVIDIA GPU Operator 集成的更多信息。虽然其中不会介绍如何为
Kubernetes 集成 NVIDIA GPU Operator，但您仍然可以按照本指南中的大部分步骤来设置底层操作系统，并通过 NVIDIA GPU
Operator Helm chart 中的 <literal>driver.enabled=false</literal> 标志来让 GPU
Operator 使用<emphasis>预安装的</emphasis>驱动程序，在这种情况下，GPU Operator
会直接选择主机上安装的驱动程序。NVIDIA 在<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">此处</link>提供了更详细的说明。</para>
</section>
<section xml:id="id-prerequisites-12">
<title>先决条件</title>
<para>如果您要学习本指南，事先需要做好以下准备：</para>
<itemizedlist>
<listitem>
<para>至少一台装有 SUSE Linux Micro 6.1 的主机，可以是物理主机，也可以是虚拟机。</para>
</listitem>
<listitem>
<para>您的主机已附加到某个订阅，只有这样，才能访问软件包 — 可在<link
xl:href="https://www.suse.com/download/sle-micro/">此处</link>进行评估。</para>
</listitem>
<listitem>
<para>已安装<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">兼容的
NVIDIA GPU</link>（或已<emphasis>完全</emphasis>直通到运行 SUSE Linux Micro 的虚拟机）。</para>
</listitem>
<listitem>
<para>root 用户访问权限 — 本章中的说明假设您是 root 用户，而<emphasis>未</emphasis>通过
<literal>sudo</literal> 提升特权。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>手动安装</title>
<para>本节介绍如何直接将 NVIDIA 驱动程序安装到 SUSE Linux Micro 操作系统上，因为 NVIDIA 开放驱动程序现在包含在核心 SUSE
Linux Micro 软件包储存库中，因此，只需安装所需的 RPM 软件包就能安装这些驱动程序。无需编译或下载可执行软件包。下面介绍如何部署支持最新
GPU 的第六代 (G06) 驱动程序（有关更多信息，请参见<link
xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">此处</link>），请选择适合您系统中的
NVIDIA GPU 的驱动程序代系。对于新式 GPU，“G06”驱动程序是最常见的选择。</para>
<para>在开始之前，需要明确一点，除了 SUSE 在 SUSE Linux Micro 中随附的 NVIDIA 开放驱动程序之外，您的配置可能还需要其他
NVIDIA 组件。这些组件可能包括 OpenGL 库、CUDA 工具包、命令行实用程序（例如
<literal>nvidia-smi</literal>）和容器集成组件（例如
<literal>nvidia-container-toolkit</literal>）。其中许多组件并非由 SUSE 提供，因为它们属于 NVIDIA
的专有软件，或者由我们而非 NVIDIA
来提供这些组件并不合理。因此，在本指南的操作步骤中，我们将配置额外的储存库以便获取上述组件，并通过一些示例说明如何使用这些工具，最终搭建出一个功能完整的系统。需要注意区分
SUSE 储存库和 NVIDIA 储存库，因为两者提供的软件包版本偶尔会出现不匹配的情况。这种情况通常发生在 SUSE
发布了新版本的开放驱动程序后，NVIDIA 储存库可能需要几天时间才能提供与之匹配的对应版本软件包。</para>
<para>我们建议您采取以下措施来确保所选驱动程序版本与您的 GPU 兼容并符合现有的任何 CUDA 要求：</para>
<itemizedlist>
<listitem>
<para>查看 <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">CUDA
发行说明</link></para>
</listitem>
<listitem>
<para>检查您要部署的驱动程序版本是否在 <link
xl:href="https://download.nvidia.com/suse/sle15sp6/x86_64/">NVIDIA
储存库</link>中有匹配的版本，并确保支持组件有可用的对应软件包版本</para>
</listitem>
</itemizedlist>
<tip>
<para>要查找 NVIDIA 开放驱动程序版本，请在目标计算机上运行 <literal>zypper se -s
nvidia-open-driver</literal>，<emphasis>或者</emphasis>在 SUSE Customer Center
上<link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.1&amp;arch=x86_64">适用于
AMD64/Intel64 的 SUSE Linux Micro 6.1</link> 中搜索“nvidia-open-driver”。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width="100%"/>
</imageobject>
<textobject><phrase>SUSE Customer Center</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>在确认 NVIDIA 储存库中提供了对应版本后，便可以在主机操作系统上安装软件包了。为此，需要打开
<literal>transactional-update</literal>
会话，它会创建底层操作系统的新读/写快照，以便我们可以对不可变平台进行更改（有关
<literal>transactional-update</literal> 的更多说明，请参见<link
xl:href="https://documentation.suse.com/sle-micro/6.1/html/Micro-transactional-updates/transactional-updates.html">此处</link>）：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>在进入 <literal>transactional-update</literal> 外壳后，添加来自 NVIDIA
的其他软件包储存库。这样就可以提取其他实用程序，例如 <literal>nvidia-smi</literal>：</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-suse-main
zypper --gpg-auto-import-keys refresh</screen>
<para>然后，可以安装驱动程序，并安装 <literal>nvidia-compute-utils</literal>
来获取其他实用程序。如果您不需要这些实用程序，可以省略其安装步骤，但为了稍后进行测试，最好现在就安装它们：</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>如果安装失败，可能表明所选驱动程序版本与 NVIDIA
在其储存库中提供的版本之间存在依赖项不匹配情况。请参见上一节来校验您的版本是否匹配。尝试安装不同的驱动程序版本。例如，如果 NVIDIA
储存库中的版本较低，您可以尝试在 install 命令中指定
<literal>nvidia-open-driver-G06-signed-kmp=550.54.14</literal>，以指定一致的版本。</para>
</note>
<para>接下来，如果您使用的 GPU <emphasis>不</emphasis>在支持列表中（支持列表可参考<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">此处</link>），可以尝试通过在模块层面启用支持来测试驱动程序是否能正常工作，但结果可能因设备而异
— 如果您使用的是<emphasis>支持的</emphasis> GPU，请跳过此步骤：</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>安装这些软件包后，请退出 <literal>transactional-update</literal> 会话：</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>在继续之前，请确保已退出 <literal>transactional-update</literal> 会话。</para>
</note>
<para>安装驱动程序后，接下来请重引导系统。由于 SUSE Linux Micro
是不可变的操作系统，因此它需要重引导至您在上一步骤中创建的新快照。驱动程序只会安装到此新快照中，因此如果系统不重引导至此新快照（会自动重引导），就无法加载驱动程序。准备就绪后，发出
reboot 命令：</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>系统成功重引导后，请重新登录并使用 <literal>nvidia-smi</literal>
工具校验驱动程序是否已成功加载，以及它是否可以访问和枚举您的 GPU：</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>此命令应显示如下所示的输出，请注意，以下示例中显示了两个 GPU：</para>
<screen language="shell" linenumbering="unnumbered">+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>在 SUSE Linux Micro 系统上安装和校验 NVIDIA 驱动程序的过程到此结束。</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>进一步验证手动安装</title>
<para>在此阶段，我们只能确认的是，在主机级别，可以访问 NVIDIA
设备，并且驱动程序可以成功加载。但是，如果我们想要确保设备正常运行，可以通过一项简单测试来验证 GPU
是否可以从用户空间应用程序接收指令，最好是通过容器和 CUDA 库接收，因为实际工作负载通常使用这种方法。为此，我们可以通过安装
<literal>nvidia-container-toolkit</literal> (<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container Toolkit</link>) 来进一步修改主机操作系统。首先，打开另一个
<literal>transactional-update</literal>
外壳，请注意，在上一步骤中我们可能是通过单个事务执行此操作，后面的章节将介绍如何完全自动地执行此操作：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>接下来，安装来自 NVIDIA Container Toolkit 储存库的
<literal>nvidia-container-toolkit</literal> 软件包：</para>
<itemizedlist>
<listitem>
<para>下面的 <literal>nvidia-container-toolkit.repo</literal> 包含稳定储存库
(<literal>nvidia-container-toolkit</literal>) 和实验性储存库
(<literal>nvidia-container-toolkit-experimental</literal>)。对于生产用途，建议使用稳定储存库。默认会禁用实验性储存库。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>准备就绪后，可以退出 <literal>transactional-update</literal> 外壳：</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>... 然后将计算机重引导至新快照：</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>如前文所述，需确保已退出 <literal>transactional-update</literal> 外壳并重引导计算机，使更改生效。</para>
</note>
<para>重引导计算机后，可以校验系统是否可以使用 NVIDIA Container Toolkit 成功枚举设备。输出应该非常详细，包含 INFO 和 WARN
消息，但不包含 ERROR 消息：</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>这可确保计算机上启动的任何容器都可以采用已发现的 NVIDIA GPU 设备。准备就绪后，可以运行基于 podman 的容器。通过
<literal>podman</literal> 执行此操作可以方便地从容器内部验证对 NVIDIA 设备的访问，稍后还可以放心地对
Kubernetes 执行同样的操作。根据 <link
xl:href="https://registry.suse.com/repositories/bci-bci-base-15sp6">SLE
BCI</link>，为 <literal>podman</literal> 授予对上一条命令处理过的带标签 NVIDIA 设备的访问权限，然后直接运行
Bash 命令：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>现在，您将从临时 podman
容器内部执行命令。该容器无权访问您的底层系统，并且是临时性的，因此我们在此处执行的所有操作都不会保存，并且您无法破坏底层主机上的任何设置。由于我们现在处于容器中，因此可以安装所需的
CUDA 库。请再次对照<link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">此页面</link>检查驱动程序的
CUDA 版本是否正确，不过，先前 <literal>nvidia-smi</literal> 命令的输出应该也会显示所需的 CUDA
版本。以下示例将安装 <emphasis>CUDA 12.3</emphasis> 并提取许多示例、演示和开发包，以便您可以全面验证 GPU：</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-suse
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>成功安装后，请不要退出容器。我们将运行 <literal>deviceQuery</literal> CUDA 示例，它会全面验证通过 CUDA
以及从容器本身内部进行 GPU 访问的情况：</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>如果成功，您应会看到如下所示的输出，请注意命令结束后返回的 <literal>Result = PASS</literal>
消息，并注意在以下输出中，系统正确识别了两个 GPU，而您的环境中可能只有一个 GPU：</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>在此处，您可以继续运行任何其他 CUDA 工作负载 — 使用编译器以及 CUDA
生态系统的任何其他组件来运行进一步的测试。完成后，可以退出容器，请注意您在容器中安装的任何内容都是临时性的（因此会丢失！），并且底层操作系统不会受到影响：</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>使用 Kubernetes 实现</title>
<para>确认已在 SUSE Linux Micro 上安装并使用 NVIDIA 开放驱动程序后，我们来了解如何在同一台计算机上配置
Kubernetes。本指南不会指导您部署 Kubernetes，但假设您已安装 <link
xl:href="https://k3s.io/">K3s</link> 或 <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>，并且已相应地配置
kubeconfig，以便能够以超级用户的身份执行标准 <literal>kubectl</literal>
命令。假设您的节点构成了单节点群集，不过，对多节点群集可以使用类似的核心步骤。首先，请确保可以正常进行
<literal>kubectl</literal> 访问：</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>此命令应会显示如下所示的输出：</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.33.3+rke2r1</screen>
<para>您会发现，k3s/rke2 安装已检测到主机上的 NVIDIA Container Toolkit，并已将 NVIDIA 运行时集成自动配置到
<literal>containerd</literal>（k3s/rke2 使用的容器运行时接口）中。这一点可以通过检查 containerd
<literal>config.toml</literal> 文件来确认：</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>此命令必须显示如下所示的内容。对应的 K3s 位置是
<literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>：</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>如果未显示这些项，则可能表示检测失败。原因可能是计算机或 Kubernetes 服务未重启。如果需要，请如前文所述手动添加这些项。</para>
</note>
<para>接下来，我们需要将作为附加 Kubernetes 运行时的 NVIDIA <literal>RuntimeClass</literal>
配置为默认设置，以确保需要访问 GPU 的任何用户 Pod 请求都可以按照 <literal>containerd</literal>
配置中指定的方式，使用 NVIDIA Container Toolkit 通过
<literal>nvidia-container-runtime</literal> 进行这种访问：</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>下一步是配置 <link xl:href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA
Device Plugin</link>，该插件会将 Kubernetes 配置为利用 NVIDIA GPU 作为群集中可用的资源，并与 NVIDIA
Container Toolkit 配合工作。此工具最初会检测底层主机上的所有功能，包括 GPU、驱动程序和其他功能（例如 GL），然后允许您请求
GPU 资源并将其用作应用程序的一部分。</para>
<para>首先，需要添加并更新 NVIDIA Device Plugin 的 Helm 储存库：</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>现在可以安装 NVIDIA Device Plugin：</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>几分钟后，您会看到一个新的 Pod 正在运行，它将在可用节点上完成检测，并根据检测到的 GPU 数量来标记节点：</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>现在，可以创建一个 NVIDIA Pod 来尝试使用此 GPU。我们来尝试在 CUDA 基准容器上执行此操作：</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>如果一切顺利，您可以在日志中看到基准测试信息：</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
        -fullscreen       (run n-body simulation in fullscreen mode)
        -fp64             (use double precision floating point values for simulation)
        -hostmem          (stores simulation data in host memory)
        -benchmark        (run benchmark to measure performance)
        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
        -compare          (compares simulation results running once on the default GPU and once on the CPU)
        -cpu              (run n-body simulation on the CPU)
        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>最后，如果您的应用程序需要 OpenGL，您可以在主机级别安装所需的 NVIDIA OpenGL 库，NVIDIA Device Plugin 和
NVIDIA Container Toolkit 可将这些库提供给容器。为此，请如下所示安装软件包：</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>需要重引导系统才能将此软件包提供给应用程序。NVIDIA Device Plugin 会通过 NVIDIA Container Toolkit
自动重新检测此软件包。</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>通过 Edge Image Builder 整合配置</title>
<para>现在您已经在 SUSE Linux Micro 上验证了应用程序和 GPU 的完整功能，现在可能希望通过<xref
linkend="components-eib"/>将所有配置整合到一个可部署/可使用的 ISO 或 RAW 磁盘映像。本指南不会介绍如何使用 Edge
Image Builder，但提供了构建此类映像所需的配置。以下是一个映像定义示例以及必要的 Kubernetes
配置文件，确保所有必需组件都能在部署时默认安装到位。下面是该示例对应的 Edge Image Builder 目录结构：</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>我们来浏览这些文件。首先，这是一个运行 K3s 的单节点群集的示例映像定义，它还会部署实用程序和 OpenGL 软件包
(<literal>eib-config-iso.yaml</literal>)：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: [snip]
kubernetes:
  version: v1.33.3+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>这只是一个示例。您可能需要根据自己的要求和期望对其进行自定义。此外，如果使用 SUSE Linux Micro，您需要提供自己的
<literal>sccRegistrationCode</literal> 来解析软件包依赖项并提取 NVIDIA 驱动程序。</para>
</note>
<para>除此之外，还需要添加其他组件，供 Kubernetes 在引导时加载。首先，EIB 目录需要一个
<literal>kubernetes</literal> 目录，其中包含用于保存配置、Helm chart 值和任何其他所需清单的子目录：</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>现在我们来通过选择 CNI（如果未选择，则默认为 Cilium）并启用 SELinux 来设置（可选的）Kubernetes 配置：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>现在确保在 Kubernetes 群集上创建 NVIDIA RuntimeClass：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>我们将使用内置的 Helm 控制器通过 Kubernetes 本身来部署 NVIDIA Device Plugin。我们需要在 chart
的值文件中提供运行时类：</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>在继续之前，我们需要抓取 NVIDIA Container Toolkit RPM 公共密钥：</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>所有必需的制品（包括 Kubernetes 二进制文件、容器映像、Helm
chart（以及所有引用的映像））都会自动实现隔离处理，这意味着在部署时，系统默认不需要互联网连接。现在您只需从 <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE 下载页面</link>抓取 SUSE
Linux Micro ISO（并将其放入 <literal>base-images</literal> 目录），然后调用 Edge Image
Builder 工具即可生成 ISO。作为示例补充，以下是用于构建映像的命令：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-config-iso.yaml</screen>
<para>有关更多说明，请参见 Edge Image Builder 的<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md">文档</link>。</para>
</section>
<section xml:id="id-resolving-issues">
<title>解决问题</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smi 找不到 GPU</title>
<para>使用 <literal>dmesg</literal> 检查内核消息。如果消息指出无法分配
<literal>NvKMSKapDevice</literal>，请采用“GPU 不受支持”临时解决方法：</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>注意</emphasis>：如果您在上述步骤中更改了内核模块配置，则需要重新加载或重引导内核模块才能使更改生效。</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="day-2-operations">
<title>Day 2 操作</title>
<partintro>
<para>本章介绍管理员如何在管理群集和下游群集上处理不同的“Day 2”操作任务。</para>
</partintro>
<chapter xml:id="day2-migration">
<title>Edge 3.4 迁移</title>
<para>本章介绍如何将<literal>管理</literal>群集和<literal>下游</literal>群集从 <literal>Edge
3.3</literal> 迁移到 <literal>Edge 3.4.0</literal>。</para>
<important>
<para>始终从 <literal>Edge 3.3</literal> 的<literal>最新 z-stream</literal> 版本执行群集迁移。</para>
<para>始终迁移到 <literal>Edge 3.4.0</literal> 版本。对于迁移后的后续升级，请参见管理群集（<xref
linkend="day2-mgmt-cluster"/>）和下游群集（<xref
linkend="day2-downstream-clusters"/>）章节。</para>
</important>
<section xml:id="day2-migration-mgmt">
<title>管理群集</title>
<para>本节将介绍以下主题：</para>
<para><xref linkend="day2-migration-mgmt-prereq"/> - 开始迁移前需要完成的先决步骤。</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller"/> - 如何使用<xref
linkend="components-upgrade-controller"/>执行<literal>管理</literal>群集迁移。</para>
<para><xref linkend="day2-migration-mgmt-fleet"/> - 如何使用<xref
linkend="components-fleet"/>执行<literal>管理</literal>群集迁移。</para>
<section xml:id="day2-migration-mgmt-prereq">
<title>先决条件</title>
<section xml:id="id-upgrade-the-bare-metal-operator-crds">
<title>升级裸机操作器 CRD</title>
<note>
<para>仅适用于需要进行 <xref linkend="components-metal3"/> chart 升级的群集。</para>
</note>
<para><literal>Metal<superscript>3</superscript></literal> Helm chart 利用 Helm 的
<link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you">CRD</link>
目录来包含<link xl:href="https://book.metal3.io/bmo/introduction.html">裸机操作器
(BMO)</link> CRD。</para>
<para>然而，这种方法存在一定的局限性，特别是无法使用 Helm 升级此目录中的 CRD。有关详细信息，请参见 <link
xl:href="https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#some-caveats-and-explanations">Helm
文档</link>。</para>
<para>因此，在将 Metal<superscript>3</superscript> 升级到与 <literal>Edge 3.4.0</literal>
兼容的版本之前，用户必须手动升级底层的 BMO CRD。</para>
<para>在安装了 <literal>Helm</literal> 且 <literal>kubectl</literal>
已配置为指向<literal>管理</literal>群集的计算机上：</para>
<orderedlist numeration="arabic">
<listitem>
<para>手动应用 BMO CRD：</para>
<screen language="bash" linenumbering="unnumbered">helm show crds oci://registry.suse.com/edge/charts/metal3 --version 304.0.16+up0.12.6 | kubectl apply -f -</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller">
<title>升级控制器</title>
<important>
<para><literal>升级控制器</literal>目前仅支持<emphasis role="strong">非隔离管理</emphasis>群集的
Edge 版本迁移。</para>
</important>
<para>本节将介绍以下主题：</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-prereq"/> -
针对<literal>升级控制器</literal>的先决条件。</para>
<para><xref linkend="day2-migration-mgmt-upgrade-controller-migration"/> -
使用<literal>升级控制器</literal>将<literal>管理</literal>群集迁移到新 Edge 版本的步骤。</para>
<section xml:id="day2-migration-mgmt-upgrade-controller-prereq">
<title>先决条件</title>
<section xml:id="id-edge-3-4-upgrade-controller">
<title>Edge 3.4 升级控制器</title>
<para>使用<literal>升级控制器</literal>之前，必须先确保其运行的版本能够迁移到目标 Edge 版本。</para>
<para>操作步骤如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para>如果您已在之前的 Edge 版本中部署了<literal>升级控制器</literal>，请升级其 chart：</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade upgrade-controller -n upgrade-controller-system oci://registry.suse.com/edge/charts/upgrade-controller --version 304.0.1+up0.1.1</screen>
</listitem>
<listitem>
<para>如果您尚<emphasis role="strong">未</emphasis>部署<literal>升级控制器</literal>，请按照<xref
linkend="components-upgrade-controller-installation"/>中所述操作。</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-migration-mgmt-upgrade-controller-migration">
<title>迁移步骤</title>
<para>使用<literal>升级控制器</literal>执行<literal>管理</literal>群集迁移本质上与执行升级类似。</para>
<para>唯一的区别是，<literal>UpgradePlan</literal> <emphasis
role="strong">必须</emphasis>指定 <literal>3.4.0</literal> 版本：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
  # Change to the namespace of your Upgrade Controller
  namespace: CHANGE_ME
spec:
  releaseVersion: 3.4.0</screen>
<para>有关如何使用上述 <literal>UpgradePlan</literal> 进行迁移的信息，请参见升级控制器升级过程（<xref
linkend="management-day2-upgrade-controller"/>）。</para>
</section>
</section>
<section xml:id="day2-migration-mgmt-fleet">
<title>Fleet</title>
<note>
<para>只要有可能，便使用<xref linkend="day2-migration-mgmt-upgrade-controller"/>进行迁移。</para>
<para>仅在<literal>升级控制器</literal>未涵盖的使用场景下，才需参考本节内容。</para>
</note>
<para>使用 <literal>Fleet</literal> 执行<literal>管理</literal>群集迁移本质上与执行升级类似。</para>
<para><emphasis role="strong">主要</emphasis>区别在于：</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">必须使用</emphasis> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link>
版本 <literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet。</para>
</listitem>
<listitem>
<para>安排升级的 chart <emphasis role="strong">必须</emphasis>升级到与 <literal>Edge
3.4.0</literal> 版本兼容的版本。有关 <literal>Edge 3.4.0</literal> 组件的列表，请参见<xref
linkend="release-notes-3-4-0"/>。</para>
</listitem>
</orderedlist>
<important>
<para>为确保 <literal>Edge 3.4.0</literal> 迁移成功，用户必须遵守上述要点。</para>
</important>
<para>鉴于上述要点，用户可参考<literal>管理</literal>群集 Fleet（<xref
linkend="management-day2-fleet"/>）文档，获取执行迁移需要完成的步骤的全面指南。</para>
</section>
</section>
<section xml:id="day2-migration-downstream">
<title>下游群集</title>
<para><xref linkend="day2-migration-downstream-fleet"/> - 如何使用<xref
linkend="components-fleet"/>执行<literal>下游</literal>群集迁移。</para>
<section xml:id="day2-migration-downstream-fleet">
<title>Fleet</title>
<para>使用 <literal>Fleet</literal> 执行<literal>下游</literal>群集迁移本质上与执行升级类似。</para>
<para><emphasis role="strong">主要</emphasis>区别在于：</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">必须使用</emphasis> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">release-3.4.0</link>
版本 <literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet。</para>
</listitem>
<listitem>
<para>安排升级的 chart <emphasis role="strong">必须</emphasis>升级到与 <literal>Edge
3.4.0</literal> 版本兼容的版本。有关 <literal>Edge 3.4.0</literal> 组件的列表，请参见<xref
linkend="release-notes-3-4-0"/>。</para>
</listitem>
</orderedlist>
<important>
<para>为确保 <literal>Edge 3.4.0</literal> 迁移成功，用户必须遵守上述要点。</para>
</important>
<para>鉴于上述要点，用户可参考<literal>下游</literal>群集 Fleet（<xref
linkend="downstream-day2-fleet"/>）文档，获取执行迁移需要完成的步骤的全面指南。</para>
</section>
</section>
</chapter>
<chapter xml:id="day2-mgmt-cluster">
<title>管理群集</title>
<para>目前，可以通过两种方式在<literal>管理</literal>群集上执行“Day 2”操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <xref linkend="components-upgrade-controller"/> - <xref
linkend="management-day2-upgrade-controller"/></para>
</listitem>
<listitem>
<para>通过 <xref linkend="components-fleet"/> - <xref
linkend="management-day2-fleet"/></para>
</listitem>
</orderedlist>
<section xml:id="management-day2-upgrade-controller">
<title>升级控制器</title>
<important>
<para><literal>升级控制器</literal>目前仅支持<emphasis role="strong">非隔离管理</emphasis>群集的
<literal>Day 2</literal> 操作。</para>
</important>
<para>本节介绍如何执行与将<literal>管理</literal>群集从一个 Edge 平台版本升级到另一个版本相关的各种 <literal>Day
2</literal> 操作。</para>
<para><literal>Day 2</literal> 操作由升级控制器（<xref
linkend="components-upgrade-controller"/>）自动执行，包括：</para>
<itemizedlist>
<listitem>
<para>SUSE Linux Micro（<xref linkend="components-slmicro"/>）操作系统升级</para>
</listitem>
<listitem>
<para><xref linkend="components-rke2"/>或<xref linkend="components-k3s"/>Kubernetes
升级</para>
</listitem>
<listitem>
<para>其他 SUSE 组件（SUSE Rancher Prime、SUSE Security 等）升级</para>
</listitem>
</itemizedlist>
<section xml:id="id-prerequisites-13">
<title>先决条件</title>
<para>升级<literal>管理</literal>群集之前，必须满足以下先决条件：</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>已在 SCC 中注册的节点</literal> - 确保群集节点的操作系统已使用相应的订阅密钥注册，且该密钥支持要升级到的 Edge
版本中指定的操作系统版本（<xref linkend="release-notes"/>）。</para>
</listitem>
<listitem>
<para><literal>升级控制器</literal> -
确保<literal>管理</literal>群集上已部署<literal>升级控制器</literal>。有关安装步骤，请参见<xref
linkend="components-upgrade-controller-installation"/>。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade">
<title>升级</title>
<orderedlist numeration="arabic">
<listitem>
<para>确定要将<literal>管理</literal>群集升级到哪个 Edge 版本（<xref linkend="release-notes"/>）。</para>
</listitem>
<listitem>
<para>在<literal>管理</literal>群集中，部署一个指定目标<literal>版本</literal>的
<literal>UpgradePlan</literal>。<literal>UpgradePlan</literal>
必须部署在<literal>升级控制器</literal>的名称空间中。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -n &lt;upgrade_controller_namespace&gt; -f - &lt;&lt;EOF
apiVersion: lifecycle.suse.com/v1alpha1
kind: UpgradePlan
metadata:
  name: upgrade-plan-mgmt
spec:
  # Version retrieved from release notes
  releaseVersion: 3.X.Y
EOF</screen>
<note>
<para>某些情况下，除了 <literal>UpgradePlan</literal> 之外，您可能还想进行其他配置。有关所有可能的配置，请参见<xref
linkend="components-upgrade-controller-extensions-upgrade-plan"/>。</para>
</note>
</listitem>
<listitem>
<para>将 <literal>UpgradePlan</literal>
部署到<literal>升级控制器</literal>的名称空间后，<literal>升级过程</literal>即会开始。</para>
<note>
<para>有关实际<literal>升级过程</literal>的详细信息，请参见<xref
linkend="components-upgrade-controller-how"/>。</para>
<para>有关如何跟踪<literal>升级过程</literal>的信息，请参见<xref
linkend="components-upgrade-controller-how-track"/>。</para>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet">
<title>Fleet</title>
<para>本节提供有关如何使用 Fleet（<xref linkend="components-fleet"/>）组件执行“Day 2”操作的信息。</para>
<para>本节将介绍以下主题：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-components"/> - 执行所有“Day 2”操作需要使用的默认组件。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-determine-use-case"/> - 概述将使用的 Fleet
自定义资源及其在不同“Day 2”操作场景中的适用性。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-upgrade-workflow"/> - 提供使用 Fleet 执行“Day
2”操作的工作流程指南。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade"/> - 说明如何使用 Fleet 进行操作系统升级。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade"/> - 说明如何使用 Fleet 进行
Kubernetes 版本升级。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade"/> - 说明如何使用 Fleet 进行 Helm
chart 升级。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-components">
<title>组件</title>
<para>下文将介绍为使用 Fleet 顺利执行“Day 2”操作而应在<literal>管理</literal>群集上设置的默认组件。</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<para><emphasis
role="strong">可选</emphasis>组件；负责管理<literal>下游群集</literal>并在<literal>管理群集</literal>上部署<literal>系统升级控制器</literal>。</para>
<para>有关详细信息，请参见<xref linkend="components-rancher"/>。</para>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>系统升级控制器 (SUC)</title>
<para><emphasis
role="strong">系统升级控制器</emphasis>负责根据通过名为<literal>计划</literal>的自定义资源提供的配置数据，在指定节点上执行任务。</para>
<para>系统会主动利用 <emphasis role="strong">SUC</emphasis> 升级操作系统和 Kubernetes 发行版。</para>
<para>有关 <emphasis role="strong">SUC</emphasis> 组件及其如何安置到 Edge 堆栈中的详细信息，请参见<xref
linkend="components-system-upgrade-controller"/>。</para>
</section>
</section>
<section xml:id="management-day2-fleet-determine-use-case">
<title>确定您的使用场景</title>
<para>Fleet 使用两种<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>来实现对
Kubernetes 和 Helm 资源的管理。</para>
<para>下文将介绍这些资源的用途，以及在“Day 2”操作情境中它们最适合的使用场景。</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para><literal>GitRepo</literal> 是一种 Fleet（<xref
linkend="components-fleet"/>）资源，它代表一个 Git 储存库，<literal>Fleet</literal>
可从中创建<literal>捆绑包</literal>。每个<literal>捆绑包</literal>都是基于
<literal>GitRepo</literal> 资源中定义的配置路径创建的。有关详细信息，请参见 <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> 文档。</para>
<para>在“Day 2”操作情境中，<literal>GitRepo</literal> 资源通常用于在利用 <emphasis>Fleet
GitOps</emphasis> 方法的<emphasis role="strong">非隔离</emphasis>环境中部署
<literal>SUC</literal> 或 <literal>SUC 计划</literal>。</para>
<para>或者，<emphasis role="strong">如果您通过本地 git 服务器镜像储存库设置</emphasis>，则
<literal>GitRepo</literal> 资源也可用于在<emphasis role="strong">隔离</emphasis>环境中部署
<literal>SUC</literal> 或 <literal>SUC 计划</literal>。</para>
</section>
<section xml:id="id-bundle">
<title>捆绑包</title>
<para><literal>捆绑包</literal>包含了要在目标群集上部署的<emphasis role="strong">原始</emphasis>
Kubernetes 资源。通常它们是基于 <literal>GitRepo</literal>
资源创建的，但在某些使用场景中也可以手动部署。有关详细信息，请参见<link
xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>文档。</para>
<para>在“Day 2”操作情境中，<literal>捆绑包</literal>资源通常用于在不使用某种形式的<emphasis>本地
GitOps</emphasis> 过程（例如<emphasis role="strong">本地 git
服务器</emphasis>）的<emphasis role="strong">隔离</emphasis>环境中部署
<literal>SUC</literal> 或 <literal>SUC 计划</literal>。</para>
<para>或者，如果您的应用场景不允许使用 <emphasis>GitOps</emphasis> 工作流程（例如需使用 Git
储存库），则<literal>捆绑包</literal>资源也可用于在<emphasis
role="strong">非隔离</emphasis>环境中部署 <literal>SUC</literal> 或 <literal>SUC
计划</literal>。</para>
</section>
</section>
<section xml:id="management-day2-upgrade-workflow">
<title>Day 2 工作流程</title>
<para>下面是在将管理群集升级到特定 Edge 版本时应遵循的“Day 2”工作流程。</para>
<orderedlist numeration="arabic">
<listitem>
<para>操作系统升级（<xref linkend="management-day2-fleet-os-upgrade"/>）</para>
</listitem>
<listitem>
<para>Kubernetes 版本升级（<xref linkend="management-day2-fleet-k8s-upgrade"/>）</para>
</listitem>
<listitem>
<para>Helm chart 升级（<xref linkend="management-day2-fleet-helm-upgrade"/>）</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade">
<title>操作系统升级</title>
<para>本节介绍如何使用<xref linkend="components-fleet"/>和<xref
linkend="components-system-upgrade-controller"/>执行操作系统升级。</para>
<para>本节将介绍以下主题：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-components"/> - 升级过程使用的其他组件。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-overview"/> - 升级过程概述。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-requirements"/> - 升级过程的要求。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-os-upgrade-plan-deployment"/> -
关于如何部署负责触发升级过程的 <literal>SUC 计划</literal>的信息。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-os-upgrade-components">
<title>组件</title>
<para>本节介绍<literal>操作系统升级</literal>过程中使用的自定义组件，这些组件与默认“Day 2”组件（<xref
linkend="management-day2-fleet-components"/>）不同。</para>
<section xml:id="management-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>特定节点上的操作系统升级由 <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>
处理。</para>
<para>根据 Edge 版本升级时操作系统所需的不同升级类型，会创建不同的服务：</para>
<itemizedlist>
<listitem>
<para>对于需要相同操作系统版本（如 <literal>6.0</literal>）的 Edge 版本，将创建
<literal>os-pkg-update.service</literal>。它使用 <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
执行<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">常规软件包升级</link>。</para>
</listitem>
<listitem>
<para>对于需要迁移操作系统版本（例如 <literal>6.0</literal> → <literal>6.1</literal>）的 Edge
版本，将创建 <literal>os-migration.service</literal>。它使用 <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
来执行：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">常规软件包升级</link>，这可确保所有软件包均为最新版本，以减少因软件包版本过旧而导致的迁移失败情况。</para>
</listitem>
<listitem>
<para>利用 <literal>zypper migration</literal> 命令进行操作系统迁移。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>上述服务通过 <literal>SUC 计划</literal>部署到每个节点，该计划必须位于需要升级操作系统的管理群集上。</para>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-overview">
<title>概述</title>
<para>通过 <literal>Fleet</literal> 和<literal>系统升级控制器 (SUC)</literal>
来为管理群集节点升级操作系统。</para>
<para><emphasis role="strong">Fleet</emphasis> 用于将 <literal>SUC
计划</literal>部署到目标群集并对其进行管理。</para>
<note>
<para><literal>SUC 计划</literal>是一种<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>，描述了
<literal>SUC</literal> 为在一组节点上执行特定任务而需要遵循的步骤。有关 <literal>SUC
计划</literal>的示例，请参见<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">上游储存库</link>。</para>
</note>
<para>通过向特定 Fleet <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">工作区</link>部署
<link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> 或<link
xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源将<literal>操作系统 SUC
计划</literal>分发到各个群集。Fleet 会获取已部署的
<literal>GitRepo/捆绑包</literal>，并将其内容（<literal>操作系统 SUC 计划</literal>）部署到目标群集。</para>
<note>
<para><literal>GitRepo/捆绑包</literal>资源始终部署在<literal>管理群集</literal>上。使用
<literal>GitRepo</literal>
还是<literal>捆绑包</literal>资源取决于具体应用场景，有关详细信息，请参见<xref
linkend="management-day2-fleet-determine-use-case"/>。</para>
</note>
<para><literal>操作系统 SUC 计划</literal>描述了以下工作流程：</para>
<orderedlist numeration="arabic">
<listitem>
<para>执行操作系统升级前，务必要<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">封锁</link>节点。</para>
</listitem>
<listitem>
<para>务必先升级<literal>控制平面</literal>节点，再升级<literal>工作</literal>节点。</para>
</listitem>
<listitem>
<para>升级群集时，务必<emphasis role="strong">逐个</emphasis>节点依序升级。</para>
</listitem>
</orderedlist>
<para>部署<literal>操作系统 SUC 计划</literal>后，工作流程如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC 协调已部署的<literal>操作系统 SUC 计划</literal>，并在<emphasis
role="strong">每个节点</emphasis>上创建一个 <literal>Kubernetes 作业</literal>。</para>
</listitem>
<listitem>
<para><literal>Kubernetes 作业</literal>创建一个 systemd.service（<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>），用于执行软件包升级或操作系统迁移。</para>
</listitem>
<listitem>
<para>所创建的 <literal>systemd.service</literal> 触发特定节点上的操作系统升级过程。</para>
<important>
<para>操作系统升级过程完成后，相应节点将<literal>重引导</literal>以应用系统更新。</para>
</important>
</listitem>
</orderedlist>
<para>下面是上述流程的示意图：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>Fleet Day2 管理操作系统升级</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-os-upgrade-requirements">
<title>要求</title>
<para><emphasis>一般：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">已在 SCC 中注册的计算机</emphasis> - 所有管理群集节点都应已注册到
<literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>。只有这样，<literal>systemd.service</literal>
才能成功连接到所需的 RPM 储存库。</para>
<important>
<para>对于需要进行操作系统版本迁移的 Edge 版本（例如 <literal>6.0</literal> →
<literal>6.1</literal>），请确保您的 SCC 密钥支持迁移到新版本。</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">确保 SUC 计划容忍度与节点容忍度相匹配</emphasis> - 如果您的 Kubernetes
群集节点具有自定义<emphasis role="strong">污点</emphasis>，请确保在 <emphasis
role="strong">SUC 计划</emphasis>中为这些污点添加<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">容忍度</link>。默认情况下，<emphasis
role="strong">SUC 计划</emphasis>仅包含<emphasis
role="strong">控制平面</emphasis>节点的容忍度。默认容忍度包括：</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>其他任何容忍度必须添加到每个计划的 <literal>.spec.tolerations</literal> 部分。与操作系统升级相关的
<emphasis role="strong">SUC 计划</emphasis>可以在 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库中的
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>
下找到。<emphasis role="strong">请确保使用有效储存库<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记中的计划。</emphasis></para>
<para>为<emphasis role="strong">控制平面</emphasis> SUC 计划定义自定义容忍度的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>隔离：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">镜像 SUSE RPM 储存库</emphasis> - 操作系统 RPM 储存库应镜像到本地，以便
<literal>systemd.service</literal> 可以访问它们。您可以使用 <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>
或 <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>
来完成该操作。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment">
<title>操作系统升级 - SUC 计划部署</title>
<important>
<para>对于之前使用此过程升级的环境，用户应确保完成以下步骤之<emphasis role="strong">一</emphasis>：</para>
<itemizedlist>
<listitem>
<para><literal>从管理群集中去除任何先前部署且与旧版 Edge 相关的 SUC 计划</literal> - 方法是从现有的
<literal>GitRepo/捆绑包</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">目标配置</link>中去除相应群集，或完全去除
<literal>GitRepo/捆绑包</literal>资源。</para>
</listitem>
<listitem>
<para><literal>重用现有的 GitRepo/捆绑包资源</literal> - 方法是将资源的修订版指向一个新标签，该标签包含目标
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>的正确
Fleet。</para>
</listitem>
</itemizedlist>
<para>这样做是为了避免旧版 Edge 的 <literal>SUC 计划</literal>之间发生冲突。</para>
<para>如果用户尝试升级，而管理群集上存在现有的 <literal>SUC 计划</literal>，他们将看到以下 Fleet 错误：</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>如<xref
linkend="management-day2-fleet-os-upgrade-overview"/>中所述，可以通过以下任意一种方式将
<literal>SUC 计划</literal>分发到目标群集来完成操作系统升级：</para>
<itemizedlist>
<listitem>
<para>Fleet <literal>GitRepo</literal> 资源 - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>。</para>
</listitem>
<listitem>
<para>Fleet <literal>捆绑包</literal>资源 - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle"/>。</para>
</listitem>
</itemizedlist>
<para>要确定使用哪个资源，请参见<xref linkend="management-day2-fleet-determine-use-case"/>。</para>
<para>对于希望通过第三方 GitOps 工具部署<literal>操作系统 SUC 计划</literal>的使用场景，请参见<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-third-party"/>。</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>SUC 计划部署 - GitRepo 资源</title>
<para>提供所需<literal>操作系统 SUC 计划</literal>的 <emphasis
role="strong">GitRepo</emphasis> 资源可通过以下方式之一进行部署：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的操作系统升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo 创建 - Rancher UI</title>
<para>要通过 Rancher UI 创建 <literal>GitRepo</literal> 资源，请遵循其官方<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">文档</link>。</para>
<para>Edge 团队维护着一个即用型 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">Fleet</link>。根据您的环境的不同，该
Fleet 可以直接使用或用作模板。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用此
Fleet。</para>
</important>
<para>对于不需要在 Fleet 附带的 <literal>SUC 计划</literal>中包含自定义更改的使用场景，用户可以直接引用
<literal>suse-edge/fleet-examples</literal> 储存库中的
<literal>os-upgrade</literal> Fleet。</para>
<para>如果需要自定义更改（例如添加自定义容忍度），用户应从单独的储存库中引用 <literal>os-upgrade</literal>
Fleet，以便能够根据需要将更改添加到 SUC 计划中。</para>
<para>有关如何配置 <literal>GitRepo</literal> 以使用
<literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet 的示例，请参见<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">此处</link>。</para>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo 创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取 <emphasis role="strong">GitRepo</emphasis> 资源：</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>编辑 <emphasis role="strong">GitRepo</emphasis> 配置：</para>
<itemizedlist>
<listitem>
<para>去除 <literal>spec.targets</literal> 部分 - 该部分仅用于下游群集。</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>将 <literal>GitRepo</literal> 的名称空间指向 <literal>fleet-local</literal> 名称空间 -
这样做是为了在管理群集上部署该资源。</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' os-upgrade-gitrepo.yaml &amp;&amp; rm -f os-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i os-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <emphasis role="strong">GitRepo</emphasis> 资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-local</literal> 名称空间下创建的 <emphasis
role="strong">GitRepo</emphasis> 资源：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-local

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>SUC 计划部署 - 捆绑包资源</title>
<para>提供所需<literal>操作系统 SUC 计划</literal>的<emphasis
role="strong">捆绑包</emphasis>资源可以通过以下方式之一进行部署：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的操作系统升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>捆绑包创建 - Rancher UI</title>
<para>Edge 团队维护着一个可在以下步骤中使用的即用型<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">捆绑包</link>。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用此捆绑包。</para>
</important>
<para>要通过 Rancher 的 UI 创建捆绑包，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>单击左上角的 <emphasis role="strong">☰ → Continuous Delivery（持续交付）</emphasis></para>
</listitem>
<listitem>
<para>转到 <emphasis role="strong">Advanced</emphasis>（高级）&gt; <emphasis
role="strong">Bundles</emphasis>（捆绑包）</para>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）</para>
</listitem>
<listitem>
<para>此处可通过以下方式之一创建捆绑包：</para>
<note>
<para>在某些使用场景中，您可能需要在捆绑包附带的 <literal>SUC
计划</literal>中包含自定义更改。请确保在以下步骤生成的捆绑包中包含这些更改。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>手动将 <literal>suse-edge/fleet-examples</literal> 中的<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">捆绑包内容</link>复制到
<emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面。</para>
</listitem>
<listitem>
<para>从所需的<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记克隆
<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库，并在 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面中选择
<emphasis role="strong">Read from File</emphasis>（从文件读取）选项。然后导航到捆绑包位置
(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)
并选择捆绑包文件。这会在<emphasis role="strong">Create from YAML</emphasis>（基于 YAML
创建）页面中自动填充捆绑包内容。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>在 Rancher UI 中编辑捆绑包：</para>
<itemizedlist>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis role="strong">名称空间</emphasis>，使其指向
<literal>fleet-local</literal> 名称空间。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis
role="strong">目标</emphasis>群集，使其指向<literal>本地</literal>（管理）群集：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>在某些使用场景中，您的<literal>本地</literal>群集的名称可能会有所不同。</para>
<para>要获取<literal>本地</literal>群集名称，请执行以下命令：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create</emphasis>（创建）</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>捆绑包创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取<emphasis role="strong">捆绑包</emphasis>资源：</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>编辑<literal>捆绑包</literal>配置：</para>
<itemizedlist>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis
role="strong">目标</emphasis>群集，使其指向<literal>本地</literal>（管理）群集：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>在某些使用场景中，您的<literal>本地</literal>群集的名称可能会有所不同。</para>
<para>要获取<literal>本地</literal>群集名称，请执行以下命令：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis role="strong">名称空间</emphasis>，使其指向
<literal>fleet-local</literal> 名称空间。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: os-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将<emphasis role="strong">捆绑包</emphasis>资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-local</literal> 名称空间下创建的<emphasis
role="strong">捆绑包</emphasis>资源：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-local</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>SUC 计划部署 - 第三方 GitOps 工作流程</title>
<para>在某些使用场景中，用户可能希望将<literal>操作系统 SUC 计划</literal>合并到自己的第三方 GitOps 工作流程（例如
<literal>Flux</literal>）中。</para>
<para>要获取所需的操作系统升级资源，首先请确定您要使用的 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记。</para>
<para>然后，便可以在
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>
中找到资源，其中：</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> 是用于<emphasis
role="strong">控制平面</emphasis>节点的 SUC 计划资源。</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> 是用于<emphasis
role="strong">工作</emphasis>节点的 SUC 计划资源。</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> 是一个包含 <literal>upgrade.sh</literal>
脚本的机密，该脚本负责创建 systemd.service（<xref
linkend="management-day2-fleet-os-upgrade-components-systemd-service"/>）。</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> 是 ConfigMap，提供
<literal>upgrade.sh</literal> 脚本使用的升级配置。</para>
</listitem>
</itemizedlist>
<important>
<para>这些<literal>计划</literal>资源由<literal>系统升级控制器</literal>解释，应部署在您要升级的每个下游群集上。有关
SUC 部署信息，请参见<xref linkend="components-system-upgrade-controller-install"/>。</para>
</important>
<para>为了更好地了解如何使用 GitOps 工作流程来部署操作系统升级的 <emphasis role="strong">SUC
计划</emphasis>，建议查看<xref
linkend="management-day2-fleet-os-upgrade-overview"/>。</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade">
<title>Kubernetes 版本升级</title>
<para>本节介绍如何使用<xref linkend="components-fleet"/>和<xref
linkend="components-system-upgrade-controller"/>执行 Kubernetes 升级。</para>
<para>本节将介绍以下主题：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-components"/> -
升级过程使用的其他组件。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-overview"/> - 升级过程概述。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-requirements"/> - 升级过程的要求。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-k8s-upgrade-plan-deployment"/> -
关于如何部署负责触发升级过程的 <literal>SUC 计划</literal>的信息。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-k8s-upgrade-components">
<title>组件</title>
<para>本节介绍 <literal>K8s 升级</literal>过程中使用的自定义组件，这些组件与默认“Day 2”组件（<xref
linkend="management-day2-fleet-components"/>）不同。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>容器映像负责升级特定节点的 RKE2 版本。</para>
<para>此组件由 <emphasis role="strong">SUC</emphasis> 根据 <emphasis role="strong">SUC
计划</emphasis>创建的 Pod 分发。该计划应位于需要进行 RKE2 升级的每个<emphasis
role="strong">群集</emphasis>上。</para>
<para>有关 <literal>rke2-upgrade</literal> 映像如何执行升级的详细信息，请参见<link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">上游</link>文档。</para>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>容器映像负责升级特定节点的 K3s 版本。</para>
<para>此组件由 <emphasis role="strong">SUC</emphasis> 根据 <emphasis role="strong">SUC
计划</emphasis>创建的 Pod 分发。该计划应位于需要进行 K3s 升级的每个<emphasis
role="strong">群集</emphasis>上。</para>
<para>有关 <literal>k3s-upgrade</literal> 映像如何执行升级的详细信息，请参见<link
xl:href="https://github.com/k3s-io/k3s-upgrade">上游</link>文档。</para>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-overview">
<title>概述</title>
<para>通过 <literal>Fleet</literal> 和<literal>系统升级控制器 (SUC)</literal> 来为管理群集节点升级
Kubernetes 发行版。</para>
<para><literal>Fleet</literal> 用于将 <literal>SUC 计划</literal>部署到目标群集并对其进行管理。</para>
<note>
<para><literal>SUC 计划</literal>是一种<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>，描述了
<emphasis role="strong">SUC</emphasis> 为在一组节点上执行特定任务而需要遵循的步骤。有关 <literal>SUC
计划</literal>的示例，请参见<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">上游储存库</link>。</para>
</note>
<para>通过向特定 Fleet <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">工作区</link>部署
<link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> 或<link
xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源将 <literal>K8s SUC
计划</literal>分发到各个群集。Fleet 会获取已部署的
<literal>GitRepo/捆绑包</literal>，并将其内容（<literal>K8s SUC 计划</literal>）部署到目标群集。</para>
<note>
<para><literal>GitRepo/捆绑包</literal>资源始终部署在<literal>管理群集</literal>上。使用
<literal>GitRepo</literal>
还是<literal>捆绑包</literal>资源取决于具体应用场景，有关详细信息，请参见<xref
linkend="management-day2-fleet-determine-use-case"/>。</para>
</note>
<para><literal>K8s SUC 计划</literal>描述了以下工作流程：</para>
<orderedlist numeration="arabic">
<listitem>
<para>执行 K8s 升级前，务必要<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">封锁</link>节点。</para>
</listitem>
<listitem>
<para>务必先升级<literal>控制平面</literal>节点，再升级<literal>工作</literal>节点。</para>
</listitem>
<listitem>
<para>始终一次升级<emphasis
role="strong">一个</emphasis><literal>控制平面</literal>节点，一次升级<emphasis
role="strong">两个</emphasis><literal>工作</literal>节点。</para>
</listitem>
</orderedlist>
<para>部署 <literal>K8s SUC 计划</literal>后，工作流程如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC 协调已部署的 <literal>K8s SUC 计划</literal>，并在<emphasis
role="strong">每个节点</emphasis>上创建一个 <literal>Kubernetes 作业</literal>。</para>
</listitem>
<listitem>
<para>根据 Kubernetes 发行版的不同，该作业将创建一个运行 rke2-upgrade（<xref
linkend="management-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>）或
k3s-upgrade（<xref
linkend="management-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>）容器映像的
Pod。</para>
</listitem>
<listitem>
<para>创建的 Pod 将执行以下工作流程：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>用 <literal>rke2-upgrade/k3s-upgrade</literal> 映像中的二进制文件替换节点上现有的
<literal>rke2/k3s</literal> 二进制文件。</para>
</listitem>
<listitem>
<para>终止正在运行的 <literal>rke2/k3s</literal> 进程。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>终止 <literal>rke2/k3s</literal> 进程会触发重启，启动运行已更新二进制文件的新进程，从而实现 Kubernetes
发行版版本的升级。</para>
</listitem>
</orderedlist>
<para>下面是上述流程的示意图：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-management-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>Fleet Day2 管理 k8s 升级</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-requirements">
<title>要求</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">备份您的 Kubernetes 发行版：</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>对于 <emphasis role="strong">RKE2 群集</emphasis>，请参见 <link
xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2 Backup and
Restore</link> 文档。</para>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s 群集</emphasis>，请参见 <link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and
Restore</link> 文档。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">确保 SUC 计划容忍度与节点容忍度相匹配</emphasis> - 如果您的 Kubernetes
群集节点具有自定义<emphasis role="strong">污点</emphasis>，请确保在 <emphasis
role="strong">SUC 计划</emphasis>中为这些污点添加<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">容忍度</link>。默认情况下，<emphasis
role="strong">SUC 计划</emphasis>仅包含<emphasis
role="strong">控制平面</emphasis>节点的容忍度。默认容忍度包括：</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>其他任何容忍度必须添加到每个计划的 <literal>.spec.tolerations</literal> 部分下。与 Kubernetes
版本升级相关的 <emphasis role="strong">SUC 计划</emphasis>可以在 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库中的以下位置找到：</para>
<itemizedlist>
<listitem>
<para>对于 <emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">请确保使用有效储存库<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记中的计划。</emphasis></para>
<para>为 RKE2 <emphasis role="strong">控制平面</emphasis> SUC 计划定义自定义容忍度的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment">
<title>K8s 升级 - SUC 计划部署</title>
<important>
<para>对于之前使用此过程升级的环境，用户应确保完成以下步骤之<emphasis role="strong">一</emphasis>：</para>
<itemizedlist>
<listitem>
<para><literal>从管理群集中去除任何先前部署且与旧版 Edge 相关的 SUC 计划</literal> - 方法是从现有的
<literal>GitRepo/捆绑包</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">目标配置</link>中去除相应群集，或完全去除
<literal>GitRepo/捆绑包</literal>资源。</para>
</listitem>
<listitem>
<para><literal>重用现有的 GitRepo/捆绑包资源</literal> - 方法是将资源的修订版指向一个新标签，该标签包含目标
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>的正确
Fleet。</para>
</listitem>
</itemizedlist>
<para>这样做是为了避免旧版 Edge 的 <literal>SUC 计划</literal>之间发生冲突。</para>
<para>如果用户尝试升级，而管理群集上存在现有的 <literal>SUC 计划</literal>，他们将看到以下 Fleet 错误：</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>如<xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>中所述，可以通过以下任意一种方式将
<literal>SUC 计划</literal>分发到目标群集来完成 Kubernetes 升级：</para>
<itemizedlist>
<listitem>
<para>Fleet GitRepo 资源（<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>）</para>
</listitem>
<listitem>
<para>Fleet 捆绑包资源（<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>）</para>
</listitem>
</itemizedlist>
<para>要确定使用哪个资源，请参见<xref linkend="management-day2-fleet-determine-use-case"/>。</para>
<para>对于希望通过第三方 GitOps 工具部署 <literal>K8s SUC 计划</literal>的使用场景，请参见<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>SUC 计划部署 - GitRepo 资源</title>
<para>提供所需 <literal>K8s SUC 计划</literal>的 <emphasis
role="strong">GitRepo</emphasis> 资源可通过以下方式之一进行部署：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的 Kubernetes 升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo 创建 - Rancher UI</title>
<para>要通过 Rancher UI 创建 <literal>GitRepo</literal> 资源，请遵循其官方<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">文档</link>。</para>
<para>Edge 团队为 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>
和 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>
这两种 Kubernetes 发行版维护着即用型 Fleet。根据您的环境的不同，这些 Fleet 可以直接使用或用作模板。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用这些
Fleet。</para>
</important>
<para>对于不需要在这些 Fleet 附带的 <literal>SUC 计划</literal>中包含自定义更改的使用场景，用户可以直接引用
<literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet。</para>
<para>如果需要自定义更改（例如添加自定义容忍度），用户应从单独的储存库中引用 Fleet，以便能够根据需要将更改添加到 SUC 计划中。</para>
<para>使用 <literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet 配置
<literal>GitRepo</literal> 资源的示例：</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo 创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取 <emphasis role="strong">GitRepo</emphasis> 资源：</para>
<itemizedlist>
<listitem>
<para>对于 <emphasis role="strong">RKE2</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>编辑 <emphasis role="strong">GitRepo</emphasis> 配置：</para>
<itemizedlist>
<listitem>
<para>去除 <literal>spec.targets</literal> 部分 - 该部分仅用于下游群集。</para>
<itemizedlist>
<listitem>
<para>对于 RKE2，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>对于 K3s，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak '/^  targets:/,$d' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval 'del(.spec.targets)' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <literal>GitRepo</literal> 的名称空间指向 <literal>fleet-local</literal> 名称空间 -
这样做是为了在管理群集上部署该资源。</para>
<itemizedlist>
<listitem>
<para>对于 RKE2，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' rke2-upgrade-gitrepo.yaml &amp;&amp; rm -f rke2-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>对于 K3s，使用以下命令：</para>
<screen language="bash" linenumbering="unnumbered"># Example using sed
sed -i.bak 's/namespace: fleet-default/namespace: fleet-local/' k3s-upgrade-gitrepo.yaml &amp;&amp; rm -f k3s-upgrade-gitrepo.yaml.bak

# Example using yq (v4+)
yq eval '.metadata.namespace = "fleet-local"' -i k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <emphasis role="strong">GitRepo</emphasis> 资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-local</literal> 名称空间下创建的 <emphasis
role="strong">GitRepo</emphasis> 资源：</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-local

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-local

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-local   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>SUC 计划部署 - 捆绑包资源</title>
<para>可通过以下方式之一部署<emphasis role="strong">捆绑包</emphasis>资源，该资源中附带了所需的
<literal>Kubernetes 升级 SUC 计划</literal>：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的 Kubernetes 升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>捆绑包创建 - Rancher UI</title>
<para>Edge 团队为 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>
和 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>
这两种 Kubernetes 发行版维护着即用型捆绑包。根据您的环境的不同，这些捆绑包可以直接使用或用作模板。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用此捆绑包。</para>
</important>
<para>要通过 Rancher 的 UI 创建捆绑包，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>单击左上角的 <emphasis role="strong">☰ → Continuous Delivery（持续交付）</emphasis></para>
</listitem>
<listitem>
<para>转到 <emphasis role="strong">Advanced</emphasis>（高级）&gt; <emphasis
role="strong">Bundles</emphasis>（捆绑包）</para>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）</para>
</listitem>
<listitem>
<para>此处可通过以下方式之一创建捆绑包：</para>
<note>
<para>在某些使用场景中，您可能需要在捆绑包附带的 <literal>SUC
计划</literal>中包含自定义更改。请确保在以下步骤生成的捆绑包中包含这些更改。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>手动将 <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>
或 <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>
的捆绑包内容从 <literal>suse-edge/fleet-examples</literal> 复制到 <emphasis
role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面。</para>
</listitem>
<listitem>
<para>从所需的<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记克隆
<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
储存库，并在 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面中选择
<emphasis role="strong">Read from File</emphasis>（从文件读取）选项。然后导航到所需的捆绑包（对于
RKE2，为
<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>；对于
K3s，为
<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>）。这会在
<emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面中自动填充捆绑包内容。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>在 Rancher UI 中编辑捆绑包：</para>
<itemizedlist>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis role="strong">名称空间</emphasis>，使其指向
<literal>fleet-local</literal> 名称空间。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis
role="strong">目标</emphasis>群集，使其指向<literal>本地</literal>（管理）群集：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>在某些使用场景中，您的<literal>本地</literal>群集的名称可能会有所不同。</para>
<para>要获取<literal>本地</literal>群集名称，请执行以下命令：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create</emphasis>（创建）</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>捆绑包创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取<emphasis role="strong">捆绑包</emphasis>资源：</para>
<itemizedlist>
<listitem>
<para>对于 <emphasis role="strong">RKE2</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>编辑<literal>捆绑包</literal>配置：</para>
<itemizedlist>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis
role="strong">目标</emphasis>群集，使其指向<literal>本地</literal>（管理）群集：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterName: local</screen>
<note>
<para>在某些使用场景中，您的<literal>本地</literal>群集的名称可能会有所不同。</para>
<para>要获取<literal>本地</literal>群集名称，请执行以下命令：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis role="strong">名称空间</emphasis>，使其指向
<literal>fleet-local</literal> 名称空间。</para>
<screen language="yaml" linenumbering="unnumbered"># Example
kind: Bundle
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: rke2-upgrade
  namespace: fleet-local
...</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将<emphasis role="strong">捆绑包</emphasis>资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-local</literal> 名称空间下创建的<emphasis
role="strong">捆绑包</emphasis>资源：</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-local

# For K3s
kubectl get bundles k3s-upgrade -n fleet-local

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="management-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>SUC 计划部署 - 第三方 GitOps 工作流程</title>
<para>在某些使用场景中，用户可能希望将 <literal>Kubernetes 升级 SUC 计划</literal>合并到自己的第三方 GitOps
工作流程（例如 <literal>Flux</literal>）中。</para>
<para>要获取所需的 K8s 升级资源，首先请确定您要使用的 <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
储存库的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记。</para>
<para>然后，便可以在以下位置找到资源：</para>
<itemizedlist>
<listitem>
<para>对于 RKE2 群集升级：</para>
<itemizedlist>
<listitem>
<para>对于<literal>控制平面</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>对于<literal>工作</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>对于 K3s 群集升级：</para>
<itemizedlist>
<listitem>
<para>对于<literal>控制平面</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>对于<literal>工作</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>这些<literal>计划</literal>资源由<literal>系统升级控制器</literal>解释，应部署在您要升级的每个下游群集上。有关
SUC 部署信息，请参见<xref linkend="components-system-upgrade-controller-install"/>。</para>
</important>
<para>为了更好地了解如何使用 GitOps 工作流程来部署 Kubernetes 版本升级的 <emphasis role="strong">SUC
计划</emphasis>，建议查看有关使用 <literal>Fleet</literal> 进行更新的过程概述（<xref
linkend="management-day2-fleet-k8s-upgrade-overview"/>）。</para>
</section>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade">
<title>Helm chart 升级</title>
<para>本节介绍如下内容：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-air-gap"/> - 包含有关如何将与 Edge
相关的 OCI chart 和映像分发到您的专用仓库的信息。</para>
</listitem>
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure"/> - 包含有关不同 Helm
chart 升级情形及其升级过程的信息。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-air-gap">
<title>为隔离环境做好准备</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet">
<title>确保您有权访问 Helm chart 的 Fleet</title>
<para>根据环境支持的情况，选择以下选项之一：</para>
<orderedlist numeration="arabic">
<listitem>
<para>将 chart 的 Fleet 资源托管在<literal>管理群集</literal>可访问的本地 git 服务器上。</para>
</listitem>
<listitem>
<para>使用 Fleet 的 CLI <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm chart 转换为捆绑包</link>，您可直接使用该捆绑包，无需将其托管在其他位置。可从 Fleet 的 <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">版本</link>页面获取其
CLI；对于 Mac 用户，可通过 <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulae 获取。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>找到 Edge 发行版本的所需资产</title>
<orderedlist numeration="arabic">
<listitem>
<para>转到“Day 2”<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>页面，找到您要将
chart 升级到的 Edge 版本，然后单击 <emphasis role="strong">Assets</emphasis>（资产）。</para>
</listitem>
<listitem>
<para>从<emphasis role="strong">“Assets”</emphasis>（资产）部分下载以下文件：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">版本文件</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">说明</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>提取 <literal>edge-release-images.txt</literal> 文件中指定的映像，并将其封装到“.tar.gz”归档中。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>提取与特定 Edge 版本相关的 OCI chart 映像，并将其封装到“.tar.gz”归档中。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>从“.tar.gz”归档加载映像，将它们重新标记并推送到专用仓库。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>接收包含 Edge OCI '.tgz' chart 软件包的目录作为参数，并将这些软件包加载到专用仓库中。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>包含与特定 Edge 版本相关的 OCI chart 映像的列表。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>包含与特定 Edge 版本相关的映像列表。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive">
<title>创建 Edge 版本映像归档</title>
<para><emphasis>在可以访问互联网的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>将 <literal>edge-save-images.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>生成映像归档：</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>这将创建一个名为 <literal>edge-images.tar.gz</literal> 的可加载归档。</para>
<note>
<para>如果指定了 <literal>-i|--images</literal> 选项，归档的名称可能会不同。</para>
</note>
</listitem>
<listitem>
<para>将此归档复制到<emphasis role="strong">隔离的</emphasis>计算机：</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive">
<title>创建 Edge OCI chart 映像归档</title>
<para><emphasis>在可以访问互联网的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>将 <literal>edge-save-oci-artefacts.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>生成 OCI chart 映像归档：</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>这将创建一个名为 <literal>oci-artefacts.tar.gz</literal> 的归档。</para>
<note>
<para>如果指定了 <literal>-a|--archive</literal> 选项，归档的名称可能会不同。</para>
</note>
</listitem>
<listitem>
<para>将此归档复制到<emphasis role="strong">隔离的</emphasis>计算机：</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine">
<title>将 Edge 版本映像加载到隔离的计算机上</title>
<para><emphasis>在隔离的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>登录到专用仓库（如果需要）：</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>将 <literal>edge-load-images.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>执行脚本，传递之前<emphasis role="strong">复制的</emphasis>
<literal>edge-images.tar.gz</literal> 归档：</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>这将从 <literal>edge-images.tar.gz</literal> 加载所有映像，并将它们重新标记并推送到
<literal>--registry</literal> 选项下指定的仓库。</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine">
<title>将 Edge OCI chart 映像加载到隔离的计算机上</title>
<para><emphasis>在隔离的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>登录到专用仓库（如果需要）：</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>将 <literal>edge-load-oci-artefacts.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>解压缩复制的 <literal>oci-artefacts.tar.gz</literal> 归档：</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>这会使用命名模板 <literal>edge-release-oci-tgz-&lt;date&gt;</literal> 生成一个目录</para>
</listitem>
<listitem>
<para>将此目录传递给 <literal>edge-load-oci-artefacts.sh</literal> 脚本，以将 Edge OCI chart
映像加载到专用仓库中：</para>
<note>
<para>此脚本假设您的环境中已预装了 <literal>Helm</literal> CLI。有关 Helm 安装说明，请参见<link
xl:href="https://helm.sh/docs/intro/install/">安装 Helm</link>。</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution">
<title>在 Kubernetes 发行版中配置专用仓库</title>
<para>对于 RKE2，请参见 <link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration</link></para>
<para>对于 K3s，请参见 <link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration</link></para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure">
<title>升级过程</title>
<para>本节重点介绍以下使用场景的 Helm 升级过程：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>手动部署的 Helm chart 无法可靠升级。我们建议使用<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster"/>中所述的方法重新部署这些
Helm chart。</para>
</important>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>我有一个新群集，想要部署和管理 Edge Helm chart</title>
<para>本节介绍如何：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>。</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>。</para>
</listitem>
<listitem>
<para><xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>。</para>
</listitem>
</orderedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>为您的 chart 准备 Fleet 资源</title>
<orderedlist numeration="arabic">
<listitem>
<para>从您要使用的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记处获取
Chart 的 Fleet 资源。</para>
</listitem>
<listitem>
<para>导航到 Helm chart Fleet
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">如果您打算使用 GitOps 工作流程</emphasis>，请将 chart Fleet 目录复制到
Git 储存库，从那里执行 GitOps。</para>
</listitem>
<listitem>
<para><emphasis role="strong">（可选）</emphasis>如果需要配置 Helm chart 的<emphasis
role="strong">值</emphasis>才能使用 Helm chart，请编辑复制的目录中
<literal>fleet.yaml</literal> 文件内的 <literal>.helm.values</literal> 配置。</para>
</listitem>
<listitem>
<para><emphasis role="strong">（可选）</emphasis>在某些使用场景中，您可能需要向 chart 的 Fleet
目录添加其他资源，使该目录能够更好地适应您的环境。有关如何增强 Fleet 目录的信息，请参见 <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git 储存库内容</link>。</para>
</listitem>
</orderedlist>
<note>
<para>在某些情况下，Fleet 为 Helm 操作设置的默认超时时长可能不足，导致发生以下错误：</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>在此类情况下，请在 <literal>fleet.yaml</literal> 文件的 <literal>helm</literal> 配置下添加
<link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>
属性。</para>
</note>
<para><literal>longhorn</literal> Helm chart 的<emphasis
role="strong">示例</emphasis>如下：</para>
<itemizedlist>
<listitem>
<para>用户 Git 储存库结构：</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>填充了用户 <literal>Longhorn</literal> 数据的 <literal>fleet.yaml</literal> 内容：</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>上面只是一些示例值，用于演示基于 <literal>longhorn</literal> chart 创建的自定义配置。<emphasis
role="strong">不应</emphasis>将它们视为 <literal>longhorn</literal> chart 的部署指南。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>部署您的 chart 的 Fleet</title>
<para>您可以使用 GitRepo（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>）或捆绑包（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>）来部署
chart 的 Fleet。</para>
<note>
<para>部署 Fleet 时，如果收到资源<literal>已修改</literal>消息，请确保在 Fleet 的
<literal>diff</literal> 部分添加相应的 <literal>comparePatches</literal>
项。有关详细信息，请参见 <link
xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore
Modified GitRepos</link>。</para>
</note>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>Fleet 的 <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>
资源包含如何访问 chart 的 Fleet 资源以及需要将这些资源应用于哪些群集的相关信息。</para>
<para>可以通过 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UI</link> 部署 <literal>GitRepo</literal>
资源，也可以通过将该资源部署到<literal>管理群集</literal>的方式手动<link
xl:href="https://fleet.rancher.io/tut-deployment">部署</link>该资源。</para>
<para>用于<emphasis role="strong">手动</emphasis>部署的 <emphasis
role="strong">Longhorn</emphasis> <literal>GitRepo</literal> 资源示例：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-local
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url</screen>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>捆绑包</title>
<para><link xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源包含需要由 Fleet
部署的原始 Kubernetes 资源。一般情况下，建议使用 <literal>GitRepo</literal> 方法，但对于无法支持本地 Git
服务器的隔离环境，<literal>捆绑包</literal>可以帮助您将 Helm chart Fleet 传播到目标群集。</para>
<para><literal>捆绑包</literal>的部署可以通过 Rancher UI（<literal>Continuous Delivery（持续交付）→
Advanced（高级）→ Bundles（捆绑包）→ Create from YAML（基于 YAML
创建）</literal>）进行，也可以通过在正确的 Fleet 名称空间中手动部署<literal>捆绑包</literal>资源来完成。有关
Fleet 名称空间的信息，请参见上游<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">文档</link>。</para>
<para>可以利用 Fleet 的<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm Chart 转换为捆绑包</link>方法创建用于 Edge Helm chart 的<literal>捆绑包</literal>。</para>
<para>下面的示例展示了如何基于 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
和 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
Helm chart Fleet
模板创建<literal>捆绑包</literal>资源，以及如何手动将此捆绑包部署到<literal>管理群集</literal>。</para>
<note>
<para>为了阐明工作流程，下面的示例使用了 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
目录结构。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>导航到 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
chart Fleet 模板：</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>创建 <literal>targets.yaml</literal> 文件，该文件将指示 Fleet 应将 Helm chart 部署到哪些群集：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
<note>
<para>在某些使用场景中，您的本地群集的名称可能会有所不同。</para>
<para>要获取本地群集名称，请执行以下命令：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>使用 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link> 将
<literal>Longhorn</literal> Helm chart Fleet 转换为捆绑包资源。</para>
<note>
<para>可从 Fleet 的<link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">版本</link><emphasis
role="strong">资产</emphasis>页面获取 Fleet 的
CLI(<literal>fleet-linux-amd64</literal>).。</para>
<para>对于 Mac 用户，有一个 <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulae。</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>导航到 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
chart Fleet 模板：</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>创建 <literal>targets.yaml</literal> 文件，该文件将指示 Fleet 应将 Helm chart 部署到哪些群集：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Match your local (management) cluster
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>使用 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link> 将
<literal>Longhorn CRD</literal> Helm chart Fleet 转换为捆绑包资源。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>将 <literal>longhorn-bundle.yaml</literal> 和
<literal>longhorn-crd-bundle.yaml</literal> 文件部署到<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>按照以上步骤操作，将 <literal>SUSE Storage</literal> 部署到所有指定的管理群集上。</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>管理部署的 Helm chart</title>
<para>通过 Fleet 完成部署后，若要进行 Helm chart 升级，请参见<xref
linkend="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>。</para>
</section>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>我想升级 Fleet 管理的 Helm chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>确定需要将 chart 升级到哪个版本，以使其与目标 Edge 版本兼容。有关每个 Edge 版本兼容的 Helm chart
版本，可参见发行说明（<xref linkend="release-notes"/>）。</para>
</listitem>
<listitem>
<para>在 Fleet 监控的 Git 储存库中，根据发行说明（<xref linkend="release-notes"/>）中所述，将 Helm chart
的 <literal>fleet.yaml</literal> 文件中的 chart <emphasis
role="strong">版本</emphasis>和<emphasis role="strong">储存库</emphasis>更改为正确的值。</para>
</listitem>
<listitem>
<para>提交更改并将其推送到储存库后，会触发所需 Helm chart 的升级</para>
</listitem>
</orderedlist>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>我要升级通过 EIB 部署的 Helm chart</title>
<para><xref linkend="components-eib"/> 通过创建 <literal>HelmChart</literal> 资源并利用
<link xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link> Helm 集成功能引入的
<literal>helm-controller</literal> 来部署 Helm chart。</para>
<para>为确保通过 <literal>EIB</literal> 部署的 Helm chart 成功升级，用户需要对相应的
<literal>HelmChart</literal> 资源执行升级操作。</para>
<para>下文提供了以下信息：</para>
<itemizedlist>
<listitem>
<para>升级过程的一般概述（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>）。</para>
</listitem>
<listitem>
<para>必要的升级步骤（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>）。</para>
</listitem>
<listitem>
<para>展示使用所述方法进行 <link xl:href="https://longhorn.io">Longhorn</link> chart
升级的示例（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>）。</para>
</listitem>
<listitem>
<para>如何使用其他 GitOps 工具完成升级过程（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>）。</para>
</listitem>
</itemizedlist>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>概述</title>
<para>通过 <literal>EIB</literal> 部署的 Helm chart 由名为 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>
的 <literal>Fleet</literal> 执行升级。</para>
<para>该 <literal>Fleet</literal> 会处理<emphasis
role="strong">用户提供的</emphasis>数据，以<emphasis role="strong">更新</emphasis>一组特定的
HelmChart 资源。</para>
<para>更新这些资源会触发 <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>，后者会<emphasis
role="strong">升级</emphasis>与修改后的 <literal>HelmChart</literal> 资源相关联的 Helm
chart。</para>
<para>用户只需执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>从本地<link xl:href="https://helm.sh/docs/helm/helm_pull/">提取</link>需要升级的每个
Helm chart 的归档。</para>
</listitem>
<listitem>
<para>将这些归档传递给 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal> 脚本，该脚本会将这些归档中的数据包含到
<literal>eib-charts-upgrader</literal> Fleet 中。</para>
</listitem>
<listitem>
<para>将 <literal>eib-charts-upgrader</literal> Fleet
部署到<literal>管理群集</literal>。此操作可以通过 <literal>GitRepo</literal>
或<literal>捆绑包</literal>资源来完成。</para>
</listitem>
</orderedlist>
<para>部署后，<literal>eib-charts-upgrader</literal> 会借助 Fleet 将其资源分发到目标管理群集。</para>
<para>这些资源包括：</para>
<orderedlist numeration="arabic">
<listitem>
<para>一组存储<emphasis role="strong">用户提供的</emphasis> Helm chart
数据的<literal>机密</literal>。</para>
</listitem>
<listitem>
<para>一个会部署 <literal>Pod</literal> 的 <literal>Kubernetes 作业</literal>，该 Pod
会挂载前面提到的<literal>机密</literal>，并根据这些机密对相应的 HelmChart 资源进行<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">修补</link>。</para>
</listitem>
</orderedlist>
<para>如前文所述，这会触发 <literal>helm-controller</literal>，进而执行实际的 Helm chart 升级。</para>
<para>下面是上述流程的示意图：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-management-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet Day2 管理 Helm EIB 升级</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>升级步骤</title>
<orderedlist numeration="arabic">
<listitem>
<para>从正确的版本<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">标记</link>处克隆
<literal>suse-edge/fleet-example</literal> 储存库。</para>
</listitem>
<listitem>
<para>创建用于存储所提取的 Helm chart 归档的目录。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>在新创建的归档目录中，<link
xl:href="https://helm.sh/docs/helm/helm_pull/">提取</link>要升级的 Helm chart 的归档：</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>从目标<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">版本标记</link>的<emphasis
role="strong">资产</emphasis>中下载
<literal>generate-chart-upgrade-data.sh</literal> 脚本。</para>
</listitem>
<listitem>
<para>执行 <literal>generate-chart-upgrade-data.sh</literal> 脚本：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>对于 <literal>--archive-dir</literal> 目录中的每个 chart 归档，该脚本都会生成一个包含 chart 升级数据的
<literal>Kubernetes 机密 YAML</literal> 文件，并将其存储在
<literal>--fleet-path</literal> 指定的 Fleet 的 <literal>base/secrets</literal>
目录中。</para>
<para><literal>generate-chart-upgrade-data.sh</literal> 脚本还会对 Fleet 进行其他修改，以确保生成的
<literal>Kubernetes 机密 YAML</literal> 文件能被 Fleet 部署的工作负载正确使用。</para>
<important>
<para>用户不应更改 <literal>generate-chart-upgrade-data.sh</literal> 脚本生成的内容。</para>
</important>
</listitem>
</orderedlist>
<para>以下步骤因您运行的环境而异：</para>
<orderedlist numeration="arabic">
<listitem>
<para>对于支持 GitOps 的环境（例如：非隔离环境，或虽是隔离环境但支持本地 Git 服务器）：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>将 <literal>fleets/day2/eib-charts-upgrader</literal> Fleet 复制到将用于 GitOps
的储存库中。</para>
<note>
<para>确保该 Fleet 包含 <literal>generate-chart-upgrade-data.sh</literal> 脚本所做的更改。</para>
</note>
</listitem>
<listitem>
<para>配置将用于提供 <literal>eib-charts-upgrader</literal> Fleet 所有资源的
<literal>GitRepo</literal> 资源。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>有关通过 Rancher UI 配置和部署 <literal>GitRepo</literal> 的信息，请参见<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">在
Rancher UI 中访问 Fleet</link>。</para>
</listitem>
<listitem>
<para>有关 <literal>GitRepo</literal> 的手动配置和部署过程，请参见<link
xl:href="https://fleet.rancher.io/tut-deployment">创建部署</link>。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>对于不支持 GitOps 的环境（例如，不允许使用本地 Git 服务器的隔离环境）：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>从 <literal>rancher/fleet</literal> <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">版本</link>页面下载
<literal>fleet-cli</literal> 二进制文件。对于 Linux，请下载
<literal>fleet-linux-amd64</literal>。对于 Mac 用户，可以使用 Homebrew Formulae -
<link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>。</para>
</listitem>
<listitem>
<para>导航到 <literal>eib-charts-upgrader</literal> Fleet：</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>创建指示 Fleet 在哪里部署资源的 <literal>targets.yaml</literal> 文件：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To map the local(management) cluster
- clusterName: local
EOF</screen>
<note>
<para>在某些使用场景中，您的<literal>本地</literal>群集的名称可能会有所不同。</para>
<para>要获取<literal>本地</literal>群集名称，请执行以下命令：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get clusters.fleet.cattle.io -n fleet-local</screen>
</note>
</listitem>
<listitem>
<para>使用 <literal>fleet-cli</literal> 将 Fleet 转换为<literal>捆绑包</literal>资源：</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>这将创建一个捆绑包 (<literal>bundle.yaml</literal>)，其中包含来自
<literal>eib-charts-upgrader</literal> Fleet 的所有模板资源。</para>
<para>有关 <literal>fleet apply</literal> 命令的详细信息，请参见 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>。</para>
<para>有关将 Fleet 转换为捆绑包的详细信息，请参见<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm Chart 转换为捆绑包</link>。</para>
</listitem>
<listitem>
<para>通过以下方式之一部署<literal>捆绑包</literal>：</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>通过 Rancher UI - 导航到 <emphasis role="strong">Continuous Delivery（持续交付）→
Advanced（高级）→ Bundles（捆绑包）→ Create from YAML（基于 YAML 创建）</emphasis>，然后粘贴
<literal>bundle.yaml</literal> 内容，或单击 <literal>Read from
File</literal>（从文件读取）选项并传递文件。</para>
</listitem>
<listitem>
<para>手动 - 在<literal>管理群集</literal>内手动部署 <literal>bundle.yaml</literal> 文件。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>执行这些步骤可成功部署 <literal>GitRepo/捆绑包</literal>资源。资源将由 Fleet
拾取，且其内容将部署到用户在之前的步骤中指定的目标群集上。有关该过程的概述，请参见<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>。</para>
<para>有关如何跟踪升级过程的信息，请参见<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>。</para>
<important>
<para>成功验证 chart 升级后，去除<literal>捆绑包/GitRepo</literal> 资源。</para>
<para>这将从<literal>管理</literal>群集中去除不再需要的升级资源，确保将来不会发生版本冲突。</para>
</important>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>示例</title>
<note>
<para>以下示例展示了如何在<literal>管理</literal>群集上将通过 <literal>EIB</literal> 部署的 Helm chart
从一个版本升级到另一个版本。请注意，本示例中使用的版本<emphasis role="strong">并非</emphasis>推荐版本。有关特定
Edge 版本的推荐版本，请参见发行说明（<xref linkend="release-notes"/>）。</para>
</note>
<para><emphasis>使用场景：</emphasis></para>
<itemizedlist>
<listitem>
<para>某<literal>管理</literal>群集正在运行旧版 <link
xl:href="https://longhorn.io">Longhorn</link>。</para>
</listitem>
<listitem>
<para>已使用以下映像定义<emphasis>代码段</emphasis>通过 EIB 部署群集：</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal> 需要升级到与 Edge 3.4 版兼容的版本。这意味着它需要升级到
<literal>107.0.0+up1.9.1</literal>。</para>
</listitem>
<listitem>
<para>假设<literal>管理群集</literal>是<emphasis role="strong">隔离式</emphasis>群集，不支持本地 Git
服务器，并且具有有效的 Rancher 设置。</para>
</listitem>
</itemizedlist>
<para>按照升级步骤（<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>）进行操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>从 <literal>release-3.4.0</literal> 标记处克隆
<literal>suse-edge/fleet-example</literal> 储存库。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>创建用于存储 <literal>Longhorn</literal> 升级归档的目录。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>提取所需的 <literal>Longhorn</literal> chart 归档版本：</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>在 <literal>archives</literal> 目录之外，从
<literal>suse-edge/fleet-examples</literal> 版本<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">标记</link>处下载
<literal>generate-chart-upgrade-data.sh</literal> 脚本。</para>
</listitem>
<listitem>
<para>目录设置如下所示：</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>执行 <literal>generate-chart-upgrade-data.sh</literal> 脚本：</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>脚本执行后的目录结构如下所示：</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>git 中更改的文件如下所示：</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>为 <literal>eib-charts-upgrader</literal> Fleet 创建<literal>捆绑包</literal>：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>首先，导航到 Fleet：</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>然后创建 <literal>targets.yaml</literal> 文件：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: local
EOF</screen>
</listitem>
<listitem>
<para>接下来，使用 <literal>fleet-cli</literal> 二进制文件将 Fleet 转换为捆绑包：</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-local -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>通过 Rancher UI 部署捆绑包：</para>
<figure>
<title>通过 Rancher UI 部署捆绑包</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 1</phrase></textobject>
</mediaobject></figure>
<para>在此处选择 <emphasis role="strong">Read from File</emphasis>（从文件读取），并找到系统上的
<literal>bundle.yaml</literal> 文件。</para>
<para>此时会在 Rancher UI 中自动填充<literal>捆绑包</literal>。</para>
<para>选择 <emphasis role="strong">Create</emphasis>（创建）。</para>
</listitem>
<listitem>
<para>成功部署后，捆绑包如下所示：</para>
<figure>
<title>已成功部署的捆绑包</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>成功部署<literal>捆绑包</literal>后，要监控升级过程，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>验证<literal>升级 Pod</literal> 的日志：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_management.png" width="100%"/>
</imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 3 管理</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>现在验证 helm-controller 针对升级过程创建的 Pod 日志：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Pod 名称将使用以下模板 -
<literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>Pod 将位于部署了 <literal>HelmChart</literal> 资源的名称空间中。在本例中为
<literal>kube-system</literal> 名称空间。</para>
<figure>
<title>成功升级的 Longhorn chart 的日志</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_management.png" width="100%"/>
</imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 4 管理</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>导航到 Rancher 的 <literal>HelmChart</literal> 部分（<literal>More Resources（更多资源）→
HelmChart</literal>），验证 <literal>HelmChart</literal> 版本是否已更新。选择部署了该 chart
的名称空间，在本例中为 <literal>kube-system</literal>。</para>
</listitem>
<listitem>
<para>最后检查 Longhorn Pod 是否正在运行。</para>
</listitem>
</orderedlist>
<para>完成上述验证后，可以确定 Longhorn Helm chart 已升级到 <literal>107.0.0+up1.9.1</literal> 版本。</para>
</section>
<section xml:id="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>使用第三方 GitOps 工具进行 Helm chart 升级</title>
<para>在某些使用场景中，用户可能希望将此升级过程与 Fleet 以外的 GitOps 工作流程（例如
<literal>Flux</literal>）配合使用。</para>
<para>要生成升级过程所需的资源，可以使用 <literal>generate-chart-upgrade-data.sh</literal>
脚本将用户提供的数据填充到 <literal>eib-charts-upgrader</literal>
Fleet。有关如何执行此操作的详细信息，请参见<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>。</para>
<para>完成所有设置后，可以使用 <link xl:href="https://kustomize.io">kustomize</link>
生成一个可在群集中部署且完整有效的解决方案：</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>如果想将解决方案包含在 GitOps 工作流程中，可以去除 <literal>fleet.yaml</literal> 文件，并将其余内容用作有效的
<literal>Kustomize</literal> 设置。只是别忘了先运行
<literal>generate-chart-upgrade-data.sh</literal> 脚本，以便其可以将您想要升级到的 Helm
chart 的数据填充到 <literal>Kustomize</literal> 设置中。</para>
<para>要了解如何使用此工作流程，可以查看<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>和<xref
linkend="management-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>。</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="day2-downstream-clusters">
<title>下游群集</title>
<important>
<para>以下步骤不适用于由 SUSE Telco Cloud （<xref
linkend="atip"/>）管理的<literal>下游</literal>群集。有关这些群集的升级指南，请参见<xref
linkend="atip-lifecycle-downstream"/>。</para>
</important>
<para>本章介绍对<literal>下游</literal>群集的不同部分执行“Day 2”操作的可能方式。</para>
<section xml:id="downstream-day2-fleet">
<title>Fleet</title>
<para>本节提供有关如何使用 Fleet（<xref linkend="components-fleet"/>）组件执行“Day 2”操作的信息。</para>
<para>本节将介绍以下主题：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-components"/> - 执行所有“Day
2”操作时需要使用的默认组件。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-determine-use-case"/> - 概述将使用的 Fleet
自定义资源及其在不同“Day 2”操作场景中的适用性。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-upgrade-workflow"/> - 提供使用 Fleet 执行“Day
2”操作的工作流程指南。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade"/> - 说明如何使用 Fleet 进行操作系统升级。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade"/> - 说明如何使用 Fleet 进行
Kubernetes 版本升级。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade"/> - 说明如何使用 Fleet 进行 Helm
chart 升级。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-components">
<title>组件</title>
<para>下文将介绍为使用 Fleet 顺利执行“Day 2”操作而应在<literal>下游</literal>群集上设置的默认组件。</para>
<section xml:id="id-system-upgrade-controller-suc-2">
<title>系统升级控制器 (SUC)</title>
<note>
<para><emphasis role="strong">必须</emphasis>在每个下游群集上部署。</para>
</note>
<para><emphasis
role="strong">系统升级控制器</emphasis>负责根据通过名为<literal>计划</literal>的自定义资源提供的配置数据，在指定节点上执行任务。</para>
<para>系统会主动利用 <emphasis role="strong">SUC</emphasis> 升级操作系统和 Kubernetes 发行版。</para>
<para>有关 <emphasis role="strong">SUC</emphasis> 组件及其如何安置到 Edge 堆栈中的详细信息，请参见<xref
linkend="components-system-upgrade-controller"/>。</para>
<para>有关如何部署 <emphasis role="strong">SUC</emphasis> 的信息，请先确定您的使用场景（<xref
linkend="downstream-day2-fleet-determine-use-case"/>），然后参见<xref
linkend="components-system-upgrade-controller-fleet-gitrepo"/>或<xref
linkend="components-system-upgrade-controller-fleet-bundle"/>。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-determine-use-case">
<title>确定您的使用场景</title>
<para>Fleet 使用两种<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>来实现对
Kubernetes 和 Helm 资源的管理。</para>
<para>下文将介绍这些资源的用途，以及在“Day 2”操作情境中它们最适合的使用场景。</para>
<section xml:id="id-gitrepo-2">
<title>GitRepo</title>
<para><literal>GitRepo</literal> 是一种 Fleet（<xref
linkend="components-fleet"/>）资源，它代表一个 Git 储存库，<literal>Fleet</literal>
可从中创建<literal>捆绑包</literal>。每个<literal>捆绑包</literal>都是基于
<literal>GitRepo</literal> 资源中定义的配置路径创建的。有关详细信息，请参见 <link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> 文档。</para>
<para>在“Day 2”操作情境中，<literal>GitRepo</literal> 资源通常用于在利用 <emphasis>Fleet
GitOps</emphasis> 方法的<emphasis role="strong">非隔离</emphasis>环境中部署
<literal>SUC</literal> 或 <literal>SUC 计划</literal>。</para>
<para>或者，<emphasis role="strong">如果您通过本地 git 服务器镜像储存库设置</emphasis>，则
<literal>GitRepo</literal> 资源也可用于在<emphasis role="strong">隔离</emphasis>环境中部署
<literal>SUC</literal> 或 <literal>SUC 计划</literal>。</para>
</section>
<section xml:id="id-bundle-2">
<title>捆绑包</title>
<para><literal>捆绑包</literal>包含了要在目标群集上部署的<emphasis role="strong">原始</emphasis>
Kubernetes 资源。通常它们是基于 <literal>GitRepo</literal>
资源创建的，但在某些使用场景中也可以手动部署。有关详细信息，请参见<link
xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>文档。</para>
<para>在“Day 2”操作情境中，<literal>捆绑包</literal>资源通常用于在不使用某种形式的<emphasis>本地
GitOps</emphasis> 过程（例如<emphasis role="strong">本地 git
服务器</emphasis>）的<emphasis role="strong">隔离</emphasis>环境中部署
<literal>SUC</literal> 或 <literal>SUC 计划</literal>。</para>
<para>或者，如果您的应用场景不允许使用 <emphasis>GitOps</emphasis> 工作流程（例如需使用 Git
储存库），则<literal>捆绑包</literal>资源也可用于在<emphasis
role="strong">非隔离</emphasis>环境中部署 <literal>SUC</literal> 或 <literal>SUC
计划</literal>。</para>
</section>
</section>
<section xml:id="downstream-day2-upgrade-workflow">
<title>Day 2 工作流程</title>
<para>下面是在将下游群集升级到特定 Edge 版本时应遵循的“Day 2”工作流程。</para>
<orderedlist numeration="arabic">
<listitem>
<para>操作系统升级（<xref linkend="downstream-day2-fleet-os-upgrade"/>）</para>
</listitem>
<listitem>
<para>Kubernetes 版本升级（<xref linkend="downstream-day2-fleet-k8s-upgrade"/>）</para>
</listitem>
<listitem>
<para>Helm chart 升级（<xref linkend="downstream-day2-fleet-helm-upgrade"/>）</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade">
<title>操作系统升级</title>
<para>本节介绍如何使用<xref linkend="components-fleet"/>和<xref
linkend="components-system-upgrade-controller"/>执行操作系统升级。</para>
<para>本节将介绍以下主题：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-components"/> - 升级过程使用的其他组件。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-overview"/> - 升级过程概述。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-requirements"/> - 升级过程的要求。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-os-upgrade-plan-deployment"/> -
关于如何部署负责触发升级过程的 <literal>SUC 计划</literal>的信息。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-os-upgrade-components">
<title>组件</title>
<para>本节介绍<literal>操作系统升级</literal>过程中使用的自定义组件，这些组件与默认“Day 2”组件（<xref
linkend="downstream-day2-fleet-components"/>）不同。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-components-systemd-service">
<title>systemd.service</title>
<para>特定节点上的操作系统升级由 <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>
处理。</para>
<para>根据 Edge 版本升级时操作系统所需的不同升级类型，会创建不同的服务：</para>
<itemizedlist>
<listitem>
<para>对于需要相同操作系统版本（如 <literal>6.0</literal>）的 Edge 版本，将创建
<literal>os-pkg-update.service</literal>。它使用 <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
执行<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">常规软件包升级</link>。</para>
</listitem>
<listitem>
<para>对于需要迁移操作系统版本（例如 <literal>6.0</literal> → <literal>6.1</literal>）的 Edge
版本，将创建 <literal>os-migration.service</literal>。它使用 <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
来执行：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">常规软件包升级</link>，这可确保所有软件包均为最新版本，以减少因软件包版本过旧而导致的迁移失败情况。</para>
</listitem>
<listitem>
<para>利用 <literal>zypper migration</literal> 命令进行操作系统迁移。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>上述服务通过 <literal>SUC 计划</literal>部署到每个节点，该计划必须位于需要升级操作系统的下游群集上。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-overview">
<title>概述</title>
<para>通过 <literal>Fleet</literal> 和<literal>系统升级控制器 (SUC)</literal>
来为下游群集节点升级操作系统。</para>
<para><emphasis role="strong">Fleet</emphasis> 用于将 <literal>SUC
计划</literal>部署到目标群集并对其进行管理。</para>
<note>
<para><literal>SUC 计划</literal>是一种<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>，描述了
<literal>SUC</literal> 为在一组节点上执行特定任务而需要遵循的步骤。有关 <literal>SUC
计划</literal>的示例，请参见<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">上游储存库</link>。</para>
</note>
<para>通过向特定 Fleet <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">工作区</link>部署
<link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> 或<link
xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源将<literal>操作系统 SUC
计划</literal>分发到各个群集。Fleet 会获取已部署的
<literal>GitRepo/捆绑包</literal>，并将其内容（<literal>操作系统 SUC 计划</literal>）部署到目标群集。</para>
<note>
<para><literal>GitRepo/捆绑包</literal>资源始终部署在<literal>管理群集</literal>上。使用
<literal>GitRepo</literal>
还是<literal>捆绑包</literal>资源取决于具体应用场景，有关详细信息，请参见<xref
linkend="downstream-day2-fleet-determine-use-case"/>。</para>
</note>
<para><literal>操作系统 SUC 计划</literal>描述了以下工作流程：</para>
<orderedlist numeration="arabic">
<listitem>
<para>执行操作系统升级前，务必要<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">封锁</link>节点。</para>
</listitem>
<listitem>
<para>务必先升级<literal>控制平面</literal>节点，再升级<literal>工作</literal>节点。</para>
</listitem>
<listitem>
<para>升级群集时，务必<emphasis role="strong">逐个</emphasis>节点依序升级。</para>
</listitem>
</orderedlist>
<para>部署<literal>操作系统 SUC 计划</literal>后，工作流程如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC 协调已部署的<literal>操作系统 SUC 计划</literal>，并在<emphasis
role="strong">每个节点</emphasis>上创建一个 <literal>Kubernetes 作业</literal>。</para>
</listitem>
<listitem>
<para><literal>Kubernetes 作业</literal>创建一个 systemd.service（<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>），用于执行软件包升级或操作系统迁移。</para>
</listitem>
<listitem>
<para>所创建的 <literal>systemd.service</literal> 触发特定节点上的操作系统升级过程。</para>
<important>
<para>操作系统升级过程完成后，相应节点将<literal>重引导</literal>以应用系统更新。</para>
</important>
</listitem>
</orderedlist>
<para>下面是上述流程的示意图：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-os-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>Fleet Day2 下游操作系统升级</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-requirements">
<title>要求</title>
<para><emphasis>一般：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">已在 SCC 中注册的计算机</emphasis> - 所有下游群集节点都应已注册到
<literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>。只有这样，<literal>systemd.service</literal>
才能成功连接到所需的 RPM 储存库。</para>
<important>
<para>对于需要进行操作系统版本迁移的 Edge 版本（例如 <literal>6.0</literal> →
<literal>6.1</literal>），请确保您的 SCC 密钥支持迁移到新版本。</para>
</important>
</listitem>
<listitem>
<para><emphasis role="strong">确保 SUC 计划容忍度与节点容忍度相匹配</emphasis> - 如果您的 Kubernetes
群集节点具有自定义<emphasis role="strong">污点</emphasis>，请确保在 <emphasis
role="strong">SUC 计划</emphasis>中为这些污点添加<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">容忍度</link>。默认情况下，<emphasis
role="strong">SUC 计划</emphasis>仅包含<emphasis
role="strong">控制平面</emphasis>节点的容忍度。默认容忍度包括：</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>其他任何容忍度必须添加到每个计划的 <literal>.spec.tolerations</literal> 部分。与操作系统升级相关的
<emphasis role="strong">SUC 计划</emphasis>可以在 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库中的
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>
下找到。<emphasis role="strong">请确保使用有效储存库<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记中的计划。</emphasis></para>
<para>为<emphasis role="strong">控制平面</emphasis> SUC 计划定义自定义容忍度的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>隔离：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">镜像 SUSE RPM 储存库</emphasis> - 操作系统 RPM 储存库应镜像到本地，以便
<literal>systemd.service</literal> 可以访问它们。您可以使用 <link
xl:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/book-rmt.html">RMT</link>
或 <link
xl:href="https://documentation.suse.com/suma/5.0/en/suse-manager/index.html">SUMA</link>
来完成该操作。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment">
<title>操作系统升级 - SUC 计划部署</title>
<important>
<para>对于之前使用此过程升级的环境，用户应确保完成以下步骤之<emphasis role="strong">一</emphasis>：</para>
<itemizedlist>
<listitem>
<para><literal>从下游群集中去除任何先前部署且与旧版 Edge 相关的 SUC 计划</literal> - 方法是从现有的
<literal>GitRepo/捆绑包</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">目标配置</link>中去除相应群集，或完全去除
<literal>GitRepo/捆绑包</literal>资源。</para>
</listitem>
<listitem>
<para><literal>重用现有的 GitRepo/捆绑包资源</literal> - 方法是将资源的修订版指向一个新标签，该标签包含目标
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>的正确
Fleet。</para>
</listitem>
</itemizedlist>
<para>这样做是为了避免旧版 Edge 的 <literal>SUC 计划</literal>之间发生冲突。</para>
<para>如果用户尝试升级，而下游群集上存在现有的 <literal>SUC 计划</literal>，他们将看到以下 Fleet 错误：</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>如<xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>中所述，可以通过以下任意一种方式将
<literal>SUC 计划</literal>分发到目标群集来完成操作系统升级：</para>
<itemizedlist>
<listitem>
<para>Fleet <literal>GitRepo</literal> 资源 - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo"/>。</para>
</listitem>
<listitem>
<para>Fleet <literal>捆绑包</literal>资源 - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle"/>。</para>
</listitem>
</itemizedlist>
<para>要确定使用哪个资源，请参见<xref linkend="downstream-day2-fleet-determine-use-case"/>。</para>
<para>对于希望通过第三方 GitOps 工具部署<literal>操作系统 SUC 计划</literal>的使用场景，请参见<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-third-party"/>。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo">
<title>SUC 计划部署 - GitRepo 资源</title>
<para>提供所需<literal>操作系统 SUC 计划</literal>的 <emphasis
role="strong">GitRepo</emphasis> 资源可通过以下方式之一进行部署：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的操作系统升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo 创建 - Rancher UI</title>
<para>要通过 Rancher UI 创建 <literal>GitRepo</literal> 资源，请遵循其官方<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">文档</link>。</para>
<para>Edge 团队维护着一个即用型 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/os-upgrade">Fleet</link>。根据您的环境的不同，该
Fleet 可以直接使用或用作模板。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用此
Fleet。</para>
</important>
<para>对于不需要在 Fleet 附带的 <literal>SUC 计划</literal>中包含自定义更改的使用场景，用户可以直接引用
<literal>suse-edge/fleet-examples</literal> 储存库中的
<literal>os-upgrade</literal> Fleet。</para>
<para>如果需要自定义更改（例如添加自定义容忍度），用户应从单独的储存库中引用 <literal>os-upgrade</literal>
Fleet，以便能够根据需要将更改添加到 SUC 计划中。</para>
<para>有关如何配置 <literal>GitRepo</literal> 以使用
<literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet 的示例，请参见<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml">此处</link>。</para>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo 创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取 <emphasis role="strong">GitRepo</emphasis> 资源：</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>编辑 <emphasis role="strong">GitRepo</emphasis> 配置，在
<literal>spec.targets</literal>
下指定所需的目标列表。默认情况下，<literal>suse-edge/fleet-examples</literal> 中的
<literal>GitRepo</literal> 资源<emphasis role="strong">不会</emphasis>映射到任何下游群集。</para>
<itemizedlist>
<listitem>
<para>为了匹配所有群集，请将默认的 <literal>GitRepo</literal> <emphasis
role="strong">目标</emphasis>更改为：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>或者，如果您要更细致地选择群集，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <emphasis role="strong">GitRepo</emphasis> 资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-default</literal> 名称空间下创建的 <emphasis
role="strong">GitRepo</emphasis> 资源：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-upgrade -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-upgrade      https://github.com/suse-edge/fleet-examples.git   release-3.4.0  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle">
<title>SUC 计划部署 - 捆绑包资源</title>
<para>提供所需<literal>操作系统 SUC 计划</literal>的<emphasis
role="strong">捆绑包</emphasis>资源可以通过以下方式之一进行部署：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的操作系统升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-rancher">
<title>捆绑包创建 - Rancher UI</title>
<para>Edge 团队维护着一个可在以下步骤中使用的即用型<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">捆绑包</link>。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用此捆绑包。</para>
</important>
<para>要通过 Rancher 的 UI 创建捆绑包，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>单击左上角的 <emphasis role="strong">☰ → Continuous Delivery（持续交付）</emphasis></para>
</listitem>
<listitem>
<para>转到 <emphasis role="strong">Advanced</emphasis>（高级）&gt; <emphasis
role="strong">Bundles</emphasis>（捆绑包）</para>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）</para>
</listitem>
<listitem>
<para>此处可通过以下方式之一创建捆绑包：</para>
<note>
<para>在某些使用场景中，您可能需要在捆绑包附带的 <literal>SUC
计划</literal>中包含自定义更改。请确保在以下步骤生成的捆绑包中包含这些更改。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>手动将 <literal>suse-edge/fleet-examples</literal> 中的<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml">捆绑包内容</link>复制到
<emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面。</para>
</listitem>
<listitem>
<para>从所需的<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记克隆
<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库，并在 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面中选择
<emphasis role="strong">Read from File</emphasis>（从文件读取）选项。然后导航到捆绑包位置
(<literal>bundles/day2/system-upgrade-controller-plans/os-upgrade</literal>)
并选择捆绑包文件。这会在<emphasis role="strong">Create from YAML</emphasis>（基于 YAML
创建）页面中自动填充捆绑包内容。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis role="strong">目标</emphasis>群集：</para>
<itemizedlist>
<listitem>
<para>为了匹配所有下游群集，请将默认的捆绑包 <literal>.spec.targets</literal> 更改为：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>有关更精细的下游群集映射，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link>。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create</emphasis>（创建）</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-bundle-manual">
<title>捆绑包创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取<emphasis role="strong">捆绑包</emphasis>资源：</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-upgrade-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/os-upgrade/os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>编辑<literal>捆绑包</literal><emphasis role="strong">目标</emphasis>配置，在
<literal>spec.targets</literal>
下提供所需的目标列表。默认情况下，<literal>suse-edge/fleet-examples</literal>
中的<literal>捆绑包</literal>资源<emphasis role="strong">不会</emphasis>映射到任何下游群集。</para>
<itemizedlist>
<listitem>
<para>为了匹配所有群集，请将默认的<literal>捆绑包</literal><emphasis
role="strong">目标</emphasis>更改为：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>或者，如果您要更细致地选择群集，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将<emphasis role="strong">捆绑包</emphasis>资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-upgrade-bundle.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-default</literal> 名称空间下创建的<emphasis
role="strong">捆绑包</emphasis>资源：</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles -n fleet-default</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-os-upgrade-plan-deployment-third-party">
<title>SUC 计划部署 - 第三方 GitOps 工作流程</title>
<para>在某些使用场景中，用户可能希望将<literal>操作系统 SUC 计划</literal>合并到自己的第三方 GitOps 工作流程（例如
<literal>Flux</literal>）中。</para>
<para>要获取所需的操作系统升级资源，首先请确定您要使用的 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记。</para>
<para>然后，便可以在
<literal>fleets/day2/system-upgrade-controller-plans/os-upgrade</literal>
中找到资源，其中：</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> 是用于<emphasis
role="strong">控制平面</emphasis>节点的 SUC 计划资源。</para>
</listitem>
<listitem>
<para><literal>plan-worker.yaml</literal> 是用于<emphasis
role="strong">工作</emphasis>节点的 SUC 计划资源。</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> 是一个包含 <literal>upgrade.sh</literal>
脚本的机密，该脚本负责创建 systemd.service（<xref
linkend="downstream-day2-fleet-os-upgrade-components-systemd-service"/>）。</para>
</listitem>
<listitem>
<para><literal>config-map.yaml</literal> 是 ConfigMap，提供
<literal>upgrade.sh</literal> 脚本使用的升级配置。</para>
</listitem>
</itemizedlist>
<important>
<para>这些<literal>计划</literal>资源由<literal>系统升级控制器</literal>解释，应部署在您要升级的每个下游群集上。有关
SUC 部署信息，请参见<xref linkend="components-system-upgrade-controller-install"/>。</para>
</important>
<para>为了更好地了解如何使用 GitOps 工作流程来部署操作系统升级的 <emphasis role="strong">SUC
计划</emphasis>，建议查看<xref
linkend="downstream-day2-fleet-os-upgrade-overview"/>。</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade">
<title>Kubernetes 版本升级</title>
<important>
<para>本节介绍<emphasis role="strong">并非</emphasis>通过 Rancher（<xref
linkend="components-rancher"/>）实例创建的下游群集的 Kubernetes 升级过程。有关如何对通过
<literal>Rancher</literal> 创建的群集进行 Kubernetes 版本升级的信息，请参见 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes</link>。</para>
</important>
<para>本节介绍如何使用<xref linkend="components-fleet"/>和<xref
linkend="components-system-upgrade-controller"/>执行 Kubernetes 升级。</para>
<para>本节将介绍以下主题：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-components"/> -
升级过程使用的其他组件。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-overview"/> - 升级过程概述。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-requirements"/> - 升级过程的要求。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment"/> -
关于如何部署负责触发升级过程的 <literal>SUC 计划</literal>的信息。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components">
<title>组件</title>
<para>本节介绍 <literal>K8s 升级</literal>过程中使用的自定义组件，这些组件与默认“Day 2”组件（<xref
linkend="downstream-day2-fleet-components"/>）不同。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade">
<title>rke2-upgrade</title>
<para>容器映像负责升级特定节点的 RKE2 版本。</para>
<para>此组件由 <emphasis role="strong">SUC</emphasis> 根据 <emphasis role="strong">SUC
计划</emphasis>创建的 Pod 分发。该计划应位于需要进行 RKE2 升级的每个<emphasis
role="strong">群集</emphasis>上。</para>
<para>有关 <literal>rke2-upgrade</literal> 映像如何执行升级的详细信息，请参见<link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">上游</link>文档。</para>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade">
<title>k3s-upgrade</title>
<para>容器映像负责升级特定节点的 K3s 版本。</para>
<para>此组件由 <emphasis role="strong">SUC</emphasis> 根据 <emphasis role="strong">SUC
计划</emphasis>创建的 Pod 分发。该计划应位于需要进行 K3s 升级的每个<emphasis
role="strong">群集</emphasis>上。</para>
<para>有关 <literal>k3s-upgrade</literal> 映像如何执行升级的详细信息，请参见<link
xl:href="https://github.com/k3s-io/k3s-upgrade">上游</link>文档。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-overview">
<title>概述</title>
<para>通过 <literal>Fleet</literal> 和<literal>系统升级控制器 (SUC)</literal> 来为下游群集节点升级
Kubernetes 发行版。</para>
<para><literal>Fleet</literal> 用于将 <literal>SUC 计划</literal>部署到目标群集并对其进行管理。</para>
<note>
<para><literal>SUC 计划</literal>是一种<link
xl:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">自定义资源</link>，描述了
<emphasis role="strong">SUC</emphasis> 为在一组节点上执行特定任务而需要遵循的步骤。有关 <literal>SUC
计划</literal>的示例，请参见<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">上游储存库</link>。</para>
</note>
<para>通过向特定 Fleet <link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">工作区</link>部署
<link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link> 或<link
xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源将 <literal>K8s SUC
计划</literal>分发到各个群集。Fleet 会获取已部署的
<literal>GitRepo/捆绑包</literal>，并将其内容（<literal>K8s SUC 计划</literal>）部署到目标群集。</para>
<note>
<para><literal>GitRepo/捆绑包</literal>资源始终部署在<literal>管理群集</literal>上。使用
<literal>GitRepo</literal>
还是<literal>捆绑包</literal>资源取决于具体应用场景，有关详细信息，请参见<xref
linkend="downstream-day2-fleet-determine-use-case"/>。</para>
</note>
<para><literal>K8s SUC 计划</literal>描述了以下工作流程：</para>
<orderedlist numeration="arabic">
<listitem>
<para>执行 K8s 升级前，务必要<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">封锁</link>节点。</para>
</listitem>
<listitem>
<para>务必先升级<literal>控制平面</literal>节点，再升级<literal>工作</literal>节点。</para>
</listitem>
<listitem>
<para>始终一次升级<emphasis
role="strong">一个</emphasis><literal>控制平面</literal>节点，一次升级<emphasis
role="strong">两个</emphasis><literal>工作</literal>节点。</para>
</listitem>
</orderedlist>
<para>部署 <literal>K8s SUC 计划</literal>后，工作流程如下：</para>
<orderedlist numeration="arabic">
<listitem>
<para>SUC 协调已部署的 <literal>K8s SUC 计划</literal>，并在<emphasis
role="strong">每个节点</emphasis>上创建一个 <literal>Kubernetes 作业</literal>。</para>
</listitem>
<listitem>
<para>根据 Kubernetes 发行版的不同，该作业将创建一个运行 rke2-upgrade（<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-rke2-upgrade"/>）或
k3s-upgrade（<xref
linkend="downstream-day2-fleet-k8s-upgrade-components-k3s-upgrade"/>）容器映像的
Pod。</para>
</listitem>
<listitem>
<para>创建的 Pod 将执行以下工作流程：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>用 <literal>rke2-upgrade/k3s-upgrade</literal> 映像中的二进制文件替换节点上现有的
<literal>rke2/k3s</literal> 二进制文件。</para>
</listitem>
<listitem>
<para>终止正在运行的 <literal>rke2/k3s</literal> 进程。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>终止 <literal>rke2/k3s</literal> 进程会触发重启，启动运行已更新二进制文件的新进程，从而实现 Kubernetes
发行版版本的升级。</para>
</listitem>
</orderedlist>
<para>下面是上述流程的示意图：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-day2-downstream-k8s-upgrade.png"
width="100%"/> </imageobject>
<textobject><phrase>Fleet Day2 下游 k8s 升级</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-requirements">
<title>要求</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">备份您的 Kubernetes 发行版：</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>对于 <emphasis role="strong">RKE2 群集</emphasis>，请参见 <link
xl:href="https://docs.rke2.io/datastore/backup_restore">RKE2 Backup and
Restore</link> 文档。</para>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s 群集</emphasis>，请参见 <link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and
Restore</link> 文档。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">确保 SUC 计划容忍度与节点容忍度相匹配</emphasis> - 如果您的 Kubernetes
群集节点具有自定义<emphasis role="strong">污点</emphasis>，请确保在 <emphasis
role="strong">SUC 计划</emphasis>中为这些污点添加<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">容忍度</link>。默认情况下，<emphasis
role="strong">SUC 计划</emphasis>仅包含<emphasis
role="strong">控制平面</emphasis>节点的容忍度。默认容忍度包括：</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>其他任何容忍度必须添加到每个计划的 <literal>.spec.tolerations</literal> 部分下。与 Kubernetes
版本升级相关的 <emphasis role="strong">SUC 计划</emphasis>可以在 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
储存库中的以下位置找到：</para>
<itemizedlist>
<listitem>
<para>对于 <emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">请确保使用有效储存库<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记中的计划。</emphasis></para>
<para>为 RKE2 <emphasis role="strong">控制平面</emphasis> SUC 计划定义自定义容忍度的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-upgrade-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment">
<title>K8s 升级 - SUC 计划部署</title>
<important>
<para>对于之前使用此过程升级的环境，用户应确保完成以下步骤之<emphasis role="strong">一</emphasis>：</para>
<itemizedlist>
<listitem>
<para><literal>从下游群集中去除任何先前部署且与旧版 Edge 相关的 SUC 计划</literal> - 方法是从现有的
<literal>GitRepo/捆绑包</literal><link
xl:href="https://fleet.rancher.io/gitrepo-targets#target-matching">目标配置</link>中去除相应群集，或完全去除
<literal>GitRepo/捆绑包</literal>资源。</para>
</listitem>
<listitem>
<para><literal>重用现有的 GitRepo/捆绑包资源</literal> - 方法是将资源的修订版指向一个新标签，该标签包含目标
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>的正确
Fleet。</para>
</listitem>
</itemizedlist>
<para>这样做是为了避免旧版 Edge 的 <literal>SUC 计划</literal>之间发生冲突。</para>
<para>如果用户尝试升级，而下游群集上存在现有的 <literal>SUC 计划</literal>，他们将看到以下 Fleet 错误：</para>
<screen language="bash" linenumbering="unnumbered">Not installed: Unable to continue with install: Plan &lt;plan_name&gt; in namespace &lt;plan_namespace&gt; exists and cannot be imported into the current release: invalid ownership metadata; annotation validation error..</screen>
</important>
<para>如<xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>中所述，可以通过以下任意一种方式将
<literal>SUC 计划</literal>分发到目标群集来完成 Kubernetes 升级：</para>
<itemizedlist>
<listitem>
<para>Fleet GitRepo 资源（<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo"/>）</para>
</listitem>
<listitem>
<para>Fleet 捆绑包资源（<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle"/>）</para>
</listitem>
</itemizedlist>
<para>要确定使用哪个资源，请参见<xref linkend="downstream-day2-fleet-determine-use-case"/>。</para>
<para>对于希望通过第三方 GitOps 工具部署 <literal>K8s SUC 计划</literal>的使用场景，请参见<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party"/>。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo">
<title>SUC 计划部署 - GitRepo 资源</title>
<para>提供所需 <literal>K8s SUC 计划</literal>的 <emphasis
role="strong">GitRepo</emphasis> 资源可通过以下方式之一进行部署：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的 Kubernetes 升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-rancher">
<title>GitRepo 创建 - Rancher UI</title>
<para>要通过 Rancher UI 创建 <literal>GitRepo</literal> 资源，请遵循其官方<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">文档</link>。</para>
<para>Edge 团队为 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/rke2-upgrade">rke2</link>
和 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/system-upgrade-controller-plans/k3s-upgrade">k3s</link>
这两种 Kubernetes 发行版维护着即用型 Fleet。根据您的环境的不同，这些 Fleet 可以直接使用或用作模板。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用这些
Fleet。</para>
</important>
<para>对于不需要在这些 Fleet 附带的 <literal>SUC 计划</literal>中包含自定义更改的使用场景，用户可以直接引用
<literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet。</para>
<para>如果需要自定义更改（例如添加自定义容忍度），用户应从单独的储存库中引用 Fleet，以便能够根据需要将更改添加到 SUC 计划中。</para>
<para>使用 <literal>suse-edge/fleet-examples</literal> 储存库中的 Fleet 配置
<literal>GitRepo</literal> 资源的示例：</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml">RKE2</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml">K3s</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-gitrepo-manual">
<title>GitRepo 创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取 <emphasis role="strong">GitRepo</emphasis> 资源：</para>
<itemizedlist>
<listitem>
<para>对于 <emphasis role="strong">RKE2</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>编辑 <emphasis role="strong">GitRepo</emphasis> 配置，在
<literal>spec.targets</literal>
下指定所需的目标列表。默认情况下，<literal>suse-edge/fleet-examples</literal> 中的
<literal>GitRepo</literal> 资源<emphasis role="strong">不会</emphasis>映射到任何下游群集。</para>
<itemizedlist>
<listitem>
<para>为了匹配所有群集，请将默认的 <literal>GitRepo</literal> <emphasis
role="strong">目标</emphasis>更改为：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>或者，如果您要更细致地选择群集，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将 <emphasis role="strong">GitRepo</emphasis> 资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-default</literal> 名称空间下创建的 <emphasis
role="strong">GitRepo</emphasis> 资源：</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   fleet-default   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle">
<title>SUC 计划部署 - 捆绑包资源</title>
<para>可通过以下方式之一部署<emphasis role="strong">捆绑包</emphasis>资源，该资源中附带了所需的
<literal>Kubernetes 升级 SUC 计划</literal>：</para>
<orderedlist numeration="arabic">
<listitem>
<para>通过 <literal>Rancher UI</literal> 部署 - <xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher"/>（如果
<literal>Rancher</literal> 可用）。</para>
</listitem>
<listitem>
<para>手动将相应资源部署（<xref
linkend="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual"/>）到<literal>管理群集</literal>。</para>
</listitem>
</orderedlist>
<para>部署后，要监控目标群集节点的 Kubernetes 升级过程，请参见<xref
linkend="components-system-upgrade-controller-monitor-plans"/>。</para>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-rancher">
<title>捆绑包创建 - Rancher UI</title>
<para>Edge 团队为 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">rke2</link>
和 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">k3s</link>
这两种 Kubernetes 发行版维护着即用型捆绑包。根据您的环境的不同，这些捆绑包可以直接使用或用作模板。</para>
<important>
<para>请务必通过有效的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记使用此捆绑包。</para>
</important>
<para>要通过 Rancher 的 UI 创建捆绑包，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>单击左上角的 <emphasis role="strong">☰ → Continuous Delivery（持续交付）</emphasis></para>
</listitem>
<listitem>
<para>转到 <emphasis role="strong">Advanced</emphasis>（高级）&gt; <emphasis
role="strong">Bundles</emphasis>（捆绑包）</para>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）</para>
</listitem>
<listitem>
<para>此处可通过以下方式之一创建捆绑包：</para>
<note>
<para>在某些使用场景中，您可能需要在捆绑包附带的 <literal>SUC
计划</literal>中包含自定义更改。请确保在以下步骤生成的捆绑包中包含这些更改。</para>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<para>手动将 <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">RKE2</link>
或 <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">K3s</link>
的捆绑包内容从 <literal>suse-edge/fleet-examples</literal> 复制到 <emphasis
role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面。</para>
</listitem>
<listitem>
<para>从所需的<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记克隆
<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
储存库，并在 <emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面中选择
<emphasis role="strong">Read from File</emphasis>（从文件读取）选项。然后导航到所需的捆绑包（对于
RKE2，为
<literal>bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>；对于
K3s，为
<literal>bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>）。这会在
<emphasis role="strong">Create from YAML</emphasis>（基于 YAML 创建）页面中自动填充捆绑包内容。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>更改<literal>捆绑包</literal>的<emphasis role="strong">目标</emphasis>群集：</para>
<itemizedlist>
<listitem>
<para>为了匹配所有下游群集，请将默认的捆绑包 <literal>.spec.targets</literal> 更改为：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>有关更精细的下游群集映射，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link>。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>选择 <emphasis role="strong">Create</emphasis>（创建）</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-bundle-manual">
<title>捆绑包创建 - 手动</title>
<orderedlist numeration="arabic">
<listitem>
<para>提取<emphasis role="strong">捆绑包</emphasis>资源：</para>
<itemizedlist>
<listitem>
<para>对于 <emphasis role="strong">RKE2</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>对于 <emphasis role="strong">K3s</emphasis> 群集：</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/refs/tags/release-3.4.0/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>编辑<literal>捆绑包</literal><emphasis role="strong">目标</emphasis>配置，在
<literal>spec.targets</literal>
下提供所需的目标列表。默认情况下，<literal>suse-edge/fleet-examples</literal>
中的<literal>捆绑包</literal>资源<emphasis role="strong">不会</emphasis>映射到任何下游群集。</para>
<itemizedlist>
<listitem>
<para>为了匹配所有群集，请将默认的<literal>捆绑包</literal><emphasis
role="strong">目标</emphasis>更改为：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>或者，如果您要更细致地选择群集，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>将<emphasis role="strong">捆绑包</emphasis>资源应用于<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>查看 <literal>fleet-default</literal> 名称空间下创建的<emphasis
role="strong">捆绑包</emphasis>资源：</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="downstream-day2-fleet-k8s-upgrade-plan-deployment-third-party">
<title>SUC 计划部署 - 第三方 GitOps 工作流程</title>
<para>在某些使用场景中，用户可能希望将 <literal>Kubernetes 升级 SUC 计划</literal>合并到自己的第三方 GitOps
工作流程（例如 <literal>Flux</literal>）中。</para>
<para>要获取所需的 K8s 升级资源，首先请确定您要使用的 <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
储存库的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记。</para>
<para>然后，便可以在以下位置找到资源：</para>
<itemizedlist>
<listitem>
<para>对于 RKE2 群集升级：</para>
<itemizedlist>
<listitem>
<para>对于<literal>控制平面</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>对于<literal>工作</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>对于 K3s 群集升级：</para>
<itemizedlist>
<listitem>
<para>对于<literal>控制平面</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>对于<literal>工作</literal>节点 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>这些<literal>计划</literal>资源由<literal>系统升级控制器</literal>解释，应部署在您要升级的每个下游群集上。有关
SUC 部署信息，请参见<xref linkend="components-system-upgrade-controller-install"/>。</para>
</important>
<para>为了更好地了解如何使用 GitOps 工作流程来部署 Kubernetes 版本升级的 <emphasis role="strong">SUC
计划</emphasis>，建议查看使用 <literal>Fleet</literal> 进行更新的过程概述（<xref
linkend="downstream-day2-fleet-k8s-upgrade-overview"/>）。</para>
</section>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade">
<title>Helm chart 升级</title>
<para>本节介绍如下内容：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-air-gap"/> - 包含有关如何将与 Edge
相关的 OCI chart 和映像分发到您的专用仓库的信息。</para>
</listitem>
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure"/> - 包含有关不同 Helm
chart 升级情形及其升级过程的信息。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-air-gap">
<title>为隔离环境做好准备</title>
<section xml:id="id-ensure-you-have-access-to-your-helm-chart-fleet-2">
<title>确保您有权访问 Helm chart 的 Fleet</title>
<para>根据环境支持的情况，选择以下选项之一：</para>
<orderedlist numeration="arabic">
<listitem>
<para>将 chart 的 Fleet 资源托管在<literal>管理群集</literal>可访问的本地 git 服务器上。</para>
</listitem>
<listitem>
<para>使用 Fleet 的 CLI <link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm chart 转换为捆绑包</link>，您可直接使用该捆绑包，无需将其托管在其他位置。可从 Fleet 的 <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">版本</link>页面获取其
CLI；对于 Mac 用户，可通过 <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulae 获取。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version-2">
<title>找到 Edge 发行版本的所需资产</title>
<orderedlist numeration="arabic">
<listitem>
<para>转到“Day 2”<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>页面，找到您要将
chart 升级到的 Edge 版本，然后单击 <emphasis role="strong">Assets</emphasis>（资产）。</para>
</listitem>
<listitem>
<para>从<emphasis role="strong">“Assets”</emphasis>（资产）部分下载以下文件：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">版本文件</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">说明</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>提取 <literal>edge-release-images.txt</literal> 文件中指定的映像，并将其封装到“.tar.gz”归档中。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>提取与特定 Edge 版本相关的 OCI chart 映像，并将其封装到“.tar.gz”归档中。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>从“.tar.gz”归档加载映像，将它们重新标记并推送到专用仓库。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>接收包含 Edge OCI '.tgz' chart 软件包的目录作为参数，并将这些软件包加载到专用仓库中。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>包含与特定 Edge 版本相关的 OCI chart 映像的列表。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>包含与特定 Edge 版本相关的映像列表。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-release-images-archive-2">
<title>创建 Edge 版本映像归档</title>
<para><emphasis>在可以访问互联网的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>将 <literal>edge-save-images.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>生成映像归档：</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>这将创建一个名为 <literal>edge-images.tar.gz</literal> 的可加载归档。</para>
<note>
<para>如果指定了 <literal>-i|--images</literal> 选项，归档的名称可能会不同。</para>
</note>
</listitem>
<listitem>
<para>将此归档复制到<emphasis role="strong">隔离的</emphasis>计算机：</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-edge-oci-chart-images-archive-2">
<title>创建 Edge OCI chart 映像归档</title>
<para><emphasis>在可以访问互联网的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>将 <literal>edge-save-oci-artefacts.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>生成 OCI chart 映像归档：</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>这将创建一个名为 <literal>oci-artefacts.tar.gz</literal> 的归档。</para>
<note>
<para>如果指定了 <literal>-a|--archive</literal> 选项，归档的名称可能会不同。</para>
</note>
</listitem>
<listitem>
<para>将此归档复制到<emphasis role="strong">隔离的</emphasis>计算机：</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-edge-release-images-to-your-air-gapped-machine-2">
<title>将 Edge 版本映像加载到隔离的计算机上</title>
<para><emphasis>在隔离的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>登录到专用仓库（如果需要）：</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>将 <literal>edge-load-images.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>执行脚本，传递之前<emphasis role="strong">复制的</emphasis>
<literal>edge-images.tar.gz</literal> 归档：</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
<note>
<para>这将从 <literal>edge-images.tar.gz</literal> 加载所有映像，并将它们重新标记并推送到
<literal>--registry</literal> 选项下指定的仓库。</para>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-the-edge-oci-chart-images-to-your-air-gapped-machine-2">
<title>将 Edge OCI chart 映像加载到隔离的计算机上</title>
<para><emphasis>在隔离的计算机上：</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>登录到专用仓库（如果需要）：</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>将 <literal>edge-load-oci-artefacts.sh</literal> 设为可执行文件：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>解压缩复制的 <literal>oci-artefacts.tar.gz</literal> 归档：</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>这会使用命名模板 <literal>edge-release-oci-tgz-&lt;date&gt;</literal> 生成一个目录</para>
</listitem>
<listitem>
<para>将此目录传递给 <literal>edge-load-oci-artefacts.sh</literal> 脚本，以将 Edge OCI chart
映像加载到专用仓库中：</para>
<note>
<para>此脚本假设您的环境中已预装了 <literal>Helm</literal> CLI。有关 Helm 安装说明，请参见<link
xl:href="https://helm.sh/docs/intro/install/">安装 Helm</link>。</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-your-private-registry-in-your-kubernetes-distribution-2">
<title>在 Kubernetes 发行版中配置专用仓库</title>
<para>对于 RKE2，请参见 <link
xl:href="https://docs.rke2.io/install/private_registry">Private Registry
Configuration</link></para>
<para>对于 K3s，请参见 <link
xl:href="https://docs.k3s.io/installation/private-registry">Private Registry
Configuration</link></para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure">
<title>升级过程</title>
<para>本节重点介绍以下使用场景的 Helm 升级过程：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/></para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart"/></para>
</listitem>
</orderedlist>
<important>
<para>手动部署的 Helm chart 无法可靠升级。我们建议使用<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster"/>中所述的方法重新部署这些
Helm chart。</para>
</important>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster">
<title>我有一个新群集，想要部署和管理 Edge Helm chart</title>
<para>本节介绍如何：</para>
<orderedlist numeration="arabic">
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare"/>。</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy"/>。</para>
</listitem>
<listitem>
<para><xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage"/>。</para>
</listitem>
</orderedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-prepare">
<title>为您的 chart 准备 Fleet 资源</title>
<orderedlist numeration="arabic">
<listitem>
<para>从您要使用的 Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">版本</link>标记处获取
Chart 的 Fleet 资源。</para>
</listitem>
<listitem>
<para>导航到 Helm chart Fleet
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)</para>
</listitem>
<listitem>
<para><emphasis role="strong">如果您打算使用 GitOps 工作流程</emphasis>，请将 chart Fleet 目录复制到
Git 储存库，从那里执行 GitOps。</para>
</listitem>
<listitem>
<para><emphasis role="strong">（可选）</emphasis>如果需要配置 Helm chart 的<emphasis
role="strong">值</emphasis>才能使用 Helm chart，请编辑复制的目录中
<literal>fleet.yaml</literal> 文件内的 <literal>.helm.values</literal> 配置。</para>
</listitem>
<listitem>
<para><emphasis role="strong">（可选）</emphasis>在某些使用场景中，您可能需要向 chart 的 Fleet
目录添加其他资源，使该目录能够更好地适应您的环境。有关如何增强 Fleet 目录的信息，请参见 <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git 储存库内容</link>。</para>
</listitem>
</orderedlist>
<note>
<para>在某些情况下，Fleet 为 Helm 操作设置的默认超时时长可能不足，导致发生以下错误：</para>
<screen language="bash" linenumbering="unnumbered">failed pre-install: context deadline exceeded</screen>
<para>在此类情况下，请在 <literal>fleet.yaml</literal> 文件的 <literal>helm</literal> 配置下添加
<link
xl:href="https://fleet.rancher.io/ref-crds#helmoptions">timeoutSeconds</link>
属性。</para>
</note>
<para><literal>longhorn</literal> Helm chart 的<emphasis
role="strong">示例</emphasis>如下：</para>
<itemizedlist>
<listitem>
<para>用户 Git 储存库结构：</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
├── longhorn
│   └── fleet.yaml
└── longhorn-crd
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>填充了用户 <literal>Longhorn</literal> 数据的 <literal>fleet.yaml</literal> 内容：</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  # timeoutSeconds: 10
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.rancher.io/"
  version: "107.0.0+up1.9.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>上面只是一些示例值，用于演示基于 <literal>longhorn</literal> chart 创建的自定义配置。<emphasis
role="strong">不应</emphasis>将它们视为 <literal>longhorn</literal> chart 的部署指南。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy">
<title>部署您的 chart 的 Fleet</title>
<para>您可以使用 GitRepo（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo"/>）或捆绑包（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle"/>）来部署
chart 的 Fleet。</para>
<note>
<para>部署 Fleet 时，如果收到资源<literal>已修改</literal>消息，请确保在 Fleet 的
<literal>diff</literal> 部分添加相应的 <literal>comparePatches</literal>
项。有关详细信息，请参见 <link
xl:href="https://fleet.rancher.io/bundle-diffs">Generating Diffs to Ignore
Modified GitRepos</link>。</para>
</note>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-gitrepo">
<title>GitRepo</title>
<para>Fleet 的 <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>
资源包含如何访问 chart 的 Fleet 资源以及需要将这些资源应用于哪些群集的相关信息。</para>
<para>可以通过 <link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Rancher
UI</link> 部署 <literal>GitRepo</literal>
资源，也可以通过将该资源部署到<literal>管理群集</literal>的方式手动<link
xl:href="https://fleet.rancher.io/tut-deployment">部署</link>该资源。</para>
<para>用于<emphasis role="strong">手动</emphasis>部署的 <emphasis
role="strong">Longhorn</emphasis> <literal>GitRepo</literal> 资源示例：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: user_repository_tag
  #
  # If using a branch
  # branch: user_repository_branch
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  - longhorn-crd
  repo: user_repository_url
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-deploy-bundle">
<title>捆绑包</title>
<para><link xl:href="https://fleet.rancher.io/bundle-add">捆绑包</link>资源包含需要由 Fleet
部署的原始 Kubernetes 资源。一般情况下，建议使用 <literal>GitRepo</literal> 方法，但对于无法支持本地 Git
服务器的隔离环境，<literal>捆绑包</literal>可以帮助您将 Helm chart Fleet 传播到目标群集。</para>
<para><literal>捆绑包</literal>的部署可以通过 Rancher UI（<literal>Continuous Delivery（持续交付）→
Advanced（高级）→ Bundles（捆绑包）→ Create from YAML（基于 YAML
创建）</literal>）进行，也可以通过在正确的 Fleet 名称空间中手动部署<literal>捆绑包</literal>资源来完成。有关
Fleet 名称空间的信息，请参见上游<link
xl:href="https://fleet.rancher.io/namespaces#gitrepos-bundles-clusters-clustergroups">文档</link>。</para>
<para>可以利用 Fleet 的<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm Chart 转换为捆绑包</link>方法创建用于 Edge Helm chart 的<literal>捆绑包</literal>。</para>
<para>下面的示例展示了如何基于 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
和 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
Helm chart Fleet
模板创建<literal>捆绑包</literal>资源，以及如何手动将此捆绑包部署到<literal>管理群集</literal>。</para>
<note>
<para>为了阐明工作流程，下面的示例使用了 <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
目录结构。</para>
</note>
<orderedlist numeration="arabic">
<listitem>
<para>导航到 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn/fleet.yaml">longhorn</link>
chart Fleet 模板：</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn</screen>
</listitem>
<listitem>
<para>创建 <literal>targets.yaml</literal> 文件，该文件将指示 Fleet 应将 Helm chart 部署到哪些群集：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>若要更精细地选择下游群集，请参见<link
xl:href="https://fleet.rancher.io/gitrepo-targets">映射到下游群集</link>。</para>
</listitem>
<listitem>
<para>使用 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link> 将
<literal>Longhorn</literal> Helm chart Fleet 转换为捆绑包资源。</para>
<note>
<para>可从 Fleet 的<link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">版本</link><emphasis
role="strong">资产</emphasis>页面获取 Fleet 的
CLI(<literal>fleet-linux-amd64</literal>).。</para>
<para>对于 Mac 用户，有一个 <link
xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>
Homebrew Formulae。</para>
</note>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-bundle &gt; longhorn-bundle.yaml</screen>
</listitem>
<listitem>
<para>导航到 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/fleets/day2/chart-templates/longhorn/longhorn-crd/fleet.yaml">longhorn-crd</link>
chart Fleet 模板：</para>
<screen language="bash" linenumbering="unnumbered">cd fleets/day2/chart-templates/longhorn/longhorn-crd</screen>
</listitem>
<listitem>
<para>创建 <literal>targets.yaml</literal> 文件，该文件将指示 Fleet 应将 Helm chart 部署到哪些群集：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# Matches all downstream clusters
- clusterSelector: {}
EOF</screen>
</listitem>
<listitem>
<para>使用 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet">fleet-cli</link> 将
<literal>Longhorn CRD</literal> Helm chart Fleet 转换为捆绑包资源。</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - longhorn-crd-bundle &gt; longhorn-crd-bundle.yaml</screen>
</listitem>
<listitem>
<para>将 <literal>longhorn-bundle.yaml</literal> 和
<literal>longhorn-crd-bundle.yaml</literal> 文件部署到<literal>管理群集</literal>：</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f longhorn-crd-bundle.yaml
kubectl apply -f longhorn-bundle.yaml</screen>
</listitem>
</orderedlist>
<para>按照以上步骤操作，将 <literal>SUSE Storage</literal> 部署到所有指定的下游群集上。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-new-cluster-manage">
<title>管理部署的 Helm chart</title>
<para>通过 Fleet 完成部署后，若要进行 Helm chart 升级，请参见<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart"/>。</para>
</section>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-fleet-managed-chart">
<title>我想升级 Fleet 管理的 Helm chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>确定需要将 chart 升级到哪个版本，以使其与目标 Edge 版本兼容。有关每个 Edge 版本兼容的 Helm chart
版本，可参见发行说明（<xref linkend="release-notes"/>）。</para>
</listitem>
<listitem>
<para>在 Fleet 监控的 Git 储存库中，根据发行说明（<xref linkend="release-notes"/>）中所述，将 Helm chart
的 <literal>fleet.yaml</literal> 文件中的 chart <emphasis
role="strong">版本</emphasis>和<emphasis role="strong">储存库</emphasis>更改为正确的值。</para>
</listitem>
<listitem>
<para>提交更改并将其推送到储存库后，会触发所需 Helm chart 的升级</para>
</listitem>
</orderedlist>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart">
<title>我要升级通过 EIB 部署的 Helm chart</title>
<para><xref linkend="components-eib"/> 通过创建 <literal>HelmChart</literal> 资源并利用
<link xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link> Helm 集成功能引入的
<literal>helm-controller</literal> 来部署 Helm chart。</para>
<para>为确保通过 <literal>EIB</literal> 部署的 Helm chart 成功升级，用户需要对相应的
<literal>HelmChart</literal> 资源执行升级操作。</para>
<para>下文提供了以下信息：</para>
<itemizedlist>
<listitem>
<para>升级过程的一般概述（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>）。</para>
</listitem>
<listitem>
<para>必要的升级步骤（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>）。</para>
</listitem>
<listitem>
<para>展示使用所述方法进行 <link xl:href="https://longhorn.io">Longhorn</link> chart
升级的示例（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>）。</para>
</listitem>
<listitem>
<para>如何使用其他 GitOps 工具完成升级过程（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party"/>）。</para>
</listitem>
</itemizedlist>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview">
<title>概述</title>
<para>通过 <literal>EIB</literal> 部署的 Helm chart 由名为 <link
xl:href="https://github.com/suse-edge/fleet-examples/tree/release-3.4.0/fleets/day2/eib-charts-upgrader">eib-charts-upgrader</link>
的 <literal>Fleet</literal> 执行升级。</para>
<para>该 <literal>Fleet</literal> 会处理<emphasis
role="strong">用户提供的</emphasis>数据，以<emphasis role="strong">更新</emphasis>一组特定的
HelmChart 资源。</para>
<para>更新这些资源会触发 <link
xl:href="https://github.com/k3s-io/helm-controller">helm-controller</link>，后者会<emphasis
role="strong">升级</emphasis>与修改后的 <literal>HelmChart</literal> 资源相关联的 Helm
chart。</para>
<para>用户只需执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>从本地<link xl:href="https://helm.sh/docs/helm/helm_pull/">提取</link>需要升级的每个
Helm chart 的归档。</para>
</listitem>
<listitem>
<para>将这些归档传递给 <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/generate-chart-upgrade-data.sh">generate-chart-upgrade-data.sh</link>
<literal>generate-chart-upgrade-data.sh</literal> 脚本，该脚本会将这些归档中的数据包含到
<literal>eib-charts-upgrader</literal> Fleet 中。</para>
</listitem>
<listitem>
<para>将 <literal>eib-charts-upgrader</literal> Fleet
部署到<literal>管理群集</literal>。此操作可以通过 <literal>GitRepo</literal>
或<literal>捆绑包</literal>资源来完成。</para>
</listitem>
</orderedlist>
<para>部署后，<literal>eib-charts-upgrader</literal> 会借助 Fleet 将其资源分发到目标下游群集。</para>
<para>这些资源包括：</para>
<orderedlist numeration="arabic">
<listitem>
<para>一组存储<emphasis role="strong">用户提供的</emphasis> Helm chart
数据的<literal>机密</literal>。</para>
</listitem>
<listitem>
<para>一个会部署 <literal>Pod</literal> 的 <literal>Kubernetes 作业</literal>，该 Pod
会挂载前面提到的<literal>机密</literal>，并根据这些机密对相应的 HelmChart 资源进行<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/">修补</link>。</para>
</listitem>
</orderedlist>
<para>如前文所述，这会触发 <literal>helm-controller</literal>，进而执行实际的 Helm chart 升级。</para>
<para>下面是上述流程的示意图：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="fleet-day2-downstream-helm-eib-upgrade.png" width="100%"/>
</imageobject>
<textobject><phrase>Fleet Day2 下游 Helm EIB 升级</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps">
<title>升级步骤</title>
<orderedlist numeration="arabic">
<listitem>
<para>从正确的版本<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">标记</link>处克隆
<literal>suse-edge/fleet-example</literal> 储存库。</para>
</listitem>
<listitem>
<para>创建用于存储所提取的 Helm chart 归档的目录。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>在新创建的归档目录中，<link
xl:href="https://helm.sh/docs/helm/helm_pull/">提取</link>要升级的 Helm chart 的归档：</para>
<screen language="bash" linenumbering="unnumbered">cd archives
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>从目标<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">版本标记</link>的<emphasis
role="strong">资产</emphasis>中下载
<literal>generate-chart-upgrade-data.sh</literal> 脚本。</para>
</listitem>
<listitem>
<para>执行 <literal>generate-chart-upgrade-data.sh</literal> 脚本：</para>
<screen language="bash" linenumbering="unnumbered">chmod +x ./generate-chart-upgrade-data.sh

./generate-chart-upgrade-data.sh --archive-dir /foo/bar/archives/ --fleet-path /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>对于 <literal>--archive-dir</literal> 目录中的每个 chart 归档，该脚本都会生成一个包含 chart 升级数据的
<literal>Kubernetes 机密 YAML</literal> 文件，并将其存储在
<literal>--fleet-path</literal> 指定的 Fleet 的 <literal>base/secrets</literal>
目录中。</para>
<para><literal>generate-chart-upgrade-data.sh</literal> 脚本还会对 Fleet 进行其他修改，以确保生成的
<literal>Kubernetes 机密 YAML</literal> 文件能被 Fleet 部署的工作负载正确使用。</para>
<important>
<para>用户不应更改 <literal>generate-chart-upgrade-data.sh</literal> 脚本生成的内容。</para>
</important>
</listitem>
</orderedlist>
<para>以下步骤因您运行的环境而异：</para>
<orderedlist numeration="arabic">
<listitem>
<para>对于支持 GitOps 的环境（例如：非隔离环境，或虽是隔离环境但支持本地 Git 服务器）：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>将 <literal>fleets/day2/eib-charts-upgrader</literal> Fleet 复制到将用于 GitOps
的储存库中。</para>
<note>
<para>确保该 Fleet 包含 <literal>generate-chart-upgrade-data.sh</literal> 脚本所做的更改。</para>
</note>
</listitem>
<listitem>
<para>配置将用于提供 <literal>eib-charts-upgrader</literal> Fleet 所有资源的
<literal>GitRepo</literal> 资源。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>有关通过 Rancher UI 配置和部署 <literal>GitRepo</literal> 的信息，请参见<link
xl:href="https://ranchermanager.docs.rancher.com/v2.12/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">在
Rancher UI 中访问 Fleet</link>。</para>
</listitem>
<listitem>
<para>有关 <literal>GitRepo</literal> 的手动配置和部署过程，请参见<link
xl:href="https://fleet.rancher.io/tut-deployment">创建部署</link>。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>对于不支持 GitOps 的环境（例如，不允许使用本地 Git 服务器的隔离环境）：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>从 <literal>rancher/fleet</literal> <link
xl:href="https://github.com/rancher/fleet/releases/tag/v0.13.1">版本</link>页面下载
<literal>fleet-cli</literal> 二进制文件。对于 Linux，请下载
<literal>fleet-linux-amd64</literal>。对于 Mac 用户，可以使用 Homebrew Formulae -
<link xl:href="https://formulae.brew.sh/formula/fleet-cli">fleet-cli</link>。</para>
</listitem>
<listitem>
<para>导航到 <literal>eib-charts-upgrader</literal> Fleet：</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>创建指示 Fleet 在哪里部署资源的 <literal>targets.yaml</literal> 文件：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
# To match all downstream clusters
- clusterSelector: {}
EOF</screen>
<para>有关如何映射目标群集的信息，请参见上游<link
xl:href="https://fleet.rancher.io/gitrepo-targets">文档</link>。</para>
</listitem>
<listitem>
<para>使用 <literal>fleet-cli</literal> 将 Fleet 转换为<literal>捆绑包</literal>资源：</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
<para>这将创建一个捆绑包 (<literal>bundle.yaml</literal>)，其中包含来自
<literal>eib-charts-upgrader</literal> Fleet 的所有模板资源。</para>
<para>有关 <literal>fleet apply</literal> 命令的详细信息，请参见 <link
xl:href="https://fleet.rancher.io/cli/fleet-cli/fleet_apply">fleet
apply</link>。</para>
<para>有关将 Fleet 转换为捆绑包的详细信息，请参见<link
xl:href="https://fleet.rancher.io/bundle-add#convert-a-helm-chart-into-a-bundle">将
Helm Chart 转换为捆绑包</link>。</para>
</listitem>
<listitem>
<para>通过以下方式之一部署<literal>捆绑包</literal>：</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>通过 Rancher UI - 导航到 <emphasis role="strong">Continuous Delivery（持续交付）→
Advanced（高级）→ Bundles（捆绑包）→ Create from YAML（基于 YAML 创建）</emphasis>，然后粘贴
<literal>bundle.yaml</literal> 内容，或单击 <literal>Read from
File</literal>（从文件读取）选项并传递文件。</para>
</listitem>
<listitem>
<para>手动 - 在<literal>管理群集</literal>内手动部署 <literal>bundle.yaml</literal> 文件。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>执行这些步骤可成功部署 <literal>GitRepo/捆绑包</literal>资源。资源将由 Fleet
拾取，且其内容将部署到用户在之前的步骤中指定的目标群集上。有关该过程的概述，请参见<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>。</para>
<para>有关如何跟踪升级过程的信息，请参见<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example"/>。</para>
<important>
<para>成功验证 chart 升级后，去除<literal>捆绑包/GitRepo</literal> 资源。</para>
<para>这将从<literal>下游</literal>群集中去除不再需要的升级资源，确保将来不会发生版本冲突。</para>
</important>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-example">
<title>示例</title>
<note>
<para>以下示例展示了如何在<literal>下游</literal>群集上将通过 <literal>EIB</literal> 部署的 Helm chart
从一个版本升级到另一个版本。请注意，本示例中使用的版本<emphasis role="strong">并非</emphasis>推荐版本。有关特定
Edge 版本的推荐版本，请参见发行说明（<xref linkend="release-notes"/>）。</para>
</note>
<para><emphasis>使用场景：</emphasis></para>
<itemizedlist>
<listitem>
<para>名为 <literal>doc-example</literal> 的群集正在运行旧版 <link
xl:href="https://longhorn.io">Longhorn</link>。</para>
</listitem>
<listitem>
<para>已使用以下映像定义<emphasis>代码段</emphasis>通过 EIB 部署群集：</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  helm:
    charts:
    - name: longhorn-crd
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    - name: longhorn
      repositoryName: rancher-charts
      targetNamespace: longhorn-system
      createNamespace: true
      version: 104.2.0+up1.7.1
      installationNamespace: kube-system
    repositories:
    - name: rancher-charts
      url: https://charts.rancher.io/
...</screen>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal> 需要升级到与 Edge 3.4 版兼容的版本。这意味着它需要升级到
<literal>107.0.0+up1.9.1</literal>。</para>
</listitem>
<listitem>
<para>假设负责管理 <literal>doc-example</literal> 群集的<literal>管理群集</literal>是<emphasis
role="strong">隔离式</emphasis>群集，不支持本地 Git 服务器，并且具有有效的 Rancher 设置。</para>
</listitem>
</itemizedlist>
<para>按照升级步骤（<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>）进行操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>从 <literal>release-3.4.0</literal> 标记处克隆
<literal>suse-edge/fleet-example</literal> 储存库。</para>
<screen language="bash" linenumbering="unnumbered">git clone -b release-3.4.0 https://github.com/suse-edge/fleet-examples.git</screen>
</listitem>
<listitem>
<para>创建用于存储 <literal>Longhorn</literal> 升级归档的目录。</para>
<screen language="bash" linenumbering="unnumbered">mkdir archives</screen>
</listitem>
<listitem>
<para>提取所需的 <literal>Longhorn</literal> chart 归档版本：</para>
<screen language="bash" linenumbering="unnumbered"># First add the Rancher Helm chart repository
helm repo add rancher-charts https://charts.rancher.io/

# Pull the Longhorn 1.9.1 CRD archive
helm pull rancher-charts/longhorn-crd --version 107.0.0+up1.9.1

# Pull the Longhorn 1.9.1 chart archive
helm pull rancher-charts/longhorn --version 107.0.0+up1.9.1</screen>
</listitem>
<listitem>
<para>在 <literal>archives</literal> 目录之外，从
<literal>suse-edge/fleet-examples</literal> 版本<link
xl:href="https://github.com/suse-edge/fleet-examples/releases/tag/release-3.4.0">标记</link>处下载
<literal>generate-chart-upgrade-data.sh</literal> 脚本。</para>
</listitem>
<listitem>
<para>目录设置如下所示：</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
|   |   |   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       └── kustomization.yaml
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
</listitem>
<listitem>
<para>执行 <literal>generate-chart-upgrade-data.sh</literal> 脚本：</para>
<screen language="bash" linenumbering="unnumbered"># First make the script executable
chmod +x ./generate-chart-upgrade-data.sh

# Then execute the script
./generate-chart-upgrade-data.sh --archive-dir ./archives --fleet-path ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
<para>脚本执行后的目录结构如下所示：</para>
<screen language="bash" linenumbering="unnumbered">.
├── archives
|   ├── longhorn-107.0.0+up1.9.1.tgz
│   └── longhorn-crd-107.0.0+up1.9.1.tgz
├── fleet-examples
...
│   ├── fleets
│   │   ├── day2
│   │   │   ├── ...
│   │   │   ├── eib-charts-upgrader
│   │   │   │   ├── base
│   │   │   │   │   ├── job.yaml
│   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   ├── patches
│   │   │   │   │   │   └── job-patch.yaml
│   │   │   │   │   ├── rbac
│   │   │   │   │   │   ├── cluster-role-binding.yaml
│   │   │   │   │   │   ├── cluster-role.yaml
│   │   │   │   │   │   ├── kustomization.yaml
│   │   │   │   │   │   └── sa.yaml
│   │   │   │   │   └── secrets
│   │   │   │   │       ├── eib-charts-upgrader-script.yaml
│   │   │   │   │       ├── kustomization.yaml
│   │   │   │   │       ├── longhorn-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   │       └── longhorn-crd-VERSION.yaml - secret created by the generate-chart-upgrade-data.sh script
│   │   │   │   ├── fleet.yaml
│   │   │   │   └── kustomization.yaml
│   │   │   └── ...
│   └── ...
└── generate-chart-upgrade-data.sh</screen>
<para>git 中更改的文件如下所示：</para>
<screen language="bash" linenumbering="unnumbered">Changes not staged for commit:
  (use "git add &lt;file&gt;..." to update what will be committed)
  (use "git restore &lt;file&gt;..." to discard changes in working directory)
        modified:   fleets/day2/eib-charts-upgrader/base/patches/job-patch.yaml
        modified:   fleets/day2/eib-charts-upgrader/base/secrets/kustomization.yaml

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-VERSION.yaml
        fleets/day2/eib-charts-upgrader/base/secrets/longhorn-crd-VERSION.yaml</screen>
</listitem>
<listitem>
<para>为 <literal>eib-charts-upgrader</literal> Fleet 创建<literal>捆绑包</literal>：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>首先，导航到 Fleet：</para>
<screen language="bash" linenumbering="unnumbered">cd ./fleet-examples/fleets/day2/eib-charts-upgrader</screen>
</listitem>
<listitem>
<para>然后创建 <literal>targets.yaml</literal> 文件：</para>
<screen language="bash" linenumbering="unnumbered">cat &gt; targets.yaml &lt;&lt;EOF
targets:
- clusterName: doc-example
EOF</screen>
</listitem>
<listitem>
<para>接下来，使用 <literal>fleet-cli</literal> 二进制文件将 Fleet 转换为捆绑包：</para>
<screen language="bash" linenumbering="unnumbered">fleet apply --compress --targets-file=targets.yaml -n fleet-default -o - eib-charts-upgrade &gt; bundle.yaml</screen>
</listitem>
<listitem>
<para>现在，在您的<literal>管理群集</literal>计算机上传输 <literal>bundle.yaml</literal>。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>通过 Rancher UI 部署捆绑包：</para>
<figure>
<title>通过 Rancher UI 部署捆绑包</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_1.png"
width="100%"/> </imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 1</phrase></textobject>
</mediaobject></figure>
<para>在此处选择 <emphasis role="strong">Read from File</emphasis>（从文件读取），并找到系统上的
<literal>bundle.yaml</literal> 文件。</para>
<para>此时会在 Rancher UI 中自动填充<literal>捆绑包</literal>。</para>
<para>选择 <emphasis role="strong">Create</emphasis>（创建）。</para>
</listitem>
<listitem>
<para>成功部署后，捆绑包如下所示：</para>
<figure>
<title>已成功部署的捆绑包</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_example_2.png"
width="100%"/> </imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 2</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>成功部署<literal>捆绑包</literal>后，要监控升级过程，请执行以下操作：</para>
<orderedlist numeration="arabic">
<listitem>
<para>验证<literal>升级 Pod</literal> 的日志：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_3_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 3 下游</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>现在验证 helm-controller 针对升级过程创建的 Pod 日志：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Pod 名称将使用以下模板 -
<literal>helm-install-longhorn-&lt;random-suffix&gt;</literal></para>
</listitem>
<listitem>
<para>Pod 将位于部署了 <literal>HelmChart</literal> 资源的名称空间中。在本例中为
<literal>kube-system</literal> 名称空间。</para>
<figure>
<title>成功升级的 Longhorn chart 的日志</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_4_downstream.png" width="100%"/>
</imageobject>
<textobject><phrase>Day2 Helm chart 升级示例 4 下游</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>导航到 Rancher 的 <literal>HelmChart</literal> 部分（<literal>More Resources（更多资源）→
HelmChart</literal>），验证 <literal>HelmChart</literal> 版本是否已更新。选择部署了该 chart
的名称空间，在本例中为 <literal>kube-system</literal>。</para>
</listitem>
<listitem>
<para>最后检查 Longhorn Pod 是否正在运行。</para>
</listitem>
</orderedlist>
<para>完成上述验证后，可以确定 Longhorn Helm chart 已升级到 <literal>107.0.0+up1.9.1</literal> 版本。</para>
</section>
<section xml:id="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-third-party">
<title>使用第三方 GitOps 工具进行 Helm chart 升级</title>
<para>在某些使用场景中，用户可能希望将此升级过程与 Fleet 以外的 GitOps 工作流程（例如
<literal>Flux</literal>）配合使用。</para>
<para>要生成升级过程所需的资源，可以使用 <literal>generate-chart-upgrade-data.sh</literal>
脚本将用户提供的数据填充到 <literal>eib-charts-upgrader</literal>
Fleet。有关如何执行此操作的详细信息，请参见<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>。</para>
<para>完成所有设置后，可以使用 <link xl:href="https://kustomize.io">kustomize</link>
生成一个可在群集中部署且完整有效的解决方案：</para>
<screen language="bash" linenumbering="unnumbered">cd /foo/bar/fleets/day2/eib-charts-upgrader

kustomize build .</screen>
<para>如果想将解决方案包含在 GitOps 工作流程中，可以去除 <literal>fleet.yaml</literal> 文件，并将其余内容用作有效的
<literal>Kustomize</literal> 设置。只是别忘了先运行
<literal>generate-chart-upgrade-data.sh</literal> 脚本，以便其可以将您想要升级到的 Helm
chart 的数据填充到 <literal>Kustomize</literal> 设置中。</para>
<para>要了解如何使用此工作流程，可以查看<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-overview"/>和<xref
linkend="downstream-day2-fleet-helm-upgrade-procedure-eib-deployed-chart-upgrade-steps"/>。</para>
</section>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-suse-telco-cloud-documentation">
<title>SUSE Telco Cloud 文档</title>
<partintro>
<para>此处为关于 SUSE Telco Cloud 的文档</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Telco Cloud</title>
<para>SUSE Telco Cloud（前身为 SUSE Edge for
Telco）是一款针对电信行业优化的计算平台，可帮助电信运营商和电信网络供应商实现创新，并加速其网络现代化进程。</para>
<para>SUSE Telco Cloud 是一套支持电信功能的完整云原生技术栈，适用于托管云原生网络功能 (CNF)，覆盖所有电信领域：分组核心网
(Packet Core)、无线接入网 (RAN)、运营支撑系统 (OSS) 和业务支撑系统 (BSS)。</para>
<itemizedlist>
<listitem>
<para>能实现电信级规模下复杂边缘堆栈配置的零接触部署及生命周期管理自动化。</para>
</listitem>
<listitem>
<para>借助电信专用配置与工作负载，在电信级硬件上持续保障质量。</para>
</listitem>
<listitem>
<para>其组件专为边缘场景打造，因此资源占用更少，并且性能功耗比更高。</para>
</listitem>
<listitem>
<para>凭借不限供应商的 API 及 100% 开源特性，保持灵活的平台策略。</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>概念和体系结构</title>
<para>SUSE Telco Cloud 是一个专为大规模托管现代化云原生电信应用程序而设计的平台，支持从核心网络到边缘网络的部署。</para>
<para>本章介绍 SUSE Telco Cloud 所采用的体系结构和组件。</para>
<section xml:id="id-suse-telco-cloud-architecture">
<title>SUSE Telco Cloud 体系结构</title>
<para>下图显示了 SUSE Telco Cloud 的总体体系结构：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 产品体系结构 1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-2">
<title>组件</title>
<para>有两个不同的区块 - 管理堆栈和运行时堆栈：</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理堆栈</emphasis>：SUSE Telco Cloud
的这一组成部分用于管理运行时堆栈的置备和生命周期，包含以下组件：</para>
<itemizedlist>
<listitem>
<para>使用 Rancher 在公有云和私有云环境中进行多群集管理（<xref linkend="components-rancher"/>）</para>
</listitem>
<listitem>
<para>使用 Metal3（<xref linkend="components-metal3"/>）、MetalLB（<xref
linkend="components-metallb"/>）和 <literal>CAPI</literal> (Cluster API)
基础架构提供程序来提供裸机支持</para>
</listitem>
<listitem>
<para>全面的租户隔离和 <literal>IDP</literal>（身份提供程序）集成</para>
</listitem>
<listitem>
<para>第三方集成和扩展大型商城</para>
</listitem>
<listitem>
<para>不限供应商的 API 和丰富的提供商生态</para>
</listitem>
<listitem>
<para>控制 SUSE Linux Micro 事务更新</para>
</listitem>
<listitem>
<para>GitOps 引擎，可以结合使用 Git 储存库和 Fleet（<xref
linkend="components-fleet"/>）来管理群集的生命周期</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">运行时栈</emphasis>：这是 SUSE Telco Cloud 中用于运行工作负载的部分。</para>
<itemizedlist>
<listitem>
<para>RKE2（<xref linkend="components-rke2"/>）是一款经过安全强化的轻量级 Kubernetes
发行版，专为边缘环境和注重合规性的电信环境优化。</para>
</listitem>
<listitem>
<para>SUSE Security（<xref
linkend="components-suse-security"/>），可用于实现映像漏洞扫描、深度数据包检测和群集内自动流量控制等安全功能。</para>
</listitem>
<listitem>
<para>基于 SUSE Storage（<xref
linkend="components-suse-storage"/>）构建的块存储，让您可以方便地使用云原生存储解决方案。</para>
</listitem>
<listitem>
<para>基于 SUSE Linux Micro（<xref
linkend="components-slmicro"/>）打造的优化操作系统，可提供安全、轻量且具备不可变特性（事务性文件系统）的容器运行环境。SUSE
Linux Micro 适用于 AArch64 和 AMD64/Intel 64
体系结构，并为电信和边缘使用场景提供<literal>实时内核</literal>支持。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>部署流程示例</title>
<para>下面是工作流程的简要示例，可帮助您了解管理组件与运行时组件之间的关系。</para>
<para>定向网络置备是可用于部署预配置了所有组件，并可以在无需人工干预的情况下运行工作负载的新下游群集的工作流程。</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>示例 1：部署装有所有组件的新管理群集</title>
<para>使用 Edge Image Builder（<xref linkend="components-eib"/>）创建包含管理堆栈的新
<literal>ISO</literal> 映像。然后，可以使用此 <literal>ISO</literal> 映像在 VM
或裸机上安装新管理群集。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 产品体系结构 2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>有关如何部署新管理群集的更多信息，请参见 SUSE Telco Cloud管理群集指南（<xref
linkend="atip-management-cluster"/>）。</para>
</note>
<note>
<para>有关如何使用 Edge Image Builder 的详细信息，请参见 Edge Image Builder 指南（<xref
linkend="quickstart-eib"/>）。</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>示例 2：使用电信配置文件部署单节点下游群集，使其能够运行电信工作负载</title>
<para>启动并运行管理群集后，我们可以使用该群集通过定向网络置备工作流程，来部署启用并配置了所有电信功能的单节点下游群集。</para>
<para>下图显示了部署该群集的概要工作流程：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 产品体系结构 3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>有关如何部署下游群集的详细信息，请参见 SUSE Telco Cloud 自动置备指南（<xref
linkend="atip-automated-provisioning"/>）。</para>
</note>
<note>
<para>有关电信功能的详细信息，请参见 SUSE Telco Cloud 电信功能指南（<xref linkend="atip-features"/>）。</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>示例 3：使用 MetalLB 作为负载平衡器部署高可用性下游群集</title>
<para>启动并运行管理群集后，我们可以使用该群集通过定向网络置备工作流程，来部署使用 <literal>MetalLB</literal>
作为负载平衡器的高可用性下游群集。</para>
<para>下图显示了部署该群集的概要工作流程：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 产品体系结构 4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>有关如何部署下游群集的详细信息，请参见 SUSE Telco Cloud 自动置备指南（<xref
linkend="atip-automated-provisioning"/>）。</para>
</note>
<note>
<para>有关 <literal>MetalLB</literal> 的详细信息，请参见<xref linkend="components-metallb"/>。</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>要求和假设</title>
<section xml:id="id-hardware">
<title>硬件</title>
<para>SUSE Telco Cloud 的硬件要求如下：</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理群集</emphasis>：管理群集包含 <literal>SUSE Linux
Micro</literal>、<literal>RKE2</literal>、<literal>SUSERancher
Prime</literal>、<literal>Metal<superscript>3</superscript></literal>
等组件，用于管理多个下游群集。服务器的硬件要求可能会因所要管理的下游群集数量而有所不同。</para>
<itemizedlist>
<listitem>
<para>服务器（<literal>VM</literal> 或<literal>裸机</literal>）的最低要求如下：</para>
<itemizedlist>
<listitem>
<para>RAM：至少 8 GB（建议至少提供 16 GB）</para>
</listitem>
<listitem>
<para>CPU：至少 2 个（建议至少提供 4 个 CPU）</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">下游群集</emphasis>：下游群集是用于运行电信工作负载的群集。需要满足特定的要求才能启用
<literal>SR-IOV</literal>、<literal>CPU 性能优化</literal>等特定电信功能。</para>
<itemizedlist>
<listitem>
<para>SR-IOV：要以直通模式将 VF（虚拟功能）附加到 CNF/VNF，NIC 必须支持 SR-IOV，并且必须在 BIOS 中启用
VT-d/AMD-Vi。</para>
</listitem>
<listitem>
<para>CPU 处理器：要运行特定的电信工作负载，应该适配 CPU 处理器型号，以启用此参考表格（<xref
linkend="atip-features"/>）中所述的大多数功能。</para>
</listitem>
<listitem>
<para>使用虚拟媒体进行安装所要满足的固件要求：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">服务器硬件</entry>
<entry align="left" valign="top">BMC 型号</entry>
<entry align="left" valign="top">管理</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>Dell 硬件</para></entry>
<entry align="left" valign="top"><para>第 15 代</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Supermicro 硬件</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>HPE 硬件</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-network">
<title>网络</title>
<para>下图显示了电信环境的典型网络体系结构作为参考：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirements1.svg"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 产品要求 1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>网络体系结构基于以下组件：</para>
<itemizedlist>
<listitem>
<para><emphasis
role="strong">管理网络</emphasis>：此网络用于管理下游群集节点。它用于进行带外管理。通常，此网络还会连接到独立的管理交换机，不过可以通过
VLAN 连接到同一服务交换机以隔离流量。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">控制平面网络</emphasis>：此网络用于下游群集节点与其上运行的服务之间的通信。此网络还用于这些节点与外部服务（例如
<literal>DHCP</literal> 或 <literal>DNS</literal>
服务器）之间的通信。在某些情况下，对于联网环境，交换机/路由器可以通过互联网处理流量。</para>
</listitem>
<listitem>
<para><emphasis role="strong">其他网络</emphasis>：在某些情况下，这些节点可以连接到其他网络以满足特定目的。</para>
</listitem>
</itemizedlist>
<note>
<para>要使用定向网络置备工作流程，管理群集必须与下游群集服务器基板管理控制器 (BMC) 建立网络连接，以便可以自动准备和置备主机。</para>
</note>
</section>
<section xml:id="id-port-requirements">
<title>端口要求</title>
<para>要使 SUSE Telco Cloud 部署正常运行，Kubernetes 管理群集和下游群集节点上需要开放多个可访问的端口。</para>
<note>
<para>具体端口列表取决于部署的可选组件和所选的部署选项（如 CNI 插件）。</para>
</note>
<section xml:id="id-management-nodes">
<title>管理节点</title>
<para>下表列出了运行管理群集的节点上开放的端口：</para>
<note>
<para>有关 CNI 插件相关端口，请参见 CNI 特有的端口要求（<xref
linkend="cni-specific-port-requirements"/>）。</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-nodes" frame="all" rowsep="1" colsep="1">
<title>管理节点的入站网络规则</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">协议</entry>
<entry align="left" valign="top">端口</entry>
<entry align="left" valign="top">来源</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>需要 SSH 访问权限的任何来源</para></entry>
<entry align="left" valign="top"><para>对管理群集节点的 SSH 访问权限</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>执行外部 TLS 终止的负载平衡器/代理</para></entry>
<entry align="left" valign="top"><para>使用外部 TLS 终止时的 Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>需要通过 TLS 访问 Rancher UI/API 的任何来源</para></entry>
<entry align="left" valign="top"><para>Rancher 代理、Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>RKE2（管理群集）服务器节点</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> 客户端端口</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>RKE2（管理群集）服务器节点</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> 对等端口</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6180</para></entry>
<entry align="left" valign="top"><para>之前 <literal>Metal<superscript>3</superscript>/ironic</literal> 指示从该公开端口（非
TLS）拉取 IPA<superscript>(2)</superscript> 内存盘映像的任何
BMC<superscript>(1)</superscript></para></entry>
<entry align="left" valign="top"><para><literal>Ironic</literal> httpd 非 TLS Web 服务器，用于提供
IPA<superscript>(2)</superscript> ISO 映像，供基于虚拟媒体的引导使用。<?asciidoc-br?>
<?asciidoc-br?>如果启用此端口，则不会开放功能等效但支持 TLS 的端口（请参见下文）</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6185</para></entry>
<entry align="left" valign="top"><para>之前 <literal>Metal<superscript>3</superscript>/ironic</literal> 指示从该公开端口
(TLS) 拉取 IPA<superscript>(2)</superscript> 内存盘映像的任何
BMC<superscript>(1)</superscript></para></entry>
<entry align="left" valign="top"><para>支持 httpd TLS 的 <literal>Ironic</literal> Web 服务器，用于提供
IPA<superscript>(2)</superscript> ISO 映像，供基于虚拟媒体的引导使用。<?asciidoc-br?>
<?asciidoc-br?>如果启用此端口，则不会开放功能等效但不支持 TLS 的端口（请参见上文）</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6385</para></entry>
<entry align="left" valign="top"><para>在“已注册”<literal>BareMetalHost</literal> 实例中部署并运行的任何
<literal>Metal<superscript>3</superscript>/ironic</literal>
IPA<superscript>(1)</superscript> 内存盘映像</para></entry>
<entry align="left" valign="top"><para>Ironic API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>任何管理群集节点；任何（管理群集外部的）Kubernetes 客户端</para></entry>
<entry align="left" valign="top"><para>Kubernetes API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6545</para></entry>
<entry align="left" valign="top"><para>任何管理群集节点</para></entry>
<entry align="left" valign="top"><para>从符合 OCI 标准的仓库 (Hauler) 拉取制品</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>RKE2 服务器和代理节点（管理群集）</para></entry>
<entry align="left" valign="top"><para>用于节点注册的 RKE2 监督 API（所有 RKE2 服务器节点上开放的端口）</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>任何管理群集节点</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal> 指标</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000-32767</para></entry>
<entry align="left" valign="top"><para>通过 <literal>spec.type: NodePort</literal> 或 <literal>spec.type:
LoadBalancer</literal> <link
xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service
API 对象</link>访问主网络上所公开服务的任何（管理群集外部的）来源</para></entry>
<entry align="left" valign="top"><para>可用的 <literal>NodePort</literal> 端口范围</para></entry>
</row>
</tbody>
</tgroup>
</table>
<para><superscript>(1)</superscript> BMC：基板管理控制器<?asciidoc-br?>
<superscript>(2)</superscript> IPA：Ironic Python Agent</para>
</section>
<section xml:id="id-downstream-nodes">
<title>下游节点</title>
<para>在 SUSE Telco Cloud 中，任何（下游）服务器要加入运行中的下游 Kubernetes 群集（或自身运行单节点下游 Kubernetes
群集），都必须经历某些 <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/baremetalhost-states.md">BaremetalHost
置备状态</link>。</para>
<itemizedlist>
<listitem>
<para>新声明的下游服务器的基板管理控制器 (BMC) 必须可通过带外网络访问。管理群集上运行的 ironic 服务会指示 BMC 执行初始步骤：</para>
<orderedlist numeration="arabic">
<listitem>
<para>在 BMC 提供的<literal>虚拟媒体</literal>中拉取并加载指定的 IPA 内存盘映像。</para>
</listitem>
<listitem>
<para>启动服务器。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<para>BMC 需公开以下端口（具体端口可能因硬件而异）：</para>
<table xml:id="table-inbound-network-rules-for-baseboard-management-controllers" frame="all" rowsep="1" colsep="1">
<title>基板管理控制器的入站网络规则</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">协议</entry>
<entry align="left" valign="top">端口</entry>
<entry align="left" valign="top">来源</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>Ironic 控制器（来自管理群集）</para></entry>
<entry align="left" valign="top"><para>Redfish API 访问 (HTTP)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>Ironic 控制器（来自管理群集）</para></entry>
<entry align="left" valign="top"><para>Redfish API 访问 (HTTPS)</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>当 BMC <literal>虚拟媒体</literal>中加载的 IPA 内存盘映像用于引导下游服务器映像时，将开始硬件检查阶段。下表列出了运行中的
IPA 内存盘映像所公开的端口：</para>
</listitem>
</itemizedlist>
<table xml:id="table-inbound-network-rules-for-downstream-nodes-provisioning-phase" frame="all" rowsep="1" colsep="1">
<title>下游节点的入站网络规则 - <literal>Metal<superscript>3</superscript>/Ironic</literal>
置备阶段</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">协议</entry>
<entry align="left" valign="top">端口</entry>
<entry align="left" valign="top">来源</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>需要通过 SSH 访问 IPA 内存盘映像的任何来源</para></entry>
<entry align="left" valign="top"><para>对正在检查的下游群集节点的 SSH 访问权限</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9999</para></entry>
<entry align="left" valign="top"><para>Ironic 控制器（来自管理群集）</para></entry>
<entry align="left" valign="top"><para>针对运行中内存盘映像的 Ironic 命令</para></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<listitem>
<para>当裸机主机完成正确置备并加入下游 Kubernetes 群集后，会公开以下端口：</para>
</listitem>
</itemizedlist>
<note>
<para>有关 CNI 插件相关端口，请参见 CNI 特有的端口要求（<xref
linkend="cni-specific-port-requirements"/>）。</para>
</note>
<table xml:id="table-inbound-network-rules-for-downstream-nodes" frame="all" rowsep="1" colsep="1">
<title>下游节点的入站网络规则</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">协议</entry>
<entry align="left" valign="top">端口</entry>
<entry align="left" valign="top">来源</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>22</para></entry>
<entry align="left" valign="top"><para>需要 SSH 访问权限的任何来源</para></entry>
<entry align="left" valign="top"><para>对下游群集节点的 SSH 访问权限</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>80</para></entry>
<entry align="left" valign="top"><para>执行外部 TLS 终止的负载平衡器/代理</para></entry>
<entry align="left" valign="top"><para>使用外部 TLS 终止时的 Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>443</para></entry>
<entry align="left" valign="top"><para>需要通过 TLS 访问 Rancher UI/API 的任何来源</para></entry>
<entry align="left" valign="top"><para>Rancher 代理、Rancher UI/API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2379</para></entry>
<entry align="left" valign="top"><para>RKE2（下游群集）服务器节点</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> 客户端端口</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>2380</para></entry>
<entry align="left" valign="top"><para>RKE2（下游群集）服务器节点</para></entry>
<entry align="left" valign="top"><para><literal>etcd</literal> 对等端口</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>6443</para></entry>
<entry align="left" valign="top"><para>任何下游群集节点；任何（下游群集外部的）Kubernetes 客户端。</para></entry>
<entry align="left" valign="top"><para>Kubernetes API</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9345</para></entry>
<entry align="left" valign="top"><para>RKE2 服务器和代理节点（下游群集）</para></entry>
<entry align="left" valign="top"><para>用于节点注册的 RKE2 监督 API（所有 RKE2 服务器节点上开放的端口）</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10250</para></entry>
<entry align="left" valign="top"><para>任何下游群集节点</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal> 指标</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>10255</para></entry>
<entry align="left" valign="top"><para>任何下游群集节点</para></entry>
<entry align="left" valign="top"><para><literal>kubelet</literal> 只读访问权限</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>TCP/UDP/SCTP</para></entry>
<entry align="left" valign="top"><para>30000-32767</para></entry>
<entry align="left" valign="top"><para>通过 <literal>spec.type: NodePort</literal> 或 <literal>spec.type:
LoadBalancer</literal> <link
xl:href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types">Service
API 对象</link>访问主网络上所公开服务的任何（下游群集外部的）来源</para></entry>
<entry align="left" valign="top"><para>可用的 <literal>NodePort</literal> 端口范围</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="cni-specific-port-requirements">
<title>CNI 特有的端口要求</title>
<para>每个受支持的 CNI 变体都有其自身的端口要求集。有关详细信息，请参见 RKE2 文档中的 <link
xl:href="https://docs.rke2.io/install/requirements#cni-specific-inbound-network-rules">CNI
Specific Inbound Network Rules</link>。</para>
<para>当 <literal>cilium</literal> 设为默认/主 CNI 插件时，如果
<literal>cilium-operator</literal> 工作负载配置为向其部署所在的 Kubernetes
群集外部公开指标，则会额外公开以下 TCP 端口。这可确保在该 Kubernetes 群集外部运行的外部
<literal>Prometheus</literal> 服务器实例仍能收集这些指标。</para>
<note>
<para>通过 rke2-cilium Helm chart 部署 <literal>cilium</literal> 时，这是默认选项。</para>
</note>
<table xml:id="table-inbound-network-rules-for-management-downstream-nodes-external-metrics-cilium-operator" frame="all" rowsep="1" colsep="1">
<title>管理/下游节点的入站网络规则 - 启用 <literal>cilium-operator</literal> 后，外部指标公开</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">协议</entry>
<entry align="left" valign="top">端口</entry>
<entry align="left" valign="top">来源</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>TCP</para></entry>
<entry align="left" valign="top"><para>9963</para></entry>
<entry align="left" valign="top"><para>（Kubernetes 群集外部的）指标收集器</para></entry>
<entry align="left" valign="top"><para>cilium-operator 指标公开</para></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>服务（DHCP、DNS 等）</title>
<para>可能需要使用一些外部服务（例如 <literal>DHCP</literal>、<literal>DNS</literal>
等），具体取决于部署环境的类型：</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">联网环境</emphasis>：在这种情况下，节点将连接到互联网（通过路由 L3
协议），外部服务将由客户提供。</para>
</listitem>
<listitem>
<para><emphasis role="strong">离线/隔离环境</emphasis>：在这种情况下，节点未建立互联网 IP
连接，因此需要通过其他服务在本地镜像定向网络置备工作流程所需的内容。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">文件服务器</emphasis>：文件服务器用于在执行定向网络置备工作流程期间存储将在下游群集节点上置备的操作系统映像。<literal>Metal<superscript>3</superscript></literal>
Helm chart 可以部署媒体服务器来存储操作系统映像 — 请查看下文内容（<xref
linkend="metal3-media-server"/>），但也可以使用现有的本地 Web 服务器。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-systemd-services">
<title>禁用 systemd 服务</title>
<para>对于电信工作负载，必须禁用或正确配置节点上运行的一些服务，以免对节点上运行的工作负载的性能产生任何影响（延迟）。</para>
<itemizedlist>
<listitem>
<para><literal>rebootmgr</literal> 是当系统中存在未完成的更新时用于配置重引导策略的服务。对于电信工作负载，必须禁用或正确配置
<literal>rebootmgr</literal> 服务，以免在系统安排了更新时重引导节点，从而避免对节点上运行的服务造成任何影响。</para>
</listitem>
</itemizedlist>
<note>
<para>有关 <literal>rebootmgr</literal> 的详细信息，请参见 <link
xl:href="https://github.com/SUSE/rebootmgr">rebootmgr GitHub 代码库</link>。</para>
</note>
<para>运行以下命令来校验使用的策略：</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>可以运行以下命令来禁用 rebootmgr：</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>也可以使用 <literal>rebootmgrctl</literal> 命令：</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para>可以使用定向网络置备工作流程来自动完成用于设置 <literal>rebootmgr</literal>
策略的此项配置。有关详细信息，请参见自动置备文档（<xref linkend="atip-automated-provisioning"/>）。</para>
</note>
<itemizedlist>
<listitem>
<para><literal>transactional-update</literal>
是一项允许在系统控制下进行自动更新的服务。对于电信工作负载，必须禁用自动更新，以免对节点上运行的服务产生任何影响。</para>
</listitem>
</itemizedlist>
<para>要禁用自动更新，可以运行：</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable transactional-update.timer
systemctl --now disable transactional-update-cleanup.timer</screen>
<itemizedlist>
<listitem>
<para><literal>fstrim</literal>
是一种允许每周自动剪裁文件系统的服务。对于电信工作负载，必须禁用自动剪裁，以免对节点上运行的服务产生任何影响。</para>
</listitem>
</itemizedlist>
<para>要禁用自动剪裁，可以运行：</para>
<screen language="shell" linenumbering="unnumbered">systemctl --now disable fstrim.timer</screen>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>设置管理群集</title>
<section xml:id="id-introduction-2">
<title>简介</title>
<para>管理群集是 SUSE Telco Cloud 的组成部分，用于管理运行时堆栈置备和生命周期。从技术角度而言，管理群集包含以下组件：</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro</literal>（操作系统），可以根据使用场景自定义某些配置，例如网络、存储、用户和内核参数。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>（Kubernetes 群集），可以根据使用场景将其配置为使用特定的 CNI 插件，例如
<literal>Multus</literal>、<literal>Cilium</literal>、<literal>Calico</literal>
等。</para>
</listitem>
<listitem>
<para><literal>Rancher</literal>（管理平台），用于管理群集的生命周期。</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>，该组件用于管理裸机节点的生命周期。</para>
</listitem>
<listitem>
<para><literal>CAPI</literal>，该组件用于管理 Kubernetes 群集（下游群集）的生命周期。<literal>RKE2 CAPI
提供程序</literal>也用于管理 RKE2 群集的生命周期。</para>
</listitem>
</itemizedlist>
<para>通过上述所有组件，管理群集可以管理下游群集的生命周期，并使用声明式方法来管理基础架构和应用程序。</para>
<note>
<para>有关 <literal>SUSE Linux Micro</literal> 的详细信息，请参见<xref
linkend="components-slmicro"/></para>
<para>有关 <literal>RKE2</literal> 的详细信息，请参见<xref linkend="components-rke2"/></para>
<para>有关 <literal>Rancher</literal> 的详细信息，请参见<xref linkend="components-rancher"/></para>
<para>有关 <literal>Metal<superscript>3</superscript></literal> 的详细信息，请参见<xref
linkend="components-metal3"/></para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>设置管理群集的步骤</title>
<para>需要执行以下步骤来设置管理群集（使用单个节点）：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 产品管理群集 1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>使用声明式方法设置管理群集需要执行以下主要步骤：</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">为联网环境准备映像（<xref
linkend="mgmt-cluster-image-preparation-connected"/>）</emphasis>：第一步是准备包含所有必要配置的清单和文件，以便在联网环境中使用。</para>
<itemizedlist>
<listitem>
<para>联网环境的目录结构（<xref linkend="mgmt-cluster-directory-structure"/>）：此步骤创建一个目录结构，供
Edge Image Builder 用来存储配置文件和映像本身。</para>
</listitem>
<listitem>
<para>管理群集定义文件（<xref
linkend="mgmt-cluster-image-definition-file"/>）：<literal>mgmt-cluster.yaml</literal>
文件是管理群集的主定义文件。其中包含有关所要创建的映像的以下信息：</para>
<itemizedlist>
<listitem>
<para>映像信息：与要使用基础映像创建的映像相关的信息。</para>
</listitem>
<listitem>
<para>操作系统：要在映像中使用的操作系统配置。</para>
</listitem>
<listitem>
<para>Kubernetes：要在群集中使用的 Helm chart 和储存库、Kubernetes 版本、网络配置以及节点。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Custom 文件夹（<xref
linkend="mgmt-cluster-custom-folder"/>）：<literal>custom</literal>
文件夹包含的配置文件和脚本供 Edge Image Builder 用来部署功能完备的管理群集。</para>
<itemizedlist>
<listitem>
<para>Files 文件夹：包含管理群集要使用的配置文件。</para>
</listitem>
<listitem>
<para>Scripts 文件夹：包含管理群集要使用的脚本。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Kubernetes 文件夹（<xref
linkend="mgmt-cluster-kubernetes-folder"/>）：<literal>kubernetes</literal>
文件夹包含管理群集要使用的配置文件。</para>
<itemizedlist>
<listitem>
<para>Manifests 文件夹：包含管理群集要使用的清单。</para>
</listitem>
<listitem>
<para>Helm：包含管理群集要使用的 Helm 值文件。</para>
</listitem>
<listitem>
<para>Config 文件夹：包含管理群集要使用的配置文件。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Network 文件夹（<xref
linkend="mgmt-cluster-network-folder"/>）：<literal>network</literal>
文件夹包含管理群集节点要使用的网络配置文件。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">为隔离环境准备映像（<xref
linkend="mgmt-cluster-image-preparation-airgap"/>）</emphasis>：此步骤将说明与非隔离场景相比，准备要在隔离场景中使用的清单和文件有哪些差别。</para>
<itemizedlist>
<listitem>
<para>定义文件中的修改（<xref linkend="mgmt-cluster-image-definition-file-airgap"/>）：必须修改
<literal>mgmt-cluster.yaml</literal> 文件，以包含
<literal>embeddedArtifactRegistry</literal> 部分，并将 <literal>images</literal>
字段设置为要包含在 EIB 输出映像中的所有容器映像。</para>
</listitem>
<listitem>
<para>custom 文件夹中的修改（<xref linkend="mgmt-cluster-custom-folder-airgap"/>）：必须修改
<literal>custom</literal> 文件夹，以包含用于在隔离环境中运行管理群集的资源。</para>
<itemizedlist>
<listitem>
<para>注册脚本：使用隔离环境时，必须去除 <literal>custom/scripts/99-register.sh</literal> 脚本。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Helm 值文件夹中的修改（<xref linkend="mgmt-cluster-helm-values-folder-airgap"/>）：必须修改
<literal>helm/values</literal> 文件夹，以包含在隔离环境中运行管理群集所需的配置。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">创建映像（<xref
linkend="mgmt-cluster-image-creation"/>）</emphasis>：此步骤使用 Edge Image Builder
工具创建映像（适用于联网场景和隔离场景）。在系统上运行 Edge Image Builder 工具之前，请先检查先决条件（<xref
linkend="components-eib"/>）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">置备管理群集（<xref
linkend="mgmt-cluster-provision"/>）</emphasis>：此步骤使用上一步中创建的映像来置备管理群集（适用于联网场景和隔离场景）。可以使用便携式计算机、服务器、VM
或任何其他带有 USB 端口的 AMD64/Intel 64 系统来执行此步骤。</para>
</listitem>
</orderedlist>
<note>
<para>有关 Edge Image Builder 的详细信息，请参见 Edge Image Builder（<xref
linkend="components-eib"/>）和 Edge Image Builder 快速入门（<xref
linkend="quickstart-eib"/>）。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>为联网环境准备映像</title>
<para>Edge Image Builder 用于为管理群集创建映像，在本文档中，我们将介绍设置管理群集所需的最低配置。</para>
<para>Edge Image Builder 在容器内运行，因此需要 <link
xl:href="https://podman.io">Podman</link> 或 <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link> 等容器运行时。在本指南中，我们假设
Podman 可用。</para>
<para>此外，作为部署高可用性管理群集的先决条件，您需要在网络中预留三个 IP 地址：</para>
<itemizedlist>
<listitem>
<para><literal>apiVIP</literal>，表示 API VIP 地址（用于访问 Kubernetes API 服务器）。</para>
</listitem>
<listitem>
<para><literal>ingressVIP</literal>，表示入口 VIP 地址（例如，供 Rancher UI 使用）。</para>
</listitem>
<listitem>
<para><literal>metal3VIP</literal>，表示 Metal3 VIP 地址。</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure">
<title>目录结构</title>
<para>运行 EIB 时，将从主机挂载一个目录，因此首先需要创建一个目录结构，供 EIB 用来存储配置文件和映像本身。此目录的结构如下：</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── os-files
│ └── var
│   └── lib
│     └── rancher
│       └── rke2
│         └── server
│           └── manifests
│             └── rke2-ingress-config.yaml
├── kubernetes
│ ├── manifests
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── longhorn.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>必须从 <link xl:href="https://scc.suse.com/">SUSE Customer Center</link> 或
<link xl:href="https://www.suse.com/download/sle-micro/">SUSE 下载页面</link>下载
<literal>SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso</literal>
映像，并且必须将其存放在 <literal>base-images</literal> 文件夹下。</para>
<para>应检查该映像的 SHA256 校验和，确保它未遭篡改。可以在映像所下载到的位置找到校验和。</para>
<para>可以在 <link xl:href="https://github.com/suse-edge/atip">SUSE Edge GitHub
代码库中的“telco-examples”文件夹下</link>找到目录结构的示例。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>管理群集定义文件</title>
<para><literal>mgmt-cluster.yaml</literal> 文件是管理群集的主定义文件。其中包含以下信息：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    - open-iscsi
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: $API_HOST
    apiVIP: $API_VIP
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para>为了解释 <literal>mgmt-cluster.yaml</literal> 定义文件中的字段和值，我们将此文件划分成了以下几个部分。</para>
<itemizedlist>
<listitem>
<para>映像部分（定义文件）：</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>其中 <literal>baseImage</literal> 是从 SUSE Customer Center 或 SUSE
下载页面下载的原始映像。<literal>outputImageName</literal> 是将用于置备管理群集的新映像的名称。</para>
<itemizedlist>
<listitem>
<para>操作系统部分（定义文件）：</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>其中 <literal>installDevice</literal> 是用于安装操作系统的设备，<literal>username</literal>
和 <literal>encryptedPassword</literal>
是用于访问系统的身份凭证，<literal>packageList</literal> 是要安装的软件包列表（在安装过程中，需要在内部使用
<literal>jq</literal>），<literal>sccRegistrationCode</literal>
是在构建时用于获取软件包和依赖项的注册代码，可从 SUSE Customer Center 获取。可以如下所示使用
<literal>openssl</literal> 命令生成加密的口令：</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>此命令会输出如下所示的内容：</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Kubernetes 部分（定义文件）：</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</screen>
<para><literal>helm</literal> 部分包含要安装的 Helm chart 列表、要使用的储存库，以及所有 chart 和储存库的版本配置。</para>
<para><literal>network</literal> 部分包含 <literal>RKE2</literal> 组件要使用的网络配置，例如
<literal>apiHost</literal> 和
<literal>apiVIP</literal>。<literal>apiVIP</literal> 必须是网络中未使用的 IP 地址，并且不属于
DHCP 池（如果使用 DHCP）。此外，如果我们在多节点群集中使用 <literal>apiVIP</literal>，apiVIP 将用于访问
Kubernetes API 服务器。<literal>apiHost</literal> 是 <literal>RKE2</literal>
组件要使用的 <literal>apiVIP</literal> 的名称解析。</para>
<para><literal>nodes</literal>
部分包含群集中要使用的节点列表。本示例使用的是单节点群集，但可通过在列表中添加更多节点（取消注释相应行）将其扩展为多节点群集。</para>
<note>
<itemizedlist>
<listitem>
<para>节点名称在群集中必须保持唯一。</para>
</listitem>
<listitem>
<para>可以选择使用 <literal>initializer</literal> 字段指定引导主机，如果不指定，列表中的第一个节点将会是引导主机。</para>
</listitem>
<listitem>
<para>需要网络配置时，节点名称必须与“Network”文件夹（<xref
linkend="mgmt-cluster-network-folder"/>）中定义的主机名相同。</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Custom 文件夹</title>
<para><literal>custom</literal> 文件夹包含以下子文件夹：</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para><literal>custom/files</literal> 文件夹包含管理群集要使用的配置文件。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal> 文件夹包含管理群集要使用的脚本。</para>
</listitem>
</itemizedlist>
<para><literal>custom/files</literal> 文件夹包含以下文件：</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>：包含
<literal>Metal<superscript>3</superscript></literal>、<literal>Rancher</literal>
和 <literal>MetalLB</literal> 的配置参数。仅当您要更改要使用的名称空间时，才需修改此文件。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>：包含要使用的
<literal>Metal<superscript>3</superscript></literal>
组件的配置（无需修改）。在将来的版本中将替换此脚本，以改用 <literal>Rancher Turtles</literal> 来简化配置。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
        EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
        EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
        apiVersion: management.cattle.io/v3
        kind: Feature
        metadata:
          name: embedded-cluster-api
        spec:
          value: false
        EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>：包含要使用的 <literal>Rancher</literal> 组件的配置（无需修改）。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>：包含用于创建 systemd
服务，以便在首次引导期间运行脚本的配置（无需修改）。</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single one
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para><literal>custom/scripts</literal> 文件夹包含以下文件：</para>
<itemizedlist>
<listitem>
<para><literal>99-alias.sh</literal> 脚本：包含管理群集在首次引导时用来加载 kubeconfig 文件的别名（无需修改）。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para><literal>99-mgmt-setup.sh</literal> 脚本：包含首次引导期间用于复制脚本的配置（无需修改）。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
        cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para><literal>99-register.sh</literal> 脚本：包含用于通过 SCC 注册代码注册系统的配置。必须正确设置
<literal>${SCC_ACCOUNT_EMAIL}</literal> 和
<literal>${SCC_REGISTRATION_CODE}</literal> 才能使用您的帐户注册系统。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
        zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Kubernetes 文件夹</title>
<para><literal>kubernetes</literal> 文件夹包含以下子文件夹：</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para><literal>kubernetes/config</literal> 文件夹包含以下文件：</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>：默认安装的 <literal>CNI</literal> 插件是
<literal>Cilium</literal>，因此不需要创建此文件夹和文件。如果您需要自定义 <literal>CNI</literal>
插件，可以使用 <literal>kubernetes/config</literal> 文件夹中的
<literal>server.yaml</literal> 文件。该文件包含以下信息：</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
system-default-registry: registry.rancher.com</screen>
</listitem>
</itemizedlist>
<note>
<para>这是一个可选文件，用于定义某些 Kubernetes 自定义设置，例如要使用的 CNI 插件，或<link
xl:href="https://docs.rke2.io/install/configuration">官方文档</link>中所述的许多选项。</para>
</note>
<para><literal>os-files/var/lib/rancher/rke2/server/manifests</literal> 文件夹包含以下文件：</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>：包含用于为管理群集创建<literal>入口</literal>服务的配置（无需修改）。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
</itemizedlist>
<note>
<para>必须通过 <literal>os-files</literal> 将 <literal>HelmChartConfig</literal> 放入
<literal>/var/lib/rancher/rke2/server/manifests</literal> 目录，而非像之前版本中所述通过
<literal>kubernetes/manifests</literal> 放入。</para>
</note>
<para><literal>kubernetes/manifests</literal> 文件夹包含以下文件：</para>
<itemizedlist>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>：包含用于创建
<literal>NeuVector</literal> 名称空间的配置（无需修改）。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>：包含用于为 <literal>MetalLB</literal> 组件创建
<literal>L2Advertisement</literal> 的配置（无需修改）。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>：包含用于为
<literal>rke2-ingress-nginx</literal> 组件创建 <literal>IPAddressPool</literal>
的配置。必须正确设置 <literal>${INGRESS_VIP}</literal>，以定义预留给
<literal>rke2-ingress-nginx</literal> 组件使用的 IP 地址。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para><literal>kubernetes/helm/values</literal> 文件夹包含以下文件：</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>：包含用于创建 <literal>Rancher</literal>
组件的配置。必须正确设置 <literal>${INGRESS_VIP}</literal>，以定义
<literal>Rancher</literal> 组件要使用的 IP 地址。用于访问 <literal>Rancher</literal> 组件的
URL 为 <literal>https://rancher-${INGRESS_VIP}.sslip.io</literal>。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>：包含用于创建 <literal>NeuVector</literal>
组件的配置（无需修改）。</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
registry: "registry.rancher.com"
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>longhorn.yaml</literal>：包含用于创建 <literal>Longhorn</literal>
组件的配置（无需修改）。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>：包含用于创建
<literal>Metal<superscript>3</superscript></literal> 组件的配置。必须正确设置
<literal>${METAL3_VIP}</literal>，以定义
<literal>Metal<superscript>3</superscript></literal> 组件要使用的 IP 地址。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>媒体服务器是 Metal<superscript>3</superscript> 中包含的可选功能（默认处于禁用状态）。要使用该 Metal3
功能，需要在前面所述的清单中配置该功能。要使用 Metal<superscript>3</superscript> 媒体服务器，请指定以下变量：</para>
<itemizedlist>
<listitem>
<para>在 global 部分，将 <literal>enable_metal3_media_server</literal> 设置为
<literal>true</literal> 以启用媒体服务器功能。</para>
</listitem>
<listitem>
<para>包含有关媒体服务器的以下配置，其中 ${MEDIA_VOLUME_PATH} 是媒体卷在媒体中的路径（例如
<literal>/home/metal3/bmh-image-cache</literal>）</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>可以使用外部媒体服务器来存储映像，如果您要将该服务器与 TLS 配合使用，则需要修改以下配置：</para>
<itemizedlist>
<listitem>
<para>将前面所述 <literal>metal3.yaml</literal> 文件中的
<literal>additionalTrustedCAs</literal> 设置为
<literal>true</literal>，以启用来自外部媒体服务器的附加可信 CA。</para>
</listitem>
<listitem>
<para>在 <literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal>
文件夹中包含以下机密配置，以存储外部媒体服务器的 CA 证书。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para><literal>additional_ca_cert</literal> 是外部媒体服务器的 base64 编码 CA
证书。可使用以下命令对证书进行编码并手动生成机密：</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>：包含用于创建 <literal>Cert-Manager</literal>
组件的配置（无需修改）。</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: true</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>Network 文件夹</title>
<para><literal>Network</literal>
文件夹中的文件数量与管理群集中的节点数量相同。在本例中，我们只有一个节点，因此此文件夹中只有一个文件，名为
<literal>mgmt-cluster-node1.yaml</literal>。该文件的名称必须与
<literal>mgmt-cluster.yaml</literal> 定义文件的上述 network/node 部分中定义的主机名一致。</para>
<para>如果您需要自定义网络配置，例如要使用特定的静态 IP 地址（无 DHCP 的方案），可以使用 <literal>network</literal>
文件夹中的 <literal>mgmt-cluster-node1.yaml</literal> 文件。该文件包含以下信息：</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>：网关 IP 地址。</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>：DNS 服务器 IP 地址。</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>：网络接口的 MAC 地址。</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>：管理群集的 IP 地址。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>如果您要使用 DHCP 获取 IP 地址，可使用以下配置（必须使用 <literal>${MGMT_MAC}</literal> 变量正确设置
<literal>MAC</literal> 地址）：</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>根据管理群集中的节点数，您可以创建更多文件（例如
<literal>mgmt-cluster-node2.yaml</literal>、<literal>mgmt-cluster-node3.yaml</literal>
等）来配置其余节点。</para>
</listitem>
<listitem>
<para><literal>routes</literal> 部分用于定义管理群集的路由表。</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>为隔离环境准备映像</title>
<para>本节介绍如何为隔离环境准备映像，其中只说明了与前面几节内容存在的差别。为隔离环境准备映像需要对上一节（为联网环境准备映像（<xref
linkend="mgmt-cluster-image-preparation-connected"/>））的内容进行以下更改：</para>
<itemizedlist>
<listitem>
<para>必须修改 <literal>mgmt-cluster.yaml</literal> 文件，以包含
<literal>embeddedArtifactRegistry</literal> 部分，并将 <literal>images</literal>
字段设置为要包含在 EIB 输出映像中的所有容器映像。</para>
</listitem>
<listitem>
<para>必须修改 <literal>mgmt-cluster.yaml</literal> 文件，以包含
<literal>rancher-turtles-airgap-resources</literal> Helm chart。</para>
</listitem>
<listitem>
<para>使用隔离环境时，必须去除 <literal>custom/scripts/99-register.sh</literal> 脚本。</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>定义文件中的修改</title>
<para>必须修改 <literal>mgmt-cluster.yaml</literal> 文件，以包含
<literal>embeddedArtifactRegistry</literal>
部分。在此部分中，<literal>images</literal> 字段必须设置为要包含在输出映像中的所有容器映像的列表。</para>
<note>
<para>以下是包含 <literal>embeddedArtifactRegistry</literal> 部分的
<literal>mgmt-cluster.yaml</literal> 文件示例。请确保列出的映像包含您所需的组件版本。</para>
</note>
<para>此外，还必须添加 <literal>rancher-turtles-airgap-resources</literal> Helm chart，以创建
<link
xl:href="https://documentation.suse.com/cloudnative/cluster-api/v0.19/en/getting-started/air-gapped-environment.html">Rancher
Turtles 隔离文档</link>中所述的资源。还需要 rancher-turtles chart 的 turtles.yaml
值文件来指定必要的配置。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/mirrored-cilium-cilium:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-cilium-operator-generic:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0
    - name: registry.rancher.com/rancher/neuvector-compliance-config:1.0.6
    - name: registry.rancher.com/rancher/neuvector-controller:5.4.5
    - name: registry.rancher.com/rancher/neuvector-enforcer:5.4.5
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.suse.com/rancher/cluster-api-addon-provider-fleet:v0.11.0
    - name: registry.rancher.com/rancher/cluster-api-operator:v0.18.1
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.7-build20250425
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher/turtles:v0.24.0
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/shell:v0.4.1
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.suse.com/rancher/cluster-api-controller:v1.10.5
    - name: registry.suse.com/rancher/cluster-api-provider-metal3:v1.10.2
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1
    - name: registry.suse.com/rancher/hardened-sriov-network-operator:v1.5.0-build20250425
    - name: registry.suse.com/rancher/ip-address-manager:v1.10.2
    - name: registry.rancher.com/rancher/kubectl:v1.32.2
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.9.5</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>custom 文件夹中的修改</title>
<itemizedlist>
<listitem>
<para>使用隔离环境时，必须去除 <literal>custom/scripts/99-register.sh</literal>
脚本。如目录结构中所示，<literal>99-register.sh</literal> 脚本并未包含在
<literal>custom/scripts</literal> 文件夹中。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-helm-values-folder-airgap">
<title>Helm 值文件夹中的修改</title>
<itemizedlist>
<listitem>
<para><literal>turtles.yaml</literal>：包含为 Rancher Turtles 指定隔离操作所需的配置，请注意，具体配置取决于
rancher-turtles-airgap-resources chart 的安装。</para>
<screen language="yaml" linenumbering="unnumbered">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"</screen>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>映像创建</title>
<para>按照前面的章节准备好目录结构后（适用于联网场景和隔离场景），运行以下命令来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>这会创建 ISO 输出映像文件，根据前面所述的映像定义，本例中该文件是
<literal>eib-mgmt-cluster-image.iso</literal>。</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>置备管理群集</title>
<para>上图包含前面介绍的所有组件，可以使用虚拟机或裸机服务器（使用虚拟媒体功能）根据此图置备管理群集。</para>
</section>
<section xml:id="mgmt-cluster-dualstack">
<title>双栈考虑因素和配置</title>
<para>前面章节中的示例提供了有关如何设置单栈 IPv4 管理群集的指导和示例。此类管理群集不受下游群集运行状态的影响，下游群集在部署后可分别配置为以
IPv4/IPv6 单栈或双栈模式运行。然而，管理群集的配置方式会直接影响置备阶段可使用的通信协议 -
在此阶段，带内和带外通信都必须遵循管理群集和下游主机支持的协议。如果部分或所有 BMC 和/或下游群集节点需使用 IPv6，则管理群集需采用双栈配置。</para>
<note>
<para>目前暂不支持单栈 IPv6 管理群集。</para>
</note>
<para>要实现双栈功能，必须为 Kubernetes 提供用于 POD 和服务的 IPv4 和 IPv6 CIDR。此外，在使用 EIB
构建管理群集映像前，还需对其他组件进行特定调整。可根据您的体系结构或要求，通过不同方式配置
Metal<superscript>3</superscript> 置备服务 (Ironic)：</para>
<itemizedlist>
<listitem>
<para>可将 Ironic 服务配置为在系统的所有接口上侦听，而非仅在单个 IP 地址上侦听。因此，只要管理群集主机在相关接口上分配了 IPv4 和 IPv6
地址，置备过程中就可能使用其中任意一个地址。请注意，目前仅能选择这些地址中的一个用于生成 URL（供裸机操作器、BMC 等其他服务使用）；因此，要实现与
BMC 的 IPv6 通信，可指示裸机操作器在处理包含 IPv6 地址的 BMH 定义时公开并传递 IPv6 URL。也就是说，当 BMC 被识别为支持
IPv6 时，将仅通过 IPv6 进行置备；其他所有情况下则通过 IPv4 进行置备。</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript> 可以使用解析为 IPv4 和 IPv6 的单个主机名，让 Ironic
使用这些地址进行绑定和创建 URL。此方法可实现简单的配置和灵活的行为（置备的每个步骤都可使用 IPv4 和 IPv6），但要求体系结构中已存在 DNS
服务器、IP 分配和记录。</para>
</listitem>
</itemizedlist>
<para>无论采用哪种方式，Kubernetes 都需要知道用于 IPv4 和 IPv6 的 CIDR。因此，您可在 EIB 目录下的
<literal>kubernetes/config/server.yaml</literal> 中添加以下行，并确保 IPv4 排在前面：</para>
<screen language="yaml" linenumbering="unnumbered">service-cidr: 10.96.0.0/12,fd12:4567:789c::/112
cluster-cidr: 193.168.0.0/18,fd12:4567:789b::/48</screen>
<para>某些容器会利用主机网络，因此需要在 <literal>network</literal> 目录下修改主机的网络配置，以启用 IPv6 连接：</para>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    next-hop-address: ${MGMT_GATEWAY_V4}
    next-hop-interface: eth0
  - destination: ::/0
    next-hop-address: ${MGMT_GATEWAY_V6}
    next-hop-interface: eth0
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
    - 2001:4860:4860::8888
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP_V4}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    address:
    - ip: ${MGMT_CLUSTER_IP_V6}
      prefix-length: 128
    dhcp: false
    autoconf: false
    enabled: true</screen>
<para>将占位符替换为网关 IP 地址、额外的 DNS 服务器（如有需要）、网络接口的 MAC 地址以及管理群集的 IP
地址。如果更喜欢使用地址自动配置，可参考以下代码，只需设置 <literal>${MGMT_MAC}</literal> 变量：</para>
<screen language="yaml" linenumbering="unnumbered">interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    enabled: true
    dhcp: true
  ipv6:
    enabled: false
    dhcp: true
    autoconf: true</screen>
<para>现在，我们从第一种方式开始，为单节点配置定义其余文件。创建
<literal>kubernetes/helm/values/metal3.yaml</literal>，内容如下：</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${MGMT_CLUSTER_IP_V4}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  listenOnAll: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort
metal3-baremetal-operator:
  baremetaloperator:
    externalHttpIPv6: ${MGMT_CLUSTER_IP_V6}</screen>
<para>创建 <literal>kubernetes/helm/values/rancher.yaml</literal>，内容如下：</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_IP_V4}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>其中，<literal>${MGMT_CLUSTER_IP_V4}</literal> 和
<literal>${MGMT_CLUSTER_IP_V6}</literal> 是之前分配给主机的 IP 地址。</para>
<para>或者，如要使用主机名替代 IP 地址，可将 <literal>kubernetes/helm/values/metal3.yaml</literal>
修改为：</para>
<screen language="yaml" linenumbering="unnumbered">global:
  provisioningHostname: `${MGMT_CLUSTER_HOSTNAME}`
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort</screen>
<para>将 <literal>kubernetes/helm/values/rancher.yaml</literal> 修改为：</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${MGMT_CLUSTER_HOSTNAME}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</screen>
<para>其中，<literal>${MGMT_CLUSTER_HOSTNAME}</literal> 应为解析为您主机 IP 地址的完全限定域名。</para>
<para>有关详细信息，请访问 <link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/mgmt-cluster/dual-stack">SUSE
Edge GitHub 代码库的“dual-stack”文件夹</link>，其中提供了示例目录结构。</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>电信功能配置</title>
<para>本章将阐释通过 SUSE Telco Cloud 部署的群集上与电信相关的功能配置。</para>
<para>将使用有关自动置备的一章（<xref linkend="atip-automated-provisioning"/>）中所述的定向网络置备部署方法。</para>
<para>本章涵盖以下主题：</para>
<itemizedlist>
<listitem>
<para>实时内核映像（<xref linkend="kernel-image-for-real-time"/>）：实时内核使用的内核映像。</para>
</listitem>
<listitem>
<para>实现低延迟和高性能需指定的内核参数（<xref
linkend="kernel-args"/>）：为了在运行电信工作负载时实现最高性能和低延迟，实时内核需使用的内核参数。</para>
</listitem>
<listitem>
<para>通过 Tuned 和内核参数实现 CPU 绑定（<xref linkend="cpu-tuned-configuration"/>）：通过内核参数和
Tuned 配置文件隔离 CPU。</para>
</listitem>
<listitem>
<para>CNI 配置（<xref linkend="cni-configuration"/>）：Kubernetes 群集使用的 CNI 配置。</para>
</listitem>
<listitem>
<para>SR-IOV 配置（<xref linkend="sriov"/>）：Kubernetes 工作负载使用的 SR-IOV 配置。</para>
</listitem>
<listitem>
<para>DPDK 配置（<xref linkend="dpdk"/>）：系统使用的 DPDK 配置。</para>
</listitem>
<listitem>
<para>vRAN 加速卡（<xref linkend="acceleration"/>）：Kubernetes 工作负载使用的加速卡配置。</para>
</listitem>
<listitem>
<para>大页（<xref linkend="huge-pages"/>）：Kubernetes 工作负载使用的大页配置。</para>
</listitem>
<listitem>
<para>Kubernetes 上的 CPU 绑定（<xref linkend="cpu-pinning-kubernetes"/>）：配置 Kubernetes
和应用程序以利用 CPU 绑定的优势。</para>
</listitem>
<listitem>
<para>可感知 NUMA 的调度配置（<xref linkend="numa-aware-scheduling"/>）：Kubernetes
工作负载使用的可感知 NUMA 的调度配置。</para>
</listitem>
<listitem>
<para>MetalLB 配置（<xref linkend="metal-lb-configuration"/>）：Kubernetes 工作负载使用的
MetalLB 配置。</para>
</listitem>
<listitem>
<para>专用仓库配置（<xref linkend="private-registry"/>）：Kubernetes 工作负载使用的专用仓库配置。</para>
</listitem>
<listitem>
<para>精确时间协议配置（<xref linkend="ptp-configuration"/>）：用于运行电信行业 PTP 配置的配置文件。</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>实时内核映像</title>
<para>实时内核映像不一定比标准内核更好。它是针对特定用例进行微调的另一种内核。经过微调的实时内核可以降低延迟，但代价是降低了吞吐量。不建议将实时内核用于一般用途，但在本例中，它是建议用于电信工作负载的内核，因为其中的延迟是一项关键考虑因素。</para>
<para>实时内核有四大特性：</para>
<itemizedlist>
<listitem>
<para>确定性执行：</para>
<para>获得更高的可预测性 —
确保关键业务流程每次都能及时完成并提供高质量的服务，即使系统负载极高的情况下也不例外。通过围隔出关键系统资源供高优先级流程使用，可以确保为时间敏感型应用程序提供更高的可预测性。</para>
</listitem>
<listitem>
<para>低抖动：</para>
<para>基于高确定性技术的低抖动有助于应用程序与现实世界保持同步。这可以为那些需要持续重复计算的服务提供帮助。</para>
</listitem>
<listitem>
<para>优先级继承：</para>
<para>优先级继承是指当较高优先级的进程在完成其任务之前需要先等待较低优先级的进程完成时，较低优先级的进程可以提升为较高优先级的功能。SUSE Linux
Enterprise Real Time 解决了任务关键型进程的优先级倒置问题。</para>
</listitem>
<listitem>
<para>线程中断：</para>
<para>在通用操作系统中以中断模式运行的进程不可抢占。在 SUSE Linux Enterprise Real Time
中，这些中断已由可中断的内核线程封装，并允许用户定义的较高优先级进程抢占硬中断和软中断。</para>
<para>在本例中，如果您安装了 <literal>SUSE Linux Micro RT</literal> 之类的实时映像，那么就已经安装了实时内核。可以从
<link xl:href="https://scc.suse.com/">SUSE Customer Center</link> 下载实时内核映像。</para>
<note>
<para>有关实时内核的详细信息，请访问 <link xl:href="https://www.suse.com/products/realtime/">SUSE
Real Time</link>。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="kernel-args">
<title>实现低延迟和高性能需指定的内核参数</title>
<para>内核参数必须进行配置，以便实时内核能够正常工作，从而为运行电信工作负载提供最佳性能和低延迟。在为此使用场景配置内核参数时，需要特别注意一些重要的概念：</para>
<itemizedlist>
<listitem>
<para>使用 SUSE 实时内核时需去除 <literal>kthread_cpus</literal>。此参数控制在哪些 CPU
上创建内核线程。它还控制允许哪些 CPU 用于 PID 1 及加载内核模块（由 kmod 用户空间辅助工具加载）。此参数无法被 SUSE
实时内核识别，因此没有任何效果。</para>
</listitem>
<listitem>
<para>通过
<literal>isolcpus</literal>、<literal>nohz_full</literal>、<literal>rcu_nocbs</literal>
和 <literal>irqaffinity</literal> 隔离 CPU 核心。有关 CPU 绑定技术的完整列表，请参见<xref
linkend="cpu-tuned-configuration"/>。</para>
</listitem>
<listitem>
<para>将 <literal>domain,nohz,managed_irq</literal> 标志添加到
<literal>isolcpus</literal> 内核参数。如果没有任何标志，<literal>isolcpus</literal> 相当于只指定
<literal>domain</literal> 标志。这将导致指定的 CPU
无法进行调度，包括内核任务。<literal>nohz</literal> 标志会停止指定 CPU 上运行的调度器节拍（如果一个 CPU
上只有一个任务可运行），<literal>managed_irq</literal> 标志可避免在指定 CPU
上路由受管理的外部（设备）中断。需要注意的是，NVMe 设备的中断请求 (IRQ)
线路由内核全权管理，因此会被路由至非隔离（系统管理）核心。例如，本节末尾提供的命令行会导致系统中仅分配 4 个队列（外加 1 个管理/控制队列）：</para>
<screen language="shell" linenumbering="unnumbered">for I in $(grep nvme0 /proc/interrupts | cut -d ':' -f1); do cat /proc/irq/${I}/effective_affinity_list; done | column
39      0       19      20      39</screen>
<para>此行为可防止磁盘 I/O 对运行在隔离核心上的任何时间敏感型应用程序造成干扰，但可能需要对以存储为重点的工作负载进行关注和精心设计。</para>
</listitem>
<listitem>
<para>调整节拍（内核的周期性定时器中断）：</para>
<itemizedlist>
<listitem>
<para><literal>skew_tick=1</literal>：有时可能同时发生多个节拍。指定
<literal>skew_tick=1</literal> 可使所有 CPU
不在同一时刻接收定时器节拍，而是在略微错开的时间点接收。这有助于减少系统抖动，从而实现更一致且更低的中断响应时间（这是对延迟敏感型应用程序的基本要求）。</para>
</listitem>
<listitem>
<para><literal>nohz=on</literal>：停止空闲 CPU 上的周期性定时器节拍。</para>
</listitem>
<listitem>
<para><literal>nohz_full=&lt;cpu-cores&gt;</literal>：停止专用于实时应用程序的指定 CPU
上的周期性定时器节拍。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>通过指定 <literal>mce=off</literal> 禁用计算机检查异常 (MCE) 处理。MCE
是处理器检测到的硬件错误，禁用它们可以避免产生冗余日志。</para>
</listitem>
<listitem>
<para>添加 <literal>nowatchdog</literal>
以禁用软锁死看门狗，该看门狗通过运行在定时器硬中断上下文中的定时器实现。当定时器到期时（即检测到软锁死），它会（在硬中断上下文中）打印警告，违背所有延迟目标。即便从未到期，它也会加入定时器列表，略微增加每次定时器中断的开销。此选项还会禁用
NMI 看门狗，因此 NMI 不会造成干扰。</para>
</listitem>
<listitem>
<para><literal>nmi_watchdog=0</literal> 会禁用 NMI（不可屏蔽中断）看门狗。当使用
<literal>nowatchdog</literal> 时，可以省略此设置。</para>
</listitem>
<listitem>
<para>RCU（读取 - 复制 - 更新）是一种内核机制，允许多个读取器以无锁方式并发访问共享数据。RCU
回调是在“宽限期”后触发的函数，确保所有先前的读取器都已完成操作，从而可以安全地回收旧数据。我们对 RCU
进行微调，特别是针对敏感工作负载，将这些回调从专用（绑定的）CPU 卸载，防止内核操作干扰关键的时间敏感型任务。</para>
<itemizedlist>
<listitem>
<para>在 <literal>rcu_nocbs</literal> 中指定绑定的 CPU，使 RCU 回调不会在这些 CPU
上运行。这有助于减轻实时工作负载的抖动和延迟。</para>
</listitem>
<listitem>
<para><literal>rcu_nocb_poll</literal> 会使无回调 CPU 定期“轮询”，以确定是否需要处理回调。这可以减少中断开销。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_cpu_stall_suppress=1</literal> 会抑制 RCU CPU
停滞警告，在高负载实时系统中，这些警告有时可能是误报。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_expedited=1</literal> 会加速 RCU 操作的宽限期，使读取端关键部分的响应更加迅速。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_normal_after_boot=1</literal> 与 rcu_expedited 一起使用时，允许
RCU 在系统引导后恢复到正常（非加速）操作模式。</para>
</listitem>
<listitem>
<para><literal>rcupdate.rcu_task_stall_timeout=0</literal> 会禁用 RCU
任务停滞检测器，防止长时间运行的 RCU 任务可能引发的警告或系统暂停。</para>
</listitem>
<listitem>
<para><literal>rcutree.kthread_prio=99</literal> 会将 RCU 回调内核线程的优先级设置为可能的最高值
(99)，确保在需要时能够及时调度该线程并处理 RCU 回调。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>为使 Metal3 和 Cluster API 成功置备/取消置备群集，请添加
<literal>ignition.platform.id=openstack</literal>。该配置供 Metal3 Python
代理使用，该代理源自 OpenStack Ironic。</para>
</listitem>
<listitem>
<para>去除 <literal>intel_pstate=passive</literal>。此选项将
<literal>intel_pstate</literal> 配置为与通用 cpufreq
调节器搭配工作，但缺点是，为了实现这一点，它会禁用硬件管理的 P 状态
(<literal>HWP</literal>)。为了减少硬件延迟，不建议将此选项用于实时工作负载。</para>
</listitem>
<listitem>
<para>将 <literal>intel_idle.max_cstate=0 processor.max_cstate=1</literal> 替换为
<literal>idle=poll</literal>。为了避免 C 状态转换，使用 <literal>idle=poll</literal>
选项来禁用 C 状态转换并将 CPU 保持在最高 C 状态。<literal>intel_idle.max_cstate=0</literal>
选项会禁用 <literal>intel_idle</literal>，因此使用 <literal>acpi_idle</literal>，然后
<literal>acpi_idle.max_cstate=1</literal> 会为 acpi_idl 设置最大 C 状态。在
AMD64/Intel 64 体系结构上，第一个 ACPI C 状态始终是<literal>轮询</literal>，但它使用
<literal>poll_idle()</literal> 函数，这可能会因为定期读取时钟并在超时后重启
<literal>do_idle()</literal> 中的主循环（也涉及清除和设置 <literal>TIF_POLL</literal>
任务标志）而导致一些细微的延迟。相比之下，<literal>idle=poll</literal>
以紧凑循环运行，通过忙等待方式等待任务被重新调度。这最大限度地减少了由退出空闲状态造成的延迟，但代价是 CPU 在空闲线程中也要保持全速运行。</para>
</listitem>
<listitem>
<para>在 BIOS 中禁用 C1E。此选项对于禁用 BIOS 中的 C1E 状态非常重要，可以避免 CPU 在空闲时进入 C1E 状态。C1E
状态是一种低功耗状态，可能会在 CPU 空闲时造成延迟。</para>
</listitem>
</itemizedlist>
<para>本文档的其余部分介绍了其他参数，包括大页和 IOMMU。</para>
<para>以下是一个包含上述调整的 32 核 Intel 服务器的内核参数示例：</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
<para>以下是一个 64 核 AMD 服务器的配置示例。在 128 个逻辑处理器 (<literal>0-127</literal>) 中，前 8 个核心
(<literal>0-7</literal>) 用于系统管理，而其余 120 个核心 (<literal>8-127</literal>)
专用于运行应用程序：</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=575291cf-74e8-42cf-8f2c-408a20dc00b8 skew_tick=1 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack amd_iommu=on iommu=pt irqaffinity=0-7 isolcpus=domain,nohz,managed_irq,8-127 nohz_full=8-127 rcu_nocbs=8-127 mce=off nohz=on net.ifnames=0 nowatchdog nmi_watchdog=0 nosoftlockup quiet rcu_nocb_poll rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll</screen>
</section>
<section xml:id="cpu-tuned-configuration">
<title>通过 Tuned 和内核参数实现 CPU 绑定</title>
<para><literal>Tuned</literal>
是一个系统调优工具，它通过各种预定义配置文件监控系统状况以优化性能。其关键特性之一是能够为特定工作负载（如实时应用程序）隔离 CPU
核心。这可防止操作系统使用这些核心，从而避免潜在的延迟增加。</para>
<para>要启用和配置此功能，首先应该为我们想要隔离的 CPU 核心创建一个配置文件。在本示例中，64 个核心中，我们将 60 个核心（<literal>1-30
和 33-62</literal>）专用于应用程序，剩余 4 个核心用于系统管理。需要注意的是，隔离 CPU
的设计在很大程度上取决于实时应用程序的需求。</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>然后我们需要修改用于隔离 CPU 核心的 GRUB 选项以及其他重要的 CPU 使用参数。请务必根据您的当前硬件规格自定义以下选项：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">参数</entry>
<entry align="left" valign="top">值</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,&#x200B;managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>隔离核心 1-30 和 33-62。<literal>domain</literal> 表示这些 CPU
属于隔离域。<literal>nohz</literal> 在这些隔离的 CPU
空闲时启用无节拍操作，以减少中断。<literal>managed_irq</literal> 使绑定的 CPU 免受 IRQ 干扰。这考虑了
<literal>irqaffinity=0-7</literal>，它已将大多数 IRQ 定向到系统管理核心。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>此选项允许内核在隔离的 CPU 之间错开定时器中断。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>启用后，内核的周期性定时器中断（即“节拍”）将在任何空闲的 CPU 核心上停止。这主要有益于系统管理
CPU（<literal>0、31、32、63</literal>）。这可节省电力并减少这些通用核心上不必要的唤醒。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>对于隔离的核心，这会停止节拍，即使 CPU 运行单个活动任务，也不例外。这意味着它使 CPU
以完全无节拍模式（或“动态节拍”）运行。内核仅在实际需要时才会传递定时器中断。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>此选项会将 RCU 回调处理从指定的 CPU 核心卸载。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocb_poll</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>如果设置此选项，无 RCU 回调的 CPU 将定期“轮询”以确定是否需要处理回调，而不是由其他 CPU 显式唤醒。这可以减少中断开销。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>此选项允许内核在系统管理核心上运行中断。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>最大限度减少因退出空闲状态造成的延迟，但代价是 CPU 在空闲线程中也要保持全速运行。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>此选项仅禁用 NMI 看门狗。如果设置了 <literal>nowatchdog</literal>，则可以省略此选项。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>此选项会禁用软锁死看门狗，该看门狗通过运行在定时器硬中断上下文中的定时器实现。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>以下命令会修改 GRUB 配置并应用上述更改，以便下次引导时使用更改的配置：</para>
<para>编辑 <literal>/etc/default/grub</literal> 文件，在其中添加上述参数，如下所示：</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>更新 GRUB 配置：</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>要验证重引导后是否应用了这些参数，可使用以下命令检查内核命令行：</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<para>另外还有一个脚本可用于调整 CPU 配置，它主要执行以下步骤：</para>
<itemizedlist>
<listitem>
<para>将 CPU 调节器设置为 <literal>performance</literal>。</para>
</listitem>
<listitem>
<para>取消将定时器迁移到隔离 CPU 的设置。</para>
</listitem>
<listitem>
<para>将 kdaemon 线程迁移到管家 CPU。</para>
</listitem>
<listitem>
<para>将隔离的 CPU 延迟设置为尽可能低的值。</para>
</listitem>
<listitem>
<para>将 vmstat 更新延迟到 300 秒。</para>
</listitem>
</itemizedlist>
<para>该脚本可在 <link
xl:href="https://raw.githubusercontent.com/suse-edge/atip/refs/heads/release-3.4/telco-examples/edge-clusters/dhcp-less/eib/custom/files/performance-settings.sh">SUSE
Telco Cloud 示例代码库</link>中获得。</para>
</section>
<section xml:id="cni-configuration">
<title>CNI 配置</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal> 是 SUSE Telco Cloud 的默认 CNI 插件。要在 RKE2 群集上启用 Cilium
作为默认插件，需要在 <literal>/etc/rancher/rke2/config.yaml</literal> 文件中进行以下配置：</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>也可以使用命令行参数来指定此配置，即，将 <literal>--cni=cilium</literal> 添加到
<literal>/etc/systemd/system/rke2-server</literal> 文件的 server 行中。</para>
<para>要使用下一节（<xref linkend="option2-sriov-helm"/>）中所述的 <literal>SR-IOV</literal>
网络操作器，请将 <literal>Multus</literal> 与另一个 CNI 插件（例如 <literal>Cilium</literal>
或 <literal>Calico</literal>）一起用作辅助插件。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</section>
<section xml:id="id-calico">
<title>Calico</title>
<para><literal>Calico</literal> 是 SUSE Edge for Telco 的另一款 CNI 插件。要在 RKE2 群集上启用
Calico 作为默认插件，需要在 <literal>/etc/rancher/rke2/config.yaml</literal>
文件中进行以下配置：</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- calico</screen>
<para>也可通过命令行参数指定此配置，即在 <literal>/etc/systemd/system/rke2-server</literal> 文件的
server 行中添加 <literal>--cni=calico</literal>。</para>
<para>要使用下一节（<xref linkend="option2-sriov-helm"/>）中所述的 <literal>SR-IOV</literal>
网络操作器，请将 <literal>Multus</literal> 与另一个 CNI 插件（例如 <literal>Cilium</literal>
或 <literal>Calico</literal>）一起用作辅助插件。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- calico</screen>
<note>
<para>有关 CNI 插件的详细信息，请参见 <link
xl:href="https://docs.rke2.io/install/network_options">Network
Options</link>。</para>
</note>
</section>
<section xml:id="id-bond-cni">
<title>Bond CNI</title>
<para>通常而言，绑定是一种将多个网络接口聚合为单个逻辑“绑定”接口的方法。这通常用于通过引入冗余网络路径提高服务可用性，也可在特定绑定模式下用于提高带宽。以下
CNI 插件支持与 multus 结合使用的 Bond CNI 插件：</para>
<itemizedlist>
<listitem>
<para>MACVLAN</para>
</listitem>
<listitem>
<para>Host Device</para>
</listitem>
<listitem>
<para>SR-IOV</para>
</listitem>
</itemizedlist>
<section xml:id="id-bond-cni-with-macvlan">
<title>Bond CNI 与 MACVLAN 结合使用</title>
<para>要将 Bond CNI 插件与 MACVLAN
结合使用，容器中需要有两个空闲接口。本示例使用“enp8s0”和“enp9s0”。首先为它们创建网络附加定义：</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp8s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-conf
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "macvlan",
          "capabilities": { "ips": true },
          "master": "enp9s0",
          "mode": "bridge",
          "ipam": {}
        }, {
          "capabilities": { "mac": true },
          "type": "tuning"
        }
      ]
    }'</screen>
<para>之后，为绑定本身添加网络附加定义。</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>此处采用的是静态 IP 地址分配方式，将绑定的地址定义为 /24
网络上的“192.168.200.100”，网关位于该网络的第一个可用地址。在绑定的网络附加定义中，我们还定义了所需的绑定类型，本示例中为
active-backup。</para>
<para>要使用此绑定，Pod 需要了解所有接口。以下是 Pod 定义示例：</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-conf",
"interface": "net1"
},
{"name": "enp9s0-conf",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
<para>请注意注解如何引用所有网络，以及如何定义接口“enp8s0 → net1”和“enp9s0 → net2”之间的映射关系。</para>
</section>
<section xml:id="id-bond-cni-with-host-device">
<title>Bond CNI 与 Host Device 结合使用</title>
<para>要将 Bond CNI 插件与 Host Device
结合使用，主机上需要有两个空闲接口。这些接口随后会映射到容器中。本示例使用“enp8s0”和“enp9s0”。首先为它们创建网络附加定义：</para>
<para><emphasis role="strong">NetworkAttachmentDefinition enp8s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp8s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp8s0",
          "ipam": {}
        }]
    }'</screen>
<para><emphasis role="strong">NetworkAttachmentDefinition enp9s0</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: enp9s0-hostdev
spec:
  config: '{
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "host-device",
          "name": "host0",
          "device": "enp9s0",
          "ipam": {}
        }]
    }'</screen>
<para>之后，为绑定本身添加网络附加定义，此过程与 MACVLAN 场景类似。</para>
<para><emphasis role="strong">NetworkAttachmentDefinition bond</emphasis></para>
<screen language="shell" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-net1
spec:
  config: '{
  "type": "bond",
  "cniVersion": "0.3.1",
  "name": "bond-net1",
  "mode": "active-backup",
  "failOverMac": 1,
  "linksInContainer": true,
  "miimon": "100",
  "mtu": 1500,
  "links": [
     {"name": "net1"},
     {"name": "net2"}
  ],
  "ipam": {
    "type": "static",
    "addresses": [
      {
        "address": "192.168.200.100/24",
        "gateway": "192.168.200.1"
      }
    ],
    "subnet": "192.168.200.0/24",
    "routes": [{
      "dst": "0.0.0.0/0"
    }]
  }
}'</screen>
<para>此处采用的是静态 IP 地址分配方式，将绑定的地址定义为 /24
网络上的“192.168.200.100”，网关位于该网络的第一个可用地址。在绑定的网络附加定义中，定义绑定类型，本示例中为
active-backup。</para>
<para>要使用此绑定，Pod 需要了解所有接口。以下是结合 Host Device 使用绑定的 Pod 定义示例：</para>
<screen language="shell" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  annotations:
        k8s.v1.cni.cncf.io/networks: '[
{"name": "enp8s0-hostdev",
"interface": "net1"
},
{"name": "enp9s0-hostdev",
"interface": "net2"
},
{"name": "bond-net1",
"interface": "bond0"
}]'
spec:
  restartPolicy: Never
  containers:
  - name: bond-test
    image: alpine:latest
    command:
      - /bin/sh
      - "-c"
      - "sleep 60m"
    imagePullPolicy: IfNotPresent</screen>
</section>
<section xml:id="id-bond-cni-with-sr-iov">
<title>Bond CNI 与 SR-IOV 结合使用</title>
<para>将 Bond CNI 与 SR-IOV 结合使用的操作相当简单。有关如何设置 SR-IOV 的详细信息，请参见<xref
linkend="sriov"/>。如前文所述，您需要创建
<literal>SriovNetworkNodePolicies</literal>，在其中定义
<literal>resourceNames</literal> 以及虚拟功能的数量等。<literal>resourceNames</literal>
供 <literal>SriovNetwork</literal> 使用，SriovNetwork 在 Pod 定义中用作接口。绑定定义与
MACVLAN 和 Host Device 场景完全相同。</para>
<note>
<para>Bond CNI 与 SR-IOV 结合使用这种方式仅适用于使用内核驱动程序的 SRIOV 虚拟功能。用户空间驱动程序 VF（如 DPDK
工作负载中使用的 VF）无法通过 Bond CNI 进行绑定。</para>
</note>
</section>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOV 允许网络适配器等设备在各种 <literal>PCIe</literal> 硬件功能之间分隔对其资源的访问。可以通过多种方式部署
<literal>SR-IOV</literal>，本节将介绍两种不同的方式：</para>
<itemizedlist>
<listitem>
<para>方式 1：使用 <literal>SR-IOV</literal> CNI 设备插件和配置映射来正确配置 SR-IOV。</para>
</listitem>
<listitem>
<para>方式 2（建议）：使用 Rancher Prime 中的 <literal>SR-IOV</literal> Helm chart 来轻松完成此部署。</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">方式 1 - 安装 SR-IOV CNI 设备插件并准备配置映射来正确配置
SR-IOV</emphasis></para>
<itemizedlist>
<listitem>
<para>为设备插件准备配置映射</para>
</listitem>
</itemizedlist>
<para>使用 <literal>lspci</literal> 命令获取用于填充配置映射的信息：</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>配置映射由一个 <literal>JSON</literal>
文件组成，该文件描述通过过滤器发现的设备，以及创建接口组所需的信息。此处的关键在于理解过滤器和组。过滤器用于发现设备，组用于创建接口。</para>
<para>可以设置过滤器：</para>
<itemizedlist>
<listitem>
<para>vendorID：<literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID：<literal>0d5c</literal>（加速卡）</para>
</listitem>
<listitem>
<para>driver：<literal>vfio-pci</literal>（驱动程序）</para>
</listitem>
<listitem>
<para>pfNames：<literal>p2p1</literal>（物理接口名称）</para>
</listitem>
</itemizedlist>
<para>还可以设置过滤器来匹配更复杂的接口语法，例如：</para>
<itemizedlist>
<listitem>
<para>pfNames：<literal>["eth1#1,2,3,4,5,6"]</literal> 或
<literal>[eth1#1-6]</literal>（物理接口名称）</para>
</listitem>
</itemizedlist>
<para>至于组，我们可为 <literal>FEC</literal> 卡创建一个组，为 <literal>Intel</literal>
卡创建另一个组，甚至可以根据具体使用场景创建一个前缀：</para>
<itemizedlist>
<listitem>
<para>resourceName：<literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix：<literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>可以发现许多组合，并创建资源组来为 Pod 分配一些 <literal>VF</literal>。</para>
<note>
<para>有关过滤器和组的详细信息，请访问 <link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sr-iov
网络设备插件</link>。</para>
</note>
<para>根据硬件和使用场景设置用于匹配接口的过滤器和组后，下面的配置映射展示了一个可使用的示例：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para>准备 <literal>daemonset</literal> 文件以部署设备插件。</para>
</listitem>
</itemizedlist>
<para>设备插件支持多种体系结构（<literal>arm</literal>、<literal>amd</literal>、<literal>ppc64le</literal>），因此同一文件可用于不同的体系结构，以便为每个体系结构部署多个
<literal>daemonset</literal>。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>应用配置映射和 <literal>daemonset</literal> 后，将部署设备插件，发现接口并将其提供给 Pod 使用。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>检查节点中发现的且可供 Pod 使用的接口：</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para><literal>FEC</literal> 为 <literal>intel.com/intel_fec_5g</literal>，值为 1。</para>
</listitem>
<listitem>
<para>如果您部署 SR-IOV 时使用了设备插件和配置映射，但未使用 Helm chart，则 <literal>VF</literal> 为
<literal>intel.com/intel_sriov_odu</literal> 或
<literal>intel.com/intel_sriov_oru</literal>。</para>
</listitem>
</itemizedlist>
<important>
<para>如果此处没有接口，则继续操作就意义不大，因为 Pod 没有可用的接口。请先检查配置映射和过滤器来解决问题。</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">方式 2（建议）- 使用 Rancher 和 Helm chart 安装 SR-IOV CNI
和设备插件</emphasis></para>
<itemizedlist>
<listitem>
<para>获取 Helm（如果没有）：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>安装 SR-IOV。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">helm install sriov-crd oci://registry.suse.com/edge/charts/sriov-crd -n sriov-network-operator
helm install sriov-network-operator oci://registry.suse.com/edge/charts/sriov-network-operator -n sriov-network-operator</screen>
<itemizedlist>
<listitem>
<para>检查已部署的资源 CRD 和 Pod：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>检查节点中的标签。</para>
</listitem>
</itemizedlist>
<para>所有资源都运行后，标签会自动出现在您的节点中：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para>查看 <literal>daemonset</literal>，您会看到新的
<literal>sriov-network-config-daemon</literal> 和
<literal>sriov-rancher-nfd-worker</literal> 处于活动状态并已准备就绪：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>几分钟后（最多可能需要 10 分钟才会更新），将检测到节点，其上已配置了 <literal>SR-IOV</literal> 功能：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>检查已检测到的接口。</para>
</listitem>
</itemizedlist>
<para>发现的接口应为网络设备的 PCI 地址。请在主机中使用 <literal>lspci</literal> 命令检查此信息。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>如果此处未检测到您的接口，请确保该接口存在于下一个配置映射中：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>如果此处未检测到您的设备，请编辑配置映射，添加要发现的正确值（如有必要，请重启
<literal>sriov-network-config-daemon</literal> daemonset）。</para>
</note>
<itemizedlist>
<listitem>
<para>创建 <literal>NetworkNode 策略</literal>来配置 <literal>VF</literal>。</para>
</listitem>
</itemizedlist>
<para>将在设备 (<literal>rootDevices</literal>) 中创建一些 <literal>VF</literal>
(<literal>numVfs</literal>)，并在其上配置驱动程序 <literal>deviceType</literal> 和
<literal>MTU</literal>：</para>
<note>
<para><literal>resourceName</literal> 字段不得包含任何特殊字符，并且在整个群集中必须是唯一的。该示例使用
<literal>deviceType: vfio-pci</literal>，因为 <literal>dpdk</literal> 将与
<literal>sr-iov</literal> 结合使用。如果您不使用 <literal>dpdk</literal>，则 deviceType
应为 <literal>deviceType: netdevice</literal>（默认值）。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>验证配置：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>创建 sr-iov 网络（可选操作，仅在需要不同的网络时才执行）：</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>检查创建的网络：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.4.0", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal>（数据平面开发包）是一组用于实现快速数据包处理的库和驱动程序。它用于加速各种 CPU
体系结构上运行的数据包处理工作负载。DPDK 包含以下组件的数据平面库和优化的网络接口控制器 (<literal>NIC</literal>)
驱动程序：</para>
<orderedlist numeration="arabic">
<listitem>
<para>队列管理器，实现无锁队列。</para>
</listitem>
<listitem>
<para>缓冲区管理器，预先分配固定大小的缓冲区。</para>
</listitem>
<listitem>
<para>内存管理器，在内存中分配对象池，并使用环来存储空闲对象；确保对象均匀分布在所有 <literal>DRAM</literal> 通道上。</para>
</listitem>
<listitem>
<para>轮询模式驱动程序 (<literal>PMD</literal>)，可在没有异步通知的情况下运行，从而降低开销。</para>
</listitem>
<listitem>
<para>作为一组库提供的数据包框架，可帮助开发数据包处理解决方案。</para>
</listitem>
</orderedlist>
<para>以下步骤说明如何启用 <literal>DPDK</literal>，以及如何从 <literal>NIC</literal> 创建供
<literal>DPDK</literal> 接口使用的 <literal>VF</literal>：</para>
<itemizedlist>
<listitem>
<para>安装 <literal>DPDK</literal> 软件包：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk dpdk-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>内核参数：</para>
</listitem>
</itemizedlist>
<para>要使用 DPDK，请采用一些驱动程序来启用内核中的某些参数：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">参数</entry>
<entry align="left" valign="top">值</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>此选项允许为 DPDK 接口使用 <literal>vfio</literal> 驱动程序。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu 或 amd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>此选项允许为 <literal>VF</literal> 使用 <literal>vfio</literal>。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>要启用这些参数，请将其添加到 <literal>/etc/default/grub</literal> 文件中：</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>更新 GRUB 配置，并重引导系统以应用更改：</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para>加载 <literal>vfio-pci</literal> 内核模块，并在 <literal>NIC</literal> 上启用
<literal>SR-IOV</literal>：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>从 <literal>NIC</literal> 创建一些虚拟功能 (<literal>VF</literal>)。</para>
</listitem>
</itemizedlist>
<para>例如，要为两个不同的 <literal>NIC</literal> 创建 <literal>VF</literal>，需要运行以下命令：</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>将新的 VF 绑定到 <literal>vfio-pci</literal> 驱动程序：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>检查是否正确应用了配置：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>vRAN 加速 (<literal>Intel ACC100/ACC200</literal>)</title>
<para>随着通信服务提供商从 4G 过渡到 5G 网络，有许多提供商正在采用虚拟化无线接入网络 (<literal>vRAN</literal>)
体系结构来提高信道容量及简化基于边缘的服务和应用程序的部署。vRAN
解决方案非常适合用于提供低延迟服务，并可以根据实时流量和网络需求灵活地增加或减少容量。</para>
<para>计算密集程度最高的 4G 和 5G 工作负载之一是 RAN 第 1 层 (<literal>L1</literal>)
<literal>FEC</literal>，它可以解决不可靠或高干扰信道上的数据传输错误。<literal>FEC</literal>
技术可以检测并纠正 4G 或 5G 数据中有限数量的错误，因此消除了重新传输的需要。由于 <literal>FEC</literal>
加速事务不包含特定的单元状态信息，因此可以轻松虚拟化，从而带来了池化优势并可实现轻松的单元迁移。</para>
<itemizedlist>
<listitem>
<para>内核参数</para>
</listitem>
</itemizedlist>
<para>要启用 <literal>vRAN</literal> 加速，需要启用以下内核参数（如果尚未启用）：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">参数</entry>
<entry align="left" valign="top">值</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>此选项允许为 DPDK 接口使用 vfio。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu 或 amd_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>此选项允许为 VF 使用 vfio。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>修改 GRUB 文件 <literal>/etc/default/grub</literal>，以将这些参数添加到内核命令行：</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="BOOT_IMAGE=/boot/vmlinuz-6.4.0-9-rt root=UUID=77b713de-5cc7-4d4c-8fc6-f5eca0a43cf9 skew_tick=1 rd.timeout=60 rd.retry=45 console=ttyS1,115200 console=tty0 default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 ignition.platform.id=openstack intel_iommu=on iommu=pt irqaffinity=0,31,32,63 isolcpus=domain,nohz,managed_irq,1-30,33-62 nohz_full=1-30,33-62 nohz=on mce=off net.ifnames=0 nosoftlockup nowatchdog nmi_watchdog=0 quiet rcu_nocb_poll rcu_nocbs=1-30,33-62 rcupdate.rcu_cpu_stall_suppress=1 rcupdate.rcu_expedited=1 rcupdate.rcu_normal_after_boot=1 rcupdate.rcu_task_stall_timeout=0 rcutree.kthread_prio=99 security=selinux selinux=1 idle=poll"</screen>
<para>更新 GRUB 配置，并重引导系统以应用更改：</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>要校验重引导后是否应用了这些参数，请检查命令行：</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>加载 vfio-pci 内核模块以启用 <literal>vRAN</literal> 加速：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>获取 Acc100 接口信息：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>将物理接口 (<literal>PF</literal>) 绑定到 <literal>vfio-pci</literal> 驱动程序：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>从物理接口 (<literal>PF</literal>) 创建虚拟功能 (<literal>VF</literal>)。</para>
</listitem>
</itemizedlist>
<para>从 <literal>PF</literal> 创建 2 个 <literal>VF</literal>，并按照以下步骤将其绑定到
<literal>vfio-pci</literal>：</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>使用建议的配置文件配置 acc100：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>检查从 FEC PF 创建的新 VF：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>大页</title>
<para>当某个进程使用 <literal>RAM</literal> 时，<literal>CPU</literal> 会将 RAM
标记为已由该进程使用。为提高效率，<literal>CPU</literal> 会以区块的形式分配
<literal>RAM</literal>，在许多平台上，默认的区块大小值为 <literal>4K</literal>
字节。这些区块称为页。页可以交换到磁盘等位置。</para>
<para>由于进程地址空间是虚拟的，<literal>CPU</literal>
和操作系统需要记住哪些页属于哪个进程，以及每个页存储在哪个位置。页数越多，内存映射的搜索时间就越长。当进程使用 <literal>1
GB</literal> 内存时，需要查找 262144 个项 (<literal>1 GB</literal> / <literal>4
K</literal>)。如果一个页表项占用 8 个字节，则需要查找 <literal>2 MB</literal> (262144 * 8) 内存。</para>
<para>当前的大多数 <literal>CPU</literal> 体系结构都支持大于默认值的页，因此减少了
<literal>CPU/操作系统</literal>需要查找的项。</para>
<itemizedlist>
<listitem>
<para>内核参数</para>
</listitem>
</itemizedlist>
<para>要启用大页，我们应添加以下内核参数。在此例中，我们配置了 40 个 1G 页面，不过大页大小和确切数量应根据应用程序的内存需求进行调整：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">参数</entry>
<entry align="left" valign="top">值</entry>
<entry align="left" valign="top">说明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>此选项允许将大页的大小设置为 1 G</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>这是先前定义的大页数量</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>这是用于获取大页的默认值</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>修改 GRUB 文件 <literal>/etc/default/grub</literal>，在
<literal>GRUB_CMDLINE_LINUX</literal> 中添加这些参数：</para>
<screen language="shell" linenumbering="unnumbered">default_hugepagesz=1G hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0</screen>
<para>更新 GRUB 配置，并重引导系统以应用更改：</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>要验证重引导后是否应用了这些参数，可以检查命令行：</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>使用大页</para>
</listitem>
</itemizedlist>
<para>要使用大页，需要挂载它们：</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>部署 Kubernetes 工作负载，并创建资源和卷：</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-kubernetes">
<title>在 Kubernetes 上进行 CPU 绑定</title>
<section xml:id="id-prerequisite">
<title>先决条件</title>
<para>必须根据<xref linkend="cpu-tuned-configuration"/>一节中所述的性能配置文件微调
<literal>CPU</literal>。</para>
</section>
<section xml:id="id-configure-kubernetes-for-cpu-pinning">
<title>配置 Kubernetes 以进行 CPU 绑定</title>
<para>配置 kubelet 参数以在 <literal>RKE2</literal> 群集中实现 CPU 管理。将以下配置块（如下例所示）添加到
<literal>/etc/rancher/rke2/config.yaml</literal> 文件中。确保在
<literal>kubelet-reserved</literal> 和 <literal>system-reserved</literal>
参数中指定系统管理 CPU 核心：</para>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=0,31,32,63"
- "system-reserved=cpu=0,31,32,63"</screen>
</section>
<section xml:id="id-leveraging-pinned-cpus-for-workloads">
<title>为工作负载使用绑定的 CPU</title>
<para>根据您在工作负载上定义的请求和限制，可以用三种方式通过 kubelet 中定义的<literal>静态策略</literal>来使用该功能：</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>BestEffort</literal> QoS 类：如果没有定义任何 <literal>CPU</literal> 请求或限制，则
Pod 将调度到系统中第一个可用的 <literal>CPU</literal> 上。</para>
<para>使用 <literal>BestEffort</literal> QoS 类的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para><literal>Burstable</literal> QoS 类：如果定义了 CPU 请求且该请求不等于限制值，或者没有定义 CPU
请求，请使用此方式。</para>
<para>使用 <literal>Burstable</literal> QoS 类的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>或</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para><literal>Guaranteed</literal> QoS 类：如果定义了 CPU 请求且该请求等于限制值，请使用此方式。</para>
<para>使用 <literal>Guaranteed</literal> QoS 类的示例如下：</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="numa-aware-scheduling">
<title>可感知 NUMA 的调度</title>
<para>非统一内存访问或非统一内存体系结构 (<literal>NUMA</literal>) 是
<literal>SMP</literal>（多处理器）体系结构中使用的物理内存设计，其中内存访问时间取决于相对于处理器的内存位置。在
<literal>NUMA</literal>
下，与访问非本地内存（即，另一个处理器本地的内存，或者在处理器之间共享的内存）相比，处理器可以更快地访问自身的本地内存。</para>
<section xml:id="id-identifying-numa-nodes">
<title>识别 NUMA 节点</title>
<para>要识别 <literal>NUMA</literal> 节点，请在系统上使用以下命令：</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>对于此示例，只有一个 <literal>NUMA</literal> 节点显示了 64 个 <literal>CPU</literal>。</para>
<para><literal>NUMA</literal> 需要在 <literal>BIOS</literal> 中启用。如果
<literal>dmesg</literal> 中没有引导过程中 NUMA 初始化的记录，则可能表示内核环缓冲区中的
<literal>NUMA</literal> 相关消息已被重写。</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>MetalLB</title>
<para><literal>MetalLB</literal> 是裸机 Kubernetes 群集的负载平衡器实现，使用
<literal>L2</literal> 和 <literal>BGP</literal>
等标准路由协议作为广告协议。它是一个网络负载平衡器，可用于向外部公开 Kubernetes 群集中的服务（因为需要在裸机上使用 Kubernetes
服务类型 <literal>LoadBalancer</literal>）。</para>
<para>要在 <literal>RKE2</literal> 群集中启用 <literal>MetalLB</literal>，需要执行以下步骤：</para>
<itemizedlist>
<listitem>
<para>使用以下命令安装 <literal>MetalLB</literal>：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/metallb
  targetNamespace: metallb-system
  version: 304.0.0+up0.14.9
  createNamespace: true
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
  version: 304.0.1+up0.3.0
  createNamespace: true
EOF</screen>
<itemizedlist>
<listitem>
<para>创建 <literal>IpAddressPool</literal> 和 <literal>L2advertisement</literal> 配置：</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para>创建端点服务来公开 <literal>VIP</literal>：</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para>检查 <literal>VIP</literal> 是否已创建并且 <literal>MetalLB</literal> Pod 是否正在运行：</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>专用仓库配置</title>
<para>可以配置 <literal>Containerd</literal> 以连接到专用仓库，然后使用专用仓库提取每个节点上的专用映像。</para>
<para>启动时，<literal>RKE2</literal> 会检查 <literal>/etc/rancher/rke2/</literal> 中是否存在
<literal>registries.yaml</literal> 文件，并指示 <literal>containerd</literal>
使用该文件中定义的任何仓库。如果您希望使用某个专用仓库，请在要使用该仓库的每个节点上以 root 身份创建此文件。</para>
<para>要添加专用仓库，请创建包含以下内容的 <literal>/etc/rancher/rke2/registries.yaml</literal> 文件：</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>或者不设置身份验证：</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>要使仓库更改生效，需要先配置此文件再在节点上启动 RKE2，或者在配置的每个节点上重启 RKE2。</para>
<note>
<para>有关详细信息，请查看 <link
xl:href="https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html#_registries_configuration_file">RKE2
containerd 仓库配置</link>。</para>
</note>
</section>
<section xml:id="ptp-configuration">
<title>精确时间协议</title>
<para>精确时间协议 (PTP) 是由电气和电子工程师协会 (IEEE)
开发的一种网络协议，旨在实现计算机网络中的亚微秒级时间同步。自诞生以来的几十年里，PTP 已在许多行业广泛应用。近年来，随着其作为 5G
网络关键要素的重要性凸显，在电信网络中的采用率不断提升。尽管 PTP
协议相对简单，但其配置会因应用场景的不同而有显著差异。为此，已定义并标准化多个配置文件。</para>
<para>本节仅介绍专用于电信行业的配置文件。因此，假设 NIC 具备时间戳能力和 PTP 硬件时钟 (PHC)。如今，所有电信级网络适配器均在硬件层面支持
PTP，但您可以通过以下命令验证此类功能：</para>
<screen language="console" linenumbering="unnumbered"># ethtool -T p1p1
Time stamping parameters for p1p1:
Capabilities:
        hardware-transmit
        software-transmit
        hardware-receive
        software-receive
        software-system-clock
        hardware-raw-clock
PTP Hardware Clock: 0
Hardware Transmit Timestamp Modes:
        off
        on
Hardware Receive Filter Modes:
        none
        all</screen>
<para>将 <literal>p1p1</literal> 替换为用于 PTP 的接口名称。</para>
<para>以下章节将提供有关如何在 SUSE Telco Cloud 中安装和配置 PTP 的指导，但要求您已熟悉基本的 PTP 概念。如需简要了解 PTP 以及
SUSE Telco Cloud 中包含的 PTP 实现，请访问 <link
xl:href="https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html">https://documentation.suse.com/sles/html/SLES-all/cha-tuning-ptp.html</link>。</para>
<section xml:id="id-install-ptp-software-components">
<title>安装 PTP 软件组件</title>
<para>在 SUSE Telco Cloud 中，PTP 功能通过 <literal>linuxptp</literal> 软件包实现，该软件包包含两个组件：</para>
<itemizedlist>
<listitem>
<para><literal>ptp4l</literal>：控制 NIC 上的 PHC 并运行 PTP 协议的守护程序</para>
</listitem>
<listitem>
<para><literal>phc2sys</literal>：将系统时钟与 NIC 上经 PTP 同步的 PHC 保持同步的守护程序</para>
</listitem>
</itemizedlist>
<para>要使系统同步正常工作，就必须运行这两个守护程序，并且必须根据您的设置正确配置它们。相关信息，请参见<xref
linkend="ptp-telco-config"/>。</para>
<para>在下游群集中集成 PTP 最简单且最佳的方式是，在 Edge Image Builder (EIB) 定义文件的
<literal>packageList</literal> 下添加 <literal>linuxptp</literal>
软件包。这样，在群集置备期间就会自动安装 PTP 控制平面软件。有关安装软件包的详细信息，请参见 EIB 文档（<xref
linkend="eib-configuring-rpm-packages"/>）。</para>
<para>以下是包含 <literal>linuxptp</literal> 的 EIB 清单示例：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: {micro-base-rt-image-raw}
  outputImageName: eibimage-slmicrort-telco.raw
operatingSystem:
  time:
    timezone: America/New_York
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
    enable:
      - ptp4l
      - phc2sys
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
      - open-iscsi
      - tuned
      - cpupower
      - linuxptp
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<note>
<para>SUSE Telco Cloud 中包含的 <literal>linuxptp</literal> 软件包默认不会启用
<literal>ptp4l</literal> 和
<literal>phc2sys</literal>。如果在置备时部署了针对特定系统的配置文件（请参见<xref
linkend="ptp-capi"/>），则应启用它们。可通过将其添加到清单的 <literal>systemd</literal>
部分来实现，如上例所示。</para>
</note>
<para>按照 EIB 文档（<xref
linkend="eib-how-to-build-image"/>）中所述的常规过程构建映像，并使用该映像部署群集。如果您是 EIB
新手，请从<xref linkend="components-eib"/>开始。</para>
</section>
<section xml:id="ptp-telco-config">
<title>为电信部署配置 PTP</title>
<para>许多电信应用场景要求严格保持偏差极小的相位和时间同步，为此定义了两个面向电信的配置文件：ITU-T G.8275.1 和 ITU-T
G.8275.2。这两个配置文件均设置了高频率同步消息，以及其他独特特性（如使用替代的最佳主时钟算法 (BMCA)）。这种特性要求
<literal>ptp4l</literal> 所使用的配置文件中包含特定设置，以下几节将提供相关参考信息。</para>
<note>
<itemizedlist>
<listitem>
<para>以下两节仅介绍时间接收器配置中的普通时钟场景。</para>
</listitem>
<listitem>
<para>对于任何此类配置文件，都必须在精心规划的 PTP 基础架构中使用。</para>
</listitem>
<listitem>
<para>要用于您的特定 PTP 网络，可能需要额外微调配置，请务必检查并根据需要调整所提供的示例。</para>
</listitem>
</itemizedlist>
</note>
<section xml:id="id-ptp-profile-itu-t-g-8275-1">
<title>PTP 配置文件 ITU-T G.8275.1</title>
<para>G.8275.1 配置文件具有以下特点：</para>
<itemizedlist>
<listitem>
<para>直接运行在以太网上，需要完整的网络支持（相邻节点/交换机必须支持 PTP）。</para>
</listitem>
<listitem>
<para>默认域设置为 24。</para>
</listitem>
<listitem>
<para>数据集比较基于 G.8275.x 算法及其在 <literal>priority2</literal> 之后的
<literal>localPriority</literal> 值。</para>
</listitem>
</itemizedlist>
<para>将以下内容复制到名为 <literal>/etc/ptp4l-G.8275.1.conf</literal> 的文件中：</para>
<screen linenumbering="unnumbered"># Telecom G.8275.1 example configuration
[global]
domainNumber                    24
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             -3
logSyncInterval                 -4
logMinDelayReqInterval          -4
announceReceiptTimeout          3
serverOnly                      0
ptp_dst_mac                     01:80:C2:00:00:0E
network_transport               L2</screen>
<para>创建好文件后，必须在 <literal>/etc/sysconfig/ptp4l</literal> 中引用该文件，才能使守护程序正确启动。可通过将
<literal>OPTIONS=</literal> 行更改为以下内容实现：</para>
<screen linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"</screen>
<para>具体说明：</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> 指定要使用的配置文件名称（此处为
<literal>/etc/ptp4l-G.8275.1.conf</literal>）。</para>
</listitem>
<listitem>
<para><literal>-i</literal> 指定要使用的接口名称，请将 <literal>$IFNAME</literal> 替换为实际接口名称。</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> 可用于在系统日志中更好地识别 ptp4l 输出，这是可选参数。</para>
</listitem>
</itemizedlist>
<para>完成上述步骤后，必须（重新）启动 <literal>ptp4l</literal> 守护程序：</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>运行以下命令，通过观察日志来检查同步状态：</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-ptp-profile-itu-t-g-8275-2">
<title>PTP 配置文件 ITU-T G.8275.2</title>
<para>G.8275.2 配置文件具有以下特点：</para>
<itemizedlist>
<listitem>
<para>运行在 IP 上，不需要完整的网络支持（相邻节点/交换机可以不支持 PTP）。</para>
</listitem>
<listitem>
<para>默认域设置为 44。</para>
</listitem>
<listitem>
<para>数据集比较基于 G.8275.x 算法及其在 <literal>priority2</literal> 之后的
<literal>localPriority</literal> 值。</para>
</listitem>
</itemizedlist>
<para>将以下内容复制到名为 <literal>/etc/ptp4l-G.8275.2.conf</literal> 的文件中：</para>
<screen linenumbering="unnumbered"># Telecom G.8275.2 example configuration
[global]
domainNumber                    44
priority2                       255
dataset_comparison              G.8275.x
G.8275.portDS.localPriority     128
G.8275.defaultDS.localPriority  128
maxStepsRemoved                 255
logAnnounceInterval             0
serverOnly                      0
hybrid_e2e                      1
inhibit_multicast_service       1
unicast_listen                  1
unicast_req_duration            60
logSyncInterval                 -5
logMinDelayReqInterval          -4
announceReceiptTimeout          2
#
# Customize the following for slave operation:
#
[unicast_master_table]
table_id                        1
logQueryInterval                2
UDPv4                           $PEER_IP_ADDRESS
[$IFNAME]
unicast_master_table            1</screen>
<para>请确保替换以下占位符：</para>
<itemizedlist>
<listitem>
<para><literal>$PEER_IP_ADDRESS</literal> - 要与之通信的下一个 PTP 节点的 IP
地址，例如提供同步的主时钟或边界时钟。</para>
</listitem>
<listitem>
<para><literal>$IFNAME</literal> - 告知 <literal>ptp4l</literal> 使用哪个接口进行 PTP。</para>
</listitem>
</itemizedlist>
<para>创建好文件后，必须在 <literal>/etc/sysconfig/ptp4l</literal> 中引用该文件以及 PTP
所用接口的名称，才能使守护程序正确启动。可通过将 <literal>OPTIONS=</literal> 行更改为以下内容实现：</para>
<screen language="shell" linenumbering="unnumbered">OPTIONS="-f /etc/ptp4l-G.8275.2.conf --message_tag ptp-8275.2"</screen>
<para>具体说明：</para>
<itemizedlist>
<listitem>
<para><literal>-f</literal> 指定要使用的配置文件名称（此处为
<literal>/etc/ptp4l-G.8275.2.conf</literal>）。</para>
</listitem>
<listitem>
<para><literal>--message_tag</literal> 可用于在系统日志中更好地识别 ptp4l 输出，这是可选参数。</para>
</listitem>
</itemizedlist>
<para>完成上述步骤后，必须（重新）启动 <literal>ptp4l</literal> 守护程序：</para>
<screen language="console" linenumbering="unnumbered"># systemctl restart ptp4l</screen>
<para>运行以下命令，通过观察日志来检查同步状态：</para>
<screen language="console" linenumbering="unnumbered"># journalctl -e -u ptp4l</screen>
</section>
<section xml:id="id-configuration-of-phc2sys">
<title>phc2sys 的配置</title>
<para>建议在配置 <literal>phc2sys</literal> 之前先完成 <literal>ptp4l</literal>
的全部配置，不过这不是强制要求。<literal>phc2sys</literal> 不需要配置文件，其执行参数可通过
<literal>/etc/sysconfig/ptp4l</literal> 中的 <literal>OPTIONS=</literal>
变量单独控制，方式与 <literal>ptp4l</literal> 类似：</para>
<screen linenumbering="unnumbered">OPTIONS="-s $IFNAME -w"</screen>
<para>其中，<literal>$IFNAME</literal> 是已在 ptp4l 中设置的接口名称，将用作系统时钟的同步源。此参数用于识别源 PHC。</para>
</section>
</section>
<section xml:id="ptp-capi">
<title>Cluster API 集成</title>
<para>当通过管理群集和定向网络置备部署群集时，可在置备时将配置文件和 <literal>/etc/sysconfig</literal>
中的两个配置变量都部署到主机上。以下是群集定义的片段，重点展示经过修改的 <literal>RKE2ControlPlane</literal>
对象，该对象会在所有主机上部署相同的 G.8275.1 配置文件：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  serverConfig:
    cni: canal
  agentConfig:
    format: ignition
    cisProfile: cis
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /etc/ptp4l-G.8275.1.conf
              overwrite: true
              contents:
                inline: |
                  # Telecom G.8275.1 example configuration
                  [global]
                  domainNumber                    24
                  priority2                       255
                  dataset_comparison              G.8275.x
                  G.8275.portDS.localPriority     128
                  G.8275.defaultDS.localPriority  128
                  maxStepsRemoved                 255
                  logAnnounceInterval             -3
                  logSyncInterval                 -4
                  logMinDelayReqInterval          -4
                  announceReceiptTimeout          3
                  serverOnly                      0
                  ptp_dst_mac                     01:80:C2:00:00:0E
                  network_transport               L2
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/ptp4l
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): ptp4l settings
                  ## Type:           string
                  ## Default:        "-i eth0 -f /etc/ptp4l.conf"
                  ## ServiceRestart: ptp4l
                  #
                  # Arguments when starting ptp4l(8).
                  #
                  OPTIONS="-f /etc/ptp4l-G.8275.1.conf -i $IFNAME --message_tag ptp-8275.1"
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /etc/sysconfig/phc2sys
              overwrite: true
              contents:
                inline: |
                  ## Path:           Network/LinuxPTP
                  ## Description:    Precision Time Protocol (PTP): phc2sys settings
                  ## Type:           string
                  ## Default:        "-s eth0 -w"
                  ## ServiceRestart: phc2sys
                  #
                  # Arguments when starting phc2sys(8).
                  #
                  OPTIONS="-s $IFNAME -w"
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>除了其他变量外，必须为上述定义补全接口名称及其他 Cluster API 对象（如<xref
linkend="atip-automated-provisioning"/>中所述）。</para>
<note>
<itemizedlist>
<listitem>
<para>仅当群集中的硬件保持统一且所有主机都需要采用相同配置（包括接口名称）时，这种方法才适用。</para>
</listitem>
<listitem>
<para>也可以使用其他方法，未来版本中将会介绍相关内容。</para>
</listitem>
</itemizedlist>
</note>
<para>至此，您的主机应已具备可正常运行的 PTP 堆栈，并将开始协商其 PTP 角色。</para>
</section>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>全自动定向网络置备</title>
<section xml:id="id-introduction-3">
<title>简介</title>
<para>定向网络置备是用于自动置备下游群集的功能。如果您有许多下游群集需要置备并希望自动完成该过程，此功能将非常有用。</para>
<para>管理群集（<xref linkend="atip-management-cluster"/>）会自动部署以下组件：</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Micro RT</literal>（操作系统），可以根据使用场景自定义网络、存储、用户和内核参数等配置。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>（Kubernetes 群集），默认的 <literal>CNI</literal> 插件为
<literal>Cilium</literal>。根据具体的应用场景，可以使用某些 <literal>CNI</literal> 插件，例如
<literal>Cilium+Multus</literal>。</para>
</listitem>
<listitem>
<para><literal>SUSE Storage</literal></para>
</listitem>
<listitem>
<para><literal>SUSE Security</literal></para>
</listitem>
<listitem>
<para><literal>MetalLB</literal> 可用作高可用性多节点群集的负载平衡器。</para>
</listitem>
</itemizedlist>
<note>
<para>有关 <literal>SUSE Linux Micro</literal> 的详细信息，请参见<xref
linkend="components-slmicro"/>。有关 <literal>RKE2</literal> 的详细信息，请参见<xref
linkend="components-rke2"/>。有关 <literal>SUSE Storage</literal>
的详细信息，请参见<xref linkend="components-suse-storage"/>。有关 <literal>SUSE
Security</literal> 的详细信息，请参见<xref linkend="components-suse-security"/>。</para>
</note>
<para>以下章节介绍了不同的定向网络置备工作流程，以及可添加到置备过程的一些附加功能：</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
<note>
<para>以下几节介绍如何使用  SUSE Telco Cloud
为不同的定向网络置备工作流程场景做好准备工作。有关不同的部署配置选项示例（包括隔离式环境、DHCP 和无 DHCP 网络、专用容器仓库等），请参见
<link
xl:href="https://github.com/suse-edge/atip/tree/release-3.4/telco-examples/edge-clusters">SUSE
Telco Cloud 代码库</link>。</para>
</note>
</section>
<section xml:id="eib-edge-image-connected">
<title>为联网场景准备下游群集映像</title>
<para>Edge Image Builder（<xref linkend="components-eib"/>）用于准备经过修改、将置备到下游群集主机上的
SLEMicro 基础映像。</para>
<para>可以通过 Edge Image Builder 完成大部分配置，但本指南仅介绍设置下游群集所需的最低限度配置。</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>联网场景的先决条件</title>
<itemizedlist>
<listitem>
<para>需要安装 <link xl:href="https://podman.io">Podman</link> 或 <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link> 等容器运行时，以便能够运行
Edge Image Builder。</para>
</listitem>
<listitem>
<para>基础映像将按照<xref linkend="guides-kiwi-builder-images"/>所述通过
<literal>Base-SelfInstall</literal> 配置文件（对于实时内核，会使用
<literal>Base-RT-SelfInstall</literal> 配置文件）构建。为 x86-64 和 aarch64
这两种体系结构构建基础映像的过程是相同的。</para>
</listitem>
</itemizedlist>
<note>
<para>构建主机的体系结构必须与待构建映像的体系结构一致。也就是说，要构建 <literal>aarch64</literal> 体系结构的映像，必须使用
<literal>aarch64</literal> 体系结构的构建主机；<literal>x86-64</literal> 体系结构同样如此 -
目前不支持跨体系结构构建。</para>
</note>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>联网场景的映像配置</title>
<para>运行 Edge Image Builder 时，将从主机挂载一个目录，因此需要创建一个目录结构来存储用于定义目标映像的配置文件。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> 是映像定义文件，有关详细信息，请参见<xref
linkend="quickstart-eib"/>。</para>
</listitem>
<listitem>
<para>基础映像文件夹将包含按照<xref linkend="guides-kiwi-builder-images"/>所述使用
<literal>Base-SelfInstall</literal> 配置文件（对于实时内核，会使用
<literal>Base-RT-SelfInstall</literal> 配置文件）生成的输出原始映像。必须将该映像复制/移动到
<literal>base-images</literal> 文件夹下。</para>
</listitem>
<listitem>
<para><literal>network</literal> 文件夹是可选的，有关详细信息，请参见<xref
linkend="add-network-eib"/>。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal> 目录包含要在首次引导时运行的脚本：</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal> 脚本，用于在部署时调整操作系统根分区的大小</para>
</listitem>
<listitem>
<para><literal>02-performance.sh</literal> 脚本（可选），用于配置系统以调优性能。</para>
</listitem>
<listitem>
<para><literal>03-sriov.sh</literal> 脚本（可选），用于为 SR-IOV 配置系统。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>custom/files</literal> 目录包含映像创建过程中要复制到该映像的
<literal>performance-settings.sh</literal> 和
<literal>sriov-auto-filler.sh</literal> 文件。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
    |   └ 01-fix-growfs.sh
    |   └ 02-performance.sh
    |   └ 03-sriov.sh
    └ files/
        └ performance-settings.sh
        └ sriov-auto-filler.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>下游群集映像定义文件</title>
<para><literal>downstream-cluster-config.yaml</literal> 文件是下游群集映像的主配置文件。下面是通过
Metal<superscript>3</superscript> 进行部署的极简示例：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>其中，<literal>$SCC_REGISTRATION_CODE</literal> 是从 <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>
中复制的注册代码，并且软件包列表包含必需的 <literal>jq</literal>。</para>
<para><literal>$ROOT_PASSWORD</literal> 是 root 用户的已加密口令，可用于测试/调试目的。可以使用
<literal>openssl passwd-6 PASSWORD</literal> 命令生成此口令</para>
<para>对于生产环境，建议使用可添加到 users 块的 SSH 密钥（需将 <literal>$USERKEY1</literal> 替换为实际 SSH
密钥）。</para>
<note>
<para><literal>arch: x86_64</literal> 为映像的体系结构。对于 arm64 体系结构，请使用 <literal>arch:
aarch64</literal>。</para>
<para><literal>net.ifnames=1</literal> 会启用<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">可预测网络接口命名</link></para>
<para>这与 metal3 chart 的默认配置相匹配，但设置必须与配置的 chart
<literal>predictableNicNames</literal> 值相匹配。</para>
<para>另请注意，<literal>ignition.platform.id=openstack</literal> 是必需的，如果不指定此参数，在
Metal<superscript>3</superscript> 自动化流程中通过 ignition 配置 SLEMicro 时将会失败。</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Growfs 脚本</title>
<para>目前，在置备后首次引导时，需要使用一个自定义脚本
(<literal>custom/scripts/01-fix-growfs.sh</literal>)
来增大文件系统，使之与磁盘大小匹配。<literal>01-fix-growfs.sh</literal> 脚本包含以下信息：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="add-custom-script-performance">
<title>性能脚本</title>
<para>以下可选脚本（<literal>custom/scripts/02-performance.sh</literal>）可用于配置系统以调优性能：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para><literal>custom/files/performance-settings.sh</literal>
是可用于配置系统以调优性能的脚本，可从以下<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">链接</link>下载。</para>
</section>
<section xml:id="add-custom-script-sriov">
<title>SR-IOV 脚本</title>
<para>以下可选脚本 (<literal>custom/scripts/03-sriov.sh</literal>) 可用于为 SR-IOV 配置系统：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para><literal>custom/files/sriov-auto-filler.sh</literal> 是可用于为 SR-IOV
配置系统的脚本，可从以下<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">链接</link>下载。</para>
<note>
<para>使用相同的方法添加要在置备过程中执行的您自己的自定义脚本。有关详细信息，请参见<xref linkend="quickstart-eib"/>。</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>电信工作负载的附加配置</title>
<para>要启用 <literal>dpdk</literal>、<literal>sr-iov</literal> 或
<literal>FEC</literal> 等电信功能，可能需要提供附加软件包，如以下示例中所示。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: eibimage-output-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
      - fstrim
      - time-sync.target
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $user1Key1
  packages:
    packageList:
      - jq
      - dpdk
      - dpdk-tools
      - libdpdk-23
      - pf-bb-config
    sccRegistrationCode: $SCC_REGISTRATION_CODE</screen>
<para>其中，<literal>$SCC_REGISTRATION_CODE</literal> 是从 <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>
中复制的注册代码，并且软件包列表包含要用于 Telco 配置文件的最低限度的软件包。</para>
<note>
<para><literal>arch: x86_64</literal> 为映像的体系结构。对于 arm64 体系结构，请使用 <literal>arch:
aarch64</literal>。</para>
</note>
</section>
<section xml:id="add-network-eib">
<title>高级网络配置的附加脚本</title>
<para>如果您需要配置静态 IP 或<xref
linkend="advanced-network-configuration"/>中所述的更高级网络方案，则需要提供以下附加配置。</para>
<para>在 <literal>network</literal> 文件夹中创建以下
<literal>configure-network.sh</literal> 文件 - 这会在首次引导时使用配置驱动器数据，并使用 <link
xl:href="https://github.com/suse-edge/nm-configurator">NM Configurator
工具</link>来配置主机网络。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>映像创建</title>
<para>按照前面的章节准备好目录结构后，运行以下命令来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>这会根据上述定义创建名为 <literal>eibimage-output-telco.raw</literal> 的输出 ISO 映像文件。</para>
<para>然后必须通过 Web 服务器提供输出映像，该服务器可以是根据管理群集文档（<xref
linkend="metal3-media-server"/>）启用的媒体服务器容器，也可以是其他某个本地可访问的服务器。在下面的示例中，此服务器是
<literal>imagecache.local:8080</literal></para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>为隔离场景准备下游群集映像</title>
<para>Edge Image Builder（<xref linkend="components-eib"/>）用于准备经过修改、将置备到下游群集主机上的
SLEMicro 基础映像。</para>
<para>可以通过 Edge Image Builder 完成大部分配置，但本指南仅介绍为隔离场景设置下游群集所需的最低限度配置。</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>隔离场景的先决条件</title>
<itemizedlist>
<listitem>
<para>需要安装 <link xl:href="https://podman.io">Podman</link> 或 <link
xl:href="https://rancherdesktop.io">Rancher Desktop</link> 等容器运行时，以便能够运行
Edge Image Builder。</para>
</listitem>
<listitem>
<para>基础映像将按照<xref linkend="guides-kiwi-builder-images"/>所述通过
<literal>Base-SelfInstall</literal> 配置文件（对于实时内核，会使用
<literal>Base-RT-SelfInstall</literal> 配置文件）构建。为 x86-64 和 aarch64
这两种体系结构构建基础映像的过程是相同的。</para>
</listitem>
<listitem>
<para>如果您要使用 SR-IOV 或任何其他需要容器映像的工作负载，则必须部署并事先配置一个本地专用仓库（设置/未设置 TLS
和/或身份验证）。此仓库用于存储 Helm chart OCI 映像及其他映像。</para>
</listitem>
</itemizedlist>
<note>
<para>构建主机的体系结构必须与待构建映像的体系结构一致。也就是说，要构建 <literal>aarch64</literal> 体系结构的映像，必须使用
<literal>aarch64</literal> 体系结构的构建主机；<literal>x86-64</literal> 体系结构同样如此 -
目前不支持跨体系结构构建。</para>
</note>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>隔离场景的映像配置</title>
<para>运行 Edge Image Builder 时，将从主机挂载一个目录，因此需要创建一个目录结构来存储用于定义目标映像的配置文件。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal>
是映像定义文件，有关详细信息，请参见<xref linkend="quickstart-eib"/>。</para>
</listitem>
<listitem>
<para>基础映像文件夹将包含按照<xref linkend="guides-kiwi-builder-images"/>所述使用
<literal>Base-SelfInstall</literal> 配置文件（对于实时内核，会使用
<literal>Base-RT-SelfInstall</literal> 配置文件）生成的输出原始映像。必须将该映像复制/移动到
<literal>base-images</literal> 文件夹下。</para>
</listitem>
<listitem>
<para><literal>network</literal> 文件夹是可选的，有关详细信息，请参见<xref
linkend="add-network-eib"/>。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal> 目录包含要在首次引导时运行的脚本：</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>01-fix-growfs.sh</literal> 脚本，用于在部署时调整操作系统根分区的大小。</para>
</listitem>
<listitem>
<para><literal>02-airgap.sh</literal> 脚本，用于在为隔离环境创建映像时，将映像复制到正确的位置。</para>
</listitem>
<listitem>
<para><literal>03-performance.sh</literal> 脚本（可选），用于配置系统以调优性能。</para>
</listitem>
<listitem>
<para><literal>04-sriov.sh</literal> 脚本（可选），用于为 SR-IOV 配置系统。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>custom/files</literal> 目录包含在映像创建过程中要复制到映像的 <literal>rke2</literal>
和 <literal>cni</literal> 映像。此外，还可以包含可选的
<literal>performance-settings.sh</literal> 和
<literal>sriov-auto-filler.sh</literal> 文件。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    |   └ performance-settings.sh
    |   └ sriov-auto-filler.sh
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh
        └ 03-performance.sh
        └ 04-sriov.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>下游群集映像定义文件</title>
<para><literal>downstream-cluster-airgap-config.yaml</literal>
文件是下游群集映像的主配置文件，上一节（<xref linkend="add-telco-feature-eib"/>）已介绍其内容。</para>
</section>
<section xml:id="id-growfs-script">
<title>Growfs 脚本</title>
<para>目前，在置备后首次引导时，需要使用一个自定义脚本
(<literal>custom/scripts/01-fix-growfs.sh</literal>)
来增大文件系统，使之与磁盘大小匹配。<literal>01-fix-growfs.sh</literal> 脚本包含以下信息：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>隔离脚本</title>
<para>在映像创建过程中，需要使用以下脚本 (<literal>custom/scripts/02-airgap.sh</literal>)
将映像复制到正确的位置：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="add-custom-script-performance2">
<title>性能脚本</title>
<para>以下可选脚本（<literal>custom/scripts/03-performance.sh</literal>）可用于配置系统以调优性能：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/performance-settings

# copy the artifacts
cp performance-settings.sh /opt/performance-settings/</screen>
<para><literal>custom/files/performance-settings.sh</literal>
是可用于配置系统以调优性能的脚本，可从以下<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/performance-settings.sh">链接</link>下载。</para>
</section>
<section xml:id="add-custom-script-sriov2">
<title>SR-IOV 脚本</title>
<para>以下可选脚本 (<literal>custom/scripts/04-sriov.sh</literal>) 可用于为 SR-IOV 配置系统：</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/sriov
# copy the artifacts
cp sriov-auto-filler.sh /opt/sriov/sriov-auto-filler.sh</screen>
<para><literal>custom/files/sriov-auto-filler.sh</literal> 是可用于为 SR-IOV
配置系统的脚本，可从以下<link
xl:href="https://github.com/suse-edge/atip/blob/release-3.4/telco-examples/edge-clusters/dhcp/eib/custom/files/sriov-auto-filler.sh">链接</link>下载。</para>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>隔离场景的自定义文件</title>
<para><literal>custom/files</literal> 目录包含映像创建过程中要复制到该映像的 <literal>rke2</literal>
和 <literal>cni</literal> 映像。为了轻松生成映像，请使用以下<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">脚本</link>和<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-release-rke2-images.txt">此处</link>的映像列表在本地准备这些映像，以生成需要包含在
<literal>custom/files</literal> 中的制品。另外，可以从<link
xl:href="https://get.rke2.io/">此处</link>下载最新的
<literal>rke2-install</literal> 脚本。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>下载映像后，目录结构应如下所示：</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>预加载包含隔离场景和 SR-IOV 所需映像的专用仓库（可选）</title>
<para>如果您要在隔离场景中使用 SR-IOV 或要使用任何其他工作负载映像，必须按照以下步骤预加载包含这些映像的本地专用仓库：</para>
<itemizedlist>
<listitem>
<para>下载、提取 helm-chart OCI 映像并将其推送到专用仓库</para>
</listitem>
<listitem>
<para>下载、提取所需的其余映像并将其推送到专用仓库</para>
</listitem>
</itemizedlist>
<para>可使用以下脚本下载、提取映像并将其推送到专用仓库。本节将通过一个示例来说明如何预加载 SR-IOV
映像，但您也可以使用相同的方法来预加载任何其他自定义映像：</para>
<orderedlist numeration="arabic">
<listitem>
<para>预加载 SR-IOV 所需的 helm-chart OCI 映像：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>必须创建一个包含所需 helm-chart OCI 映像的列表：</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:304.0.2+up1.5.0
edge/sriov-crd-chart:304.0.2+up1.5.0
EOF</screen>
</listitem>
<listitem>
<para>使用以下<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-oci-artefacts.sh">脚本</link>和上面创建的列表生成本地
tarball 文件：</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/charts/sriov-network-operator:304.0.2+up1.5.0
Pulled: registry.suse.com/edge/charts/sriov-crd:304.0.2+up1.5.0
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-304.0.2+up1.5.0.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-304.0.2+up1.5.0.tgz</screen>
</listitem>
<listitem>
<para>使用以下<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-oci-artefacts.sh">脚本</link>将
tarball 文件上载到您的专用仓库（如 <literal>myregistry:5000</literal>），以便将上一步下载的 helm
chart OCI 映像预加载到您的仓库中：</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>预加载 SR-IOV 所需的其余映像：</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>在这种情况下，必须包含电信工作负载的 sr-iov 容器映像（例如，作为参考，您可以从 <link
xl:href="https://github.com/suse-edge/charts/blob/main/charts/sriov-network-operator/1.5.0/values.yaml">helm-chart
值</link>获取这些映像）</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.3.0-build20240816
rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
rancher/hardened-sriov-cni:v2.8.1-build20240820
rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
EOF</screen>
</listitem>
<listitem>
<para>必须使用以下<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-save-images.sh">脚本</link>和上面创建的列表，在本地生成包含所需映像的
tarball 文件：</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -l ./edge-release-images.txt -s registry.suse.com
Image pull success: registry.suse.com/rancher/hardened-sriov-network-operator:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-config-daemon:v1.3.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-cni:v2.8.1-build20240820
Image pull success: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.1.1-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v3.7.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.6.0-build20240816
Image pull success: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.3.0-build20240816
Creating edge-images.tar.gz with 7 images</screen>
</listitem>
<listitem>
<para>使用以下<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.4.0/scripts/day2/edge-load-images.sh">脚本</link>将
tarball 文件上载到您的专用仓库（如
<literal>myregistry:5000</literal>），以便将上一步下载的映像预加载到您的专用仓库中：</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>为隔离场景创建映像</title>
<para>按照前面的章节准备好目录结构后，运行以下命令来构建映像：</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>这会根据上述定义创建名为 <literal>eibimage-output-telco.raw</literal> 的输出 ISO 映像文件。</para>
<para>然后必须通过 Web 服务器提供输出映像，该服务器可以是根据管理群集文档（<xref
linkend="metal3-media-server"/>）启用的媒体服务器容器，也可以是其他某个本地可访问的服务器。在下面的示例中，此服务器是
<literal>imagecache.local:8080</literal>。</para>
</section>
</section>
<section xml:id="single-node">
<title>使用定向网络置备来置备下游群集（单节点）</title>
<para>本节介绍用于通过定向网络置备自动置备单节点下游群集的工作流程。这是自动置备下游群集的最简单的方法。</para>
<para><emphasis role="strong">要求</emphasis></para>
<itemizedlist>
<listitem>
<para>如前面的章节（<xref linkend="eib-edge-image-connected"/>）所述使用
<literal>EIB</literal> 生成的、附带用于设置下游群集的最低限度配置的映像，必须位于管理群集上您按照<xref
linkend="metal3-media-server"/>一节所述配置的确切路径中。</para>
</listitem>
<listitem>
<para>管理服务器已创建并可在后续章节中使用。有关详细信息，请参见管理群集相关的一章<xref
linkend="atip-management-cluster"/>。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">工作流程</emphasis></para>
<para>下图显示了用于通过定向网络置备自动置备单节点下游群集的工作流程：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 单节点群集自动置备 1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>可以执行两个不同的步骤来使用定向网络置备自动置备单节点下游群集：</para>
<orderedlist numeration="arabic">
<listitem>
<para>登记裸机主机，使其在置备过程中可用。</para>
</listitem>
<listitem>
<para>置备裸机主机，以安装并配置操作系统和 Kubernetes 群集。</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">登记裸机主机</emphasis></para>
<para>第一步是在管理群集中登记新的裸机主机，使其可供置备。为此，必须在管理群集中创建以下文件
(<literal>bmh-example.yaml</literal>)，以指定要使用的 <literal>BMC</literal>
身份凭证以及要登记的 <literal>BaremetalHost</literal> 对象：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>其中：</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal> — 新裸机主机的 <literal>BMC</literal> 用户名。</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal> — 新裸机主机的 <literal>BMC</literal> 口令。</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal> — 要使用的新裸机主机的 <literal>MAC</literal> 地址。</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal> — 裸机主机 <literal>BMC</literal> 的
<literal>URL</literal>（例如
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>）。要了解有关硬件提供商支持的不同选项的详细信息，请访问<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">此链接</link>。</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>体系结构必须是 <literal>x86_64</literal> 或
<literal>aarch64</literal>，具体取决于要注册的裸机主机的体系结构。</para>
</listitem>
<listitem>
<para>如果未在映像构建时或通过 <literal>BareMetalHost</literal>
定义指定主机的网络配置，系统将使用自动配置机制（DHCP、DHCPv6、SLAAC）。有关详细信息或复杂配置，请参见<xref
linkend="advanced-network-configuration"/>。</para>
</listitem>
</itemizedlist>
</note>
<para>创建该文件后，必须在管理群集中执行以下命令，才能在管理群集中开始登记新的裸机主机：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>随后会登记新裸机主机对象，其状态将从正在注册依次变为正在检查和可用。可使用以下命令检查状态变化：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para>在验证 <literal>BMC</literal> 身份凭证之前，<literal>BaremetalHost</literal>
对象将一直处于<literal>正在注册</literal>状态。验证身份凭证后，<literal>BaremetalHost</literal>
对象的状态将更改为<literal>正在检查</literal>，此步骤可能需要一段时间（最长 20
分钟），具体取决于所用的硬件。在检查阶段，将检索硬件信息并更新 Kubernetes 对象。使用以下命令检查信息：<literal>kubectl
get bmh -o yaml</literal>。</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">置备步骤</emphasis></para>
<para>裸机主机已登记并可供使用后，下一步是置备裸机主机，以安装并配置操作系统和 Kubernetes 群集。为此，必须在管理群集中创建以下文件
(<literal>capi-provisioning-example.yaml</literal>) 并在其中包含以下信息（可以通过合并以下块来生成
<literal>capi-provisioning-example.yaml</literal>）。</para>
<note>
<para>只需将 <literal>${...}</literal> 中的值替换为实际值。</para>
</note>
<para>下面的块是群集定义，可在其中使用 <literal>pods</literal> 和 <literal>services</literal>
块来配置网络。此外，它还包含对要使用的控制平面和基础架构（使用
<literal>Metal<superscript>3</superscript></literal> 提供程序）对象的引用。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:bad:cafe::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:bad:bad:cafe::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<note>
<itemizedlist>
<listitem>
<para>单栈和双栈部署均受支持，如果为仅 IPv4 群集，可从上述定义中去除 IPv6 CIDR。</para>
</listitem>
<listitem>
<para>单栈 IPv6 部署目前处于技术预览阶段，暂未提供正式支持。</para>
</listitem>
</itemizedlist>
</note>
<para><literal>Metal3Cluster</literal> 对象指定要配置的控制平面端点（请替换
<literal>${DOWNSTREAM_CONTROL_PLANE_IPV4}</literal>），以及
<literal>noCloudProvider</literal>（因为使用的是裸机节点）。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>
对象指定要使用的控制平面配置，<literal>Metal3MachineTemplate</literal>
对象指定要使用的控制平面映像。此外，它还包含有关要使用的复本数（在本示例中为 1）以及要使用的 <literal>CNI</literal>
插件（在本示例中为 <literal>Cilium</literal>）的信息。agentConfig 块包含要使用的
<literal>Ignition</literal> 格式，以及用于配置 <literal>RKE2</literal> 节点的
<literal>additionalUserData</literal>，其中包含诸如 systemd 服务（名为
<literal>rke2-preinstall.service</literal>）之类的信息，该服务会在置备过程中使用 Ironic 信息自动替换
<literal>BAREMETALHOST_UUID</literal> 和 <literal>node-name</literal>。为了启用
Cilium 相关的 Multus 功能，需在 <literal>rke2</literal> 服务器清单目录中创建名为
<literal>rke2-cilium-config.yaml</literal> 的文件，并在其中包含要使用的配置。最后一部分信息包含要使用的
Kubernetes 版本。<literal>${RKE2_VERSION}</literal> 是要使用的
<literal>RKE2</literal> 版本，请替换此值（例如替换为 <literal>v1.33.3+rke2r1</literal>）。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para><literal>Metal3MachineTemplate</literal> 对象指定以下信息：</para>
<itemizedlist>
<listitem>
<para><literal>dataTemplate</literal>，用作对模板的引用。</para>
</listitem>
<listitem>
<para><literal>hostSelector</literal>，在与登记过程中创建的标签匹配时使用。</para>
</listitem>
<listitem>
<para><literal>image</literal>，用作对上一节（<xref
linkend="eib-edge-image-connected"/>）中使用 <literal>EIB</literal>
生成的映像的引用；<literal>checksum</literal> 和
<literal>checksumType</literal>，用于验证映像。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal> 对象指定下游群集的 <literal>metaData</literal>。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>通过合并上述块创建该文件后，必须在管理群集中执行以下命令才能开始置备新的裸机主机：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>使用定向网络置备来置备下游群集（多节点）</title>
<para>本节介绍用于通过定向网络置备和
<literal>MetalLB</literal>（用作负载平衡器策略）自动置备多节点下游群集的工作流程。这是自动置备下游群集的最简单的方法。下图显示了用于通过定向网络置备和
<literal>MetalLB</literal> 自动置备多节点下游群集的工作流程。</para>
<para><emphasis role="strong">要求</emphasis></para>
<itemizedlist>
<listitem>
<para>如前面的章节（<xref linkend="eib-edge-image-connected"/>）所述使用
<literal>EIB</literal> 生成的、附带用于设置下游群集的最低限度配置的映像，必须位于管理群集上您按照<xref
linkend="metal3-media-server"/>一节所述配置的确切路径中。</para>
</listitem>
<listitem>
<para>管理服务器已创建并可在后续章节中使用。有关详细信息，请参见管理群集相关的一章：<xref
linkend="atip-management-cluster"/>。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">工作流程</emphasis></para>
<para>下图显示了用于通过定向网络置备自动置备多节点下游群集的工作流程：</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png"
width="100%"/> </imageobject>
<textobject><phrase>ATIP 多节点群集自动置备 1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>登记三个裸机主机，使其在置备过程中可用。</para>
</listitem>
<listitem>
<para>置备三个裸机主机，以使用 <literal>MetalLB</literal> 安装并配置操作系统和 Kubernetes 群集。</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">登记裸机主机</emphasis></para>
<para>第一步是在管理群集中登记三个裸机主机，使其可供置备。为此，必须在管理群集中创建以下文件（<literal>bmh-example-node1.yaml</literal>、<literal>bmh-example-node2.yaml</literal>
和 <literal>bmh-example-node3.yaml</literal>），以指定要使用的 <literal>BMC</literal>
身份凭证，以及要在管理群集中登记的 <literal>BaremetalHost</literal> 对象。</para>
<note>
<itemizedlist>
<listitem>
<para>只需将 <literal>${...}</literal> 中的值替换为实际值。</para>
</listitem>
<listitem>
<para>本节只会指导您完成一个主机的置备过程。这些步骤同样适用于另外两个节点。</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>其中:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal> — 第一个裸机主机的 BMC 用户名。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal> — 第一个裸机主机的 BMC 口令。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal> — 第一个裸机主机的要使用的 MAC 地址。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal> — 第一个裸机主机 BMC 的 URL（例如
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>）。URL
的主机部分可以是 IP 地址（v4 或
v6），也可以是域名（取决于现有体系结构是否支持）。要了解有关硬件提供商支持的不同选项的详细信息，请访问此<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">链接</link>。</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>如果未在映像构建时或通过 <literal>BareMetalHost</literal>
定义指定主机的网络配置，系统将使用自动配置机制（DHCP、DHCPv6、SLAAC）。有关详细信息或复杂配置，请参见<xref
linkend="advanced-network-configuration"/>。</para>
</listitem>
<listitem>
<para>单栈 IPv6 群集目前处于技术预览阶段，暂未提供正式支持。</para>
</listitem>
<listitem>
<para>体系结构必须是 <literal>x86_64</literal> 或
<literal>aarch64</literal>，具体取决于要注册的裸机主机的体系结构。</para>
</listitem>
<listitem>
<para>所有现代服务器都配备支持双栈的 BMC，但在双栈环境中投入生产使用前，应验证其 IPv6 支持情况（以及虚拟媒体功能是否可能支持使用主机名）。</para>
</listitem>
</itemizedlist>
</note>
<para>创建该文件后，必须在管理群集中执行以下命令，才能在管理群集中开始登记裸机主机：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>随后会登记新裸机主机对象，其状态将从正在注册依次变为正在检查和可用。可使用以下命令检查状态变化：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para>在验证 <literal>BMC</literal> 身份凭证之前，<literal>BaremetalHost</literal>
对象将一直处于<literal>正在注册</literal>状态。验证身份凭证后，<literal>BaremetalHost</literal>
对象的状态将更改为<literal>正在检查</literal>，此步骤可能需要一段时间（最长 20
分钟），具体取决于所用的硬件。在检查阶段，将检索硬件信息并更新 Kubernetes 对象。使用以下命令检查信息：<literal>kubectl
get bmh -o yaml</literal>。</para>
</note>
<para><emphasis role="strong">置备步骤</emphasis></para>
<para>三个裸机主机已登记并可供使用后，下一步是置备裸机主机，以安装和配置操作系统与 Kubernetes
群集，并创建用于管理该操作系统和群集的负载平衡器。为此，必须在管理群集中创建以下文件
(<literal>capi-provisioning-example.yaml</literal>) 并在其中包含以下信息（可以通过合并以下块来生成
capi-provisioning-example.yaml）。</para>
<note>
<itemizedlist>
<listitem>
<para>只需将 <literal>${...}</literal> 中的值替换为实际值。</para>
</listitem>
<listitem>
<para><literal>VIP</literal> 地址是未分配给任何节点的预留 IP 地址，用于配置负载平衡器。在双栈群集中，可同时指定 IPv4 和
IPv6 地址，但在以下示例中，将优先使用 IPv4 地址。</para>
</listitem>
</itemizedlist>
</note>
<para>下面的内容为群集定义，可在其中使用 <literal>pods</literal> 和 <literal>services</literal>
块来配置群集网络。此外，它还包含对要使用的控制平面和基础架构（使用
<literal>Metal<superscript>3</superscript></literal> 提供程序）对象的引用。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
        - fd00:1234:4321::/48
    services:
      cidrBlocks:
        - 10.96.0.0/12
        - fd00:5678:8765:4321::/112
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<note>
<para>单栈和双栈部署均受支持，如果为仅 IPv4 群集，可从后续章节中去除 IPv6 CIDR 和 IPv6 VIP 地址</para>
</note>
<para><literal>Metal3Cluster</literal> 对象指定使用已预留的 <literal>VIP</literal> 地址（请替换
<literal>${EDGE_VIP_ADDRESS_IPV4}</literal>）的待配置控制平面端点，以及
<literal>noCloudProvider</literal>（因为使用了三个裸机节点）。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS_IPV4}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>
对象指定要使用的控制平面配置，<literal>Metal3MachineTemplate</literal> 对象指定要使用的控制平面映像。</para>
<itemizedlist>
<listitem>
<para>要使用的复本数（在本例中为 3）。</para>
</listitem>
<listitem>
<para>负载平衡器要使用的通告模式（<literal>address</literal> 使用 L2 实现），以及要使用的地址（请将
<literal>${EDGE_VIP_ADDRESS}</literal> 替换为 <literal>VIP</literal> 地址）。</para>
</listitem>
<listitem>
<para><literal>serverConfig</literal> 包含要使用的 <literal>CNI</literal> 插件（本示例中为
<literal>Cilium</literal>），以及要在 <literal>tlsSan</literal> 下列出的额外
<literal>VIP</literal> 地址和名称。</para>
</listitem>
<listitem>
<para>agentConfig 块包含要使用的 <literal>Ignition</literal> 格式以及用于配置
<literal>RKE2</literal> 节点的 <literal>additionalUserData</literal>，其中的信息如下：</para>
<itemizedlist>
<listitem>
<para>名为 <literal>rke2-preinstall.service</literal> 的 systemd 服务，用于在置备过程中使用 Ironic
信息自动替换 <literal>BAREMETALHOST_UUID</literal> 和 <literal>node-name</literal>。</para>
</listitem>
<listitem>
<para><literal>storage</literal> 块，其中包含用于安装 <literal>MetalLB</literal> 和
<literal>endpoint-copier-operator</literal> 的 Helm chart。</para>
</listitem>
<listitem>
<para><literal>metalLB</literal> 自定义资源文件包含要使用的 <literal>IPaddressPool</literal> 和
<literal>L2Advertisement</literal>（请将
<literal>${EDGE_VIP_ADDRESS_IPV4}</literal> 替换为 <literal>VIP</literal> 地址）。</para>
</listitem>
<listitem>
<para>用于配置 <literal>kubernetes-vip</literal> 服务的 <literal>end-svc.yaml</literal>
文件，<literal>MetalLB</literal> 使用该服务来管理 <literal>VIP</literal> 地址。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>最后一部分信息包含要使用的 Kubernetes 版本。<literal>${RKE2_VERSION}</literal> 是要使用的
<literal>RKE2</literal> 版本，需替换此值（例如替换为 <literal>v1.33.3+rke2r1</literal>）。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS_IPV4}
      - ${EDGE_VIP_ADDRESS_IPV6}
      - https://${EDGE_VIP_ADDRESS_IPV4}.sslip.io
      - https://${EDGE_VIP_ADDRESS_IPV6}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            # https://docs.rke2.io/networking/multus_sriov#using-multus-with-cilium
            - path: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChartConfig
                  metadata:
                    name: rke2-cilium
                    namespace: kube-system
                  spec:
                    valuesContent: |-
                      cni:
                        exclusive: false
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/endpoint-copier-operator
                    targetNamespace: endpoint-copier-operator
                    version: 304.0.1+up0.3.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/metallb
                    targetNamespace: metallb-system
                    version: 304.0.0+up0.14.9
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS_IPV4}/32
                      - ${EDGE_VIP_ADDRESS_IPV6}/128
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ipFamilyPolicy: PreferDualStack
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "Node-multinode-cluster"</screen>
<para><literal>Metal3MachineTemplate</literal> 对象指定以下信息：</para>
<itemizedlist>
<listitem>
<para><literal>dataTemplate</literal>，用作对模板的引用。</para>
</listitem>
<listitem>
<para><literal>hostSelector</literal>，在与登记过程中创建的标签匹配时使用。</para>
</listitem>
<listitem>
<para><literal>image</literal>，用作对上一节（<xref
linkend="eib-edge-image-connected"/>）中使用 <literal>EIB</literal>
生成的映像的引用；<literal>checksum</literal> 和
<literal>checksumType</literal>，用于验证映像。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-output-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-output-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal> 对象指定下游群集的 <literal>metaData</literal>。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>以下 YAML 文件是工作节点的示例配置。</para>
<para>一个 <literal>MachineDeployment</literal>：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: multinode-cluster
    nodepool: nodepool-0
  name: multinode-cluster-workers
  namespace: default
spec:
  clusterName: multinode-cluster
  replicas: 3
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: multinode-cluster
      nodepool: nodepool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: multinode-cluster
        nodepool: nodepool-0
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: RKE2ConfigTemplate
          name: multinode-cluster-workers
      clusterName: multinode-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: multinode-cluster-workers
      nodeDrainTimeout: 0s
      version: ${RKE2_VERSION}</screen>
<para>`RKE2ConfigTemplate` 对象指定要用于多节点群集工作节点的配置模板。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: RKE2ConfigTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        nodeName: "Node-multinode-cluster-worker"
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target</screen>
<para><literal>Metal3MachineTemplate</literal> 对象包含工作节点的
<literal>dataTemplate</literal>、<literal>hostSelector</literal> 和
<literal>image</literal> 相关引用：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slmicro-rt-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal> 对象指定下游群集工作节点的
<literal>metaData</literal>：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-workers-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>合并上述各部分创建文件后，在管理群集中运行以下命令，开始置备三个新的裸机主机：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>高级网络配置</title>
<para>定向网络置备工作流程允许在下游群集中使用静态 IP、绑定、VLAN 等网络配置。</para>
<para>以下章节将介绍使用高级网络配置置备下游群集需额外执行的步骤。</para>
<para><emphasis role="strong">要求</emphasis></para>
<itemizedlist>
<listitem>
<para>使用 <literal>EIB</literal> 生成的映像必须包含<xref linkend="add-network-eib"/>一节中所述的
network 文件夹和脚本。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">配置</emphasis></para>
<para>在继续操作之前，请参考以下章节，了解登记和置备主机所需执行的步骤：</para>
<itemizedlist>
<listitem>
<para>使用定向网络置备来置备下游群集（单节点）（<xref linkend="single-node"/>）</para>
</listitem>
<listitem>
<para>使用定向网络置备来置备下游群集（多节点）（<xref linkend="multi-node"/>）</para>
</listitem>
</itemizedlist>
<para>任何高级网络配置都必须在登记时通过 <literal>BareMetalHost</literal> 主机定义以及包含
<literal>nmstate</literal> 格式 <literal>networkData</literal>
块的关联机密来应用。以下示例文件定义了一个包含所需 <literal>networkData</literal> 的机密，该机密会为下游群集主机请求静态
<literal>IP</literal> 和 <literal>VLAN</literal>：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      identifier: mac-address
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>如您所见，该示例展示了启用配备静态 IP 的接口的配置，以及使用基础接口启用 VLAN 的配置，在实际应用时，需根据您的基础架构将以下变量替换为实际值：</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal> — 用于边缘群集的控制平面接口（例如
<literal>eth0</literal>）。包含 <literal>identifier: mac-address</literal>
后，系统会通过 MAC 地址自动检查命名，因此可使用任何接口名称。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP}</literal> — 用作边缘群集端点的 IP 地址（必须与 kubeapi-server
端点匹配）。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX}</literal> — 用于边缘群集的 CIDR（例如，如果要使用
<literal>/24</literal> 或 <literal>255.255.255.0</literal>，请设为
<literal>/24</literal>）。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY}</literal> — 用于边缘群集的网关（例如
<literal>192.168.100.1</literal>）。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_MAC}</literal> — 用于控制平面接口的 MAC 地址（例如
<literal>00:0c:29:3e:3e:3e</literal>）。</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal> — 用于边缘群集的 DNS（例如
<literal>192.168.100.2</literal>）。</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal> — 用于边缘群集的 VLAN ID（例如 <literal>100</literal>）。</para>
</listitem>
</itemizedlist>
<para>任何其他符合 <literal>nmstate</literal> 标准的定义都可用于配置下游群集的网络，以适应特定要求。例如，可以指定静态双栈配置：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mac-address: ${CONTROLPLANE_MAC}
      ipv4:
        enabled: true
        dhcp: false
        address:
        - ip: ${CONTROLPLANE_IP_V4}
          prefix-length: ${CONTROLPLANE_PREFIX_V4}
      ipv6:
        enabled: true
        dhcp: false
        autoconf: false
        address:
        - ip: ${CONTROLPLANE_IP_V6}
          prefix-length: ${CONTROLPLANE_PREFIX_V6}
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V4}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
      - destination: ::/0
        next-hop-address: ${CONTROLPLANE_GATEWAY_V6}
        next-hop-interface: ${CONTROLPLANE_INTERFACE}
    dns-resolver:
      config:
        server:
        - ${DNS_SERVER_V4}
        - ${DNS_SERVER_V6}</screen>
<para>与前面的示例一样，请根据您的基础架构将以下变量替换为实际值：</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE_IP_V4}</literal> - 分配给主机的 IPv4 地址</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V4}</literal> - 主机 IP 所属网络的 IPv4 前缀</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_IP_V6}</literal> - 分配给主机的 IPv6 地址</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_PREFIX_V6}</literal> - 主机 IP 所属网络的 IPv6 前缀</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V4}</literal> - 与默认路由匹配的流量所用网关的 IPv4 地址</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_GATEWAY_V6}</literal> - 与默认路由匹配的流量所用网关的 IPv6 地址</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE_INTERFACE}</literal> -
地址要分配到的接口的名称，该接口同时供与默认路由匹配的出站流量使用（适用于 IPv4 和 IPv6）</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER_V4}</literal> 和/或 <literal>${DNS_SERVER_V6}</literal>
- 要使用的 DNS 服务器的 IP 地址（可指定单项或多项，支持 IPv4 和/或 IPv6 地址）</para>
</listitem>
</itemizedlist>
<note>
<itemizedlist>
<listitem>
<para>您可访问 <link
xl:href="https://github.com/suse-edge/atip/tree/main/telco-examples/edge-clusters">SUSE
Telco Cloud 示例代码库</link>获取复杂示例，包括仅 IPv6 和双栈配置。</para>
</listitem>
<listitem>
<para>单栈 IPv6 部署目前处于技术预览阶段，暂未提供正式支持。</para>
</listitem>
</itemizedlist>
</note>
<para>最后，无论网络配置详细信息如何，都要确保通过在 <literal>BaremetalHost</literal> 对象中附加
<literal>preprovisioningNetworkDataName</literal> 来引用该机密，以便成功在管理群集中登记主机。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: example-demo
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<itemizedlist>
<listitem>
<para>如果需要部署多节点群集，必须对每个节点执行相同的过程。</para>
</listitem>
<listitem>
<para>目前不支持 <literal>Metal3DataTemplate</literal>、<literal>networkData</literal> 和
<literal>Metal3 IPAM</literal>；只有通过静态机密进行的配置才完全受支持。</para>
</listitem>
<listitem>
<para>体系结构必须是 <literal>x86_64</literal> 或
<literal>aarch64</literal>，具体取决于要注册的裸机主机的体系结构。</para>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="add-telco">
<title>电信功能（DPDK、SR-IOV、CPU 隔离、大页、NUMA 等）</title>
<para>定向网络置备工作流程允许将下游群集中使用的电信功能自动化，以便在这些服务器上运行电信工作负载。</para>
<para><emphasis role="strong">要求</emphasis></para>
<itemizedlist>
<listitem>
<para>如前面的章节（<xref linkend="eib-edge-image-connected"/>）所述使用
<literal>EIB</literal> 生成的映像必须位于管理群集上您按照<xref
linkend="metal3-media-server"/>一节所述配置的确切路径中。</para>
</listitem>
<listitem>
<para>使用 <literal>EIB</literal> 生成的映像必须包含<xref
linkend="add-telco-feature-eib"/>一节中所述的特定电信软件包。</para>
</listitem>
<listitem>
<para>管理服务器已创建并可在后续章节中使用。有关详细信息，请参见管理群集相关的一章：<xref
linkend="atip-management-cluster"/>。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">配置</emphasis></para>
<para>基于以下两个章节的内容登记和置备主机：</para>
<itemizedlist>
<listitem>
<para>使用定向网络置备来置备下游群集（单节点）（<xref linkend="single-node"/>）</para>
</listitem>
<listitem>
<para>使用定向网络置备来置备下游群集（多节点）（<xref linkend="multi-node"/>）</para>
</listitem>
</itemizedlist>
<para>本节将介绍以下电信功能：</para>
<itemizedlist>
<listitem>
<para>DPDK 和 VF 创建</para>
</listitem>
<listitem>
<para>工作负载使用的 SR-IOV 和 VF 分配</para>
</listitem>
<listitem>
<para>CPU 隔离和性能调优</para>
</listitem>
<listitem>
<para>大页配置</para>
</listitem>
<listitem>
<para>内核参数调优</para>
</listitem>
</itemizedlist>
<note>
<para>有关电信功能的详细信息，请参见<xref linkend="atip-features"/>。</para>
</note>
<para>启用上述电信功能所需做出的更改都可以在 <literal>capi-provisioning-example.yaml</literal> 置备文件的
<literal>RKE2ControlPlane</literal>
块中完成。<literal>capi-provisioning-example.yaml</literal> 文件中的其余信息与置备相关章节（<xref
linkend="single-node-provision"/>）中提供的信息相同。</para>
<para>明确地说，为启用电信功能而需在该块 (<literal>RKE2ControlPlane</literal>) 中进行的更改如下：</para>
<itemizedlist>
<listitem>
<para>指定 <literal>preRKE2Commands</literal>，该参数用于在 <literal>RKE2</literal>
安装过程开始之前执行命令。本例使用 <literal>modprobe</literal> 命令启用
<literal>vfio-pci</literal> 和 <literal>SR-IOV</literal> 内核模块。</para>
</listitem>
<listitem>
<para>指定 ignition 文件
<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>，该文件用于定义要创建且向工作负载公开的接口、驱动程序及
<literal>VF</literal> 数量。</para>
<itemizedlist>
<listitem>
<para>只有 <literal>sriov-custom-auto-config</literal> 配置映射中的值可以替换为实际值。</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal> — 用于第一个 <literal>PF</literal>
接口的资源名称（例如 <literal>sriov-resource-du1</literal>）。它将添加到前缀
<literal>rancher.io</literal> 的后面，供工作负载用作标签（例如
<literal>rancher.io/sriov-resource-du1</literal>）。</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal> — 要使用的第一个 <literal>PF</literal>
接口的名称（例如 <literal>eth0</literal>）。</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal> — 要使用的第一个物理功能 <literal>PF</literal>
的名称。可以使用此名称生成更复杂的过滤器（例如 <literal>eth0#2-5</literal>）。</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal> — 用于第一个 <literal>VF</literal>
接口的驱动程序名称（例如 <literal>vfio-pci</literal>）。</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal> — 要为第一个 <literal>PF</literal> 接口创建的
<literal>VF</literal> 数量（例如 <literal>8</literal>）。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>提供 <literal>/var/sriov-auto-filler.sh</literal>，用作高层级配置映射
<literal>sriov-custom-auto-config</literal> 与包含底层硬件信息的
<literal>sriovnetworknodepolicy</literal>
之间的转换器。创建此脚本的目的是简化用户操作，让其无需提前了解复杂的硬件信息。不需要在此文件中进行更改，但如果我们需要启用
<literal>sr-iov</literal> 并创建 <literal>VF</literal>，则应该提供此脚本。</para>
</listitem>
<listitem>
<para>用于启用以下功能的内核参数：</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>参数</para></entry>
<entry align="left" valign="top"><para>值</para></entry>
<entry align="left" valign="top"><para>说明</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>domain,nohz,&#x200B;managed_irq,1-30,33-62</para></entry>
<entry align="left" valign="top"><para>隔离核心 1-30 和 33-62。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>允许内核在隔离的 CPU 之间错开定时器中断。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>允许内核在系统空闲时在单个 CPU 上运行定时器节拍周期。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>内核引导参数是当前用于配置完整 dynticks 及 CPU 隔离的主接口。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>允许内核在系统空闲时在单个 CPU 上运行 RCU 回调。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>允许内核在系统空闲时在单个 CPU 上运行中断。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>idle</para></entry>
<entry align="left" valign="top"><para>poll</para></entry>
<entry align="left" valign="top"><para>最大限度地减少因退出空闲状态造成的延迟。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>允许为 dpdk 接口使用 vfio。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>允许为 VF 使用 vfio。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>允许将大页的大小设置为 1 G。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>先前定义的大页数量。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>用于启用大页的默认值。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nowatchdog</para></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"><para>禁用看门狗。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nmi_watchdog</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>禁用 NMI 看门狗。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>以下 systemd 服务用于启用下述功能：</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal>，用于在置备过程中使用 Ironic 信息自动替换
<literal>BAREMETALHOST_UUID</literal> 和 <literal>node-name</literal>。</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal>，用于启用 <literal>CPU</literal>
核心隔离（例如 <literal>1-30,33-62</literal>）。</para>
</listitem>
<listitem>
<para><literal>performance-settings.service</literal>，用于调优 CPU 性能。</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal>，用于执行以下操作：安装
<literal>sriov</literal> Helm chart，等待自定义资源创建完成，运行
<literal>/var/sriov-auto-filler.sh</literal> 以替换配置映射
<literal>sriov-custom-auto-config</literal> 中的值，并创建工作负载要使用的
<literal>sriovnetworknodepolicy</literal>。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>${RKE2_VERSION}</literal> 是要使用的 <literal>RKE2</literal>
版本，需替换此值（例如替换为 <literal>v1.33.3+rke2r1</literal>）。</para>
</listitem>
</itemizedlist>
<para>做出上述所有更改后，<literal>capi-provisioning-example.yaml</literal> 中的
<literal>RKE2ControlPlane</literal> 块如下所示：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-crd
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/charts/sriov-network-operator
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                    createNamespace: true
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>通过合并上述块创建该文件后，必须在管理群集中执行以下命令才能开始使用电信功能置备新的下游群集：</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>专用仓库</title>
<para>可以配置专用仓库作为工作负载所使用映像的镜像。</para>
<para>为此，我们可以创建机器，并在其中包含有关下游群集要使用的专用仓库的信息。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para><literal>tls.crt</literal>、<literal>tls.key</literal> 和
<literal>ca.crt</literal> 是用于对专用仓库进行身份验证的证书。<literal>username</literal> 和
<literal>password</literal> 是用于对专用仓库进行身份验证的身份凭证。</para>
<note>
<para>在机密中使用
<literal>tls.crt</literal>、<literal>tls.key</literal>、<literal>ca.crt</literal>、<literal>username</literal>
和 <literal>password</literal> 之前，必须对它们进行 base64 编码。</para>
</note>
<para>做出上述所有更改后，<literal>capi-provisioning-example.yaml</literal> 中的
<literal>RKE2ControlPlane</literal> 块如下所示：</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: calico
    cniMultusEnable: true
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<para>其中，<literal>registry.example.com</literal> 是下游群集使用的专用仓库的示例名称，应将其替换为实际值。</para>
</section>
<section xml:id="airgap-deployment">
<title>在隔离场景中置备下游群集</title>
<para>定向网络置备工作流程允许在隔离场景中自动置备下游群集。</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>隔离场景的要求</title>
<orderedlist numeration="arabic">
<listitem>
<para>使用 <literal>EIB</literal>
生成的<literal>原始</literal>映像必须包含用于在隔离场景中运行下游群集的特定容器映像（helm-chart OCI
和容器映像）。有关详细信息，请参见<xref linkend="eib-edge-image-airgap"/>。</para>
</listitem>
<listitem>
<para>如果使用 SR-IOV 或任何其他自定义工作负载，则必须按照有关预加载专用仓库的一节（<xref
linkend="preload-private-registry"/>）所述，在专用仓库中预加载用于运行工作负载的映像。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>在隔离场景中登记裸机主机</title>
<para>在管理群集中登记裸机主机的过程与上前面的章节（<xref linkend="enroll-bare-metal-host"/>）中所述的过程相同。</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>在隔离场景中置备下游群集</title>
<para>需要进行一些重大更改才能在隔离场景中置备下游群集：</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>capi-provisioning-example.yaml</literal> 文件中的
<literal>RKE2ControlPlane</literal> 必须包含
<literal>spec.agentConfig.airGapped: true</literal> 指令。</para>
</listitem>
<listitem>
<para>必须按照有关专用仓库的一节（<xref linkend="atip-private-registry"/>）所述，将专用仓库配置包含在
<literal>capi-provisioning-airgap-example.yaml</literal> 文件的
<literal>RKE2ControlPlane</literal> 块中。</para>
</listitem>
<listitem>
<para>如果您使用的是 SR-IOV 或任何其他需要安装 helm-chart 的 <literal>AdditionalUserData</literal>
配置（combustion 脚本），则必须修改配置内容以引用专用仓库，而不是使用公共仓库。</para>
</listitem>
</orderedlist>
<para>以下示例显示了 <literal>capi-provisioning-airgap-example.yaml</literal> 文件的
<literal>AdditionalUserData</literal> 块中的 SR-IOV 配置，以及为了引用专用仓库而需进行的修改。</para>
<itemizedlist>
<listitem>
<para>专用仓库机密引用</para>
</listitem>
<listitem>
<para>在 Helm-Chart 定义中使用专用仓库而不是公共 OCI 映像。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  version: ${RKE2_VERSION}
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: calico
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: 304.0.2+up1.5.0
              mode: 0644
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - iommu=pt
            - idle=poll
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=domain,nohz,managed_irq,${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nowatchdog
            - nohz=on
            - nmi_watchdog=0
            - skew_tick=1
            - quiet
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: performance-settings.service
              enabled: true
              contents: |
                [Unit]
                Description=performance-settings
                Wants=network-online.target
                After=network.target network-online.target cpu-partitioning.service
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "/opt/performance-settings/performance-settings.sh"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=1800
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --timeout=30m --all ; do sleep 10 ; done"
                ExecStartPost=/bin/sh -c "/opt/sriov/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>生命周期操作</title>
<para>本章介绍通过 SUSE Telco Cloud 部署的群集的生命周期管理操作。</para>
<section xml:id="id-management-cluster-upgrades">
<title>管理群集升级</title>
<para>管理群集的升级涉及到多个组件。有关需要升级的常规组件的列表，请参见 <literal>Day 2</literal> 管理群集（<xref
linkend="day2-mgmt-cluster"/>）文档。</para>
<para>下面介绍了针对此设置组件的升级过程。</para>
<para><emphasis role="strong">升级 Metal<superscript>3</superscript></emphasis></para>
<para>要升级 <literal>Metal<superscript>3</superscript></literal>，请使用以下命令更新 Helm
储存库缓存，并从 Helm chart 储存库提取用于安装
<literal>Metal<superscript>3</superscript></literal> 的最新 chart：</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>然后，最简单的升级方式是将当前配置导出到某个文件，然后使用该文件升级
<literal>Metal<superscript>3</superscript></literal>
版本。如果需要对新版本进行任何更改，可以先编辑该文件，然后再升级。</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=304.0.16+up0.12.6</screen>
</section>
<section xml:id="atip-lifecycle-downstream">
<title>下游群集升级</title>
<para>升级下游群集涉及到更新多个组件。以下章节介绍了每个组件的升级过程。</para>
<para><emphasis role="strong">升级操作系统</emphasis></para>
<para>对于此过程，请参考此章节(<xref linkend="eib-edge-image-connected"/>)来构建包含新操作系统版本的新映像。使用
<literal>EIB</literal> 生成此新映像后，下一置备阶段将使用提供的新操作系统版本。下一步骤将使用新映像来升级节点。</para>
<para><emphasis role="strong">升级 RKE2 群集</emphasis></para>
<para>需要做出以下更改才能使用自动化工作流程升级 <literal>RKE2</literal> 群集：</para>
<itemizedlist>
<listitem>
<para>按照<xref linkend="single-node-provision"/>中所述更改
<literal>capi-provisioning-example.yaml</literal> 中的
<literal>RKE2ControlPlane</literal> 块：</para>
<itemizedlist>
<listitem>
<para>指定所需的 <literal>rolloutStrategy</literal>。</para>
</listitem>
<listitem>
<para>将 <literal>RKE2</literal> 群集的版本更改为新版本（请替换以下代码中的
<literal>${RKE2_NEW_VERSION}</literal>）。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  version: ${RKE2_NEW_VERSION}
  replicas: 1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  registrationMethod: "control-plane-endpoint"
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>按照<xref linkend="single-node-provision"/>中所述更改
<literal>capi-provisioning-example.yaml</literal> 中的
<literal>Metal3MachineTemplate</literal> 块：</para>
<itemizedlist>
<listitem>
<para>将映像名称与校验和更改为在上一步骤中生成的新版本。</para>
</listitem>
<listitem>
<para>将指令 <literal>nodeReuse</literal> 设置为 <literal>true</literal>，以避免创建新节点。</para>
</listitem>
<listitem>
<para>将指令 <literal>automatedCleaningMode</literal> 设置为
<literal>metadata</literal>，以启用节点自动清理。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>完成这些更改后，可以使用以下命令将 <literal>capi-provisioning-example.yaml</literal> 文件应用于群集：</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-troubleshooting-3">
<title>查错</title>
<partintro>
<para>本节提供诊断和解决常见 SUSE Edge 部署及操作问题的指导。内容涵盖多个主题，包括针对特定组件的查错步骤、关键工具及相关日志位置。</para>
</partintro>
<chapter xml:id="general-troubleshooting-principles">
<title>通用查错原则</title>
<para>在深入了解特定组件问题之前，请考虑以下通用原则：</para>
<itemizedlist>
<listitem>
<para><emphasis
role="strong">检查日志</emphasis>：日志是主要的信息来源。大多数情况下，错误消息会自行说明问题，且包含失败原因的提示。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查时钟</emphasis>：系统间的时钟差异可能导致各种错误。确保时钟同步。可通过 EIB
配置在引导时强制时钟同步，请参见“配置操作系统时间”（<xref linkend="quickstart-eib"/>）。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">引导问题</emphasis>：如果系统在引导过程中陷入停滞状态，则记录显示的最后一条消息。可访问控制台（物理连接或通过
BMC）查看引导消息。</para>
</listitem>
<listitem>
<para><emphasis role="strong">网络问题</emphasis>：验证网络接口配置 (<literal>ip
a</literal>)、路由表 (<literal>ip
route</literal>)，测试与其他节点及外部服务的连通性（<literal>ping</literal>、<literal>nc</literal>）。确保防火墙规则未封锁必要端口。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证组件状态</emphasis>：使用 <literal>kubectl get</literal>
和 <literal>kubectl describe</literal> 查看 Kubernetes 资源。使用 <literal>kubectl
get events --sort-by='.lastTimestamp' -n &lt;namespace&gt;</literal> 查看特定
Kubernetes 名称空间的事件。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证服务状态</emphasis>：使用 <literal>systemctl status
&lt;service&gt;</literal> 检查 systemd 服务状态。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查语法</emphasis>：软件对配置文件的结构和语法有特定要求。例如，对于 YAML 文件，可使用
<literal>yamllint</literal> 或类似工具验证语法正确性。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">隔离问题</emphasis>：尝试将问题缩小到特定组件或层级（例如，网络、存储、操作系统、Kubernetes、Metal<superscript>3</superscript>、Ironic
等）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">文档参考</emphasis>：始终参考官方 <link
xl:href="https://documentation.suse.com/suse-edge/">SUSE Edge
文档</link>及上游文档以获取详细信息。</para>
</listitem>
<listitem>
<para><emphasis role="strong">版本</emphasis>：SUSE Edge 是经过明确设计且全面测试的 SUSE
各组件集合版本。每个 SUSE Edge 版本所含各组件的版本信息可在 <link
xl:href="https://documentation.suse.com/suse-edge/support-matrix/html/support-matrix/index.html">SUSE
Edge 支持矩阵</link>中找到。</para>
</listitem>
<listitem>
<para><emphasis role="strong">已知问题</emphasis>：每个 SUSE Edge
版本的发行说明中都含有“已知问题”部分，包含有关将在未来版本中修复但可能影响当前版本的问题的信息。</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="troubleshooting-kiwi">
<title>Kiwi 查错</title>
<para>Kiwi 用于生成更新的 SUSE Linux Micro 映像，供 Edge Image Builder 使用。</para>
<itemizedlist>
<title>常见问题</title>
<listitem>
<para><emphasis role="strong">SL Micro
版本不匹配</emphasis>：构建主机的操作系统版本必须与待构建的操作系统版本匹配（例如，SL Micro 6.0 主机 → SL Micro
6.0 映像）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">SELinux 处于强制模式</emphasis>：由于存在某些限制，目前需要临时禁用 SELinux
才能使用 Kiwi 构建映像。请使用 <literal>getenforce</literal> 检查 SELinux 状态，并在运行构建过程前使用
<literal>setenforce 0</literal> 将其禁用。</para>
</listitem>
<listitem>
<para><emphasis role="strong">构建主机未注册</emphasis>：构建过程会使用构建主机订阅从 SUSE SCC
提取软件包。如果主机未注册，构建将会失败。</para>
</listitem>
<listitem>
<para><emphasis role="strong">循环设备测试失败</emphasis>：首次执行 Kiwi
构建过程时，启动后不久就会失败，并显示“ERROR: Early loop device test failed, please retry the
container run.”，这是由于底层主机系统上正在创建的循环设备无法立即在容器映像内可见所致。重新运行该 Kiwi 构建过程，通常即可顺利进行。</para>
</listitem>
<listitem>
<para><emphasis role="strong">权限缺失</emphasis>：构建过程需要以 root 用户（或通过 sudo）运行。</para>
</listitem>
<listitem>
<para><emphasis role="strong">权限错误</emphasis>：运行容器时，应为构建过程指定
<literal>--privileged</literal> 标志。请仔细检查是否指定了该标志。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>日志</title>
<listitem>
<para><emphasis role="strong">构建容器日志</emphasis>：检查构建容器的日志。生成的日志存放在用于存储制品的目录中。也可通过
docker logs 或 podman logs 获取必要信息。</para>
</listitem>
<listitem>
<para><emphasis role="strong">临时构建目录</emphasis>：Kiwi
会在构建过程中创建临时目录。如果主要输出的信息不充足，可在这些目录中查看中间日志或制品。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>查错步骤</title>
<listitem>
<para><emphasis role="strong">查看 <literal>build-image</literal>
输出</emphasis>：控制台输出中的错误消息通常具有明确指示性。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查构建环境</emphasis>：确保运行 Kiwi 的计算机满足 Kiwi
本身的所有先决条件（例如，docker/podman、SELinux、充足的磁盘空间）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查构建容器日志</emphasis>：查看失败容器的日志以获取更详细的错误信息（见上文）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证定义文件</emphasis>：如果使用自定义 Kiwi
映像定义文件，请仔细检查文件是否有拼写错误或语法问题。</para>
</listitem>
</orderedlist>
<note>
<para>请参考 <link
xl:href="https://documentation.suse.com/appliance/kiwi-9/html/kiwi/troubleshooting.html">Kiwi
查错指南</link>。</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-image-builder">
<title>Edge Image Builder (EIB) 查错</title>
<para>EIB 用于创建自定义 SUSE Edge 映像。</para>
<itemizedlist>
<title>常见问题</title>
<listitem>
<para><emphasis role="strong">错误的 SCC 代码</emphasis>：确保 EIB 定义文件中使用的 SCC 代码与 SL
Micro 版本和体系结构相匹配。</para>
</listitem>
<listitem>
<para><emphasis role="strong">依赖项缺失</emphasis>：确保构建环境中没有缺失的软件包或工具。</para>
</listitem>
<listitem>
<para><emphasis role="strong">映像大小不正确</emphasis>：对于原始映像，必须指定
<literal>diskSize</literal> 参数，其值很大程度上取决于映像中包含的映像、RPM 及其他制品。</para>
</listitem>
<listitem>
<para><emphasis role="strong">权限</emphasis>：如果在 custom/files
目录中存储脚本，则确保脚本具有可执行权限，因为这些文件仅在 Combustion 阶段可用，EIB 不会对其进行修改。</para>
</listitem>
<listitem>
<para><emphasis
role="strong">操作系统组依赖项</emphasis>：创建需要设置自定义用户和组的映像时，应明确预先创建作为“<literal>primaryGroup</literal>”的组。</para>
</listitem>
<listitem>
<para><emphasis role="strong">操作系统用户的 SSH 密钥需要主目录</emphasis>：创建需要设置带 SSH
密钥的用户的映像时，还需使用 <literal>createHomeDir=true</literal> 创建主文件夹。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Combustion 问题</emphasis>：EIB 依赖 Combustion
来自定义操作系统并部署其他所有 SUSE Edge 组件，包括存放在 custom/scripts
文件夹中的自定义脚本。需要注意的是，Combustion 过程在 <literal>initrd</literal>
阶段执行，因此脚本执行时系统尚未完全引导。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Podman 计算机大小</emphasis>：如<xref
linkend="tips-and-tricks"/>中所述，在非 Linux 操作系统上，需要验证 Podman 计算机是否有充足的
CPU/内存来运行 EIB 容器。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>日志</title>
<listitem>
<para><emphasis role="strong">EIB 输出</emphasis>：<literal>eib build</literal>
命令的控制台输出可提供极为重要的信息。</para>
</listitem>
<listitem>
<para><emphasis role="strong">构建容器日志</emphasis>：检查构建容器的日志。生成的日志存放在用于存储制品的目录中。也可通过
<literal>docker logs</literal> 或 <literal>podman logs</literal> 获取必要信息。</para>
<note>
<para>有关详细信息，请参见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">调试</link>。</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">临时构建目录</emphasis>：EIB
会在构建过程中创建临时目录。如果主要输出的信息不充足，可在这些目录中查看中间日志或制品。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Combustion 日志</emphasis>：如果使用 EIB
构建的映像因任何原因无法引导，可以使用 root 外壳。连接到主机控制台（物理连接或通过 BMC 等方式），使用 <literal>journalctl
-u combustion</literal> 检查 Combustion 日志，或使用 <literal>journalctl</literal>
查看所有操作系统日志，确定失败的根本原因。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>查错步骤</title>
<listitem>
<para><emphasis role="strong">查看 <literal>eib-build</literal>
输出</emphasis>：控制台输出中的错误消息通常具有明确指示性。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查构建环境</emphasis>：确保运行 EIB 的计算机满足 EIB
本身的所有先决条件（例如，docker/podman、充足的磁盘空间）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查构建容器日志</emphasis>：查看失败容器的日志以获取更详细的错误信息（见上文）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证 <literal>eib</literal> 配置</emphasis>：仔细检查
<literal>eib</literal> 配置文件是否有拼写错误，或者其中的源文件/构建脚本路径是否不正确。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">单独测试组件</emphasis>：如果 EIB 构建涉及自定义脚本或阶段，单独运行它们以隔离故障。</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>请参见 <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/debugging.md">Edge
Image Builder 调试</link>。</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-edge-networking">
<title>Edge 网络 (NMC) 查错</title>
<para>SL Micro EIB 映像中注入了 NMC，后者用于在系统引导时通过 Combustion 配置 Edge 主机的网络。在 Metal3
工作流程中，它也会作为检查过程的一部分执行。当主机首次引导时或在 Metal3 检查过程中，可能会发生问题。</para>
<itemizedlist>
<title>常见问题</title>
<listitem>
<para><emphasis role="strong">主机首次引导时无法正常引导</emphasis>：格式错误的网络定义文件可能导致 Combustion
阶段失败，进而使主机回退到 root 外壳。</para>
</listitem>
<listitem>
<para><emphasis role="strong">未正确生成文件</emphasis>：确保网络文件符合 <link
xl:href="https://nmstate.io/examples.html">NMState</link> 格式。</para>
</listitem>
<listitem>
<para><emphasis role="strong">网络接口配置不正确</emphasis>：确保 MAC 地址与主机上使用的接口匹配。</para>
</listitem>
<listitem>
<para><emphasis role="strong">接口名称不匹配</emphasis>：内核参数
<literal>net.ifnames=1</literal> 会启用<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">网络接口的可预测命名方案</link>，因此不再使用
<literal>eth0</literal> 命名方案，而是采用 <literal>enp2s0</literal> 等其他命名方案。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>日志</title>
<listitem>
<para><emphasis role="strong">Combustion 日志</emphasis>：由于 nmc 在 Combustion
阶段使用，可在待置备的主机上使用 <literal>journalctl -u combustion</literal> 检查 Combustion
日志。</para>
</listitem>
<listitem>
<para><emphasis role="strong">NetworkManager 日志</emphasis>：在
Metal<superscript>3</superscript> 部署工作流程中，nmc 是 IPA 执行过程的一个环节，它通过 systemd 的
ExecStartPre 功能作为 NetworkManager 服务的依赖项运行。可以在 IPA 主机上使用 <literal>journalctl
-u NetworkManager</literal> 检查 NetworkManager 日志（参见<xref
linkend="troubleshooting-directed-network-provisioning"/>，了解如何在主机通过 IPA
引导时访问主机）。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>查错步骤</title>
<listitem>
<para><emphasis role="strong">验证 yaml 语法</emphasis>：nmc 配置文件是 yaml 文件，可使用
<literal>yamllint</literal> 或类似工具检查其语法正确性。</para>
</listitem>
<listitem>
<para><emphasis role="strong">手动运行 nmc</emphasis>：由于 nmc 是 EIB 容器的一部分，可使用本地 Podman
命令来调试问题。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>创建一个用于存储 nmc 文件的临时文件夹。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p ${HOME}/tmp/foo</screen>
</listitem>
<listitem>
<para>将 nmc 文件保存到该位置。</para>
<screen language="shell" linenumbering="unnumbered">❯ tree --noreport ${HOME}/tmp/foo
/Users/johndoe/tmp/foo
├── host1.example.com.yaml
└── host2.example.com.yaml</screen>
</listitem>
<listitem>
<para>以 nmc 为入口点执行 generate 命令来运行 EIB 容器，模拟 nmc 在 combustion 阶段执行的任务：</para>
<screen language="shell" linenumbering="unnumbered">podman run -it --rm -v ${HOME}/tmp/foo:/tmp/foo:Z --entrypoint=/usr/bin/nmc registry.suse.com/edge/3.3/edge-image-builder:1.2.0 generate --config-dir /tmp/foo --output-dir /tmp/foo/

[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host2.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc::generate_conf] Generating config from "/tmp/foo/host1.example.com.yaml"...
[2025-06-04T11:58:37Z INFO  nmc] Successfully generated and stored network config</screen>
</listitem>
<listitem>
<para>观察临时文件夹中生成的日志和文件。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-phone-home-scenarios">
<title>自主回连场景查错</title>
<para>自主回连场景涉及使用 Elemental 回连管理群集，以及使用 EIB 创建包含 elemental-registration
组件的操作系统映像。当主机首次引导时、在 EIB 构建过程中或尝试注册到管理群集时，可能会发生问题。</para>
<itemizedlist>
<title>常见问题</title>
<listitem>
<para><emphasis role="strong">系统注册失败</emphasis>：节点未在 UI 中注册。确保主机正常引导、能够与 Rancher
通信、时钟保持同步，并且 Elemental 服务正常工作。</para>
</listitem>
<listitem>
<para><emphasis role="strong">系统置备失败</emphasis>：节点已注册但置备失败。确保主机能够与 Rancher
通信、时钟保持同步，并且 Elemental 服务正常工作。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>日志</title>
<listitem>
<para><emphasis role="strong">系统日志</emphasis>：<literal>journalctl</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Elemental-system-agent
日志</emphasis>：<literal>journalctl -u elemental-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s/RKE2 日志</emphasis>：<literal>journalctl -u k3s 或
journalctl -u rke2-server</literal>（或 <literal>rke2-agent</literal>）</para>
</listitem>
<listitem>
<para><emphasis role="strong">Elemental Operator Pod</emphasis>：<literal>kubectl
logs -n cattle-elemental-system -l app=elemental-operator</literal></para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>查错步骤</title>
<listitem>
<para><emphasis role="strong">查看日志</emphasis>：检查 Elemental Operator Pod
日志以确定是否存在任何问题。如果节点已引导，则检查主机日志。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查 MachineRegistration 和 TPM</emphasis>：默认会使用 TPM
进行<link
xl:href="https://elemental.docs.rancher.com/authentication/">身份验证</link>，但对于没有
TPM 的主机，也有替代方案。</para>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="troubleshooting-directed-network-provisioning">
<title>定向网络置备查错</title>
<para>定向网络置备场景涉及使用 Metal<superscript>3</superscript> 和 CAPI 组件置备下游群集，以及使用 EIB
创建操作系统映像。当主机首次引导时，或者在检查或置备过程中，可能会发生问题。</para>
<itemizedlist>
<title>常见问题</title>
<listitem>
<para><emphasis role="strong">固件过旧</emphasis>：验证物理主机上使用的所有固件是否为最新版本，包括 BMC 固件（有时
Metal<superscript>3</superscript> <link
xl:href="https://book.metal3.io/bmo/supported_hardware#redfish-and-its-variants">需要特定版本/更新的固件</link>）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">置备因 SSL 错误失败</emphasis>：如果提供映像的 Web 服务器使用 https，需要配置
Metal<superscript>3</superscript> 以在 IPA 映像中注入并信任证书。请参见<xref
linkend="mgmt-cluster-kubernetes-folder"/>了解如何将
<literal>ca-additional.crt</literal> 文件包含到 Metal<superscript>3</superscript>
chart 中。</para>
</listitem>
<listitem>
<para><emphasis role="strong">使用 IPA 引导主机时的证书问题</emphasis>：某些服务器供应商在将虚拟媒体 ISO
映像挂接到 BMC 时会验证 SSL 连接，这可能导致出现问题，因为针对 Metal3
部署生成的证书是自我签名证书。在这种情况下，主机在引导时可能会回退到 UEFI 外壳。请参见<xref
linkend="disabling-tls-for-virtualmedia-iso-attachment"/>了解如何解决此问题。</para>
</listitem>
<listitem>
<para><emphasis role="strong">名称或标签引用错误</emphasis>：如果群集通过错误的名称或标签引用节点，群集可能显示为已部署，但
BMH 仍处于“可用”状态。请仔细检查 BMH 关联对象的引用。</para>
</listitem>
<listitem>
<para><emphasis role="strong">BMC 通信问题</emphasis>：确保管理群集上运行的
Metal<superscript>3</superscript> Pod 能够访问待置备主机的 BMC（通常 BMC 网络的限制极为严格）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">裸机主机状态不正确</emphasis>：BMH 对象在其生命周期（<link
xl:href="https://book.metal3.io/bmo/state_machine">状态机流转过程</link>）中会经历不同状态（正在检查、正在准备、已置备等）。如果检测到状态不正确，请使用以下命令检查
BMH 对象的 <literal>status</literal> 字段以获取更多信息：<literal>kubectl get bmh
&lt;name&gt; -o jsonpath=’{.status}’| jq</literal>。</para>
</listitem>
<listitem>
<para><emphasis role="strong">主机无法取消置备</emphasis>：如果主机取消置备失败，可尝试为 BMH
对象添加“detached”注解后再尝试去除，具体命令为 <literal>kubectl annotate bmh/&lt;BMH&gt;
baremetalhost.metal3.io/detached=””</literal>。</para>
</listitem>
<listitem>
<para><emphasis role="strong">映像错误</emphasis>：验证通过 EIB
为下游群集构建的映像是否可用、是否具有正确的校验和，且其大小不会导致解压失败或超出磁盘容量。</para>
</listitem>
<listitem>
<para><emphasis role="strong">磁盘大小不匹配</emphasis>：默认情况下，磁盘不会自动扩容至其最大容量。如<xref
linkend="growfs-script"/>所述，通过 EIB 为下游群集主机构建映像时，需要包含一个 growfs 脚本。</para>
</listitem>
<listitem>
<para><emphasis role="strong">清理过程卡住</emphasis>：清理过程会重试多次。如果由于主机问题导致清理过程无法继续，需要先将
BMH 对象的 <literal>automatedCleanMode</literal> 字段设置为
<literal>disabled</literal>，以禁用清理功能。</para>
<warning>
<para>当清理过程耗时过长或失败时，不建议手动去除终结器。这样做会从 Kubernetes 中去除主机记录，但会在 Ironic
中残留该记录。后台当前运行的操作会继续，再次添加主机时可能会因发生冲突而失败。</para>
</warning>
</listitem>
<listitem>
<para><emphasis role="strong">Metal3/Rancher Turtles/CAPI Pod
问题</emphasis>：所有必需组件的部署流程如下：</para>
<itemizedlist>
<listitem>
<para>Rancher Turtles 控制器部署 CAPI 操作器控制器。</para>
</listitem>
<listitem>
<para>CAPI 操作器控制器随后部署提供程序控制器（CAPI 核心、CAPM3 和 RKE2 控制平面/引导组件）。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>验证所有 Pod 是否正常运行，若非如此，请检查日志。</para>
<itemizedlist>
<title>日志</title>
<listitem>
<para><emphasis role="strong">Metal<superscript>3</superscript> 日志</emphasis>：检查不同
Pod 的日志。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n metal3-system -l app.kubernetes.io/component=baremetal-operator
kubectl logs -n metal3-system -l app.kubernetes.io/component=ironic</screen>
<note>
<para>metal3-ironic Pod 至少包含 4
个不同的容器（<literal>ironic-httpd</literal>、`ironic-log-watch`、<literal>ironic</literal>
和 <literal>ironic-ipa-downloader</literal> (init)）。带 <literal>-c</literal>
标志运行 <literal>kubectl logs</literal> 可校验其中每个容器的日志。</para>
</note>
<note>
<para><literal>ironic-log-watch</literal>
容器会在检查/置备主机后从主机公开控制台日志（前提是网络连接允许将这些日志发送回管理群集）。当发生置备错误，但您无法直接访问 BMC
控制台日志时，此功能非常有用。</para>
</note>
</listitem>
<listitem>
<para><emphasis role="strong">Rancher Turtles 日志</emphasis>：检查不同 Pod 的日志。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs -n rancher-turtles-system -l control-plane=controller-manager
kubectl logs -n rancher-turtles-system -l app.kubernetes.io/name=cluster-api-operator
kubectl logs -n rke2-bootstrap-system -l cluster.x-k8s.io/provider=bootstrap-rke2
kubectl logs -n rke2-control-plane-system -l cluster.x-k8s.io/provider=control-plane-rke2
kubectl logs -n capi-system -l cluster.x-k8s.io/provider=cluster-api
kubectl logs -n capm3-system -l cluster.x-k8s.io/provider=infrastructure-metal3</screen>
</listitem>
<listitem>
<para><emphasis role="strong">BMC 日志</emphasis>：BMC 通常会提供一个可用于执行大多数交互操作的
UI，其中一般都会有一个“日志”部分，您可在其中查看潜在问题（无法访问映像、硬件故障等）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">控制台日志</emphasis>：连接到 BMC 控制台（通过 BMC Web
UI、串行接口等），检查实时写入的日志中的错误。</para>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>查错步骤</title>
<listitem>
<para><emphasis role="strong">检查 <literal>BareMetalHost</literal> 状态</emphasis>：</para>
<itemizedlist>
<listitem>
<para>运行 <literal>kubectl get bmh -A</literal> 可显示当前状态。查找
<literal>provisioning</literal>、<literal>ready</literal>、<literal>error</literal>、<literal>registering</literal>。</para>
</listitem>
<listitem>
<para>运行 <literal>kubectl describe bmh -n &lt;namespace&gt;
&lt;bmh_name&gt;</literal> 可提供详细事件和状态，解释 BMH 可能出现卡死状态的原因。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">测试 RedFish 连通性</emphasis>：</para>
<itemizedlist>
<listitem>
<para>从 Metal<superscript>3</superscript> 控制平面使用 <literal>curl</literal> 测试能否通过
redfish 连接到 BMC。</para>
</listitem>
<listitem>
<para>确保在 <literal>BareMetalHost-Secret</literal> 定义中提供了正确的 BMC 身份凭证。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">验证 Turtles/CAPI/Metal3 Pod
状态</emphasis>：分别使用以下命令确保管理群集上的容器处于正常运行状态：<literal>kubectl get pods -n
metal3-system</literal> 和 <literal>kubectl get pods -n
rancher-turtles-system</literal>（另请参见
<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>
和 <literal>rke2-control-plane-system</literal>）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证待置备的主机可访问 Ironic 端点</emphasis>：待置备的主机需要能够访问 Ironic
端点以向 Metal<superscript>3</superscript> 回传状态信息。请使用 <literal>kubectl get svc
-n metal3-system metal3-metal3-ironic</literal> 获取 IP，然后通过
<literal>curl/nc</literal> 尝试访问该端点。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证 BMC 可访问 IPA 映像</emphasis>：IPA 由 Ironic 端点提供，BMC
必须能够访问 IPA，因为后者用作虚拟 CD。</para>
</listitem>
<listitem>
<para><emphasis role="strong">验证待置备的主机可访问操作系统映像</emphasis>：运行 IPA
时，待置备主机自身必须能够访问用于置备该主机的映像，因为该映像会被临时下载并写入磁盘。</para>
</listitem>
<listitem>
<para><emphasis role="strong">检查 Metal<superscript>3</superscript>
组件日志</emphasis>：请参见上文。</para>
</listitem>
<listitem>
<para><emphasis role="strong">重新触发 BMH 检查</emphasis>：如果检查失败或可用主机的硬件发生变化，可通过为 BMH
对象添加注解 <literal>inspect.metal3.io: ""</literal> 触发新的检查过程。有关详细信息，请参见 <link
xl:href="https://book.metal3.io/bmo/inspect_annotation">Metal<superscript>3</superscript>
控制检查</link>指南。</para>
</listitem>
<listitem>
<para><emphasis role="strong">裸机 IPA 控制台</emphasis>：您可以通过以下几种方法来排查 IPA 问题：</para>
<itemizedlist>
<listitem>
<para>启用“自动登录”：这样 root 用户在连接到 IPA 控制台时便会自动登录。</para>
<warning>
<para>此功能仅用于调试目的，因为这会授予对主机的完全访问权限。</para>
</warning>
<para>要启用自动登录功能，应将 Metal3 helm 的 <literal>global.ironicKernelParams</literal> 设置为
<literal>console=ttyS0 suse.autologin=ttyS0</literal>（根据控制台的不同，可将
<literal>ttyS0</literal> 替换为其他值）。然后需要重新部署 Metal<superscript>3</superscript>
chart。（注意：<literal>ttyS0</literal> 是示例，具体的值应与实际终端匹配。例如在裸机上，该值通常为
<literal>tty1</literal>，可通过查看主机引导时 IPA 内存盘的控制台输出来确定，因为其中的
<literal>/etc/issue</literal> 会列显控制台名称。）</para>
<para>另一种方法是更改 <literal>metal3-system</literal> 名称空间内
<literal>ironic-bmo</literal> configmap 中的
<literal>IRONIC_KERNEL_PARAMS</literal> 参数。通过 <literal>kubectl</literal>
edit 进行操作会更简单，但更新 chart 时，该参数会被覆盖。之后需要使用 <literal>kubectl delete pod -n
metal3-system -l app.kubernetes.io/component=ironic</literal> 重启
Metal<superscript>3</superscript> Pod。</para>
</listitem>
<listitem>
<para>为 IPA 的 root 用户注入 SSH 密钥。</para>
<warning>
<para>此功能仅用于调试目的，因为这会授予对主机的完全访问权限。</para>
</warning>
<para>可通过 Metal<superscript>3</superscript> Helm 的
<literal>debug.ironicRamdiskSshKey</literal> 参数注入 root 用户的 SSH 密钥。然后需要重新部署
Metal<superscript>3</superscript> chart。</para>
<para>另一种方法是更改 <literal>metal3-system</literal> 名称空间内 <literal>ironic-bmo
configmap</literal> 中的 <literal>IRONIC_RAMDISK_SSH_KEY</literal> 参数。通过
<literal>kubectl</literal> edit 进行操作会更简单，但更新 chart 时，该参数会被覆盖。之后需要使用
<literal>kubectl delete pod -n metal3-system -l
app.kubernetes.io/component=ironic</literal> 重启
Metal<superscript>3</superscript> Pod。</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<para>请参见 <link
xl:href="https://cluster-api.sigs.k8s.io/user/troubleshooting">CAPI
查错</link>指南和 <link
xl:href="https://book.metal3.io/troubleshooting">Metal<superscript>3</superscript>
查错</link>指南。</para>
</note>
</chapter>
<chapter xml:id="troubleshooting-other-components">
<title>对其他组件查错</title>
<para>其他 SUSE Edge 组件的查错指南可在相应组件的官方文档中找到：</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://documentation.suse.com/smart/micro-clouds/html/SLE-Micro-5.5-admin/index.html#id-1.10">SUSE
Linux Micro 查错</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.rke2.io/known_issues">RKE2 已知问题</link></para>
</listitem>
<listitem>
<para><link xl:href="https://docs.k3s.io/known-issues">K3s 已知问题</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/troubleshooting/general-troubleshooting">Rancher
常规查错</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://documentation.suse.com/multi-linux-manager/5.1/en/docs/administration/troubleshooting/tshoot-intro.html">SUSE
Multi-Linux Manager 查错</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://elemental.docs.rancher.com/troubleshooting-support/">Elemental
支持</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://turtles.docs.rancher.com/turtles/stable/en/troubleshooting/troubleshooting.html">Rancher
Turtles 查错</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://longhorn.io/docs/1.9.1/troubleshoot/troubleshooting/">Longhorn
查错</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://open-docs.neuvector.com/next/troubleshooting/troubleshooting/">Neuvector
查错</link></para>
</listitem>
<listitem>
<para><link xl:href="https://fleet.rancher.io/troubleshooting">Fleet 查错</link></para>
</listitem>
</itemizedlist>
<para>您还可以访问 <link xl:href="https://www.suse.com/support/kb/">SUSE 知识库</link>。</para>
</chapter>
<chapter xml:id="collecting-diagnostics-for-support">
<title>收集支持团队所需的诊断信息</title>
<para>联系 SUSE 支持团队时，提供全面的诊断信息至关重要。</para>
<itemizedlist>
<title>需收集的基本信息</title>
<listitem>
<para><emphasis role="strong">详细问题描述</emphasis>：发生了什么、何时发生、操作过程、预期行为及实际行为。</para>
</listitem>
<listitem>
<para><emphasis role="strong">重现步骤</emphasis>：能否稳定重现问题？如果可以，请列出确切步骤。</para>
</listitem>
<listitem>
<para><emphasis role="strong">组件版本</emphasis>：SUSE Edge
版本、各组件版本（RKE2/K3、EIB、Metal<superscript>3</superscript>、Elemental 等）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">相关日志</emphasis>：</para>
<itemizedlist>
<listitem>
<para><literal>journalctl</literal> 输出（尽可能按服务过滤，或提供完整的引导日志）。</para>
</listitem>
<listitem>
<para>Kubernetes Pod 日志 (kubectl logs)。</para>
</listitem>
<listitem>
<para>Metal³/Elemental 组件日志。</para>
</listitem>
<listitem>
<para>EIB 构建日志及其他日志。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">系统信息</emphasis>：</para>
<itemizedlist>
<listitem>
<para><literal>uname -a</literal></para>
</listitem>
<listitem>
<para><literal>df -h</literal></para>
</listitem>
<listitem>
<para><literal>ip a</literal></para>
</listitem>
<listitem>
<para><literal>/etc/os-release</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">配置文件</emphasis>：Elemental、Metal<superscript>3</superscript>、EIB
的相关配置文件，例如 Helm chart 值文件、configmap 等。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Kubernetes 信息</emphasis>：节点、服务、部署等。</para>
</listitem>
<listitem>
<para><emphasis role="strong">受影响的 Kubernetes
对象</emphasis>：BMH、MachineRegistration 等。</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>收集方式</title>
<listitem>
<para><emphasis role="strong">对于日志</emphasis>：将命令输出重定向到文件（例如，使用
<literal>journalctl -u k3s &gt; k3s_logs.txt</literal>）。</para>
</listitem>
<listitem>
<para><emphasis role="strong">对于 Kubernetes 资源</emphasis>：使用 <literal>kubectl get
&lt;resource&gt; -o yaml &gt; &lt;resource_name&gt;.yaml</literal> 获取详细的
YAML 定义。</para>
</listitem>
<listitem>
<para><emphasis role="strong">对于系统信息</emphasis>：收集上述命令的输出。</para>
</listitem>
<listitem>
<para><emphasis role="strong">对于 SL Micro</emphasis>：参考 <link
xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-adm-support-slemicro.html">SUSE
Linux Micro 查错指南</link>文档，了解如何使用 <literal>supportconfig</literal>
收集支持团队所需的系统信息。</para>
</listitem>
<listitem>
<para><emphasis role="strong">对于 RKE2/Rancher</emphasis>：参考<link
xl:href="https://www.suse.com/support/kb/doc/?id=000020191">《The Rancher
v2.x Linux log collector script》</link>一文，运行 Rancher v2.x Linux 日志收集器脚本。</para>
</listitem>
</itemizedlist>
<formalpara>
<title>联系支持团队</title>
<para>有关如何联系 SUSE 支持团队的详细信息，请参见<link
xl:href="https://www.suse.com/support/kb/doc/?id=000019452">《How-to
effectively work with SUSE Technical Support》</link>一文，以及 <link
xl:href="https://www.suse.com/support/handbook/">SUSE Technical Support
Handbook</link> 页面中提供的支持手册。</para>
</formalpara>
</chapter>
</part>
<part xml:id="id-appendix">
<title>附录</title>
<chapter xml:id="id-release-notes">
<title>发行说明</title>
<section xml:id="release-notes">
<title>摘要</title>
<para>SUSE Edge 3.4
是一套紧密集成且经过全面验证的端到端解决方案，旨在应对边缘基础架构和云原生应用程序部署的独特挑战。其核心目标是提供一个既有明确设计导向、又具备高度灵活性与可扩缩性的安全平台，覆盖从初始部署映像构建、节点置备与接入，到应用程序部署、可观测性及生命周期管理的全流程。</para>
<para>该解决方案的设计考虑到了客户的需求和期望千差万别，因此不存在“以一应百”的边缘平台。边缘部署促使我们解决并不断设想出一些极具挑战性的问题，包括大规模可伸缩性、网络受限情况下的可用性、物理空间限制、新的安全威胁和攻击途径、硬件体系结构和系统资源的差异、部署旧式基础架构和应用程序并与之连接的要求，以及使用寿命较长的客户解决方案。</para>
<para>SUSE Edge 完全基于一流的开源软件构建而成，这既符合我们 30 年来交付安全、稳定且经过认证的 SUSE Linux
平台的历史，也延续了我们通过 Rancher 产品组合提供高可可缩放且功能丰富的 Kubernetes 管理方案的经验。SUSE Edge
依托这些既有能力，提供能够满足众多市场领域需求的功能，包括零售、医疗、交通、物流、电信、智能制造业以及工业物联网等。</para>
<para>有关 SUSE Edge 产品支持生命周期更新的详细信息，请参见 <link
xl:href="https://www.suse.com/lifecycle/#suse-edge-33">Product Support
Lifecycle</link>。</para>
<note>
<para>SUSE Telco Cloud（前身为 SUSE Edge for Telco）是 SUSE Edge
的衍生产品，它经过进一步优化并包含更多组件，使平台能够满足电信行业场景的需求。除非另有明确说明，所有发行说明均适用于 SUSE Edge 3.4 和
SUSE Telco Cloud 3.4。</para>
</note>
</section>
<section xml:id="id-about">
<title>简介</title>
<para>除非明确指定和说明，否则下面的发行说明在所有体系结构中都是相同的，并且最新版本以及所有其他 SUSE 产品的发行说明始终在 <link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>
上在线提供。</para>
<para>说明项只会列出一次，但是，非常重要并且属于多个章节的说明项可能会在多处提到。发行说明通常只会列出两个后续版本之间发生的更改。本发行说明可能重复了旧产品版本的发行说明中的某些重要说明项。为了方便识别这些说明项，发行说明中会通过一条备注来指出这一点。</para>
<para>但需注意，重复列出的内容仅出于方便查阅目的提供。因此，如果您跳过了一个或多个版本，还需查看被跳过版本的发行说明。如果您仅阅读当前版本的发行说明，可能会遗漏影响系统运行的重要变更。SUSE
Edge 版本格式为 x.y.z，其中“x”表示主要版本，“y”表示次要版本，“z”表示补丁版本（也称为“z-stream”）。SUSE Edge
产品生命周期基于特定的次要版本（如“3.4”）定义，但在生命周期内会通过后续补丁更新（如“3.4.1”）持续优化。</para>
<note>
<para>各个 SUSE Edge z-stream
版本紧密集成，并已作为版本受控的堆栈进行全面测试。将任何一个组件升级到其他版本（而不是上面列出的版本）都可能会导致系统停机。虽然可以在未经测试的配置中运行
Edge 群集，但我们不建议这样做，而且这样可能需要花费更长的时间来通过支持渠道获得问题的解决方法。</para>
</note>
</section>
<section xml:id="release-notes-3-4-0">
<title>3.4.0 版本</title>
<para>发布日期：2025 年 9 月 24 日</para>
<para>全面支持结束日期：2026 年 3 月 20 日</para>
<para>维护支持结束日期：2027 年 9 月 20 日</para>
<para>生命周期终止日期：2027 年 9 月 21 日</para>
<para>概述：SUSE Edge 3.4.0 是 SUSE Edge 3.4 系列版本中的第一个版本。</para>
<section xml:id="id-new-features">
<title>新功能</title>
<itemizedlist>
<listitem>
<para>已更新到 Kubernetes 1.33 和 Rancher Prime 2.12</para>
</listitem>
<listitem>
<para>更新了 Rancher Turtles、Cluster API 和 Metal3/Ironic 版本</para>
</listitem>
<listitem>
<para>已更新到 SUSE Storage (Longhorn) 1.9.1，详见<link
xl:href="https://longhorn.io/docs/1.9.1/">发行说明</link></para>
</listitem>
<listitem>
<para>现在可通过定向网络置备流程更灵活地部署 AArch64 下游群集，详情请参见<xref
linkend="atip-automated-provisioning"/></para>
</listitem>
<listitem>
<para>现已全面支持双栈群集部署（单栈 IPv6 仍处于<xref linkend="tech-previews"/>阶段）</para>
</listitem>
<listitem>
<para>MetalLB 的 BGP 模式现已作为技术预览功能提供，详情请参见<xref linkend="tech-previews"/>和<xref
linkend="guides-metallb-k3s-l3"/></para>
</listitem>
<listitem>
<para>Edge Image Builder 已更新到 1.3.0，详见<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/RELEASE_NOTES.md">上游发行说明</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>Bug 和安全修复</title>
<itemizedlist>
<listitem>
<para>Rancher Prime 2.12 包含多项 bug 修复，详见<link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.12.1">上游 Rancher
发行说明</link></para>
</listitem>
<listitem>
<para>Rancher Prime 2.12 修复了与 AppVersion 相关的问题，该问题曾影响扩展升级可用性判断，进而影响与 Edge chart
的兼容，详见<link
xl:href="https://github.com/rancher/dashboard/issues/14204">上游问题</link></para>
</listitem>
<listitem>
<para>SUSE Storage (Longhorn) 1.9.1 包含多项 bug 修复，详见<link
xl:href="https://github.com/longhorn/longhorn/releases/tag/v1.9.1">上游
Longhorn bug 修复</link></para>
</listitem>
<listitem>
<para>更新的 Metal<superscript>3</superscript> chart 修复了检查期间可能为绑定接口收集错误 MAC
地址的问题，详见<link
xl:href="https://bugs.launchpad.net/ironic-python-agent/+bug/2103450">上游 IPA
问题</link></para>
</listitem>
<listitem>
<para>更新的 Metal<superscript>3</superscript> chart 修复了 ConfigMap
更新时部署可能无法正确重启的问题，详见<link
xl:href="https://github.com/suse-edge/charts/issues/219">上游问题</link></para>
</listitem>
<listitem>
<para>Rancher Turtles 更新包含一项修复，解决了 RKE2 CAPI 提供程序未应用 MachineTemplate
ownerReferences 的问题，详见<link
xl:href="https://github.com/rancher/cluster-api-provider-rke2/issues/500">上游问题</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-known-issues-8">
<title>已知问题</title>
<warning>
<para>如果要部署新群集，请先按照<xref linkend="guides-kiwi-builder-images"/>所述构建全新映像。现在，无论是要创建
AMD64/Intel 64 还是 AArch64 体系结构的群集（包括管理群集和下游群集），都必须先执行此步骤。</para>
</warning>
<itemizedlist>
<listitem>
<para>通过 Edge Image Builder 部署时，如果将 <literal>HelmChartConfigs</literal> 清单放入
<literal>kubernetes/manifests</literal> 配置目录，可能会导致部署失败。建议改为通过 EIB os-files
接口将所有 <literal>HelmChartConfigs</literal> 放置在
<literal>/var/lib/rancher/{rke2/k3s}/server/manifests/</literal>
目录中，示例请参见<xref
linkend="mgmt-cluster-directory-structure"/>。若不如此操作，可能导致节点首次启动时处于
<literal>NotReady</literal> 状态，详见 <link
xl:href="https://github.com/rancher/rke2/issues/8357">RKE2 问题 #8357</link></para>
</listitem>
<listitem>
<para>在 RKE2/K3s 1.31、1.32 和 1.33 版本中，用于存储 CNI 配置的 <literal>/etc/cni</literal>
目录在某些与 <literal>overlayfs</literal> 相关的状况下，可能无法触发
<literal>containerd</literal> 感知该目录下文件的写入操作（请参见 <link
xl:href="https://github.com/rancher/rke2/issues/8356">RKE2 问题
#8356</link>）。这会导致 RKE2/K3s 部署停滞在等待 CNI 启动的阶段，且 RKE2/K3s 节点持续处于
<literal>NotReady</literal> 状态。通过在节点上执行 <literal>kubectl describe node
&lt;affected_node&gt;</literal> 可查看此问题：</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Conditions:
  Type   Status  LastHeartbeatTime                LastTransitionTime               Reason           Message
  ----   ------  -----------------                ------------------               ------           -------
  Ready  False   Thu, 05 Jun 2025 17:41:28 +0000  Thu, 05 Jun 2025 14:38:16 +0000  KubeletNotReady  container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized</screen>
<para>临时解决方法是在 RKE2 启动前将 tmpfs 卷挂载到 <literal>/etc/cni</literal> 目录。这会避免使用
overlayfs（overlayfs 会导致 containerd 漏收通知），且每次节点重启及 Pod
初始化容器重新运行时，配置都会被重写。如果使用 EIB，可在 <literal>custom/scripts</literal> 目录中添加
<literal>04-tmpfs-cni.sh</literal> 脚本（如此处[<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom">https://github.com/suse-edge/edge-image-builder/blob/release-1.2/docs/building-images.md#custom</link>]所述），脚本内容如下：</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash
mkdir -p /etc/cni
mount -t tmpfs -o mode=0700,size=5M tmpfs /etc/cni
echo "tmpfs /etc/cni tmpfs defaults,size=5M,mode=0700 0 0" &gt;&gt; /etc/fstab</screen>
<itemizedlist>
<listitem>
<para>使用 Elemental 接入远程主机时，<literal>dbus.service</literal> 与
<literal>elemental-system-agent.service</literal> 之间可能出现竞态条件，导致远程主机上的
<literal>rancher-system-agent.service</literal> 启动失败，并出现类似以下的错误。（详情请参见 <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/784">Edge
Image Builder 问题 #784</link>）</para>
</listitem>
</itemizedlist>
<screen language="bash" linenumbering="unnumbered">Sep 19 19:38:07 elementalvm elemental-system-agent[3671]: time="2025-09-19T19:38:07Z" level=info msg="[6b20fe64c854da2639804884b34129bb8f718eb59578111da58d9de1509c24db_1:stderr]: Failed to restart rancher-system-agent.service: Message recipient disconnected from message bus without replying"</screen>
<para>临时解决方法是创建如下 systemd 覆盖文件</para>
<screen language="bash" linenumbering="unnumbered">[Unit]
Wants=dbus.service network-online.target
After=dbus.service network-online.target time-sync.target

[Service]
ExecStartPre=/bin/bash -c 'echo "Waiting for dbus to become active..." | systemd-cat -p info -t elemental-system-agent; sleep 15; timeout 300 bash -c "while ! systemctl is-active --quiet dbus.service; do sleep 15; done"'</screen>
<para>并使用名为 <literal>30a-copy-elemental-system-agent-override.sh</literal>
的自定义脚本，在 Combustion 阶段 EIB 的 <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/pkg/combustion/templates/31-elemental-register.sh.tpl">31-elemental-register.sh</link>
脚本运行之前，将该覆盖文件放置到
<literal>/etc/systemd/system/elemental-system-agent.service.d</literal> 中。</para>
<screen language="bash" linenumbering="unnumbered">#!/bin/bash

/bin/mkdir -p /etc/systemd/system/elemental-system-agent.service.d
/bin/cp -f elemental-system-agent-override.conf /etc/systemd/system/elemental-system-agent.service.d/override.conf</screen>
</section>
<section xml:id="id-component-versions">
<title>组件版本</title>
<para>下表介绍了构成 3.4.0 版本的各个组件，包括组件版本、Helm chart
版本（如果适用），以及可从中拉取已发布二进制格式制品的来源。有关用法和部署示例，请参见相关文档。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>名称</para></entry>
<entry align="left" valign="top"><para>版本</para></entry>
<entry align="left" valign="top"><para>Helm Chart 版本</para></entry>
<entry align="left" valign="top"><para>制品位置（URL/映像）</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Linux Micro</para></entry>
<entry align="left" valign="top"><para>6.1（最新）</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SUSE Linux Micro
下载页面</link><?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso (sha256
70b9be28f2d92bc3b228412e4fc2b1d5026e691874b728e530b8063522158854)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-SelfInstall-GM.install.iso (sha256
9ce83e4545d4b36c7c6a44f7841dc3d9c6926fe32dbff694832e0fbd7c496e9d)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-GM.raw.xz (sha256
36e3efa55822113840dd76fdf6914e933a7b7e88a1dce5cb20c424ccf2fb4430)<?asciidoc-br?>
SL-Micro.x86_64-6.1-Base-RT-GM.raw.xz (sha256
2ee66735da3e1da107b4878e73ae68f5fb7309f5ec02b5dfdb94e254fda8415e)<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Multi-Linux Manager</para></entry>
<entry align="left" valign="top"><para>5.0.5</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE Multi-Linux
Manager 下载页面</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.33.3%2Bk3s1">上游
K3s 版本</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.33.3</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.33.3%2Brke2r1">上游
RKE2 版本</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para>2.12.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://charts.rancher.com/server-charts/prime/index.yaml">Rancher
Prime Helm 储存库</link><?asciidoc-br?> <link
xl:href="https://github.com/rancher/rancher/releases/download/v2.12.1/rancher-images.txt">Rancher
2.12.1 容器映像</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Storage (Longhorn)</para></entry>
<entry align="left" valign="top"><para>1.9.1</para></entry>
<entry align="left" valign="top"><para>107.0.0+up1.9.1</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher chart Helm
储存库</link><?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-backing-image-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.61<?asciidoc-br?>
registry.suse.com/rancher/mirrored-longhornio-longhorn-cli:v1.9.1<?asciidoc-br?></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Security</para></entry>
<entry align="left" valign="top"><para>5.4.5</para></entry>
<entry align="left" valign="top"><para>107.0.0+up2.8.7</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher chart Helm
储存库</link><?asciidoc-br?>
registry.suse.com/rancher/neuvector-controller:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-enforcer:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-manager:5.4.5<?asciidoc-br?>
registry.suse.com/rancher/neuvector-compliance-config:1.0.6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-registry-adapter:0.1.8<?asciidoc-br?>
registry.suse.com/rancher/neuvector-scanner:6<?asciidoc-br?>
registry.suse.com/rancher/neuvector-updater:0.0.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles (CAPI)</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles:304.0.6_up0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/rancher/turtles:v0.24.0<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-controller:v1.10.2<?asciidoc-br?>
registry.rancher.com/rancher/cluster-api-metal3-ipam-controller:v1.10.2<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-controller:v1.10.5<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1<?asciidoc-br?>
registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Turtles 隔离资源</para></entry>
<entry align="left" valign="top"><para>0.24.0</para></entry>
<entry align="left" valign="top"><para>304.0.6+up0.24.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/rancher-turtles-airgap-resources:304.0.6_up0.24.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>0.11.5</para></entry>
<entry align="left" valign="top"><para>304.0.16+up0.12.6</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metal3:304.0.16_up0.12.6<?asciidoc-br?>
registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic:29.0.4.3<?asciidoc-br?>
registry.suse.com/edge/3.4/ironic-ipa-downloader:3.0.9<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.9</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.14.9</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/metallb:304.0.0_up0.14.9<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-controller:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/metallb-speaker:v0.14.8<?asciidoc-br?>
registry.suse.com/edge/3.4/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/3.4/frr-k8s:v0.0.14</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>1.7.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.7.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.7.3</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental 仪表板扩展</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para>3.0.1</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/ui-plugin-charts/tree/4.0.0/charts/elemental/3.0.1">Elemental
扩展 Helm chart</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.3.0</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/edge-image-builder:1.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.3.3</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.3">NMConfigurator
上游版本</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.5.2</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-operator:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-api:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-controller:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportproxy:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-exportserver:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-handler:1.5.2<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/virt-launcher:1.5.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt 仪表板扩展</para></entry>
<entry align="left" valign="top"><para>1.3.2</para></entry>
<entry align="left" valign="top"><para>304.0.3+up1.3.2</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/kubevirt-dashboard-extension:304.0.3_up1.3.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.62.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.6.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/cdi:304.0.1_up0.6.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-operator:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-controller:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-importer:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-cloner:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-apiserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadserver:1.62.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.7/cdi-uploadproxy:1.62.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.3.0</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.3.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/endpoint-copier-operator:304.0.1_up0.3.0<?asciidoc-br?>
registry.suse.com/edge/3.4/endpoint-copier-operator:0.3.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri（已弃用）</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>304.0.0+up0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/akri:304.0.0_up0.12.20<?asciidoc-br?>
registry.suse.com/edge/charts/akri-dashboard-extension:304.0.0_up1.3.1<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/3.4/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SR-IOV 网络操作器</para></entry>
<entry align="left" valign="top"><para>1.5.0</para></entry>
<entry align="left" valign="top"><para>304.0.2+up1.5.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/sriov-network-operator:304.0.2_up1.5.0<?asciidoc-br?>
registry.suse.com/edge/charts/sriov-crd:304.0.2_up1.5.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>系统升级控制器</para></entry>
<entry align="left" valign="top"><para>0.16.0</para></entry>
<entry align="left" valign="top"><para>107.0.0</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://charts.rancher.io/index.yaml">Rancher chart Helm
储存库</link><?asciidoc-br?>
registry.suse.com/rancher/system-upgrade-controller:v0.16.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>升级控制器</para></entry>
<entry align="left" valign="top"><para>0.1.1</para></entry>
<entry align="left" valign="top"><para>304.0.1+up0.1.1</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/charts/upgrade-controller:304.0.1_up0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/upgrade-controller:0.1.1<?asciidoc-br?>
registry.suse.com/edge/3.4/kubectl:1.33.4<?asciidoc-br?>
registry.suse.com/edge/3.4/release-manifest:3.4.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Kiwi 构建器</para></entry>
<entry align="left" valign="top"><para>10.2.12.0</para></entry>
<entry align="left" valign="top"><para>不适用</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/3.4/kiwi-builder:10.2.12.0</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-deprecated-features">
<title>已弃用功能</title>
<para>除非另有说明，否则以下功能适用于 3.4.0 版本及后续所有 z-stream 版本。</para>
<itemizedlist>
<listitem>
<para>Akri 在之前的 Edge 版本中为技术预览功能，目前已弃用，计划在未来版本中去除。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="tech-previews">
<title>技术预览</title>
<para>除非另有说明，否则以下功能适用于 3.4.0 版本及后续所有 z-stream 版本。</para>
<itemizedlist>
<listitem>
<para>单栈 IPv6 部署为技术预览功能，不纳入标准支持范围。</para>
</listitem>
<listitem>
<para>下游部署中的精确时间协议 (PTP) 为技术预览功能，不纳入标准支持范围。</para>
</listitem>
<listitem>
<para>MetalLB 的 BGP 模式为技术预览功能，不纳入标准支持范围。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-component-verification">
<title>组件验证</title>
<para>可以使用软件材料清单 (SBOM) 数据来验证上述组件 - 例如，如下所述使用 <literal>cosign</literal>：</para>
<para>从 <link xl:href="https://www.suse.com/support/security/keys/">SUSE
签名密钥来源</link>下载 SUSE Edge 容器公共密钥：</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>验证容器映像哈希，例如，使用 <literal>crane</literal> 进行验证：</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/3.4/baremetal-operator:0.10.2.1 --platform linux/amd64
sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374</screen>
<note>
<para>对于多体系结构映像，获取摘要时还需要指定平台，例如 <literal>--platform linux/amd64</literal> 或
<literal>--platform linux/arm64</literal>。如果不这样做，后续步骤中会出现错误 (<literal>Error:
no matching attestations</literal>)。</para>
</note>
<para>使用 <literal>cosign</literal> 进行验证：</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 &gt; /dev/null
#
Verification for registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The signatures were verified against the specified public key</screen>
<para>按照 <link xl:href="https://www.suse.com/support/security/sbom/">SUSE SBOM
文档</link>中所述提取 SBOM 数据：</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/3.4/baremetal-operator@sha256:310d939f8ae4b547710195b9671a4e9ff417420c0856103dd728b051788b5374 | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>升级步骤</title>
<para>有关如何升级到新版本的详细信息，请参见<xref linkend="day-2-operations"/>。</para>
</section>
<section xml:id="id-product-support-lifecycle">
<title>产品支持生命周期</title>
<para>SUSE Edge 拥有 SUSE 屡获殊荣的支持服务作为后盾。SUSE
是公认的技术领导者，在提供企业级品质支持服务方面有着久经验证的历史。有关详细信息，请参见 <link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>
和支持策略页面 (<link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>)。如果您在提交支持案例、SUSE
如何划分严重性级别或支持范围方面有任何疑问，请参见技术支持手册 (<link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>)。</para>
<para>SUSE Edge“3.4”版本提供 24 个月的生产支持，其中前 6 个月提供“全面支持”，随后 18
个月提供“维护支持”。在这些支持阶段结束后，产品将进入“生命周期终止”(EOL) 状态，不再提供任何支持。有关各生命周期阶段的详细信息，请参见下表：</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">全面支持（6 个月）</emphasis></para></entry>
<entry align="left" valign="top"><para>在全面支持期内，会发布紧急和选定的高优先级 bug 修复，其他所有补丁（非紧急修复、功能增强、新功能）将按常规发布计划发布。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">维护支持（18 个月）</emphasis></para></entry>
<entry align="left" valign="top"><para>在此期间，仅通过补丁发布关键修复。其他 bug 修复可能由 SUSE 酌情发布，但不做保证。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">生命周期终止 (EOL)</emphasis></para></entry>
<entry align="left" valign="top"><para>产品版本到达生命周期终止日期后，客户可根据产品许可协议继续使用该产品，但 SUSE 的支持计划不再适用于已进入生命周期终止状态的产品版本。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>除非明确说明，否则列出的所有组件均被视为正式发布 (GA) 版本，并涵盖在 SUSE
的标准支持范围内。某些组件可能以“技术预览”版本的形式列出，SUSE
通过此类版本让客户提前体验尚未正式发布的特性和功能以进行评估，但这些组件没有标准支持政策的保障，不建议将其用于生产使用场景。SUSE
非常欢迎大家提供针对技术预览组件的改进意见和建议，但如果技术预览功能无法满足客户的需求或者达不到我们要求的成熟度，SUSE
保留在正式发布之前弃用该功能的权利。</para>
<para>请注意，SUSE 必须偶尔弃用功能或更改 API 规范。弃用功能或更改 API
的原因包括相应功能已更新或由新的实现取代、有新的功能集、上游技术不再可用，或者上游社区引入了不兼容的更改。在给定的次要版本 (x.z)
中，这种情况预期永远不会发生，因此所有 z-stream 版本都将保持 API 兼容性和原有功能。SUSE
将尽力在发行说明中提供弃用警告并发出充足的通告，同时提供解决方法、建议和缓解措施，以最大程度地减少服务中断。</para>
<para>SUSE Edge 团队也欢迎社区提出反馈，相关问题可在 <link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>
中的相应代码储存库内提出。</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>获取源代码</title>
<para>本 SUSE 产品包含根据 GNU 通用公共许可证 (GPL) 和其他各种开源许可证授权给 SUSE 的组件。根据 GPL 要求，SUSE 必须提供与
GPL 授权组件对应的源代码，此外还须严格遵守其他所有开源许可要求。因此，SUSE 会公开所有源代码，这些源代码通常可在 SUSE Edge
GitHub 代码库 (<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>)、依赖组件的
SUSE Rancher GitHub 代码库 (<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)
中找到；具体到 SUSE Linux Micro，其源代码可在 <link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>
的“Medium 2”处下载。</para>
</section>
<section xml:id="id-legal-notices">
<title>法律声明</title>
<para>SUSE 不对本文档的内容或其使用作出任何陈述或保证，并明确否认对适销性或任何特定用途的适用性作出任何明示或暗示保证。此外，SUSE
保留随时修订本出版物和更改其内容的权利，且无义务将此类修订或更改通知任何个人或实体。</para>
<para>此外，SUSE 不对任何软件作出任何陈述或保证，并明确否认对适销性或任何特定用途的适用性作出任何明示或暗示保证。此外，SUSE 保留随时更改 SUSE
软件任何或所有部分的权利，且无义务将此类更改通知任何个人或实体。</para>
<para>根据本协议提供的任何产品或技术信息可能受到美国出口管制和其他国家/地区贸易法的约束。您同意遵守所有出口管制法规，并在出口、再出口或进口可交付产品之前取得所有必要的许可证或分类证书。您同意不向当前美国出口排除清单上的实体，或美国出口法中规定的任何禁运国家/地区或支持恐怖主义的国家/地区出口或再出口。您同意不将可交付产品用于受禁核武器、导弹或生化武器的最终用途。有关出口
SUSE 软件的详细信息，请访问 <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>。如果您未能取得任何必要的出口许可，SUSE
对此不承担任何责任。</para>
<para><emphasis role="strong">版权所有 © 2024 SUSE LLC。</emphasis></para>
<para>本发行说明文档根据 Creative Commons Attribution-NoDerivatives 4.0 国际许可证
(CC-BY-ND-4.0) 授权。您应已随本文档收到了一份许可证。如果没有，请访问 <link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>。</para>
<para>SUSE 拥有与本文档所述产品中体现的技术相关的知识产权。具体而言，这些知识产权可能包括但不限于 <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>
中列出的一项或多项美国专利，以及美国和其他国家/地区的一项或多项其他专利或正在申请的专利。</para>
<para>有关 SUSE 商标，请参见 SUSE 商标和服务标记列表 (<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>)。所有第三方商标均是其各自所有者的财产。有关
SUSE 品牌信息和使用要求，请参见 <link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link> 上发布的准则。</para>
</section>
</chapter>
</part>
</book>
