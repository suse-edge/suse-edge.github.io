<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>SUSE Edge Documentation</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>2024-07-10</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="id-suse-edge-documentation">
<title>SUSE Edge Documentation</title>
<para>Welcome to the SUSE Edge documentation. You will find quick start guides,
validated designs, guidance on using components, third-party integrations,
and best practices for managing your edge computing infrastructure and
workloads.</para>
<section xml:id="id-what-is-suse-edge">
<title>What is SUSE Edge?</title>
<para>SUSE Edge is a purpose-built, tightly integrated, and comprehensively
validated end-to-end solution for addressing the unique challenges of the
deployment of infrastructure and cloud-native applications at the edge. Its
driving focus is to provide an opinionated, yet highly flexible, highly
scalable, and secure platform that spans initial deployment image building,
node provisioning and onboarding, application deployment, observability, and
complete lifecycle operations. The platform is built on best-of-breed open
source software from the ground up, consistent with both our 30-year history
in delivering secure, stable, and certified SUSE Linux platforms and our
experience in providing highly scalable and feature-rich Kubernetes
management with our Rancher portfolio. SUSE Edge builds on-top of these
capabilities to deliver functionality that can address a wide number of
market segments, including retail, medical, transportation, logistics,
telecommunications, smart manufacturing, and Industrial IoT.</para>
</section>
<section xml:id="id-design-philosophy">
<title>Design Philosophy</title>
<para>The solution is designed with the notion that there is no
"one-size-fits-all" edge platform due to customers’ widely varying
requirements and expectations. Edge deployments push us to solve, and
continually evolve, some of the most challenging problems, including massive
scalability, restricted network availability, physical space constraints,
new security threats and attack vectors, variations in hardware architecture
and system resources, the requirement to deploy and interface with legacy
infrastructure and applications, and customer solutions that have extended
lifespans. Since many of these challenges are different from traditional
ways of thinking, e.g. deployment of infrastructure and applications within
data centers or in the public cloud, we have to look into the design in much
more granular detail, and rethinking many common assumptions.</para>
<para>For example, we find value in minimalism, modularity, and ease of
operations. Minimalism is important for edge environments since the more
complex a system is, the more likely it is to break. When looking at
hundreds of locations, up to hundreds of thousands, complex systems will
break in complex ways.  Modularity in our solution allows for more user
choice while removing unneeded complexity in the deployed platform. We also
need to balance these with the ease of operations. Humans may make mistakes
when repeating a process thousands of times, so the platform should make
sure any potential mistakes are recoverable, eliminating the need for
on-site technician visits, but also strive for consistency and
standardization.</para>
</section>
<section xml:id="id-which-quick-start-should-you-use">
<title>Which Quick Start should you use?</title>
<para>Due to the varying set of operating environments and lifecycle requirements,
we’ve implemented support for a number of distinct deployment patterns that
loosely align to market segments and use-cases that SUSE Edge operates
in. We have documented a quickstart guide for each of these deployment
patterns to help you get familiar with the SUSE Edge platform based around
your needs. The three deployment patterns that we support today are
described below, with a link to the respective quickstart page.</para>
<section xml:id="id-direct-network-provisioning">
<title>Direct network provisioning</title>
<para>Direct network provisioning is where you know the details of the hardware
you wish to deploy to and have direct access to the out-of-band management
interface to orchestrate and automate the entire provisioning process. In
this scenario, our customers expect a solution to be able to provision edge
sites fully automated from a centralized location, going much further than
the creation of a boot image by minimizing the manual operations at the edge
location; simply rack, power, and attach the required networks to the
physical hardware, and the automation process powers up the machine via the
out-of-band management (e.g. via the Redfish API) and handles the
provisioning, onboarding, and deployment of infrastructure without user
intervention. The key for this to work is that the systems are known to the
administrators; they know which hardware is in which location, and that
deployment is expected to be handled centrally.</para>
<para>This solution is the most robust since you are directly interacting with the
hardware’s management interface, are dealing with known hardware, and have
fewer constraints on network availability. Functionality wise, this solution
extensively uses Cluster API and Metal<superscript>3</superscript> for
automated provisioning from baremetal, through operating system, Kubernetes,
and layered applications, and provides the ability to link into the rest of
the common lifecycle management capabilities of SUSE Edge
post-deployment. The quickstart for this solution can be found in <xref
linkend="quickstart-metal3"/>.</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>"Phone home" network provisioning</title>
<para>Sometimes you are operating in an environment where the central management
cluster cannot manage the hardware directly (for example, your remote
network is behind a firewall or there is no out-of-band management
interface; common in "PC" type hardware often found at the edge). In this
scenario, we provide tooling to remotely provision clusters and their
workloads with no need to know where hardware is being shipped when it is
bootstrapped. This is what most people think of when they think about edge
computing; it’s the thousands or tens of thousands of somewhat unknown
systems booting up at edge locations and securely phoning home, validating
who they are, and receiving their instructions on what they’re supposed to
do. Our requirements here expect provisioning and lifecycle management with
very little user-intervention other than either pre-imaging the machine at
the factory, or simply attaching a boot image, e.g. via USB, and switching
the system on. The primary challenges in this space are addressing scale,
consistency, security, and lifecycle of these devices in the wild.</para>
<para>This solution provides a great deal of flexibility and consistency in the
way that systems are provisioned and on-boarded, regardless of their
location, system type or specification, or when they’re powered on for the
first time. SUSE Edge enables full flexibility and customization of the
system via Edge Image Builder, and leverages the registration capabilities
Rancher’s Elemental offering for node on-boarding and Kubernetes
provisioning, along with SUSE Manager for operating system patching. The
quick start for this solution can be found in <xref
linkend="quickstart-elemental"/>.</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>Image-based provisioning</title>
<para>For customers that need to operate in standalone, air-gapped, or network
limited environments, SUSE Edge provides a solution that enables customers
to generate fully customized installation media that contains all of the
required deployment artifacts to enable both single-node and multi-node
highly-available Kubernetes clusters at the edge, including any workloads or
additional layered components required, all without any network connectivity
to the outside world, and without the intervention of a centralized
management platform. The user-experience follows closely to the "phone home"
solution in that installation media is provided to the target systems, but
the solution will "bootstrap in-place". In this scenario, it’s possible to
attach the resulting clusters into Rancher for ongoing management
(i.e. going from a "disconnected" to "connected" mode of operation without
major reconfiguration or redeployment), or can continue to operate in
isolation. Note that in both cases the same consistent mechanism for
automating lifecycle operations can be applied.</para>
<para>Furthermore, this solution can be used to quickly create management clusters
that may host the centralized infrastructure that supports both the
"directed network provisioning" and "phone home network provisioning" models
as it can be the quickest and most simple way to provision all types of Edge
infrastructure. This solution heavily utilizes the capabilities of SUSE Edge
Image Builder to create fully customized and unattended installation media;
the quickstart can be found in <xref linkend="quickstart-eib"/>.</para>
</section>
</section>
<section xml:id="id-components-used-in-suse-edge">
<title>Components used in SUSE Edge</title>
<para>SUSE Edge is comprised of both existing SUSE components, including those
from the Linux and Rancher teams, along with additional features and
components built by the Edge team to enable SUSE to address both the
infrastructure requirements and intricacies. The list of components, along
with a link to a high-level description of each and how it’s used in SUSE
Edge can be found below:</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Rancher Dashboard Extensions (<xref
linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SLE Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>Akri (<xref linkend="components-akri"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>Longhorn (<xref linkend="components-longhorn"/>)</para>
</listitem>
<listitem>
<para>NeuVector (<xref linkend="components-neuvector"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>Quick Starts</title>
<partintro>
<para>Quick Starts here</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>BMC automated deployments with Metal<superscript>3</superscript></title>
<para>Metal<superscript>3</superscript> is a <link
xl:href="https://metal3.io/">CNCF project</link> which provides bare-metal
infrastructure management capabilities for Kubernetes.</para>
<para>Metal<superscript>3</superscript> provides Kubernetes-native resources to
manage the lifecycle of bare-metal servers which support management via
out-of-band protocols such as <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>It also has mature support for <link
xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link> which
enables management of infrastructure resources across multiple
infrastructure providers via broadly adopted vendor-neutral APIs.</para>
<section xml:id="id-why-use-this-method">
<title>Why use this method</title>
<para>This method is useful for scenarios where the target hardware supports
out-of-band management, and a fully automated infrastructure management flow
is desired.</para>
<para>A management cluster is configured to provide declarative APIs that enable
inventory and state management of downstream cluster bare-metal servers,
including automated inspection, cleaning and provisioning/deprovisioning.</para>
</section>
<section xml:id="id-high-level-architecture">
<title>High-level architecture</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.png"
width=""/> </imageobject>
<textobject><phrase>quickstart metal3 architecture</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>Prerequisites</title>
<para>There are some specific constraints related to the downstream cluster server
hardware and networking:</para>
<itemizedlist>
<listitem>
<para>Management cluster</para>
<itemizedlist>
<listitem>
<para>Must have network connectivity to the target server management/BMC API</para>
</listitem>
<listitem>
<para>Must have network connectivity to the target server control plane network</para>
</listitem>
<listitem>
<para>For multi-node management clusters, an additional reserved IP address is
required</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Hosts to be controlled</para>
<itemizedlist>
<listitem>
<para>Must support out-of-band management via Redfish, iDRAC or iLO interfaces</para>
</listitem>
<listitem>
<para>Must support deployment via virtual media (PXE is not currently supported)</para>
</listitem>
<listitem>
<para>Must have network connectivity to the management cluster for access to the
Metal<superscript>3</superscript> provisioning APIs</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Some tools are required, these can be installed either on the management
cluster, or on a host which can access it.</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>,
<link xl:href="https://helm.sh">Helm</link> and <link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para>A container runtime such as <link xl:href="https://podman.io">Podman</link>
or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link></para>
</listitem>
</itemizedlist>
<para>The <literal>SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz</literal> OS image
file must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE
Customer Center</link> or the <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE Download
page</link>.</para>
<section xml:id="id-setup-management-cluster">
<title>Setup Management Cluster</title>
<para>The basic steps to install a management cluster and use
Metal<superscript>3</superscript> are:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Install an RKE2 management cluster</para>
</listitem>
<listitem>
<para>Install Rancher</para>
</listitem>
<listitem>
<para>Install a storage provider</para>
</listitem>
<listitem>
<para>Install the Metal<superscript>3</superscript> dependencies</para>
</listitem>
<listitem>
<para>Install CAPI dependencies</para>
</listitem>
<listitem>
<para>Build a SLEMicro OS image for downstream cluster hosts</para>
</listitem>
<listitem>
<para>Register BareMetalHost CRs to define the bare-metal inventory</para>
</listitem>
<listitem>
<para>Create a downstream cluster by defining CAPI resources</para>
</listitem>
</orderedlist>
<para>This guide assumes an existing RKE2 cluster and Rancher (including
cert-manager) has been installed, for example by using Edge Image Builder
(<xref linkend="components-eib"/>).</para>
<tip>
<para>The steps here can also be fully automated as described in the ATIP
management cluster documentation (<xref
linkend="atip-management-cluster"/>).</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Installing Metal<superscript>3</superscript> dependencies</title>
<para>If not already installed as part of the Rancher installation, cert-manager
must be installed and running.</para>
<para>A persistent storage provider must be installed. Longhorn is recommended but
local-path can also be used for dev/PoC environments. The instructions below
assume a StorageClass has been <link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">marked
as default</link>, otherwise additional configuration for the
Metal<superscript>3</superscript> chart is required.</para>
<para>An additional IP is required, which is managed by <link
xl:href="https://metallb.universe.tf/">MetalLB</link> to provide a
consistent endpoint for the Metal<superscript>3</superscript> management
services.  This IP must be part of the control plane subnet and reserved for
static configuration (not part of any DHCP pool).</para>
<tip>
<para>If the management cluster is a single node, the requirement for an
additional floating IP managed via MetalLB can be avoided, see Single-node
configuration (<xref linkend="id-single-node-configuration"/>)</para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>First, we install MetalLB:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/metallb-chart \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>Then we define an <literal>IPAddressPool</literal> and
<literal>L2Advertisment</literal> using the reserved IP, defined as
<literal>STATIC_IRONIC_IP</literal> below:</para>
<screen language="yaml" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>Now Metal<superscript>3</superscript> can be installed:</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/metal3-chart \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="${STATIC_IRONIC_IP}"</screen>
</listitem>
<listitem>
<para>It can take around two minutes for the initContainer to run on this
deployment, so ensure the pods are all running before proceeding:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para>Do not proceed to the following steps until all pods in the
<literal>metal3-system</literal> namespace are running</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Installing cluster API dependencies</title>
<para>First, we need to disable the Rancher-embedded CAPI controller:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: management.cattle.io/v3
kind: Feature
metadata:
  name: embedded-cluster-api
spec:
  value: false
EOF

kubectl delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
kubectl wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s</screen>
<para>Then, to use the SUSE images, a configuration file is needed:</para>
<screen language="bash" linenumbering="unnumbered">mkdir ~/.cluster-api
cat &gt;  ~/.cluster-api/clusterctl.yaml &lt;&lt;EOF
images:
  all:
    repository: registry.suse.com/edge
EOF</screen>
<para>Install <link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">clusterctl</link>
1.6.x, after which we will install the core, infrastructure, bootstrap and
control plane providers as follows:</para>
<screen language="bash" linenumbering="unnumbered">clusterctl init --core "cluster-api:v1.6.2" --infrastructure "metal3:v1.6.0" --bootstrap "rke2:v0.2.6" --control-plane "rke2:v0.2.6"</screen>
<para>After some time, the controller pods should be running in the
<literal>capi-system</literal>, <literal>capm3-system</literal>,
<literal>rke2-bootstrap-system</literal> and
<literal>rke2-control-plane-system</literal> namespaces.</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>Prepare downstream cluster image</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) is used to prepare a
modified SLEMicro base image which is provisioned on downstream cluster
hosts.</para>
<para>Much of the configuration via Edge Image Builder is possible, but in this
guide, we cover the minimal configurations necessary to set up the
downstream cluster.</para>
<section xml:id="id-image-configuration">
<title>Image configuration</title>
<para>When running Edge Image Builder, a directory is mounted from the host, so it
is necessary to create a directory structure to to store the configuration
files used to define the target image.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> is the image definition
file, see <xref linkend="quickstart-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The base image when downloaded is <literal>xz</literal> compressed, which
must be uncompressed with <literal>unxz</literal> and copied/moved under the
<literal>base-images</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>network</literal> folder is optional, see <xref
linkend="metal3-add-network-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The custom/scripts directory contains scripts to be run on first-boot;
currently a <literal>growfs.sh</literal> script is required to resize the OS
root partition on deployment</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>Downstream cluster image definition file</title>
<para>The <literal>downstream-cluster-config.yaml</literal> file is the main
configuration file for the downstream cluster image. The following is a
minimal example for deployment via Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</screen>
<para><literal>${ROOT_PASSWORD}</literal> is the encrypted password for the root
user, which can be useful for test/debugging.  It can be generated with the
<literal>openssl passwd -6 PASSWORD</literal> command</para>
<para>For the production environments, it is recommended to use the SSH keys that
can be added to the users block replacing the <literal>${USERKEY1}</literal>
with the real SSH keys.</para>
<note>
<para><literal>net.ifnames=1</literal> enables <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link></para>
<para>This matches the default configuration for the metal3 chart, but the setting
must match the configured chart <literal>predictableNicNames</literal>
value.</para>
<para>Also note <literal>ignition.platform.id=openstack</literal> is mandatory,
without this argument SLEMicro configuration via ignition will fail in the
Metal<superscript>3</superscript> automated flow.</para>
</note>
</section>
<section xml:id="id-growfs-script">
<title>Growfs script</title>
<para>Currently is a custom script (<literal>custom/scripts/growfs.sh</literal>)
which is required to grow the file system to the match the disk size on
first-boot after provisioning. The <literal>growfs.sh</literal> script
contains the following information:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>Add your own custom scripts to be executed during the provisioning process
using the same approach.  For more information, see <xref
linkend="quickstart-eib"/>.</para>
<para>The bug related to this workaround is <link
xl:href="https://bugzilla.suse.com/show_bug.cgi?id=1217430">https://bugzilla.suse.com/show_bug.cgi?id=1217430</link></para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>Image creation</title>
<para>Once the directory structure is prepared following the previous sections,
run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>This creates the output image file named
<literal>SLE-Micro-eib-output.raw</literal>, based on the definition
described above.</para>
<para>The output image must then be made available via a webserver, either the
media-server container enabled via the Metal3 chart (<xref
linkend="metal3-media-server"/>)  or some other locally accessible server.
In the examples below, we refer to this server as
<literal>imagecache.local:8080</literal></para>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>Adding BareMetalHost inventory</title>
<para>Registering bare-metal servers for automated deployment requires creating
two resources: a Secret storing BMC access credentials and a
Metal<superscript>3</superscript> BareMetalHost resource defining the BMC
connection and other details:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>Note the following:</para>
<itemizedlist>
<listitem>
<para>The Secret username/password must be base64 encoded. Note this should not
include any trailing newlines (for example, use <literal>echo -n</literal>,
not just <literal>echo</literal>!)</para>
</listitem>
<listitem>
<para>The <literal>cluster-role</literal> label may be set now or later on cluster
creation. In the example below, we expect <literal>control-plane</literal>
or <literal>worker</literal></para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal> must be a valid MAC that matches the
control plane NIC of the host</para>
</listitem>
<listitem>
<para>The <literal>bmc</literal> address is the connection to the BMC management
API, the following are supported:</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/&lt;SYSTEM ID&gt;</literal>: Redfish virtual
media, for example, SuperMicro</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/System.Embedded.1</literal>: Dell iDRAC</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>See the <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">Upstream
API docs</link> for more details on the BareMetalHost API</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>Configuring Static IPs</title>
<para>The BareMetalHost example above assumes DHCP provides the controlplane
network configuration, but for scenarios where manual configuration is
needed such as static IPs it is possible to provide additional
configuration, as described below.</para>
<section xml:id="metal3-add-network-eib">
<title>Additional script for static network configuration</title>
<para>When creating the base image with Edge Image Builder, in the
<literal>network</literal> folder, create the following
<literal>configure-network.sh</literal> file.</para>
<para>This consumes configuration drive data on first-boot, and configures the
host networking using the <link
xl:href="https://github.com/suse-edge/nm-configurator">NM Configurator
tool</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>Additional secret with host network configuration</title>
<para>An additional secret containing data in the <link
xl:href="https://nmstate.io/">nmstate</link> format supported by NM
Configurator (<xref linkend="components-nmc"/>) can be defined for each
host.</para>
<para>The secret is then referenced in the <literal>BareMetalHost</literal>
resource via the <literal>preprovisioningNetworkDataName</literal> spec
field.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>although optional in the nmstate API, the mac-address is mandatory for
configuration via NM Configurator and must be provided.</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>BareMetalHost preparation</title>
<para>After creating the BareMetalHost resource and associated secrets as
described above, a host preparation workflow is triggered:</para>
<itemizedlist>
<listitem>
<para>A ramdisk image is booted by virtualmedia attachment to the target host BMC</para>
</listitem>
<listitem>
<para>The ramdisk inspects hardware details, and prepares the host for
provisioning (for example by cleaning disks of previous data)</para>
</listitem>
<listitem>
<para>On completion of this process, hardware details in the BareMetalHost
<literal>status.hardware</literal> field are updated and can be verified</para>
</listitem>
</itemizedlist>
<para>This process can take several minutes, but when completed you should see the
BareMetalHost state become <literal>available</literal>:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>Creating downstream clusters</title>
<para>We now create Cluster API resources which define the downstream cluster, and
Machine resources which will cause the BareMetalHost resources to be
provisioned, then bootstrapped to form an RKE2 cluster.</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>Control plane deployment</title>
<para>To deploy the controlplane we define a yaml manifest similar to the one
below, which contains the following resources:</para>
<itemizedlist>
<listitem>
<para>Cluster resource defines the cluster name, networks, and type of
controlplane/infrastructure provider (in this case RKE2/Metal3)</para>
</listitem>
<listitem>
<para>Metal3Cluster defines the controlplane endpoint (host IP for single-node,
LoadBalancer endpoint for multi-node, this example assumes single-node)</para>
</listitem>
<listitem>
<para>RKE2ControlPlane defines the RKE2 version and any additional configuration
needed during cluster bootstrapping</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate defines the OS Image to be applied to the
BareMetalHost resources, and the hostSelector defines which BareMetalHosts
to consume</para>
</listitem>
<listitem>
<para>Metal3DataTemplate defines additional metaData to be passed to the
BareMetalHost (note networkData is not currently supported in the Edge
solution)</para>
</listitem>
</itemizedlist>
<para>Note for simplicity this example assumes a single-node controlplane, where
the BareMetalHost is configured with an IP of
<literal>192.168.125.200</literal> - for more advanced multi-node examples
please see the ATIP documentation (<xref
linkend="atip-automated-provisioning"/>)</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    version: v1.28.9+rke2r1
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>When the example above has been copied and adapted to suit your environment,
it can be applied via <literal>kubectl</literal> then the cluster status can
be monitored with <literal>clusterctl</literal></para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned - status can be checked via clusterctl
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>Worker/Compute deployment</title>
<para>Similar to the controlplane we define a yaml manifest, which contains the
following resources:</para>
<itemizedlist>
<listitem>
<para>MachineDeployment defines the number of replicas (hosts) and the
bootstrap/infrastructure provider (in this case RKE2/Metal3)</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplate describes the RKE2 version and first-boot configuration
for agent host bootstrapping</para>
</listitem>
<listitem>
<para>Metal3MachineTemplate defines the OS Image to be applied to the
BareMetalHost resources, and the hostSelector defines which BareMetalHosts
to consume</para>
</listitem>
<listitem>
<para>Metal3DataTemplate defines additional metaData to be passed to the
BareMetalHost (note networkData is not currently supported in the Edge
solution)</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.28.9+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.28.9+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>When the example above has been copied and adapted to suit your environment,
it can be applied via <literal>kubectl</literal> then the cluster status can
be monitored with <literal>clusterctl</literal></para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait some time for the compute/agent hosts to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>Cluster deprovisioning</title>
<para>The downstream cluster may be deprovisioned by deleting the resources
applied in the creation steps above:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>This triggers deprovisioning of the BareMetalHost resources, which may take
several minutes, after which they should be in available state again:</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>The upstream <link
xl:href="https://github.com/metal3-io/ip-address-manager">IP Address
Management controller</link> is currently not supported, because it’s not
yet compatible with our choice of network configuration tooling and
first-boot toolchain in SLEMicro.</para>
</listitem>
<listitem>
<para>Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are
not currently supported.</para>
</listitem>
<listitem>
<para>Only deployment via redfish-virtualmedia is currently supported.</para>
</listitem>
<listitem>
<para>Deployed clusters are not currently imported into Rancher</para>
</listitem>
<listitem>
<para>Due to disabling the Rancher embedded CAPI controller, a management cluster
configured for Metal<superscript>3</superscript> as described above cannot
also be used for other cluster provisioning methods such as Elemental (<xref
linkend="components-elemental"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-planned-changes">
<title>Planned changes</title>
<itemizedlist>
<listitem>
<para>Deployed clusters imported into Rancher, this is planned via <link
xl:href="https://turtles.docs.rancher.com/">Rancher Turtles</link> in future</para>
</listitem>
<listitem>
<para>Aligning with Rancher Turtles is also expected to remove the requirement to
disable the Rancher embedded CAPI, so other cluster methods should be
possible via the management cluster.</para>
</listitem>
<listitem>
<para>Enable support of the IPAM resources and configuration via networkData
fields</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>Additional resources</title>
<para>The ATIP Documentation (<xref linkend="atip"/>) has examples of more
advanced usage of Metal<superscript>3</superscript> for telco use-cases.</para>
<section xml:id="id-single-node-configuration">
<title>Single-node configuration</title>
<para>For test/PoC environments where the management cluster is a single node, it
is possible to avoid the requirement for an additional floating IP managed
via MetalLB.</para>
<para>In this mode, the endpoint for the management cluster APIs is the IP of the
management cluster, therefore it should be reserved when using DHCP or
statically configured to ensure the management cluster IP does not change -
referred to as <literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal> below.</para>
<para>To enable this scenario the metal3 chart values required are as follows:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="id-disabling-tls-for-virtualmedia-iso-attachment">
<title>Disabling TLS for virtualmedia ISO attachment</title>
<para>Some server vendors verify the SSL connection when attaching virtual-media
ISO images to the BMC, which can cause a problem because the generated
certificates for the Metal3 deployment are self-signed, to work around this
issue it’s possible to disable TLS only for the virtualmedia disk attachment
with metal3 chart values as follows:</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>An alternative solution is to configure the BMCs with the CA cert - in this
case you can read the certificates from the cluster using
<literal>kubectl</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>The certificate can then be configured on the server BMC console, although
the process for that is vendor specific (and not possible for all vendors,
in which case the <literal>enable_vmedia_tls</literal> flag may be
required).</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Remote host onboarding with Elemental</title>
<para>This section documents the "phone home network provisioning" solution as
part of SUSE Edge, where we use Elemental to assist with node
onboarding. Elemental is a software stack enabling remote host registration
and centralized full cloud-native OS management with Kubernetes. In the SUSE
Edge stack we use the registration feature of Elemental to enable remote
host onboarding into Rancher so that hosts can be integrated into a
centralized management platform and from there, deploy and manage Kubernetes
clusters along with layered components, applications, and their lifecycle,
all from a common place.</para>
<para>This approach can be useful in scenarios where the devices that you want to
control are not on the same network as the upstream cluster or do not have a
out-of-band management controller onboard to allow more direct control, and
where you’re booting many different "unknown" systems at the edge, and need
to securely onboard and manage them at scale. This is a common scenario for
use cases in retail, industrial IoT, or other spaces where you have little
control over the network your devices are being installed in.</para>
<section xml:id="id-high-level-architecture-2">
<title>High-level architecture</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.png"
width=""/> </imageobject>
<textobject><phrase>quickstart elemental architecture</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>Resources needed</title>
<para>The following describes the minimum system and environmental requirements to
run through this quickstart:</para>
<itemizedlist>
<listitem>
<para>A host for the centralized management cluster (the one hosting Rancher and
Elemental):</para>
<itemizedlist>
<listitem>
<para>Minimum 8 GB RAM and 20 GB disk space for development or testing (see <link
xl:href="https://ranchermanager.docs.rancher.com/pages-for-subheaders/installation-requirements#hardware-requirements">here</link>
for production use)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>A target node to be provisioned, i.e. the edge device (a virtual machine can
be used for demoing or testing purposes)</para>
<itemizedlist>
<listitem>
<para>Minimum 4GB RAM, 2 CPU cores, and 20 GB disk</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>A resolvable host name for the management cluster or a static IP address to
use with a service like sslip.io</para>
</listitem>
<listitem>
<para>A host to build the installation media via Edge Image Builder</para>
<itemizedlist>
<listitem>
<para>Running SLES 15 SP5, openSUSE Leap 15.5, or another compatible operating
system that supports Podman.</para>
</listitem>
<listitem>
<para>With <link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>,
<link xl:href="https://podman.io">Podman</link>, and <link
xl:href="https://helm.sh">Helm</link> installed</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>A USB flash drive to boot from (if using physical hardware)</para>
</listitem>
</itemizedlist>
<note>
<para>Existing data found on target machines will be overwritten as part of the
process, please make sure you backup any data on any USB storage devices and
disks attached to target deployment nodes.</para>
</note>
<para>This guide is created using a Digital Ocean droplet to host the upstream
cluster and an Intel NUC as the downstream device. For building the
installation media, SUSE Linux Enterprise Server is used.</para>
</section>
<section xml:id="id-how-to-use-elemental">
<title>How to use Elemental</title>
<para>The basic steps to install and use Elemental are:</para>
<itemizedlist>
<listitem>
<para><xref linkend="build-bootstrap-cluster"/></para>
</listitem>
<listitem>
<para><xref linkend="install-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="install-elemental"/></para>
</listitem>
<listitem>
<para><xref linkend="build-installation-media"/></para>
</listitem>
<listitem>
<para><xref linkend="boot-downstream-nodes"/></para>
</listitem>
<listitem>
<para><xref linkend="create-downstream-clusters"/></para>
</listitem>
</itemizedlist>
<section xml:id="build-bootstrap-cluster">
<title>Build bootstrap cluster</title>
<para>Start by creating a cluster capable of hosting Rancher and Elemental. This
cluster needs to be routable from the network that the downstream nodes are
connected to.</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Create Kubernetes cluster</title>
<para>If you are using a hyperscaler (such as Azure, AWS or Google Cloud), the
easiest way to set up a cluster is using their built-in tools. For the sake
of conciseness in this guide, we do not detail the process of each of these
options.</para>
<para>If you are installing onto bare metal or another hosting service where you
need to also provide the Kubernetes distribution itself, we recommend using
<link xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>.</para>
</section>
<section xml:id="id-set-up-dns">
<title>Set up DNS</title>
<para>Before continuing, you need to set up access to your cluster. As with the
setup of the cluster itself, how you configure DNS will be different
depending on where it is being hosted.</para>
<tip>
<para>If you do not want to handle setting up DNS records (for example, this is
just an ephemeral test server), you can use a service like <link
xl:href="https://sslip.io">sslip.io</link> instead. With this service, you
can resolve any IP address with <literal>&lt;address&gt;.sslip.io</literal>.</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Install Rancher</title>
<para>To install Rancher, you need to get access to the Kubernetes API of the
cluster you just created. This looks differently depending on what
distribution of Kubernetes is being used.</para>
<para>For RKE2, the kubeconfig file will have been written to
<literal>/etc/rancher/rke2/rke2.yaml</literal>.  Save this file as
<literal>~/.kube/config</literal> on your local system.  You may need to
edit the file to include the correct externally routable IP address or host
name.</para>
<para>Install Rancher easily with the commands from the <link
xl:href="https://ranchermanager.docs.rancher.com/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster">Rancher
Documentation</link>:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Install <link xl:href="https://cert-manager.io">cert-manager</link>:</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime

kubectl create namespace cattle-system

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml
helm repo add jetstack https://charts.jetstack.io

helm repo update

helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Windows</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime

kubectl create namespace cattle-system

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml

helm repo add jetstack https://charts.jetstack.io

helm repo update
helm install cert-manager jetstack/cert-manager `
  --namespace cert-manager `
  --create-namespace</screen>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<para>Then install Rancher itself:</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt;</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Windows</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm install rancher rancher-prime/rancher `
  --namespace cattle-system `
  --set hostname=&lt;DNS or sslip from above&gt; `
  --set replicas=1 `
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt;</screen>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
<note>
<para>If this is intended to be a production system, please use cert-manager to
configure a real certificate (such as one from Let’s Encrypt).</para>
</note>
<para>Browse to the host name you set up and log in to Rancher with the
<literal>bootstrapPassword</literal> you used. You will be guided through a
short setup process.</para>
</section>
<section xml:id="install-elemental">
<title>Install Elemental</title>
<para>With Rancher installed, you can now install the Elemental operator and
required CRD’s. The Helm chart for Elemental is published as an OCI artifact
so the installation is a little simpler than other charts.  It can be
installed from either the same shell you used to install Rancher or in the
browser from within Rancher’s shell.</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.4.4

helm install --create-namespace -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.4.4</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>(Optionally) Install the Elemental UI extension</title>
<orderedlist numeration="arabic">
<listitem>
<para>To use the Elemental UI, log in to your Rancher instance, click the
three-dot menu in the upper left:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width=""/> </imageobject>
<textobject><phrase>Installing Elemental extension1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>From the "Available" tab on this page, click "Install" on the Elemental
card:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width=""/> </imageobject>
<textobject><phrase>Installing Elemental extension 2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Confirm that you want to install the extension:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width=""/> </imageobject>
<textobject><phrase>Installing Elemental extension 3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>After it installs, you will be prompted to reload the page.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width=""/> </imageobject>
<textobject><phrase>Installing Elemental extension 4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Once you reload, you can access the Elemental extension through the "OS
Management" global app.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width=""/> </imageobject>
<textobject><phrase>Accessing Elemental extension</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-elemental">
<title>Configure Elemental</title>
<para>For simplicity, we recommend setting the variable <literal>$ELEM</literal>
to the full path of where you want the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>To allow machines to register to Elemental, we need to create a
<literal>MachineRegistration</literal> object in the
<literal>fleet-default</literal> namespace.</para>
<para>Let us create a basic version of this object:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>The <literal>cat</literal> command escapes each <literal>$</literal> with a
backslash (<literal>\</literal>) so that Bash does not template them. Remove
the backslashes if copying manually.</para>
</note>
<para>Once the object is created, find and note the endpoint that gets assigned:</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>Alternatively, this can also be done from the UI.</para>
<variablelist>
<varlistentry>
<term>UI Extension</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>From the OS Management extension, click "Create Registration Endpoint":</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png" width=""/>
</imageobject>
<textobject><phrase>Click Create Registration</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Give this configuration a name.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png" width=""/>
</imageobject>
<textobject><phrase>Add Name</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>You can ignore the Cloud Configuration field as the data here is overridden
by the following steps with Edge Image Builder.</para>
</note>
</listitem>
<listitem>
<para>Next, scroll down and click "Add Label" for each label you want to be on the
resource that gets created when a machine registers. This is useful for
distinguishing machines.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png" width=""/>
</imageobject>
<textobject><phrase>Add Labels</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Lastly, click "Create" to save the configuration.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-create.png" width=""/>
</imageobject>
<textobject><phrase>Click Create</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI Extension</term>
<listitem>
<para>If you just created the configuration, you should see the Registration URL
listed and can click "Copy" to copy the address:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width=""/>
</imageobject>
<textobject><phrase>Copy URL</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>If you clicked away from that screen, you can click "Registration Endpoints"
in the left menu, then click the name of the endpoint you just created.</para>
</tip>
</listitem>
</varlistentry>
</variablelist>
<para>This URL is used in the next step.</para>
</section>
</section>
<section xml:id="build-installation-media">
<title>Build the installation media</title>
<para>While the current version of Elemental has a way to build its own
installation media, in SUSE Edge 3.0 we do this with the Edge Image Builder
instead, so the resulting system is built with <link
xl:href="https://www.suse.com/products/micro/">SLE Micro</link> as the base
Operating System.</para>
<tip>
<para>For more details on the Edge Image Builder, check out the Getting Started
Guide for it (<xref linkend="quickstart-eib"/>) and also the Component
Documentation (<xref linkend="components-eib"/>).</para>
</tip>
<para>From a Linux system with Podman installed, run:</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.0
image:
    imageType: iso
    arch: x86_64
    baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para>The unencoded password is <literal>eib</literal>.</para>
</listitem>
<listitem>
<para>The <literal>cat</literal> command escapes each <literal>$</literal> with a
backslash (<literal>\</literal>) so that Bash does not template them. Remove
the backslashes if copying manually.</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file eib-config.yaml</screen>
<para>If you are booting a physical device, we need to burn the image to a USB
flash drive. This can be done with:</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>Boot the downstream nodes</title>
<para>Now that we have created the installation media, we can boot our downstream
nodes with it.</para>
<para>For each of the systems that you want to control with Elemental, add the
installation media and boot the device. After installation, it will reboot
and register itself.</para>
<para>If you are using the UI extension, you should see your node appear in the
"Inventory of Machines."</para>
<note>
<para>Do not remove the installation medium until you’ve seen the login prompt;
during first-boot files are still accessed on the USB stick.</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>Create downstream clusters</title>
<para>There are two objects we need to create when provisioning a new cluster
using Elemental.</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>The first is the <literal>MachineInventorySelectorTemplate</literal>. This
object allows us to specify a mapping between clusters and the machines in
the inventory.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Create a selector which will match any machine in the inventory with a
label:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>Apply the resource to the cluster:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>Obtain the name of the machine and add the matching label:</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>Create a simple single-node K3s cluster resource and apply it to the
cluster:</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.28.9+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI Extension</term>
<listitem>
<para>The UI extension allows for a few shortcuts to be taken. Note that managing
multiple locations may involve too much manual work.</para>
<orderedlist numeration="arabic">
<listitem>
<para>As before, open the left three-dot menu and select "OS Management." This
brings you back to the main screen for managing your Elemental systems.</para>
</listitem>
<listitem>
<para>On the left sidebar, click "Inventory of Machines." This opens the inventory
of machines that have registered.</para>
</listitem>
<listitem>
<para>To create a cluster from these machines, select the systems you want, click
the "Actions" drop-down list, then "Create Elemental Cluster." This opens
the Cluster Creation dialog while also creating a MachineSelectorTemplate to
use in the background.</para>
</listitem>
<listitem>
<para>On this screen, configure the cluster you want to be built. For this quick
start, K3s v1.28.9+k3s1 is selected and the rest of the options are left as
is.</para>
<tip>
<para>You may need to scroll down to see more options.</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>After creating these objects, you should see a new Kubernetes cluster spin
up using the new node you just installed with.</para>
<tip>
<para>To allow for easier grouping of systems, you could add a startup script that
finds something in the environment that is known to be unique to that
location.</para>
<para>For example, if you know that each location will have a unique subnet, you
can write a script that finds the network prefix and adds a label to the
corresponding MachineInventory.</para>
<para>This would typically be custom to your system’s design but could look like:</para>
<screen language="bash" linenumbering="unnumbered">INET=`ip addr show dev eth0 | grep "inet\ "`
elemental-register --label "network=$INET" \
 --label "network=$INET" /oem/registration</screen>
</tip>
</section>
</section>
<section xml:id="id-node-reset">
<title>Node Reset</title>
<para>SUSE Rancher Elemental supports the ability to perform a "node reset" which
can optionally trigger when either a whole cluster is deleted from Rancher,
a single node is deleted from a cluster, or a node is manually deleted from
the machine inventory. This is useful when you want to reset and clean-up
any orphaned resources and want to automatically bring the cleaned node back
into the machine inventory so it can be reused. This is not enabled by
default, and thus any system that is removed, will not be cleaned up
(i.e. data will not be removed, and any Kubernetes cluster resources will
continue to operate on the downstream clusters) and it will require manual
intervention to wipe data and re-register the machine to Rancher via
Elemental.</para>
<para>If you wish for this functionality to be enabled by default, you need to
make sure that your <literal>MachineRegistration</literal> explicitly
enables this by adding <literal>config.elemental.reset.enabled:
true</literal>, for example:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>Then, all systems registered with this
<literal>MachineRegistration</literal> will automatically receive the
<literal>elemental.cattle.io/resettable: 'true'</literal> annotation in
their configuration. If you wish to do this manually on individual nodes,
e.g. because you’ve got an existing <literal>MachineInventory</literal> that
doesn’t have this annotation, or you have already deployed nodes, you can
modify the <literal>MachineInventory</literal> and add the
<literal>resettable</literal> configuration, for example:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>In SUSE Edge 3.0, the Elemental Operator puts down a marker on the operating
system that will trigger the cleanup process automatically; it will stop all
Kubernetes services, remove all persistent data, uninstall all Kubernetes
services, cleanup any remaining Kubernetes/Rancher directories, and force a
re-registration to Rancher via the original Elemental
<literal>MachineRegistration</literal> configuration. This happens
automaticaly, there is no need for any manual intervention. The script that
gets called can be found in
<literal>/opt/edge/elemental_node_cleanup.sh</literal> and is triggered via
<literal>systemd.path</literal> upon the placement of the marker, so its
execution is immediate.</para>
<warning>
<para>Using the <literal>resettable</literal> functionality assumes that the
desired behavior when removing a node/cluster from Rancher is to wipe data
and force a re-registration. Data loss is guaranteed in this situation, so
only use this if you’re sure that you want automatic reset to be performed.</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>Next steps</title>
<para>Here are some recommended resources to research after using this guide:</para>
<itemizedlist>
<listitem>
<para>End-to-end automation in <xref linkend="components-fleet"/></para>
</listitem>
<listitem>
<para>Additional network configuration options in <xref linkend="components-nmc"/></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Standalone clusters with Edge Image Builder</title>
<para>Edge Image Builder (EIB) is a tool that streamlines the process of
generating customized, ready-to-boot (CRB) disk images for bootstrapping
machines, even in fully air-gapped scenarios. EIB is used to create
deployment images for use in all three of the SUSE Edge deployment
footprints, as it’s flexible enough to offer the smallest customizations,
e.g. adding a user or setting the timezone, through offering a
comprehensively configured image that sets up, for example, complex
networking configurations, deploys multi-node Kubernetes clusters, deploys
customer workloads, and registers to the centralized management platform via
Rancher/Elemental and SUSE Manager. EIB runs as in a container image, making
it incredibly portable across platforms and ensuring that all of the
required dependencies are self-contained, having a very minimal impact on
the installed packages of the system that’s being used to operate the tool.</para>
<para>For more information, read the Edge Image Builder Introduction (<xref
linkend="components-eib"/>).</para>
<section xml:id="id-prerequisites-2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>An x86_64 physical host (or virtual machine) running SLES 15 SP5, openSUSE
Leap 15.5, or openSUSE Tumbleweed.</para>
</listitem>
<listitem>
<para>An available container runtime (e.g. Podman)</para>
</listitem>
<listitem>
<para>A downloaded copy of the latest SLE Micro 5.5 SelfInstall "GM2" ISO image
found <link xl:href="https://www.suse.com/download/sle-micro/">here</link>.</para>
</listitem>
</itemizedlist>
<note>
<para>Other operating systems may function so long as a compatible container
runtime is available, but testing on other platforms has not been
extensive. The documentation focuses on Podman, but the same functionality
should be able to be achieved with Docker.</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>Getting the EIB Image</title>
<para>The EIB container image is publicly available and can be downloaded from the
SUSE Edge registry by running the following command on your image build
host:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/edge-image-builder:1.0.2</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>Creating the image configuration directory</title>
<para>As EIB runs within a container, we need to mount a configuration directory
from the host, enabling you to specify your desired configuration, and
during the build process EIB has access to any required input files and
supporting artifacts. This directory must follow a specific structure. Let’s
create it, assuming that this directory will exist in your home directory,
and called "eib":</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>In the previous step we created a "base-images" directory that will host the
SLE Micro 5.5 input image, let’s ensure that the downloaded image is copied
over to the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/downloads/SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>During the EIB run, the original base image is <emphasis
role="strong">not</emphasis> modified; a new and customized version is
created with the desired configuration in the root of the EIB config
directory.</para>
</note>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>Creating the image definition file</title>
<para>The definition file describes the majority of configurable options that the
Edge Image Builder supports, a full example of options can be found <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/pkg/image/testdata/full-valid-example.yaml">here</link>,
and we would recommend that you take a look at the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md">upstream
building images guide</link> for more comprehensive examples than the one
we’re going to run through below. Let’s start with a very basic definition
file for our OS image:</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>This definition specifies that we’re generating an output image for an
<literal>x86_64</literal> based system. The image that will be used as the
base for further modification is an <literal>iso</literal> image named
<literal>slemicro.iso</literal>, expected to be located at
<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>. It also outlines
that after EIB finishes modifying the image, the output image will be named
<literal>eib-image.iso</literal>, and by default will reside in
<literal>$CONFIG_DIR</literal>.</para>
<para>Now our directory structure should look like:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>In the following sections we’ll walk through a few examples of common
operations:</para>
<section xml:id="id-configuring-os-users">
<title>Configuring OS Users</title>
<para>EIB allows you to preconfigure users with login information, such as
passwords or SSH keys, including setting a fixed root password. As part of
this example we’re going to fix the root password, and the first step is to
use <literal>OpenSSL</literal> to create a one-way encrypted password:</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>This will output something similar to:</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>We can then add a section in the definition file called
<literal>operatingSystem</literal> with a <literal>users</literal> array
inside it. The resulting file should look like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>It’s also possible to add additional users, create the home directories, set
user-id’s, add ssh-key authentication, and modify group information. Please
refer to the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md">upstream
building images guide</link> for further examples.</para>
</note>
</section>
<section xml:id="id-configuring-rpm-packages">
<title>Configuring RPM packages</title>
<para>One of the major features of EIB is to provide a mechanism to add additional
software packages to the image, so when the installation completes the
system is able to leverage the installed packages right away. EIB permits
users to specify the following:</para>
<itemizedlist>
<listitem>
<para>Packages by their name within a list in the image definition</para>
</listitem>
<listitem>
<para>Network repositories to search for these packages in</para>
</listitem>
<listitem>
<para>SUSE Customer Center (SCC) credentials to search official SUSE repositories
for the listed packages</para>
</listitem>
<listitem>
<para>Via an <literal>$CONFIG_DIR/rpms</literal> directory, side-load custom RPM’s
that don’t exist in network repositories</para>
</listitem>
<listitem>
<para>Via the same directory (<literal>$CONFIG_DIR/rpms/gpg-keys</literal>),
GPG-keys to enable validation of third party packages</para>
</listitem>
</itemizedlist>
<para>EIB will then run through a package resolution process at image build time,
taking the base image as the input, and attempts to pull and install all
supplied packages, either specified via the list or provided locally. EIB
downloads all of the packages, including any dependencies into a repository
that exists within the output image and instructs the system to install
these during the first boot process. Doing this process during the image
build guarantees that the packages will successfully install during
first-boot on the desired platform, e.g. the node at the edge. This is also
advantageous in environments where you want to bake the additional packages
into the image rather than pull them over the network when in operation,
e.g. for air-gapped or restricted network environments.</para>
<para>As a simple example to demonstrate this, we are going to install the
<literal>nvidia-container-toolkit</literal> RPM package found in the third
party vendor-supported NVIDIA repository:</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>The resulting definition file looks like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>The above is a simple example, but for completeness, download the NVIDIA
package signing key before running the image generation:</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>Adding in additional RPM’s via this method is meant for the addition of
supported third party components or user-supplied (and maintained) packages;
this mechanism should not be used to add packages that would not usually be
supported on SLE Micro. If this mechanism is used to add components from
openSUSE repositories (which are not supported), including from newer
releases or service packs, you may end up with an unsupported configuration,
especially when dependency resolution results in core parts of the operating
system being replaced, even though the resulting system may appear to
function as expected. If you’re unsure, contact your SUSE representative for
assistance in determining the supportability of your desired configuration.</para>
</warning>
<note>
<para>A more comprehensive guide with additional examples can be found in the
<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/installing-packages.md">upstream
installing packages guide</link>.</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Configuring Kubernetes cluster and user workloads</title>
<para>Another feature of EIB is the ability to use it to automate the deployment
of both single-node and multi-node highly-available Kubernetes clusters that
"bootstrap in place", i.e. don’t require any form of centralized management
infrastructure to coordinate. The primary driver behind this approach is for
air-gapped deployments, or network restricted environments, but it also
serves as a way of quickly bootstrapping standalone clusters, even if full
and unrestricted network access is available.</para>
<para>This method enables not only the deployment of the customized operating
system, but also the ability to specify Kubernetes configuration, any
additional layered components via Helm charts, and any user workloads via
supplied Kubernetes manifests. However, the design principle behind using
this method is that we default to assuming that the user is wanting to
air-gap and therefore any items specified in the image definition will be
pulled into the image, which includes user-supplied workloads, where EIB
will make sure that any discovered images that are required by definitions
supplied are copied locally, and are served by the embedded image registry
in the resulting deployed system.</para>
<para>In this next example, we’re going to take our existing image definition and
will specify a Kubernetes configuration (in this example it doesn’t list the
systems and their roles, so we default to assuming single-node), which will
instruct EIB to provision a single-node RKE2 Kubernetes cluster. To show the
automation of both the deployment of both user-supplied workloads (via
manifest) and layered components (via Helm), we are going to install
KubeVirt via the SUSE Edge Helm chart, as well as NGINX via a Kubernetes
manifest. The additional configuration we need to append to the existing
image definition is as follows:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.28.9+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt-chart
        version: 0.2.4
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge</screen>
<para>The resulting full definition file should now look like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.28.9+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt-chart
        version: 0.2.4
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge</screen>
<note>
<para>Further examples of options such as multi-node deployments, custom
networking, and Helm chart options/values can be found in the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#kubernetes">upstream
documentation</link>.</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>Configuring the network</title>
<para>In the last example in this quickstart, let’s configure the network that
will be brought up when a system is provisioned with the image generated by
EIB. It’s important to understand that unless a network configuration is
supplied, the default model is that DHCP will be used on all interfaces
discovered at boot time. However, this is not always a desirable
configuration, especially if DHCP is not available and you need to provide
static configurations, or you need to set up more complex networking
constructs, e.g. bonds, LACP, and VLAN’s, or need to override certain
parameters, e.g. hostnames, DNS servers, and routes.</para>
<para>EIB provides the ability to provide either per-node configurations (where
the system in question is uniquely identified by its MAC address), or an
override for supplying an identical configuration to each machine, which is
more useful when the system MAC addresses aren’t known. An additional tool
is used by EIB called Network Manager Configurator, or
<literal>nmc</literal> for short, which is a tool built by the SUSE Edge
team to allow custom networking configurations to be applied based on the
<link xl:href="https://nmstate.io/">nmstate.io</link> declarative network
schema, and at boot time will identify the node it’s booting on and will
apply the desired network configuration prior to any services coming up.</para>
<para>We’ll now apply a static network configuration for a system with a single
interface by describing the desired network state in a node-specific file
(based on the desired hostname) in the required <literal>network</literal>
directory:</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>The above example is set up for the default
<literal>192.168.122.0/24</literal> subnet assuming that testing is being
executed on a virtual machine, please adapt to suit your environment, not
forgetting the MAC address. As the same image can be used to provision
multiple nodes, networking configured by EIB (via <literal>nmc</literal>) is
dependent on it being able to uniquely identify the node by its MAC address,
and hence during boot <literal>nmc</literal> will apply the correct
networking configuration to each machine. This means that you’ll need to
know the MAC addresses of the systems you want to install
onto. Alternatively, the default behavior is to rely on DHCP, but you can
utilize the <literal>configure-network.sh</literal> hook to apply a common
configuration to all nodes - see the networking guide (<xref
linkend="components-nmc"/>) for further details.</para>
</warning>
<para>The resulting file structure should look like:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>The network configuration we just created will be parsed and the necessary
NetworkManager connection files will be automatically generated and inserted
into the new installation image that EIB will create. These files will be
applied during the provisioning of the host, resulting in a complete network
configuration.</para>
<note>
<para>Please refer to the Edge Networking component (<xref
linkend="components-nmc"/>) for a more comprehensive explanation of the
above configuration and examples of this feature.</para>
</note>
</section>
</section>
<section xml:id="id-building-the-image">
<title>Building the image</title>
<para>Now that we’ve got a base image and an image definition for EIB to consume,
let’s go ahead and build the image. For this, we simply use
<literal>podman</literal> to call the EIB container with the "build"
command, specifying the definition file:</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file iso-definition.yaml</screen>
<para>The output of the command should be similar to:</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Downloading file: dl-manifest-1.yaml 100%  (498/498 B, 5.9 MB/s)
Populating Embedded Artifact Registry... 100%  (3/3, 11 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 98 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 100 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100%  (34/34 MB, 101 MB/s)
Downloading file: sha256sum-amd64.txt 100%  (3.9/3.9 kB, 1.5 MB/s)
Downloading file: dl-manifest-1.yaml 100%  (498/498 B, 7.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>The built ISO image is stored at
<literal>$CONFIG_DIR/eib-image.iso</literal>:</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>Each build creates a time-stamped folder in
<literal>$CONFIG_DIR/_build/</literal> that includes the logs of the build,
the artifacts used during the build, and the <literal>combustion</literal>
and <literal>artefacts</literal> directories which contain all the scripts
and artifacts that are added to the CRB image.</para>
<para>The contents of this directory should look like:</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:1.14.2-registry.tar.zst
│   │   │   ├── rancher_kubectl:v1.28.7-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.5_virt-operator:1.1.1-150500.8.12.1-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   └── x86_64
│   │   │       │       ├── nvidia-container-toolkit-1.15.0-1.x86_64.rpm
│   │   │       │       ├── ...
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt-chart.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt-chart
│   │       └── kubevirt-0.2.4.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>If the build fails, <literal>eib-build.log</literal> is the first log that
contains information. From there, it will direct you to the component that
failed for debugging.</para>
<para>At this point, you should have a ready-to-use image that will:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Deploy SLE Micro 5.5</para>
</listitem>
<listitem>
<para>Configure the root password</para>
</listitem>
<listitem>
<para>Install the <literal>nvidia-container-toolkit</literal> package</para>
</listitem>
<listitem>
<para>Configure an embedded container registry to serve content locally</para>
</listitem>
<listitem>
<para>Install single-node RKE2</para>
</listitem>
<listitem>
<para>Configure static networking</para>
</listitem>
<listitem>
<para>Install KubeVirt</para>
</listitem>
<listitem>
<para>Deploy a user-supplied manifest</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>Debugging the image build process</title>
<para>If the image build process fails, refer to the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/debugging.md">upstream
debugging guide</link>.</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>Testing your newly built image</title>
<para>For instructions on how to test the newly built CRB image, refer to the
<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/testing-guide.md">upstream
image testing guide</link>.</para>
</section>
</chapter>
</part>
<part xml:id="id-components-used">
<title>Components Used</title>
<partintro>
<para>List of components for Edge</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para>See Rancher upstream documentation at <link
xl:href="https://ranchermanager.docs.rancher.com">https://ranchermanager.docs.rancher.com</link>.</para>
<para>Rancher is a powerful open-source Kubernetes management platform that
streamlines the deployment, operations and monitoring of Kubernetes clusters
across multiple environments. Whether you manage clusters on premises, in
the cloud, or at the edge, Rancher provides a unified and centralized
platform for all your Kubernetes needs.</para>
<section xml:id="id-key-features-of-rancher">
<title>Key Features of Rancher</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Multi-cluster management:</emphasis> Rancher’s
intuitive interface lets you manage Kubernetes clusters from anywhere—public
clouds, private data centers and edge locations.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Security and compliance:</emphasis> Rancher enforces
security policies, role-based access control (RBAC), and compliance
standards across your Kubernetes landscape.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Simplified cluster operations:</emphasis> Rancher
automates cluster provisioning, upgrades and troubleshooting, simplifying
Kubernetes operations for teams of all sizes.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Centralized application catalog:</emphasis> The
Rancher application catalog offers a diverse range of Helm charts and
Kubernetes Operators, making it easy to deploy and manage containerized
applications.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Continuous delivery:</emphasis> Rancher supports
GitOps and CI/CD pipelines, enabling automated and streamlined application
delivery processes.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>Rancher’s use in SUSE Edge</title>
<para>Rancher provides several core functionalities to the SUSE Edge stack:</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Centralized Kubernetes management</title>
<para>In typical edge deployments with numerous distributed clusters, Rancher acts
as a central control plane for managing these Kubernetes clusters. It offers
a unified interface for provisioning, upgrading, monitoring, and
troubleshooting, simplifying operations, and ensuring consistency.</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>Simplified cluster deployment</title>
<para>Rancher streamlines Kubernetes cluster creation on the lightweight SLE Micro
(SUSE Linux Enterprise Micro) operating system, easing the rollout of edge
infrastructure with robust Kubernetes capabilities.</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>Application deployment and management</title>
<para>The integrated Rancher application catalog can simplify deploying and
managing containerized applications across SUSE Edge clusters, enabling
seamless edge workload deployment.</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>Security and policy enforcement</title>
<para>Rancher provides policy-based governance tools, role-based access control
(RBAC), and integration with external authentication providers. This helps
SUSE Edge deployments maintain security and compliance, critical in
distributed environments.</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>Best practices</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>Rancher includes Fleet as a built-in component to allow manage cluster
configurations and application deployments with code stored in git.</para>
</section>
<section xml:id="id-observability">
<title>Observability</title>
<para>Rancher includes built-in monitoring and logging tools like Prometheus and
Grafana for comprehensive insights into your cluster health and performance.</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize
base SLE Micro OS images.  Follow <xref linkend="rancher-install"/> for an
air-gapped installation of Rancher on top of Kubernetes clusters provisioned
by EIB.</para>
</section>
<section xml:id="id-additional-resources-2">
<title>Additional Resources</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancher Documentation</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher Community</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Helm Charts</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Kubernetes Operators</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Rancher Dashboard Extensions</title>
<para>Extensions allow users, developers, partners, and customers to extend and
enhance the Rancher UI. SUSE Edge 3.0 provides KubeVirt and Akri dashboard
extensions.</para>
<para>See <literal><link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions">Rancher
documentation</link></literal> for general information about Rancher
Dashboard Extensions.</para>
<section xml:id="id-prerequisites-3">
<title>Prerequisites</title>
<para>To enable extensions Rancher requires ui-plugin operator to be
installed. When using the Rancher Dashboard UI, navigate to <emphasis
role="strong">Extensions</emphasis> in the left navigation <emphasis
role="strong">Configuration</emphasis> section. If the ui-plugin operator is
not installed you’ll get a prompt asking to enable the extensions support as
described <literal><link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions#installing-extensions">here</link></literal>.</para>
<para>The operator can be also installed using Helm:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/
helm upgrade --create-namespace -n cattle-ui-plugin-system \
  --install ui-plugin-operator rancher-charts/ui-plugin-operator
helm upgrade --create-namespace -n cattle-ui-plugin-system \
  --install ui-plugin-operator-crd rancher-charts/ui-plugin-operator-crd</screen>
<para>Or with Fleet by creating a dedicated GitRepo resource. For more information
see <link xl:href="fleet.xml">Fleet</link> section and <literal><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/main/gitrepos/rancher-ui-plugin-operator-gitrepo.yaml">fleet-examples</link></literal>
repository.</para>
</section>
<section xml:id="id-installation">
<title>Installation</title>
<para>All SUSE Edge 3.0 components including dashboard extensions are distributed
as OCI artifacts. Rancher Dashboard Apps/Marketplace does not support OCI
based Helm repositories <literal><link
xl:href="https://github.com/rancher/dashboard/issues/9815">yet</link></literal>.
Therefore, to install SUSE Edge Extensions you can use Helm or Fleet:</para>
<section xml:id="id-installing-with-helm">
<title>Installing with Helm</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/kubevirt-dashboard-extension-chart --version 1.0.0 --namespace cattle-ui-plugin-system

# Akri extension
helm install akri-dashboard-extension oci://registry.suse.com/edge/akri-dashboard-extension-chart --version 1.0.0 --namespace cattle-ui-plugin-system</screen>
<note>
<para>The extensions need to be installed in
<literal>cattle-ui-plugin-system</literal> namespace.</para>
</note>
<note>
<para>After an extension is installed, Rancher Dashboard UI needs to be reloaded.</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Installing with Fleet</title>
<para>Installing Dashboard Extensions with Fleet requires defining a
<literal>gitRepo</literal> resource which points to a Git repository with
custom <literal>fleet.yaml</literal> bundle configuration file(s).</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/akri-dashboard-extension-chart
  version: "1.0.0"</screen>
<screen language="yaml" linenumbering="unnumbered"># Akri extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: akri-dashboard-extension
  chart: oci://registry.suse.com/edge/akri-dashboard-extension-chart
  version: "1.0.0"</screen>
<note>
<para>The <literal>releaseName</literal> property is required and needs to match
the extension name to get the extension correctly installed by
ui-plugin-operator.</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>For more information see <link xl:href="fleet.xml">Fleet</link> section and
<literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>
repository.</para>
<para>Once the Extensions are installed they are listed in <emphasis
role="strong">Extensions</emphasis> section under <emphasis
role="strong">Installed</emphasis> tabs. Since they are not installed via
Apps/Marketplace, they are marked with <literal>Third-Party</literal> label.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width=""/> </imageobject>
<textobject><phrase>installed dashboard extensions</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>KubeVirt Dashboard Extension</title>
<para>KubeVirt Extension provides basic virtual machine management for Rancher
dashboard UI. It’s capabilities are described in <link
xl:href="virtualization.xml#kubevirt-dashboard-extension">Edge
Virtualization</link>.</para>
</section>
<section xml:id="id-akri-dashboard-extension">
<title>Akri Dashboard Extension</title>
<para>Akri is a Kubernetes Resource Interface that lets you easily expose
heterogeneous leaf devices (such as IP cameras and USB devices) as resources
in a Kubernetes cluster, while also supporting the exposure of embedded
hardware resources such as GPUs and FPGAs. Akri continually detects nodes
that have access to these devices and schedules workloads based on them.</para>
<para>Akri Dashboard Extension allows you to use Rancher Dashboard user interface
to manage and monitor leaf devices and run workloads once these devices are
discovered.</para>
<para>Extension capabilities are further described in <link
xl:href="akri.xml#akri-dashboard-extension">Akri section</link>.</para>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link xl:href="https://fleet.rancher.io">Fleet</link> is a container
management and deployment engine designed to offer users more control on the
local cluster and constant monitoring through GitOps. Fleet focuses not only
on the ability to scale, but it also gives users a high degree of control
and visibility to monitor exactly what is installed on the cluster.</para>
<para>Fleet can manage deployments from Git of raw Kubernetes YAML, Helm charts,
Kustomize, or any combination of the three. Regardless of the source, all
resources are dynamically turned into Helm charts, and Helm is used as the
engine to deploy all resources in the cluster. As a result, users can enjoy
a high degree of control, consistency and auditability of their clusters.</para>
<para>For information about how Fleet works, see <link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/architecture">this
page</link>.</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Installing Fleet with Helm</title>
<para>Fleet comes built-in to Rancher, but it can be also <link
xl:href="https://fleet.rancher.io/installation">installed</link> as a
standalone application on any Kubernetes cluster using Helm.</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>Using Fleet with Rancher</title>
<para>Rancher uses Fleet to deploy applications across managed
clusters. Continuous delivery with Fleet introduces GitOps at scale,
designed to manage applications running on large numbers of clusters.</para>
<para>Fleet shines as an integrated part of Rancher. Clusters managed with Rancher
automatically get the Fleet agent deployed as part of the
installation/import process and the cluster is immediately available to be
managed by Fleet.</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Accessing Fleet in the Rancher UI</title>
<para>Fleet comes preinstalled in Rancher and is managed by the <emphasis
role="strong">Continuous Delivery</emphasis> option in the Rancher UI. For
additional information on Continuous Delivery and other Fleet
troubleshooting tips, refer <link
xl:href="https://fleet.rancher.io/troubleshooting">here</link>.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width=""/>
</imageobject>
<textobject><phrase>fleet dashboard</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Continuous Delivery section consists of following items:</para>
<section xml:id="id-dashboard">
<title>Dashboard</title>
<para>An overview page of all GitOps repositories across all workspaces. Only the
workspaces with repositories are displayed.</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos</title>
<para>A list of GitOps repositories in the selected workspace. Select the active
workspace using the drop-down list at the top of the page.</para>
</section>
<section xml:id="id-clusters">
<title>Clusters</title>
<para>A list of managed clusters. By default, all Rancher-managed clusters are
added to the <literal>fleet-default</literal>
workspace. <literal>fleet-local</literal> workspace includes the local
(management) cluster. From here, it is possible to <literal>Pause</literal>
or <literal>Force update</literal> the clusters or move the cluster into
another workspace. Editing the cluster allows to update labels and
annotations used for grouping the clusters.</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups</title>
<para>This section allows custom grouping of the clusters within the workspace
using selectors.</para>
</section>
<section xml:id="id-advanced">
<title>Advanced</title>
<para>The "Advanced" section allows to manage workspaces and other related Fleet
resources.</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Example of installing KubeVirt with Rancher and Fleet using Rancher
dashboard</title>
<orderedlist numeration="arabic">
<listitem>
<para>Create a Git repository containing the <literal>fleet.yaml</literal> file:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/kubevirt-chart"
  version: "0.2.4"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>In the Rancher dashboard, navigate to <emphasis role="strong">☰ &gt;
Continuous Delivery &gt; Git Repos</emphasis> and click <literal>Add
Repository</literal>.</para>
</listitem>
<listitem>
<para>The Repository creation wizard guides through creation of the Git
repo. Provide <emphasis role="strong">Name</emphasis>, <emphasis
role="strong">Repository URL</emphasis> (referencing the Git repository
created in the previous step) and select the appropriate branch or
revision. In the case of a more complex repository, specify <emphasis
role="strong">Paths</emphasis> to use multiple directories in a single
repository.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width=""/>
</imageobject>
<textobject><phrase>fleet create repo1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Click <literal>Next</literal>.</para>
</listitem>
<listitem>
<para>In the next step, you can define where the workloads will get
deployed. Cluster selection offers several basic options: you can select no
clusters, all clusters, or directly choose a specific managed cluster or
cluster group (if defined). The "Advanced" option allows to directly edit
the selectors via YAML.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width=""/>
</imageobject>
<textobject><phrase>fleet create repo2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>Click <literal>Create</literal>. The repository gets created. From now on,
the workloads are installed and kept in sync on the clusters matching the
repository definition.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>Debugging and troubleshooting</title>
<para>The "Advanced" navigation section provides overviews of lower-level Fleet
resources. <link xl:href="https://fleet.rancher.io/ref-bundle-stages">A
bundle</link> is an internal resource used for the orchestration of
resources from Git. When a Git repo is scanned, it produces one or more
bundles.</para>
<para>To find bundles relevant to a specific repository, go to the Git repo detail
page and click the <literal>Bundles</literal> tab.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width=""/>
</imageobject>
<textobject><phrase>fleet repo bundles</phrase></textobject>
</mediaobject>
</informalfigure>
<para>For each cluster, the bundle is applied to a BundleDeployment resource that
is created. To view BundleDeployment details, click the
<literal>Graph</literal> button in the upper right of the Git repo detail
page.  A graph of <emphasis role="strong">Repo &gt; Bundles &gt;
BundleDeployments</emphasis> is loaded. Click the BundleDeployment in the
graph to see its details and click the <literal>Id</literal> to view the
BundleDeployment YAML.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width=""/>
</imageobject>
<textobject><phrase>fleet repo graph</phrase></textobject>
</mediaobject>
</informalfigure>
<para>For additional information on Fleet troubleshooting tips, refer <link
xl:href="https://fleet.rancher.io/troubleshooting">here</link>.</para>
</section>
<section xml:id="id-fleet-examples">
<title>Fleet examples</title>
<para>The Edge team maintains a <link
xl:href="https://github.com/suse-edge/fleet-examples">repository</link> with
examples of installing Edge projects with Fleet.</para>
<para>The Fleet project includes a <link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>
repository that covers all use cases for <link
xl:href="https://fleet.rancher.io/gitrepo-content">Git repository
structure</link>.</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SLE Micro</title>
<para>See <link xl:href="https://documentation.suse.com/sle-micro/5.5/">SLE Micro
official documentation</link></para>
<blockquote>
<para>SUSE Linux Enterprise Micro is a lightweight and secure operating system for
the edge. It merges the enterprise-hardened components of SUSE Linux
Enterprise with the features that developers want in a modern, immutable
operating system. As a result, you get a reliable infrastructure platform
with best-in-class compliance that is also simple to use.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-sle-micro">
<title>How does SUSE Edge use SLE Micro?</title>
<para>We use SLE Micro as the base operating system for our platform stack. This
provides us with a secure, stable and minimal base for building upon.</para>
<para>SLE Micro is unique in its use of file system (Btrfs) snapshots to allow for
easy rollbacks in case something goes wrong with an upgrade. This allows for
secure remote upgrades for the entire platform even without physical access
in case of issues.</para>
</section>
<section xml:id="id-best-practices-2">
<title>Best practices</title>
<section xml:id="id-installation-media">
<title>Installation media</title>
<para>SUSE Edge uses the Edge Image Builder (<xref linkend="components-eib"/>) to
preconfigure the SLE Micro self-install installation image.</para>
</section>
<section xml:id="id-local-administration">
<title>Local administration</title>
<para>SLE Micro comes with Cockpit to allow the local management of the host
through a Web application.</para>
<para>This service is disabled by default but can be started by enabling the
systemd service <literal>cockpit.socket</literal>.</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>There is no desktop environment available in SLE Micro at the moment but a
containerized solution is in development.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>
is a CNCF project which provides bare-metal infrastructure management
capabilities for Kubernetes.</para>
<para>Metal<superscript>3</superscript> provides Kubernetes-native resources to
manage the lifecycle of bare-metal servers which support management via
out-of-band protocols such as <link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>.</para>
<para>It also has mature support for <link
xl:href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)</link> which
enables management of infrastructure resources across multiple
infrastructure providers via broadly adopted vendor-neutral APIs.</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>How does SUSE Edge use Metal3?</title>
<para>This method is useful for scenarios where the target hardware supports
out-of-band management, and a fully automated infrastructure management flow
is desired.</para>
<para>This method provides declarative APIs that enable inventory and state
management of bare-metal servers, including automated inspection, cleaning
and provisioning/deprovisioning.</para>
</section>
<section xml:id="id-known-issues-3">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>The upstream <link
xl:href="https://github.com/metal3-io/ip-address-manager">IP Address
Management controller</link> is currently not supported, because it is not
yet compatible with our choice of network configuration tooling.</para>
</listitem>
<listitem>
<para>Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are
not supported.</para>
</listitem>
<listitem>
<para>Only deployment via redfish-virtualmedia is currently supported.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para>See the <link
xl:href="https://github.com/suse-edge/edge-image-builder">Official
Repository</link>.</para>
<para>Edge Image Builder (EIB) is a tool that streamlines the generation of
customized, ready-to-boot (CRB) disk images for bootstrapping
machines. These images enable the end-to-end deployment of the entire SUSE
software stack with a single image.</para>
<para>Whilst EIB can create CRB images for all provisioning scenarios, EIB
demonstrates a tremendous value in air-gapped deployments with limited or
completely isolated networks.</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>How does SUSE Edge use Edge Image Builder?</title>
<para>SUSE Edge uses EIB for the simplified and quick configuration of customized
SLE Micro images for a variety of scenarios. These scenarios include the
bootstrapping of virtual and bare-metal machines with:</para>
<itemizedlist>
<listitem>
<para>Fully air-gapped deployments of K3s/RKE2 Kubernetes (single &amp;
multi-node)</para>
</listitem>
<listitem>
<para>Fully air-gapped Helm chart and Kubernetes manifest deployments</para>
</listitem>
<listitem>
<para>Registration to Rancher via Elemental API</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>Customized networking (for example, static IP, host name, VLAN’s, bonding,
etc.)</para>
</listitem>
<listitem>
<para>Customized operating system configurations (for example, users, groups,
passwords, SSH keys, proxies, NTP, custom SSL certificates, etc.)</para>
</listitem>
<listitem>
<para>Air-gapped installation of host-level and side-loaded RPM packages
(including dependency resolution)</para>
</listitem>
<listitem>
<para>Registration to SUSE Manager for OS management</para>
</listitem>
<listitem>
<para>Embedded container images</para>
</listitem>
<listitem>
<para>Kernel command-line arguments</para>
</listitem>
<listitem>
<para>Systemd units to be enabled/disabled at boot time</para>
</listitem>
<listitem>
<para>Custom scripts and files for any manual tasks</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>Getting started</title>
<para>Comprehensive documentation for the usage and testing of Edge Image Builder
can be found <link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.0/docs">here</link>.</para>
<para>Additionally, here is a quick start guide (<xref linkend="quickstart-eib"/>)
for Edge Image Builder covering a basic deployment scenario.</para>
</section>
<section xml:id="id-known-issues-4">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>EIB air-gaps Helm charts through templating the Helm charts and parsing all
the images within the template. If a Helm chart does not keep all of its
images within the template and instead side-loads the images, EIB will not
be able to air-gap those images automatically. The solution to this is to
manually add any undetected images to the
<literal>embeddedArtifactRegistry</literal> section of the definition file.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Edge Networking</title>
<para>This section describes the approach to network configuration in the SUSE
Edge solution.  We will show how to configure NetworkManager on SLE Micro in
a declarative manner, and explain how the related tools are integrated.</para>
<section xml:id="id-overview-of-networkmanager">
<title>Overview of NetworkManager</title>
<para>NetworkManager is a tool that manages the primary network connection and
other connection interfaces.</para>
<para>NetworkManager stores network configurations as connection files that
contain the desired state.  These connections are stored as files in the
<literal>/etc/NetworkManager/system-connections/</literal> directory.</para>
<para>Details about NetworkManager can be found in the <link
xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-nm-configuration.html">upstream
SLE Micro documentation</link>.</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>Overview of nmstate</title>
<para>nmstate is a widely adopted library (with an accompanying CLI tool) which
offers a declarative API for network configurations via a predefined schema.</para>
<para>Details about nmstate can be found in the <link
xl:href="https://nmstate.io/">upstream documentation</link>.</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>Enter: NetworkManager Configurator (nmc)</title>
<para>The network customization options available in SUSE Edge are achieved via a
CLI tool called NetworkManager Configurator or <emphasis>nmc</emphasis> for
short.  It is leveraging the functionality provided by the nmstate library
and, as such, it is fully capable of configuring static IP addresses, DNS
servers, VLANs, bonding, bridges, etc.  This tool allows us to generate
network configurations from predefined desired states and to apply those
across many different nodes in an automated fashion.</para>
<para>Details about the NetworkManager Configurator (nmc) can be found in the
<link xl:href="https://github.com/suse-edge/nm-configurator">upstream
repository</link>.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>How does SUSE Edge use NetworkManager Configurator?</title>
<para>SUSE Edge utilizes <emphasis>nmc</emphasis> for the network customizations
in the various different provisioning models:</para>
<itemizedlist>
<listitem>
<para>Custom network configurations in the Direct Network Provisioning scenarios
(<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>Declarative static configurations in the Image Based Provisioning scenarios
(<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Configuring with Edge Image Builder</title>
<para>Edge Image Builder (EIB) is a tool which enables configuring multiple hosts
with a single OS image.  In this section we’ll show how you can use a
declarative approach to describe the desired network states, how those are
converted to the respective NetworkManager connections, and are then applied
during the provisioning process.</para>
<section xml:id="id-prerequisites-4">
<title>Prerequisites</title>
<para>If you’re following this guide, it’s assumed that you’ve got the following
already available:</para>
<itemizedlist>
<listitem>
<para>An x86_64 physical host (or virtual machine) running SLES 15 SP5 or openSUSE
Leap 15.5</para>
</listitem>
<listitem>
<para>An available container runtime (e.g. Podman)</para>
</listitem>
<listitem>
<para>A copy of the SLE Micro 5.5 RAW image found <link
xl:href="https://www.suse.com/download/sle-micro/">here</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Getting the Edge Image Builder container image</title>
<para>The EIB container image is publicly available and can be downloaded from the
SUSE Edge registry by running:</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/edge-image-builder:1.0.2</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>Creating the image configuration directory</title>
<para>Let’s start with creating the configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>We will now ensure that the downloaded base image copy is moved over to the
configuration directory:</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SLE-Micro.x86_64-5.5.0-Default-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIB is never going to modify the base image input.</para>
</note>
</blockquote>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>Creating the image definition file</title>
<para>The definition file describes the majority of configurable options that the
Edge Image Builder supports.</para>
<para>Let’s start with a very basic definition file for our OS image:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.0
image:
  arch: x86_64
  imageType: raw
  baseImage: SLE-Micro.x86_64-5.5.0-Default-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para>The <literal>image</literal> section is required, and it specifies the input
image, its architecture and type, as well as what the output image will be
called.  The <literal>operatingSystem</literal> section is optional, and
contains configuration to enable login on the provisioned systems with the
<literal>root/eib</literal> username/password.</para>
<blockquote>
<note>
<para>Feel free to use your own encrypted password by running <literal>openssl
passwd -6 &lt;password&gt;</literal>.</para>
</note>
</blockquote>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>Defining the network configurations</title>
<para>The desired network configurations are not part of the image definition file
that we just created.  We’ll now populate those under the special
<literal>network/</literal> directory. Let’s create it:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>As previously mentioned, the NetworkManager Configurator
(<emphasis>nmc</emphasis>) tool expects an input in the form of predefined
schema.  You can find how to set up a wide variety of different networking
options in the <link xl:href="https://nmstate.io/examples.html">upstream
NMState examples documentation</link>.</para>
<para>This guide will explain how to configure the networking on three different
nodes:</para>
<itemizedlist>
<listitem>
<para>A node which uses two Ethernet interfaces</para>
</listitem>
<listitem>
<para>A node which uses network bonding</para>
</listitem>
<listitem>
<para>A node which uses a network bridge</para>
</listitem>
</itemizedlist>
<warning>
<para>Using completely different network setups is not recommended in production
builds, especially if configuring Kubernetes clusters.  Networking
configurations should generally be homogeneous amongst nodes or at least
amongst roles within a given cluster.  This guide is including various
different options only to serve as an example reference.</para>
</warning>
<blockquote>
<note>
<para>The following assumes a default <literal>libvirt</literal> network with an
IP address range <literal>192.168.122.1/24</literal>.  Adjust accordingly if
this differs in your environment.</para>
</note>
</blockquote>
<para>Let’s create the desired states for the first node which we will call
<literal>node1.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>In this example we define a desired state of two Ethernet interfaces (eth0
and eth3), their requested IP addresses, routing, and DNS resolution.</para>
<warning>
<para>You must ensure that the MAC addresses of all Ethernet interfaces are
listed.  Those are used during the provisioning process as the identifiers
of the nodes and serve to determine which configurations should be applied.
This is how we are able to configure multiple nodes using a single ISO or
RAW image.</para>
</warning>
<para>Next up is the second node which we will call
<literal>node2.suse.com</literal> and which will use network bonding:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>In this example we define a desired state of two Ethernet interfaces (eth0
and eth1) which are not enabling IP addressing, as well as a bond with a
round-robin policy and its respective address which is going to be used to
forward the network traffic.</para>
<para>Lastly, we’ll create the third and final desired state file which will be
utilizing a network bridge and which we’ll call
<literal>node3.suse.com</literal>:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
<blockquote>
<note>
<para>The names of the files under the <literal>network/</literal> directory are
intentional.  They correspond to the hostnames which will be set during the
provisioning process.</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>Building the OS image</title>
<para>Now that all the necessary configurations are in place, we can build the
image by simply running:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>The snippet above tells us that the <literal>Network</literal> component has
successfully been configured, and we can proceed with provisioning our edge
nodes.</para>
<blockquote>
<note>
<para>A log file (<literal>network-config.log</literal>) and the respective
NetworkManager connection files can be inspected in the resulting
<literal>_build</literal> directory under a timestamped directory for the
image run.</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>Provisioning the edge nodes</title>
<para>Let’s copy the resulting RAW image:</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>You will notice that we copied the built image four times but only specified
the network configurations for three nodes.  This is because we also want to
showcase what will happen if we provision a node which does not match any of
the desired configurations.</para>
<blockquote>
<note>
<para>This guide will use virtualization for the node provisioning
examples. Ensure the necessary extensions are enabled in the BIOS (see <link
xl:href="https://documentation.suse.com/sles/15-SP5/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">here</link>
for details).</para>
</note>
</blockquote>
<para>We will be using <literal>virt-install</literal> to create virtual machines
using the copied raw disks.  Each virtual machine will be using 10 GB of RAM
and 6 vCPUs.</para>
<section xml:id="id-provisioning-the-first-node">
<title>Provisioning the first node</title>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>It is important that we create the network interfaces with the same MAC
addresses as the ones in the desired state we described above.</para>
</note>
</blockquote>
<para>Once the operation is complete, we will see something similar to the
following:</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Enterprise Micro 5.5  (x86_64) - Kernel 5.14.21-150500.55.19-default (ttyS0).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>We’re now able to log in with the <literal>root:eib</literal> credentials
pair.  We’re also able to SSH into the host if we prefer that over the
<literal>virsh console</literal> we’re presented with here.</para>
<para>Once logged in, let’s confirm that all the settings are in place.</para>
<para>Verify that the hostname is properly set:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>Verify that the routing is properly configured:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>Verify that Internet connection is available:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>Verify that exactly two Ethernet interfaces are configured and only one of
those is active:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>You’ll notice that the second interface is <literal>eth1</literal> instead
of the predefined <literal>eth3</literal> in our desired networking state.
This is the case because the NetworkManager Configurator
(<emphasis>nmc</emphasis>) is able to detect that the OS has given a
different name for the NIC with MAC address
<literal>34:8a:b1:4b:16:e2</literal> and it adjusts its settings
accordingly.</para>
<para>Verify this has indeed happened by inspecting the Combustion phase of the
provisioning:</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>We will now provision the rest of the nodes, but we will only show the
differences in the final configuration.  Feel free to apply any or all of
the above checks for all nodes you are about to provision.</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>Provisioning the second node</title>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>Once the virtual machine is up and running, we can confirm that this node is
using bonded interfaces:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>Confirm that the routing is using the bond:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>Ensure that the static connection files are properly utilized:</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>Provisioning the third node</title>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>Once the virtual machine is up and running, we can confirm that this node is
using a network bridge:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>Confirm that the routing is using the bridge:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>Ensure that the static connection files are properly utilized:</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>Provisioning the fourth node</title>
<para>Lastly, we will provision a node which will not match any of the predefined
configurations by a MAC address.  In these cases, we will default to DHCP to
configure the network interfaces.</para>
<para>Let’s create the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>Once the virtual machine is up and running, we can confirm that this node is
using a random IP address for its network interface:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>Verify that nmc failed to apply static configurations for this node:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>Verify that the Ethernet interface was configured via DHCP:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="id-unified-node-configurations">
<title>Unified node configurations</title>
<para>There are occasions where relying on known MAC addresses is not an
option. In these cases we can opt for the so-called <emphasis>unified
configuration</emphasis> which allows us to specify settings in an
<literal>_all.yaml</literal> file which will then be applied across all
provisioned nodes.</para>
<para>We will build and provision an edge node using different configuration
structure. Follow all steps starting from <xref
linkend="image-config-dir-creation"/> up until <xref
linkend="default-network-definition"/>.</para>
<para>In this example we define a desired state of two Ethernet interfaces (eth0
and eth1) - one using DHCP, and one assigned a static IP address.</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file definition.yaml</screen>
<para>Once the image is successfully built, let’s create a virtual machine using
it:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>The provisioning process might take a few minutes. Once it’s finished, log
in to the system with the provided credentials.</para>
<para>Verify that the routing is properly configured:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>Verify that Internet connection is available:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Verify that the Ethernet interfaces are configured and active:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>Custom network configurations</title>
<para>We have already covered the default network configuration for Edge Image
Builder which relies on the NetworkManager Configurator.  However, there is
also the option to modify it via a custom script. Whilst this option is very
flexible and is also not MAC address dependant, its limitation stems from
the fact that using it is much less convenient when bootstrapping multiple
nodes with a single image.</para>
<blockquote>
<note>
<para>It is recommended to use the default network configuration via files
describing the desired network states under the <literal>/network</literal>
directory.  Only opt for custom scripting when that behaviour is not
applicable to your use case.</para>
</note>
</blockquote>
<para>We will build and provision an edge node using different configuration
structure. Follow all steps starting from <xref
linkend="image-config-dir-creation"/> up until <xref
linkend="default-network-definition"/>.</para>
<para>In this example, we will create a custom script which applies static
configuration for the <literal>eth0</literal> interface on all provisioned
nodes, as well as removing and disabling the automatically created wired
connections by NetworkManager. This is beneficial in situations where you
want to make sure that every node in your cluster has an identical
networking configuration, and as such you do not need to be concerned with
the MAC address of each node prior to image creation.</para>
<para>Let’s start by storing the connection file in the
<literal>/custom/files</literal> directory:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>Now that the static configuration is created, we will also create our custom
network script:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>The nmc binary will still be included by default, so it can also be used in
the <literal>configure-network.sh</literal> script if necessary.</para>
</note>
</blockquote>
<warning>
<para>The custom script must always be provided under
<literal>/network/configure-network.sh</literal> in the configuration
directory. If present, all other files will be ignored.  It is NOT possible
to configure a network by working with both static configurations in YAML
format and a custom script simultaneously.</para>
</warning>
<para>The configuration directory at this point should look like the following:</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file definition.yaml</screen>
<para>Once the image is successfully built, let’s create a virtual machine using
it:</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>The provisioning process might take a few minutes. Once it’s finished, log
in to the system with the provided credentials.</para>
<para>Verify that the routing is properly configured:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>Verify that Internet connection is available:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>Verify that an Ethernet interface is statically configured using our
connection file and is active:</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elemental is a software stack enabling centralized and full cloud-native OS
management with Kubernetes. The Elemental stack consists of a number of
components that either reside on Rancher itself, or on the edge nodes. The
core components are:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> - The core operator
that resides on Rancher and handles registration requests from clients.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis> - The client that runs
on the edge nodes allowing registration via the
<literal>elemental-operator</literal>.</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis> - An agent that
resides on the edge nodes; its configuration is fed from
<literal>elemental-register</literal> and it receives a
<literal>plan</literal> for configuring the
<literal>rancher-system-agent</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis> - Once the edge node
has fully registered, this takes over from
<literal>elemental-system-agent</literal> and waits for further
<literal>plans</literal> from Rancher Manager (e.g. for Kubernetes
installation).</para>
</listitem>
</itemizedlist>
<para>See <link xl:href="https://elemental.docs.rancher.com/">Elemental upstream
documentation</link> for full information about Elemental and its
relationship to Rancher.</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>How does SUSE Edge use Elemental?</title>
<para>We use portions of Elemental for managing remote devices where
Metal<superscript>3</superscript> is not an option (for example, there is no
BMC, or the device is behind a NAT gateway). This tooling allows for an
operator to bootstrap their devices in a lab before knowing when or where
they will be shipped to. Namely, we leverage the
<literal>elemental-register</literal> and
<literal>elemental-system-agent</literal> components to enable the
onboarding of SLE Micro hosts to Rancher for "phone home" network
provisioning use-cases. When using Edge Image Builder (EIB) to create
deployment images, the automatic registration through Rancher via Elemental
can be achieved by specifying the registration configuration in the
configuration directory for EIB.</para>
<note>
<para>In SUSE Edge 3.0 we do <emphasis role="strong">not</emphasis> leverage the
operating system management aspects of Elemental, and therefore it’s not
possible to manage your operating system patching via Rancher. Instead of
using the Elemental tools to build deployment images, SUSE Edge uses the
Edge Image Builder tooling, which consumes the registration configuration.</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>Best practices</title>
<section xml:id="id-installation-media-2">
<title>Installation media</title>
<para>The SUSE Edge recommended way of building deployments image that can
leverage Elemental for registration to Rancher in the "phone home network
provisioning" deployment footprint is to follow the instructions detailed in
the remote host onboarding with Elemental (<xref
linkend="quickstart-elemental"/>) quickstart.</para>
</section>
<section xml:id="id-labels">
<title>Labels</title>
<para>Elemental tracks its inventory with the <literal>MachineInventory</literal>
CRD and provides a way to select inventory, e.g. for selecting machines to
deploy Kubernetes clusters to, based on labels. This provides a way for
users to predefine most (if not all) of their infrastructure needs prior to
hardware even being purchased. Also, since nodes can add/remove labels on
their respective inventory object (by re-running
<literal>elemental-register</literal> with the additional flag
<literal>--label "FOO=BAR"</literal>), we can write scripts that will
discover and let Rancher know where a node is booted.</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>The Elemental UI does not currently know how to build installation media or
update non-"Elemental Teal" operating systems. This should be addressed in
future releases.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-akri">
<title>Akri</title>
<para>Akri is a CNCF-Sandbox project that aims to discover leaf devices to present
those as Kubernetes native resource.  It also allows scheduling a pod or a
job for each discovered device.  Devices can be node-local or networked, and
can use a wide variety of protocols.</para>
<para>Akri’s upstream documentation is available at: <link
xl:href="https://docs.akri.sh">https://docs.akri.sh</link></para>
<section xml:id="id-how-does-suse-edge-use-akri">
<title>How does SUSE Edge use Akri?</title>
<warning>
<para>Akri is currently tech-preview in the SUSE Edge stack.</para>
</warning>
<para>Akri is available as part of the Edge Stack whenever there is a need to
discover and schedule workload against leaf devices.</para>
<section xml:id="id-installing-akri">
<title>Installing Akri</title>
<para>Akri is available as a Helm chart within the Edge Helm repository.  The
recommended way of configuring Akri is by using the given Helm chart to
deploy the different components (agent, controller, discovery-handlers), and
then use your preferred deployment mechanism to deploy Akri’s Configuration
CRDs.</para>
</section>
<section xml:id="id-configuring-akri">
<title>Configuring Akri</title>
<para>Akri is configured using a <literal>akri.sh/Configuration</literal> object,
this object takes in all information about how to discover the devices, as
well as what to do when a matching one is discovered.</para>
<para>Here is an example configuration breakdown with all fields explained:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: akri.sh/v0
kind: Configuration
metadata:
  name: sample-configuration
spec:</screen>
<para>This part describes the configuration of the discovery handler, you have to
specify its name (the handlers available as part of Akri’s chart are
<literal>udev</literal>, <literal>opcua</literal>,
<literal>onvif</literal>).  The <literal>discoveryDetails</literal> is
handler specific, refer to the handler’s documentation on how to configure
it.</para>
<screen language="yaml" linenumbering="unnumbered">  discoveryHandler:
    name: debugEcho
    discoveryDetails: |+
      descriptions:
        - "foo"
        - "bar"</screen>
<para>This section defines the workload to be deployed for every discovered
device.  The example shows a minimal version of a <literal>Pod</literal>
configuration in <literal>brokerPodSpec</literal>, all usual fields of a
Pod’s spec can be used here.  It also shows the Akri specific syntax to
request the device in the <literal>resources</literal> section.</para>
<para>You can alternatively use a Job instead of a Pod, using the
<literal>brokerJobSpec</literal> key instead, and providing the spec part of
a Job to it.</para>
<screen language="yaml" linenumbering="unnumbered">  brokerSpec:
    brokerPodSpec:
      containers:
      - name: broker-container
        image: rancher/hello-world
        resources:
          requests:
            "{{PLACEHOLDER}}" : "1"
          limits:
            "{{PLACEHOLDER}}" : "1"</screen>
<para>These two sections show how to configure Akri to deploy a service per broker
(<literal>instanceService</literal>), or pointing to all brokers
(<literal>configurationService</literal>).  These are containing all
elements pertaining to a usual Service.</para>
<screen language="yaml" linenumbering="unnumbered">  instanceServiceSpec:
    type: ClusterIp
    ports:
    - name: http
      port: 80
      protocol: tcp
      targetPort: 80
  configurationServiceSpec:
    type: ClusterIp
    ports:
    - name: https
      port: 443
      protocol: tcp
      targetPort: 443</screen>
<para>The <literal>brokerProperties</literal> field is a key/value store that will
be exposed as additional environment variables to any pod requesting a
discovered device.</para>
<para>The capacity is the allowed number of concurrent users of a discovered
device.</para>
<screen language="yaml" linenumbering="unnumbered">  brokerProperties:
    key: value
  capacity: 1</screen>
</section>
<section xml:id="id-writing-and-deploying-additional-discovery-handlers">
<title>Writing and deploying additional Discovery Handlers</title>
<para>In case the protocol used by your device isn’t covered by an existing
discovery handler, you can write your own using <link
xl:href="https://docs.akri.sh/development/handler-development">this
guide</link></para>
</section>
<section xml:id="akri-dashboard-extension">
<title>Akri Rancher Dashboard Extension</title>
<para>Akri Dashboard Extension allows you to use Rancher Dashboard user interface
to manage and monitor leaf devices and run workloads once these devices are
discovered.</para>
<para>Once the extension is installed you can navigate to any Akri-enabled managed
cluster using cluster explorer. Under <emphasis
role="strong">Akri</emphasis> navigation group you can see Configurations
and Instances sections.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configurations.png"
width=""/> </imageobject>
<textobject><phrase>akri extension configurations</phrase></textobject>
</mediaobject>
</informalfigure>
<para>The configurations list provides information about Configuration Discovery
Handler and number of instances. Clicking the name opens a configuration
detail page.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-detail.png"
width=""/> </imageobject>
<textobject><phrase>akri extension configuration detail</phrase></textobject>
</mediaobject>
</informalfigure>
<para>You can also edit or create a new Configuration. Extension allows you to
select discovery handler, set up Broker Pod or Job, configure Configuration
and Instance services and set the Configuration capacity.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-edit.png"
width=""/> </imageobject>
<textobject><phrase>akri extension configuration edit</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Discovered devices are listed in the <emphasis
role="strong">Instances</emphasis> list.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instances-list.png"
width=""/> </imageobject>
<textobject><phrase>akri extension instances list</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Clicking the Instance name opens a detail page allowing to view the
workloads and instance service.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instance-detail.png"
width=""/> </imageobject>
<textobject><phrase>akri extension instance detail</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link xl:href="https://k3s.io/">K3s</link> is a highly available, certified
Kubernetes distribution designed for production workloads in unattended,
resource-constrained, remote locations or inside IoT appliances.</para>
<para>It is packaged as a single and small binary, so installations and updates
are fast and easy.</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>How does SUSE Edge use K3s</title>
<para>K3s can be used as the Kubernetes distribution backing the SUSE Edge stack.
It is meant to be installed on a SLE Micro operating system.</para>
<para>Using K3s as the SUSE Edge stack Kubernetes distribution is only recommended
when etcd as a backend does not fit your constraints. If etcd as a backend
is possible, it is better to use RKE2 (<xref linkend="components-rke2"/>).</para>
</section>
<section xml:id="id-best-practices-4">
<title>Best practices</title>
<section xml:id="id-installation-2">
<title>Installation</title>
<para>The recommended way of installing K3s as part of the SUSE Edge stack is by
using Edge Image Builder (EIB). See its documentation (<xref
linkend="components-eib"/>) for more details on how to configure it to
deploy K3s.</para>
<para>It automatically supports HA setup, as well as Elemental setup.</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>Fleet for GitOps workflow</title>
<para>The SUSE Edge stack uses Fleet as its preferred GitOps tool.  For more
information around its installation and use, refer to the Fleet section
(<xref linkend="components-fleet"/>) in this documentation.</para>
</section>
<section xml:id="id-storage-management">
<title>Storage management</title>
<para>K3s comes with local-path storage preconfigured, which is suitable for
single-node clusters.  For clusters spanning over multiple nodes, we
recommend using Longhorn (<xref linkend="components-longhorn"/>).</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>Load balancing and HA</title>
<para>If you installed K3s using EIB, this part is already covered by the EIB
documentation in the HA section.</para>
<para>Otherwise, you need to install and configure MetalLB as per our MetalLB
documentation (<xref linkend="guides-metallb-k3s"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para>See <link xl:href="https://docs.rke2.io/">RKE2 official
documentation</link>.</para>
<para>RKE2 is a fully conformant Kubernetes distribution that focuses on security
and compliance by:</para>
<itemizedlist>
<listitem>
<para>Providing defaults and configuration options that allow clusters to pass the
CIS Kubernetes Benchmark v1.6 or v1.23 with minimal operator intervention</para>
</listitem>
<listitem>
<para>Enabling FIPS 140-2 compliance</para>
</listitem>
<listitem>
<para>Regularly scanning components for CVEs using <link
xl:href="https://trivy.dev">trivy</link> in the RKE2 build pipeline</para>
</listitem>
</itemizedlist>
<para>RKE2 launches control plane components as static pods, managed by the
kubelet. The embedded container runtime is containerd.</para>
<para>Note: RKE2 is also known as RKE Government in order to convey another use
case and sector it currently targets.</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2 vs K3s</title>
<para>K3s is a fully compliant and lightweight Kubernetes distribution focused on
Edge, IoT, ARM or just for situations where a PhD in K8s clusterology is
infeasible.</para>
<para>RKE2 combines the best of both worlds from the 1.x version of RKE (hereafter
referred to as RKE1) and K3s.</para>
<para>From K3s, it inherits the usability, ease of operation and deployment model.</para>
<para>From RKE1, it inherits close alignment with upstream Kubernetes. In places,
K3s has diverged from upstream Kubernetes in order to optimize for edge
deployments, but RKE1 and RKE2 can stay closely aligned with upstream.</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>How does SUSE Edge use RKE2?</title>
<para>RKE2 is a fundamental piece of the SUSE Edge stack. It sits on top of SUSE
Linux Micro (<xref linkend="components-slmicro"/>), providing a standard
Kubernetes interface required to deploy Edge workloads with minimal
footprint.</para>
</section>
<section xml:id="id-best-practices-5">
<title>Best practices</title>
<section xml:id="id-installation-3">
<title>Installation</title>
<para>The recommended way of installing RKE2 as part of the SUSE Edge stack is by
using Edge Image Builder (EIB). See the EIB documentation (<xref
linkend="components-eib"/>) for more details on how to configure it to
deploy RKE2.</para>
<para>EIB is flexible enough to support any parameter required by RKE2, such as
specifying the RKE2 version, the <link
xl:href="https://docs.rke2.io/reference/server_config">servers</link> or the
<link
xl:href="https://docs.rke2.io/reference/linux_agent_config">agents</link>
configuration, covering all the Edge use cases.</para>
<para>For other use cases involving Metal<superscript>3</superscript>, RKE2 is
also being used and installed. In those particular cases, the <link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">Cluster
API Provider RKE2</link> automatically deploys RKE2 on clusters being
provisioned with Metal<superscript>3</superscript> using the Edge Stack.</para>
<para>In those cases, the RKE2 configuration must be applied on the different CRDs
involved. An example of how to provide a different CNI using the
<literal>RKE2ControlPlane</literal> CRD looks like:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>For more information about the Metal<superscript>3</superscript> use cases,
see <xref linkend="components-metal3"/>.</para>
</section>
<section xml:id="id-high-availability">
<title>High availability</title>
<para>For HA deployments, EIB automatically deploys and configures MetalLB (<xref
linkend="components-metallb"/>) and the <link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link> to expose the RKE2 API endpoint externally.</para>
</section>
<section xml:id="id-networking">
<title>Networking</title>
<para>The supported CNI for the Edge Stack is <link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link> with optionally
adding the meta-plugin <link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>,
but RKE2 supports <link
xl:href="https://docs.rke2.io/install/network_options">a few others</link>
as well.</para>
</section>
<section xml:id="id-storage">
<title>Storage</title>
<para>RKE2 does not provide any kind of persistent storage class or operators. For
clusters spanning over multiple nodes, it is recommended to use Longhorn
(<xref linkend="components-longhorn"/>).</para>
</section>
</section>
</chapter>
<chapter xml:id="components-longhorn">
<title>Longhorn</title>
<para>Longhorn is a lightweight, reliable and user-friendly distributed block
storage system designed for Kubernetes.  As an open source project, Longhorn
was initially developed by Rancher Labs and is currently incubated under the
CNCF.</para>
<section xml:id="id-prerequisites-5">
<title>Prerequisites</title>
<para>If you are following this guide, it assumes that you have the following
already available:</para>
<itemizedlist>
<listitem>
<para>At least one host with SLE Micro 5.5 installed; this can be physical or
virtual</para>
</listitem>
<listitem>
<para>A Kubernetes cluster installed; either K3s or RKE2</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-longhorn">
<title>Manual installation of Longhorn</title>
<section xml:id="id-installing-open-iscsi">
<title>Installing Open-iSCSI</title>
<para>A core requirement of deploying and using Longhorn is the installation of
the <literal>open-iscsi</literal> package and the <literal>iscsid</literal>
daemon running on all Kubernetes nodes.  This is necessary, since Longhorn
relies on <literal>iscsiadm</literal> on the host to provide persistent
volumes to Kubernetes.</para>
<para>Let’s install it:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>It is important to note that once the operation is completed, the package is
only installed into a new snapshot as SLE Micro is an immutable operating
system.  In order to load it and for the <literal>iscsid</literal> daemon to
start running, we must reboot into that new snapshot that we just created.
Issue the reboot command when you are ready:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>For additional help installing open-iscsi, refer to the <link
xl:href="https://longhorn.io/docs/1.6.1/deploy/install/#installing-open-iscsi">official
Longhorn documentation</link>.</para>
</tip>
</section>
<section xml:id="id-installing-longhorn">
<title>Installing Longhorn</title>
<para>There are several ways to install Longhorn on your Kubernetes clusters.
This guide will follow through the Helm installation, however feel free to
follow the <link
xl:href="https://longhorn.io/docs/1.6.1/deploy/install/">official
documentation</link> if another approach is desired.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Add the Longhorn Helm repository:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add longhorn https://charts.longhorn.io</screen>
</listitem>
<listitem>
<para>Fetch the latest charts from the repository:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para>Install Longhorn in the longhorn-system namespace:</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.6.1</screen>
</listitem>
<listitem>
<para>Confirm that the deployment succeeded:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-longhorn-volumes">
<title>Creating Longhorn volumes</title>
<para>Longhorn utilizes Kubernetes resources called
<literal>StorageClass</literal> in order to automatically provision
<literal>PersistentVolume</literal> objects for pods.  Think of
<literal>StorageClass</literal> as a way for administrators to describe the
<emphasis>classes</emphasis> or <emphasis>profiles</emphasis> of storage
they offer.</para>
<para>Let’s create a <literal>StorageClass</literal> with some default options:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para>Now that we have our <literal>StorageClass</literal> in place, we need a
<literal>PersistentVolumeClaim</literal> referencing it.  A
<literal>PersistentVolumeClaim</literal> (PVC) is a request for storage by a
user. PVCs consume <literal>PersistentVolume</literal> resources.  Claims
can request specific sizes and access modes (e.g., they can be mounted once
read/write or many times read-only).</para>
<para>Let’s create a <literal>PersistentVolumeClaim</literal>:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>That’s it! Once we have the <literal>PersistentVolumeClaim</literal>
created, we can proceed with attaching it to a <literal>Pod</literal>.  When
the <literal>Pod</literal> is deployed, Kubernetes creates the Longhorn
volume and binds it to the <literal>Pod</literal> if storage is available.</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>The concept of storage in Kubernetes is a complex, but important topic. We
briefly mentioned some of the most common Kubernetes resources, however, we
suggest to familiarize yourself with the <link
xl:href="https://longhorn.io/docs/1.6.1/terminology/">terminology
documentation</link> that Longhorn offers.</para>
</tip>
<para>In this example, the result should look something like this:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>Accessing the UI</title>
<para>If you installed Longhorn with kubectl or Helm, you need to set up an
Ingress controller to allow external traffic into the
cluster. Authentication is not enabled by default. If the Rancher catalog
app was used, Rancher automatically created an Ingress controller with
access control (the rancher-proxy).</para>
<orderedlist numeration="arabic">
<listitem>
<para>Get the Longhorn’s external service IP address:</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para>Once you have retrieved the <literal>longhorn-frontend</literal> IP address,
you can start using the UI by navigating to it in your browser.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize
base SLE Micro OS images.  We are going to demonstrate how to do so for
provisioning an RKE2 cluster with Longhorn on top of it.</para>
<para>Let’s create the definition file:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.0
image:
  imageType: iso
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 1.6.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.longhorn.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>Customizing any of the Helm chart values is possible via a separate file
provided under <literal>helm.charts[].valuesFile</literal>.  Refer to the
<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#kubernetes">upstream
documentation</link> for details.</para>
</note>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>After the image is built, you can use it to install your OS on a physical or
virtual host.  Once the provisioning is complete, you are able to log in to
the system using the <literal>root:eib</literal> credentials pair.</para>
<para>Ensure that Longhorn has been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>This installation will not work for completely air-gapped environments.  In
those cases, please refer to <xref linkend="longhorn-install"/>.</para>
</note>
</section>
</chapter>
<chapter xml:id="components-neuvector">
<title>NeuVector</title>
<para>NeuVector is a security solution for Kubernetes that provides L7 network
security, runtime security, supply chain security, and compliance checks in
a cohesive package.</para>
<para>NeuVector is deployed as a platform of several containers that communicate
with each other on various ports and interfaces. The following are the
different containers deployed:</para>
<itemizedlist>
<listitem>
<para>Manager. A stateless container which presents the Web-based
console. Typically, only one is needed and this can run anywhere. Failure of
the Manager does not affect any of the operations of the controller or
enforcer. However, certain notifications (events) and recent connection data
are cached in memory by the Manager so viewing of these would be affected.</para>
</listitem>
<listitem>
<para>Controller. The ‘control plane’ for NeuVector must be deployed in an HA
configuration, so configuration is not lost in a node failure. These can run
anywhere, although customers often choose to place these on ‘management’,
master or infra nodes because of their criticality.</para>
</listitem>
<listitem>
<para>Enforcer. This container is deployed as a DaemonSet so one Enforcer is on
every node to be protected. Typically deploys to every worker node but
scheduling can be enabled for master and infra nodes to deploy there as
well. Note: If the Enforcer is not on a cluster node and connections come
from a pod on that node, NeuVector labels them as ‘unmanaged’ workloads.</para>
</listitem>
<listitem>
<para>Scanner. Performs the vulnerability scanning using the built-in CVE
database, as directed by the Controller. Multiple scanners can be deployed
to increase scanning capacity. Scanners can run anywhere but are often run
on the nodes where the controllers run. See below for sizing considerations
of scanner nodes. A scanner can also be invoked independently when used for
build-phase scanning, for example, within a pipeline that triggers a scan,
retrieves the results, and stops the scanner. The scanner contains the
latest CVE database so should be updated daily.</para>
</listitem>
<listitem>
<para>Updater. The updater triggers an update of the scanner through a Kubernetes
cron job when an update of the CVE database is desired. Please be sure to
configure this for your environment.</para>
</listitem>
</itemizedlist>
<para>A more in-depth NeuVector onboarding and best practices documentation can be
found <link
xl:href="https://open-docs.neuvector.com/deploying/production/NV_Onboarding_5.0.pdf">here</link>.</para>
<section xml:id="id-how-does-suse-edge-use-neuvector">
<title>How does SUSE Edge use NeuVector?</title>
<para>SUSE Edge provides a leaner configuration of NeuVector as a starting point
for edge deployments.</para>
<para>Find the NeuVector configuration changes <link
xl:href="https://github.com/suse-edge/charts/blob/main/packages/neuvector-core/generated-changes/patch/values.yaml.patch">here</link>.</para>
</section>
<section xml:id="id-important-notes">
<title>Important notes</title>
<itemizedlist>
<listitem>
<para>The <literal>Scanner</literal> container must have enough memory to pull the
image to be scanned into memory and expand it. To scan images exceeding 1
GB, increase the scanner’s memory to slightly above the largest expected
image size.</para>
</listitem>
<listitem>
<para>High network connections expected in Protect mode. The
<literal>Enforcer</literal> requires CPU and memory when in Protect (inline
firewall blocking) mode to hold and inspect connections and possible payload
(DLP). Increasing memory and dedicating a CPU core to the
<literal>Enforcer</literal> can ensure adequate packet filtering capacity.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize
base SLE Micro OS images.  Follow <xref linkend="neuvector-install"/> for an
air-gapped installation of NeuVector on top of Kubernetes clusters
provisioned by EIB.</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para>See <link xl:href="https://metallb.universe.tf/">MetalLB official
documentation</link>.</para>
<blockquote>
<para>MetalLB is a load-balancer implementation for bare-metal Kubernetes
clusters, using standard routing protocols.</para>
<para>In bare-metal environments, setting up network load balancers is notably
more complex than in cloud environments. Unlike the straightforward API
calls in cloud setups, bare metal requires either dedicated network
appliances or a combination of load balancers and Virtual IP (VIP)
configurations to manage High Availability (HA) or address the potential
Single Point of Failure (SPOF) inherent in a single node load
balancer. These configurations are not easily automated, posing challenges
in Kubernetes deployments where components dynamically scale up and down.</para>
<para>MetalLB addresses these challenges by harnessing the Kubernetes model to
create LoadBalancer type services as if they were operating in a cloud
environment, even on bare-metal setups.</para>
<para>There are two different approaches, via <link
xl:href="https://metallb.universe.tf/concepts/layer2/">L2 mode</link> (using
ARP <emphasis>tricks</emphasis>) or via <link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>. Mainly L2
does not need any special network gear but BGP is in general better. It
depends on the use cases.</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>How does SUSE Edge use MetalLB?</title>
<para>SUSE Edge uses MetalLB in two key ways:</para>
<itemizedlist>
<listitem>
<para>As a Load Balancer Solution: MetalLB serves as the Load Balancer solution
for bare-metal machines.</para>
</listitem>
<listitem>
<para>For an HA K3s/RKE2 Setup: MetalLB allows for load balancing the Kubernetes
API using a Virtual IP address.</para>
</listitem>
</itemizedlist>
<note>
<para>In order to be able to expose the API, the
<literal>endpoint-copier-operator</literal> is used to keep in sync the K8s
API endpoints from the 'kubernetes' service to a 'kubernetes-vip'
LoadBalancer service.</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>Best practices</title>
<para>Installation of MetalLB in L2 mode is detailed in the MetalLB guide (<xref
linkend="guides-metallb-k3s"/>).</para>
<para>A guide on installing MetalLB in front of the kube-api-server to achieve HA
setups can be found in the MetalLB in front of the Kubernetes API server
(<xref linkend="guides-metallb-kubernetes"/>) tutorial.</para>
</section>
<section xml:id="id-known-issues-6">
<title>Known issues</title>
<itemizedlist>
<listitem>
<para>K3S LoadBalancer Solution: K3S comes with its Load Balancer solution,
<literal>Klipper</literal>. To use MetalLB, Klipper must be disabled. This
can be done by starting the K3s server with the <literal>--disable
servicelb</literal> option, as described in the <link
xl:href="https://docs.k3s.io/networking">K3s documentation</link>.</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>This section describes how you can use Edge Virtualization to run virtual
machines on your edge nodes. It is important to point out that Edge
Virtualization is not a comprehensive solution and has limited features; it
attempts to solve requirements for lightweight virtualization where basic
virtual machine capabilities are required. SUSE provides a more
comprehensive virtualization (and hyperconverged infrastructure) solution
with <link xl:href="https://harvesterhci.io/">Harvester</link>.</para>
<para>SUSE Edge Virtualization supports two methods of running virtual machines:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Deploying the virtual machines manually via libvirt+qemu-kvm at the host
level</para>
</listitem>
<listitem>
<para>Deploying the KubeVirt operator for Kubernetes-based management of virtual
machines</para>
</listitem>
</orderedlist>
<para>Both options are valid, but only the second one is covered below. If you
want to use the standard out-of-the box virtualization mechanisms provided
by SLE Micro, a comprehensive guide can be found <link
xl:href="https://documentation.suse.com/sles/15-SP5/html/SLES-all/chap-virtualization-introduction.html">here</link>,
and whilst it was primarily written for SUSE Linux Enterprise Server, the
concepts are almost identical.</para>
<para>This guide initially explains how to deploy the additional virtualization
components onto a system that has already been pre-deployed, but follows
with a section that describes how to embed this configuration in the initial
deployment via Edge Image Builder. If you do not want to run through the
basics and set things up manually, skip right ahead to that section.</para>
<section xml:id="id-kubevirt-overview">
<title>KubeVirt overview</title>
<para>KubeVirt allows for managing Virtual Machines with Kubernetes alongside the
rest of your containerized workloads. It does this by running the user space
portion of the Linux virtualization stack in a container. This minimizes the
requirements on the host system, allowing for easier setup and management.</para>
<informalexample>
<para>Details about KubeVirt’s architecture can be found in <link
xl:href="https://kubevirt.io/user-guide/architecture/">the upstream
documentation.</link></para>
</informalexample>
</section>
<section xml:id="id-prerequisites-6">
<title>Prerequisites</title>
<para>If you are following this guide, we assume you have the following already
available:</para>
<itemizedlist>
<listitem>
<para>At least one physical host with SLE Micro 5.5+ installed, and with
virtualization extensions enabled in the BIOS (see <link
xl:href="https://documentation.suse.com/sles/15-SP5/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">here</link>
for details).</para>
</listitem>
<listitem>
<para>Across your nodes, a K3s/RKE2 Kubernetes cluster already deployed and with
an appropriate <literal>kubeconfig</literal> that enables superuser access
to the cluster.</para>
</listitem>
<listitem>
<para>Access to the root user — these instructions assume you are the root user,
and <emphasis>not</emphasis> escalating your privileges via
<literal>sudo</literal>.</para>
</listitem>
<listitem>
<para>You have <link xl:href="https://helm.sh/docs/intro/install/">Helm</link>
available locally with an adequate network connection to be able to push
configurations to your Kubernetes cluster and download the required images.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Manual installation of Edge Virtualization</title>
<para>This guide will not walk you through the deployment of Kubernetes, but it
assumes that you have either installed the SUSE Edge-appropriate version of
<link xl:href="https://k3s.io/">K3s</link> or <link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> and that you
have your kubeconfig configured accordingly so that standard
<literal>kubectl</literal> commands can be executed as the superuser. We
assume your node forms a single-node cluster, although there are no
significant differences expected for multi-node deployments.</para>
<para>SUSE Edge Virtualization is deployed via three separate Helm charts,
specifically:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>: The core virtualization
components, that is, Kubernetes CRDs, operators and other components
required for enabling Kubernetes to deploy and manage virtual machines.</para>
</listitem>
<listitem>
<para><emphasis role="strong">KubeVirt Dashboard Extension</emphasis>: An optional
Rancher UI extension that allows basic virtual machine management, for
example, starting/stopping of virtual machines as well as accessing the
console.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>: An
additional component that enables persistent-storage integration for
KubeVirt, providing capabilities for virtual machines to use existing
Kubernetes storage back-ends for data, but also allowing users to import or
clone data volumes for virtual machines.</para>
</listitem>
</itemizedlist>
<para>Each of these Helm charts is versioned according to the SUSE Edge release
you are currently using. For production/supported usage, employ the
artifacts that can be found in the SUSE Registry.</para>
<para>First, ensure that your <literal>kubectl</literal> access is working:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.28.9+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.28.9+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.28.9+rke2r1</screen>
<para>Now you can proceed to install the <emphasis
role="strong">KubeVirt</emphasis> and <emphasis role="strong">Containerized
Data Importer (CDI)</emphasis> Helm charts:</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/kubevirt-chart --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/cdi-chart --namespace cdi-system --create-namespace</screen>
<para>In a few minutes, you should have all KubeVirt and CDI components
deployed. You can validate this by checking all the deployed resources in
the <literal>kubevirt-system</literal> and <literal>cdi-system</literal>
namespace.</para>
<para>Verify KubeVirt resources:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>Verify CDI resources:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para>To verify that the <literal>VirtualMachine</literal> custom resource
definitions (CRDs) are deployed, you can validate with:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para>This should print out the definition of the
<literal>VirtualMachine</literal> object, which should print as follows:</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>Deploying virtual machines</title>
<para>Now that KubeVirt and CDI are deployed, let us define a simple virtual
machine based on <link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE
Tumbleweed</link>. This virtual machine has the most simple of
configurations, using standard "pod networking" for a networking
configuration identical to any other pod. It also employs non-persistent
storage, ensuring the storage is ephemeral, just like in any container that
does not have a <link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>This should print that a <literal>VirtualMachine</literal> was created:</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>This <literal>VirtualMachine</literal> definition is minimal, specifying
little about the configuration. It simply outlines that it is a machine type
"<link xl:href="https://wiki.qemu.org/Features/Q35">q35</link>" with 2 GB of
memory that uses a disk image based on an ephemeral <literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>
(that is, a disk image that is stored in a container image from a remote
image repository), and specifies a base64 encoded cloudInit disk, which we
only use for user creation and password enforcement at boot time (use
<literal>base64 -d</literal> to decode it).</para>
<blockquote>
<note>
<para>This virtual machine image is only for testing. The image is not officially
supported and is only meant as a documentation example.</para>
</note>
</blockquote>
<para>This machine takes a few minutes to boot as it needs to download the
openSUSE Tumbleweed disk image, but once it has done so, you can view
further details about the virtual machine by checking the virtual machine
information:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>This should print the node that the virtual machine was started on, and the
IP address of the virtual machine. Remember, since it uses pod networking,
the reported IP address will be just like any other pod, and routable as
such:</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>When running these commands on the Kubernetes cluster nodes themselves, with
a CNI that routes traffic directly to pods (for example, Cilium), you should
be able to <literal>ssh</literal> directly to the machine itself. Substitute
the following IP address with the one that was assigned to your virtual
machine:</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>Once you are in this virtual machine, you can play around, but remember that
it is limited in terms of resources, and only has 1 GB disk space. When you
are finished, <literal>Ctrl-D</literal> or <literal>exit</literal> to
disconnect from the SSH session.</para>
<para>The virtual machine process is still wrapped in a standard Kubernetes
pod. The <literal>VirtualMachine</literal> CRD is a representation of the
desired virtual machine, but the process in which the virtual machine is
actually started is via the <literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>
pod, a standard Kubernetes pod, just like any other application. For every
virtual machine started, you can see there is a
<literal>virt-launcher</literal> pod:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>This should then show the one <literal>virt-launcher</literal> pod for the
Tumbleweed machine that we have defined:</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>If we take a look into this <literal>virt-launcher</literal> pod, you see it
is executing <literal>libvirt</literal> and <literal>qemu-kvm</literal>
processes. We can enter the pod itself and have a look under the covers,
noting that you need to adapt the following command for your pod name:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Once you are in the pod, try running <literal>virsh</literal> commands along
with looking at the processes. You will see the
<literal>qemu-system-x86_64</literal> binary running, along with certain
processes for monitoring the virtual machine. You will also see the location
of the disk image and how the networking is plugged (as a tap device):</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>Finally, let us delete this virtual machine to clean up:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>Using virtctl</title>
<para>Along with the standard Kubernetes CLI tooling, that is,
<literal>kubectl</literal>, KubeVirt comes with an accompanying CLI utility
that allows you to interface with your cluster in a way that bridges some
gaps between the virtualization world and the world that Kubernetes was
designed for. For example, the <literal>virtctl</literal> tool provides the
capability of managing the lifecycle of virtual machines (starting,
stopping, restarting, etc.), providing access to the virtual consoles,
uploading virtual machine images, as well as interfacing with Kubernetes
constructs such as services, without using the API or CRDs directly.</para>
<para>Let us download the latest stable version of the <literal>virtctl</literal>
tool:</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.1.0
$ wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64</screen>
<para>If you are using a different architecture or a non-Linux machine, you can
find other releases <link
xl:href="https://github.com/kubevirt/kubevirt/releases">here</link>. You
need to make this executable before proceeding, and it may be useful to move
it to a location within your <literal>$PATH</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-${VERSION}-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>You can then use the <literal>virtctl</literal> command-line tool to create
virtual machines. Let us replicate our previous virtual machine, noting that
we are piping the output directly into <literal>kubectl apply</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>This should then show the virtual machine running (it should start a lot
quicker this time given that the container image will be cached):</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>Now we can use <literal>virtctl</literal> to connect directly to the virtual
machine:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para>There are plenty of other commands that can be used by
<literal>virtctl</literal>. For example, <literal>virtctl console</literal>
can give you access to the serial console if networking is not working, and
you can use <literal>virtctl guestosinfo</literal> to get comprehensive OS
information, subject to the guest having the
<literal>qemu-guest-agent</literal> installed and running.</para>
<para>Finally, let us pause and resume the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para>You find that the <literal>VirtualMachine</literal> object shows as
<emphasis role="strong">Paused</emphasis> and the
<literal>VirtualMachineInstance</literal> object shows as <emphasis
role="strong">Running</emphasis> but <emphasis
role="strong">READY=False</emphasis>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>You also find that you can no longer connect to the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>Let us resume the virtual machine and try again:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>Now we should be able to re-establish a connection:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>Finally, let us remove the virtual machine:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>Simple ingress networking</title>
<para>In this section, we show how you can expose virtual machines as standard
Kubernetes services and make them available via the Kubernetes ingress
service, for example, <link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINX
with RKE2</link> or <link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">Traefik
with K3s</link>. This document assumes that these components are already
configured appropriately and that you have an appropriate DNS pointer, for
example, via a wild card, to point at your Kubernetes server nodes or your
ingress virtual IP for proper ingress resolution.</para>
<blockquote>
<note>
<para>In SUSE Edge 3.0+, if you are using K3s in a multi-server node
configuration, you might have needed to configure a MetalLB-based VIP for
Ingress; this is not required for RKE2.</para>
</note>
</blockquote>
<para>In the example environment, another openSUSE Tumbleweed virtual machine is
deployed, cloud-init is used to install NGINX as a simple Web server at boot
time, and a simple message is configured to be returned to verify that it
works as expected when a call is made. To see how this is done, simply
<literal>base64 -d</literal> the cloud-init section in the output below.</para>
<para>Let us create this virtual machine now:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>When this virtual machine has successfully started, we can use the
<literal>virtctl</literal> command to expose the
<literal>VirtualMachineInstance</literal> with an external port of
<literal>8080</literal> and a target port of <literal>80</literal> (where
NGINX listens by default). We use the <literal>virtctl</literal> command
here as it understands the mapping between the virtual machine object and
the pod. This creates a new service for us:</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>We will then have an appropriate service automatically created:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>Next, if you then use <literal>kubectl create ingress</literal>, we can
create an ingress object that points to this service. Adapt the URL (known
as the "host" in the <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link>
object) here to match your DNS configuration and ensure that you point it to
port <literal>8080</literal>:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>With DNS being configured correctly, you should be able to curl the URL
immediately:</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>Let us clean up by removing this virtual machine and its service and ingress
resources:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Using the Rancher UI extension</title>
<para>SUSE Edge Virtualization provides a UI extension for Rancher Manager,
enabling basic virtual machine management using the Rancher dashboard UI.</para>
<section xml:id="id-installation-4">
<title>Installation</title>
<para>See <link xl:href="rancher-dashboard-extensions.xml">Rancher Dashboard
Extensions section</link> for installation guidance.</para>
</section>
<section xml:id="kubevirt-dashboard-extension">
<title>Using KubeVirt Rancher Dashboard Extension</title>
<para>The extension introduces a new <emphasis role="strong">KubeVirt</emphasis>
section to the Cluster Explorer. This section is added to any managed
cluster which has KubeVirt installed.</para>
<para>The extension allows you to directly interact with two KubeVirt resources:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Virtual Machine instances</literal> — A resource representing a
single running virtual machine instance.</para>
</listitem>
<listitem>
<para><literal>Virtual Machines</literal> — A resource used to manage virtual
machines lifecycle.</para>
</listitem>
</orderedlist>
<section xml:id="id-creating-a-virtual-machine">
<title>Creating a virtual machine</title>
<orderedlist numeration="arabic">
<listitem>
<para>Navigate to <emphasis role="strong">Cluster Explorer</emphasis> clicking
KubeVirt-enabled managed cluster in the left navigation.</para>
</listitem>
<listitem>
<para>Navigate to <emphasis role="strong">KubeVirt &gt; Virtual
Machines</emphasis> page and click <literal>Create from YAML</literal> in
the upper right of the screen.</para>
</listitem>
<listitem>
<para>Fill in or paste a virtual machine definition and press
<literal>Create</literal>. Use virtual machine definition from Deploying
Virtual Machines section as an inspiration.</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width=""/>
</imageobject>
<textobject><phrase>virtual machines page</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-starting-and-stopping-virtual-machines">
<title>Starting and stopping virtual machines</title>
<para>Start and stop virtual machines using the action menu accessed from the
<emphasis role="strong">⋮</emphasis> drop-down list to the right of each
virtual machine or use group actions at the top of the list by selecting
virtual machines to perform the action on.</para>
<para>It is possible to run start and stop actions only on the virtual machines
which have <literal>spec.running</literal> property defined. In case when
<literal>spec.runStrategy</literal> is used, it is not possible to directly
start and stop such a machine. For more information, see <link
xl:href="https://kubevirt.io/user-guide/virtual_machines/run_strategies/#run-strategies">KubeVirt
documentation</link>.</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>Accessing virtual machine console</title>
<para>The "Virtual machines" list provides a <literal>Console</literal> drop-down
list that allows to connect to the machine using <emphasis role="strong">VNC
or Serial Console</emphasis>. This action is only available to running
machines.</para>
<para>In some cases, it takes a short while before the console is accessible on a
freshly started virtual machine.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width=""/>
</imageobject>
<textobject><phrase>vnc console ui</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Installing with Edge Image Builder</title>
<para>SUSE Edge is using <xref linkend="components-eib"/> in order to customize
base SLE Micro OS images.  Follow <xref linkend="kubevirt-install"/> for an
air-gapped installation of both KubeVirt and CDI on top of Kubernetes
clusters provisioned by EIB.</para>
</section>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>How-To Guides</title>
<partintro>
<para>How-to guides and best practices</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>MetalLB on K3s (using L2)</title>
<para>MetalLB is a load-balancer implementation for bare-metal Kubernetes
clusters, using standard routing protocols.</para>
<para>In this guide, we demonstrate how to deploy MetalLB in layer 2 mode.</para>
<section xml:id="id-why-use-this-method-2">
<title>Why use this method</title>
<para>In layer 2 mode, one node assumes the responsibility of advertising a
service to the local network. From the network’s perspective, it simply
looks like that machine has multiple IP addresses assigned to its network
interface.</para>
<para>The major advantage of the layer 2 mode is its universality: it works on any
Ethernet network, with no special hardware required, not even fancy routers.</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>MetalLB on K3s (using L2)</title>
<para>In this quick start, L2 mode will be used, so it means we do not need any
special network gear but just a couple of free IPs in our network range,
ideally outside of the DHCP pool so they are not assigned.</para>
<para>In this example, our DHCP pool is
<literal>192.168.122.100-192.168.122.200</literal> (yes, three IPs, see
Traefik and MetalLB (<xref linkend="traefik-and-metallb"/>) for the reason
of the extra IP) for a <literal>192.168.122.0/24</literal> network, so
anything outside this range is OK (besides the gateway and other hosts that
can be already running!)</para>
</section>
<section xml:id="id-prerequisites-7">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>A K3s cluster where MetalLB is going to be deployed.</para>
</listitem>
</itemizedlist>
<warning>
<para>K3S comes with its own service load balancer named Klipper. You <link
xl:href="https://metallb.universe.tf/configuration/k3s/">need to disable it
to run MetalLB</link>. To disable Klipper, K3s needs to be installed using
the <literal>--disable=servicelb</literal> flag.</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>A couple of free IPs in our network range. In this case,
<literal>192.168.122.10-192.168.122.12</literal></para>
</listitem>
</itemizedlist>
<section xml:id="id-deployment">
<title>Deployment</title>
<para>MetalLB leverages Helm (and other methods as well), so:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add metallb https://metallb.github.io/metallb
helm install --create-namespace -n metallb-system metallb metallb/metallb

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>Configuration</title>
<para>At this point, the installation is completed. Now it is time to <link
xl:href="https://metallb.universe.tf/configuration/">configure</link> using
our example values:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>Now, it is ready to be used. You can customize many things for L2 mode, such
as:</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">IPv6
And Dual Stack Services</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">Control
automatic address allocation</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">Reduce
the scope of address allocation to specific namespaces and services</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">Limiting
the set of nodes where the service can be announced from</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">Specify
network interfaces that LB IP can be announced from</link></para>
</listitem>
</itemizedlist>
<para>And a lot more for <link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>.</para>
</section>
<section xml:id="traefik-and-metallb">
<title>Traefik and MetalLB</title>
<para>Traefik is deployed by default with K3s (<link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">it can
be disabled</link> with <literal>--disable=traefik</literal>) and it is by
default exposed as <literal>LoadBalancer</literal> (to be used with
Klipper). However, as Klipper needs to be disabled, Traefik service for
ingress is still a <literal>LoadBalancer</literal> type. So at the moment of
deploying MetalLB, the first IP will be assigned automatically to Traefik
Ingress.</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>This will be applied later (<xref linkend="ingress-with-metallb"/>) in the
process.</para>
</section>
<section xml:id="id-usage">
<title>Usage</title>
<para>Let us create an example deployment:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>And finally, the service:</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>Let us see it in action:</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
</section>
</section>
<section xml:id="ingress-with-metallb">
<title>Ingress with MetalLB</title>
<para>As Traefik is already serving as an ingress controller, we can expose any
HTTP/HTTPS traffic via an <literal>Ingress</literal> object such as:</para>
<screen language="yaml" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>And then:</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>Also, to verify that MetalLB works correctly, <literal>arping</literal> can
be used as:</para>
<para><literal>arping hellok3s.${IP}.sslip.io</literal></para>
<para>Expected result:</para>
<screen language="console" linenumbering="unnumbered">ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>In the example above, the traffic flows as follows:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal> is resolved to the actual IP.</para>
</listitem>
<listitem>
<para>Then the traffic is handled by the <literal>metallb-speaker</literal> pod.</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal> redirects the traffic to the
<literal>traefik</literal> controller.</para>
</listitem>
<listitem>
<para>Finally, Traefik forwards the request to the
<literal>hello-kubernetes</literal> service.</para>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>MetalLB in front of the Kubernetes API server</title>
<para>This guide demonstrates using a MetalLB service to expose the K3s API
externally on an HA K3s cluster with three control-plane nodes.  To achieve
this, a Kubernetes Service of type <literal>LoadBalancer</literal> and
Endpoints will be manually created. The Endpoints keep the IPs of all
control plane nodes available in the cluster.  For the Endpoint to be
continuously synchronized with the events occurring in the cluster
(adding/removing a node or a node goes offline), the <link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link> will be deployed. The operator monitors the events
happening in the default <literal>kubernetes</literal> Endpoint and updates
the managed one automatically to keep them in sync.  Since the managed
Service is of type <literal>LoadBalancer</literal>,
<literal>MetalLB</literal> assigns it a static
<literal>ExternalIP</literal>. This <literal>ExternalIP</literal> will be
used to communicate with the API Server.</para>
<section xml:id="id-prerequisites-8">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<para>Three hosts to deploy K3s on top.</para>
<itemizedlist>
<listitem>
<para>Ensure the hosts have different host names.</para>
</listitem>
<listitem>
<para>For testing, these could be virtual machines</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>At least 2 available IPs in the network (one for the Traefik and one for the
managed service).</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-k3s">
<title>Installing K3s</title>
<note>
<para>If you do not want a fresh cluster but want to use an existing one, skip
this step and proceed to the next one.</para>
</note>
<para>First, a free IP in the network must be reserved that will be used later for
<literal>ExternalIP</literal> of the managed Service.</para>
<para>SSH to the first host and install <literal>K3s</literal> in cluster mode as:</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>Make sure that <literal>--disable=servicelb</literal> flag is provided in
the <literal>k3s server</literal> command.</para>
</note>
<important>
<para>From now on, the commands should be run on the local machine.</para>
</important>
<para>To access the API server from outside, the IP of the K3s VM will be used.</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
scp ${NODE_IP}:/etc/rancher/k3s/k3s.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-k3s-cluster">
<title>Configuring an existing K3s cluster</title>
<note>
<para>This step is valid only if you intend to use an existing K3s cluster.</para>
</note>
<para>To use an existing K3s cluster, the <literal>servicelb</literal> LB should
be disabled and also <literal>tls-san</literal> flags modified.</para>
<para>To change the K3s flags, <literal>/etc/systemd/system/k3s.service</literal>
should be modified on all the VMs in the cluster.</para>
<para>The flags should be inserted in the <literal>ExecStart</literal>. For
example:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>Then the following commands should be executed for K3s to load the new
configurations:</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart k3s</screen>
</section>
<section xml:id="id-installing-metallb">
<title>Installing MetalLB</title>
<para>To deploy <literal>MetalLB</literal>, the <link
xl:href="https://suse-edge.github.io/docs/quickstart/metallb">MetalLB on
K3s</link> guide can be used.</para>
<para><emphasis role="strong">NOTE:</emphasis> Ensure that the IP addresses of the
<literal>ip-pool</literal> IPAddressPool do not overlap with the IP
addresses previously selected for the <literal>LoadBalancer</literal>
service.</para>
<para>Create a separate <literal>IpAddressPool</literal> that will be used only
for the managed Service.</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Installing the Endpoint Copier Operator</title>
<screen language="bash" linenumbering="unnumbered">helm repo add endpoint-copier-operator \
 https://suse-edge.github.io/endpoint-copier-operator

helm install --create-namespace -n endpoint-copier-operator \
 endpoint-copier-operator endpoint-copier-operator/endpoint-copier-operator</screen>
<para>The command above will deploy three different resources in the cluster:</para>
<orderedlist numeration="arabic">
<listitem>
<para>The <literal>endpoint-copier-operator</literal> operator Deployment with two
replicas. One will be the leader and the other will take over the leader
role if needed.</para>
</listitem>
<listitem>
<para>A Kubernetes service called <literal>kubernetes-vip</literal> in the
<literal>default</literal> namespace that will be a copy of the
<literal>kubernetes</literal> Service but from type
<literal>LoadBalancer</literal>.</para>
</listitem>
<listitem>
<para>An Endpoint resource called <literal>kubernetes-vip</literal> in the
<literal>default</literal> namespace that will be a copy of the
<literal>kubernetes</literal> Endpoint.</para>
</listitem>
</orderedlist>
<para>Verify that the <literal>kubernetes-vip</literal> Service has the correct IP
address:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para>Ensure that the <literal>kubernetes-vip</literal> and
<literal>kubernetes</literal> Endpoints resources in the
<literal>default</literal> namespace point to the same IPs.</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpoints kubernetes kubernetes-vip</screen>
<para>If everything is correct, the last thing left is to use the
<literal>VIP_SERVICE_IP</literal> in our <literal>Kubeconfig</literal>.</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>From now on, all the <literal>kubectl</literal> will go through the
<literal>kubernetes-vip</literal> service.</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>Adding control-plane nodes</title>
<para>To monitor the entire process, two more terminal tabs can be opened.</para>
<para>First terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>Second terminal:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpoints</screen>
<para>Now execute the commands below on the second and third nodes.</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Air-gapped deployments with Edge Image Builder</title>
<section xml:id="id-intro">
<title>Intro</title>
<para>This guide will show how to deploy several of the SUSE Edge components
completely air-gapped on SLE Micro 5.5 utilizing Edge Image Builder(EIB)
(<xref linkend="components-eib"/>). With this, you’ll be able to boot into a
customized, ready to boot (CRB) image created by EIB and have the specified
components deployed on either a RKE2 or K3s cluster without an Internet
connection or any manual steps. This configuration is highly desirable for
customers that want to pre-bake all artifacts required for deployment into
their OS image, so they are immediately available on boot.</para>
<para>We will cover an air-gapped installation of:</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-neuvector"/></para>
</listitem>
<listitem>
<para><xref linkend="components-longhorn"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIB will parse and pre-download all images referenced in the provided Helm
charts and Kubernetes manifests. However, some of those may be attempting to
pull container images and create Kubernetes resources based on those at
runtime. In these cases we have to manually specify the necessary images in
the definition file if we want to set up a completely air-gapped
environment.</para>
</warning>
</section>
<section xml:id="id-prerequisites-9">
<title>Prerequisites</title>
<para>If you’re following this guide, it’s assumed that you are already familiar
with EIB (<xref linkend="components-eib"/>). If not, please follow the quick
start guide (<xref linkend="quickstart-eib"/>) to better understand the
concepts shown in practice below.</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Libvirt Network Configuration</title>
<note>
<para>To demo the air-gapped deployment, this guide will be done using a simulated
air-gapped <literal>libvirt</literal> network and the following
configuration will be tailored to that. For your own deployments, you may
have to modify the <literal>host1.local.yaml</literal> configuration that
will be introduced in the next step.</para>
</note>
<para>If you would like to use the same <literal>libvirt</literal> network
configuration, follow along. If not, skip to <xref
linkend="config-dir-creation"/>.</para>
<para>Let’s create an isolated network configuration with an IP address range
<literal>192.168.100.2/24</literal> for DHCP:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>Now, the only thing left is to create the network and start it:</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>Base Directory Configuration</title>
<para>The base directory configuration is the same across all different
components, so we will set it up here.</para>
<para>We will first create the necessary subdirectories:</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>Make sure to add whichever base image you plan to use into the
<literal>base-images</literal> directory. This guide will focus on the Self
Install ISO found <link
xl:href="https://www.suse.com/download/sle-micro/">here</link>.</para>
<para>Let’s copy the downloaded image:</para>
<screen language="shell" linenumbering="unnumbered">cp SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIB is never going to modify the base image input.</para>
</note>
<para>Let’s create a file containing the desired network configuration:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>This configuration ensures the following are present on the provisioned
systems (using the specified MAC address):</para>
<itemizedlist>
<listitem>
<para>an Ethernet interface with a static IP address</para>
</listitem>
<listitem>
<para>routing</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>hostname (<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>The resulting file structure should now look like:</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>Base Definition File</title>
<para>Edge Image Builder is using <emphasis>definition files</emphasis> to modify
the SLE Micro images. These files contain the majority of configurable
options.  Many of these options will be repeated across the different
component sections, so we will list and explain those here.</para>
<tip>
<para>Full list of customization options in the definition file can be found in
the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#image-definition-file">upstream
documentation</link></para>
</tip>
<para>We will take a look at the following fields which will be present in all
definition files:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para>The <literal>image</literal> section is required, and it specifies the input
image, its architecture and type, as well as what the output image will be
called.</para>
<para>The <literal>operatingSystem</literal> section is optional, and contains
configuration to enable login on the provisioned systems with the
<literal>root/eib</literal> username/password.</para>
<para>The <literal>kubernetes</literal> section is optional, and it defines the
Kubernetes type and version. We are going to use Kubernetes 1.28.9 and RKE2
by default.  Use <literal>kubernetes.version: v1.28.9+k3s1</literal> if K3s
is desired instead. Unless explicitly configured via the
<literal>kubernetes.nodes</literal> field, all clusters we bootstrap in this
guide will be single-node ones.</para>
<para>The <literal>embeddedArtifactRegistry</literal> section will include all
images which are only referenced and pulled at runtime for the specific
component.</para>
</section>
<section xml:id="rancher-install">
<title>Rancher Installation</title>
<note>
<para>The Rancher (<xref linkend="components-rancher"/>) deployment that will be
demonstrated will be highly slimmed down for demonstration purposes. For
your actual deployments, additional artifacts may be necessary depending on
your configuration.</para>
</note>
<para>The <link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.8.4">Rancher
v2.8.4</link> release assets contain a <literal>rancher-images.txt</literal>
file which lists all the images required for an air-gapped installation.</para>
<para>There are about 602 container images in total which means that the resulting
CRB image would be roughly 28GB+. For our Rancher installation, we will
strip down that list to the smallest working configuration. From there, you
can add back any images you may need for your deployments.</para>
<para>We will create the definition file and include the stripped down image list:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  network:
    apiVIP: 192.168.100.151
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.14.2/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.14.2
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url:  https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v4.0.2
    - name: registry.rancher.com/rancher/calico-cni:v3.27.0-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.0.13
    - name: registry.rancher.com/rancher/coreos-kube-state-metrics:v1.9.7
    - name: registry.rancher.com/rancher/coreos-prometheus-config-reloader:v0.38.1
    - name: registry.rancher.com/rancher/coreos-prometheus-operator:v0.38.1
    - name: registry.rancher.com/rancher/flannel-cni:v0.3.0-rancher9
    - name: registry.rancher.com/rancher/fleet-agent:v0.9.4
    - name: registry.rancher.com/rancher/fleet:v0.9.4
    - name: registry.rancher.com/rancher/gitjob:v0.9.7
    - name: registry.rancher.com/rancher/grafana-grafana:7.1.5
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240410
    - name: registry.rancher.com/rancher/hardened-calico:v3.27.3-build20240423
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.10-build20240124
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.4.1-build20240325
    - name: registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240305
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.22.28-build20240125
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.9-k3s1-build20240418
    - name: registry.rancher.com/rancher/hardened-flannel:v0.25.1-build20240423
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240401
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.28.9-rke2r1-build20240416
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.0.2-build20240208
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.14.1-build20230926
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.6.3-build20240208
    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1
    - name: registry.rancher.com/rancher/istio-kubectl:1.5.10
    - name: registry.rancher.com/rancher/jimmidyson-configmap-reload:v0.3.0
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.8.3-build20240228
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.7
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.1
    - name: registry.rancher.com/rancher/kubectl:v1.28.7
    - name: registry.rancher.com/rancher/library-nginx:1.19.2-alpine
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.26
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher112
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.4.4
    - name: registry.rancher.com/rancher/nginx-ingress-controller:nginx-1.9.6-rancher1
    - name: registry.rancher.com/rancher/pause:3.6
    - name: registry.rancher.com/rancher/prom-alertmanager:v0.21.0
    - name: registry.rancher.com/rancher/prom-node-exporter:v1.0.1
    - name: registry.rancher.com/rancher/prom-prometheus:v2.18.2
    - name: registry.rancher.com/rancher/prometheus-auth:v0.2.2
    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.0-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.0-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.8.4
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v3.0.1
    - name: registry.rancher.com/rancher/rancher-webhook:v0.4.5
    - name: registry.rancher.com/rancher/rancher:v2.8.4
    - name: registry.rancher.com/rancher/rke-tools:v0.1.96
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.29.3-build20240412
    - name: registry.rancher.com/rancher/rke2-runtime:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.2.15
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.6-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.1
    - name: registry.rancher.com/rancher/ui-plugin-catalog:1.3.0
    - name: registry.rancher.com/rancher/ui-plugin-operator:v0.1.1
    - name: registry.rancher.com/rancher/webhook-receiver:v0.2.5
    - name: registry.rancher.com/rancher/kubectl:v1.20.2</screen>
<para>As compared to the full list of 602 container images, this slimmed down
version only contains 62 which makes the new CRB image only about 7GB.</para>
<para>We also need to create a Helm values file for Rancher:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para>Setting the <literal>systemDefaultRegistry</literal> to
<literal>registry.rancher.com</literal> allows Rancher to automatically look
for images in the embedded artifact registry started within the CRB image at
boot. Omitting this field may result in failure to find the container images
on the node.</para>
</warning>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 17 MB/s)
Populating Embedded Artifact Registry... 100% (69/69, 26 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (780/780 MB, 115 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 108 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 117 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 34 MB/s)
Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 106 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>Once a node using the built image is provisioned, we can verify the Rancher
installation:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -A --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has
been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAMESPACE                         NAME                                                        READY   STATUS      RESTARTS   AGE
cattle-fleet-local-system         pod/fleet-agent-68f4d5d5f7-tdlk7                            1/1     Running     0          34s
cattle-fleet-system               pod/fleet-controller-85564cc978-pbtvk                       1/1     Running     0          5m51s
cattle-fleet-system               pod/gitjob-9dc58fb5b-7cwsw                                  1/1     Running     0          5m51s
cattle-provisioning-capi-system   pod/capi-controller-manager-5c57b4b8f7-wlp5k                1/1     Running     0          4m52s
cattle-system                     pod/helm-operation-4fk5c                                    0/2     Completed   0          37s
cattle-system                     pod/helm-operation-6zgbq                                    0/2     Completed   0          4m54s
cattle-system                     pod/helm-operation-cjds5                                    0/2     Completed   0          5m37s
cattle-system                     pod/helm-operation-kt5c2                                    0/2     Completed   0          5m21s
cattle-system                     pod/helm-operation-ppgtw                                    0/2     Completed   0          5m30s
cattle-system                     pod/helm-operation-tvcwk                                    0/2     Completed   0          5m54s
cattle-system                     pod/helm-operation-wpxd4                                    0/2     Completed   0          53s
cattle-system                     pod/rancher-58575f9575-svrg2                                1/1     Running     0          6m34s
cattle-system                     pod/rancher-webhook-5c6556f7ff-vgmkt                        1/1     Running     0          5m19s
cert-manager                      pod/cert-manager-6c69f9f796-fkm8f                           1/1     Running     0          7m14s
cert-manager                      pod/cert-manager-cainjector-584f44558c-wg7p6                1/1     Running     0          7m14s
cert-manager                      pod/cert-manager-webhook-76f9945d6f-lv2nv                   1/1     Running     0          7m14s
endpoint-copier-operator          pod/endpoint-copier-operator-58964b659b-l64dk               1/1     Running     0          7m16s
endpoint-copier-operator          pod/endpoint-copier-operator-58964b659b-z9t9d               1/1     Running     0          7m16s
kube-system                       pod/cilium-fht55                                            1/1     Running     0          7m32s
kube-system                       pod/cilium-operator-558bbf6cfd-gwfwf                        1/1     Running     0          7m32s
kube-system                       pod/cilium-operator-558bbf6cfd-qsxb5                        0/1     Pending     0          7m32s
kube-system                       pod/cloud-controller-manager-host1.local                    1/1     Running     0          7m21s
kube-system                       pod/etcd-host1.local                                        1/1     Running     0          7m8s
kube-system                       pod/helm-install-cert-manager-fvbtt                         0/1     Completed   0          8m12s
kube-system                       pod/helm-install-endpoint-copier-operator-5kkgw             0/1     Completed   0          8m12s
kube-system                       pod/helm-install-metallb-zfphb                              0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rancher-nc4nt                              0/1     Completed   2          8m12s
kube-system                       pod/helm-install-rke2-cilium-7wq87                          0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-coredns-nl4gc                         0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-ingress-nginx-svjqd                   0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-metrics-server-gqgqz                  0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-snapshot-controller-crd-r6b5p         0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-snapshot-controller-ss9v4             0/1     Completed   1          8m12s
kube-system                       pod/helm-install-rke2-snapshot-validation-webhook-vlkpn     0/1     Completed   0          8m12s
kube-system                       pod/kube-apiserver-host1.local                              1/1     Running     0          7m29s
kube-system                       pod/kube-controller-manager-host1.local                     1/1     Running     0          7m30s
kube-system                       pod/kube-proxy-host1.local                                  1/1     Running     0          7m30s
kube-system                       pod/kube-scheduler-host1.local                              1/1     Running     0          7m42s
kube-system                       pod/rke2-coredns-rke2-coredns-6c8d9bb6d-qlwc8               1/1     Running     0          7m31s
kube-system                       pod/rke2-coredns-rke2-coredns-autoscaler-55fb4bbbcf-j5r2z   1/1     Running     0          7m31s
kube-system                       pod/rke2-ingress-nginx-controller-4h2mm                     1/1     Running     0          7m3s
kube-system                       pod/rke2-metrics-server-544c8c66fc-lsrc6                    1/1     Running     0          7m15s
kube-system                       pod/rke2-snapshot-controller-59cc9cd8f4-4wx75               1/1     Running     0          7m14s
kube-system                       pod/rke2-snapshot-validation-webhook-54c5989b65-5kp2x       1/1     Running     0          7m15s
metallb-system                    pod/metallb-controller-5895d8446d-z54lm                     1/1     Running     0          7m15s
metallb-system                    pod/metallb-speaker-fxwgk                                   1/1     Running     0          7m15s

NAMESPACE                         NAME                                              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)
         AGE
cattle-fleet-system               service/gitjob                                    ClusterIP      10.43.30.8      &lt;none&gt;            80/TCP
         5m51s
cattle-provisioning-capi-system   service/capi-webhook-service                      ClusterIP      10.43.7.100     &lt;none&gt;            443/TCP
         4m52s
cattle-system                     service/rancher                                   ClusterIP      10.43.100.229   &lt;none&gt;            80/TCP,443/TCP
         6m34s
cattle-system                     service/rancher-webhook                           ClusterIP      10.43.121.133   &lt;none&gt;            443/TCP
         5m19s
cert-manager                      service/cert-manager                              ClusterIP      10.43.140.65    &lt;none&gt;            9402/TCP
         7m14s
cert-manager                      service/cert-manager-webhook                      ClusterIP      10.43.108.158   &lt;none&gt;            443/TCP
         7m14s
default                           service/kubernetes                                ClusterIP      10.43.0.1       &lt;none&gt;            443/TCP
         8m26s
default                           service/kubernetes-vip                            LoadBalancer   10.43.138.138   192.168.100.151   9345:31006/TCP,6443:31599/TCP   8m21s
kube-system                       service/cilium-agent                              ClusterIP      None            &lt;none&gt;            9964/TCP
         7m32s
kube-system                       service/rke2-coredns-rke2-coredns                 ClusterIP      10.43.0.10      &lt;none&gt;            53/UDP,53/TCP
         7m31s
kube-system                       service/rke2-ingress-nginx-controller-admission   ClusterIP      10.43.157.19    &lt;none&gt;            443/TCP
         7m3s
kube-system                       service/rke2-metrics-server                       ClusterIP      10.43.4.123     &lt;none&gt;            443/TCP
         7m15s
kube-system                       service/rke2-snapshot-validation-webhook          ClusterIP      10.43.91.161    &lt;none&gt;            443/TCP
         7m16s
metallb-system                    service/metallb-webhook-service                   ClusterIP      10.43.71.192    &lt;none&gt;            443/TCP
         7m15s

NAMESPACE        NAME                                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system      daemonset.apps/cilium                          1         1         1       1            1           kubernetes.io/os=linux   7m32s
kube-system      daemonset.apps/rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux   7m3s
metallb-system   daemonset.apps/metallb-speaker                 1         1         1       1            1           kubernetes.io/os=linux   7m15s

NAMESPACE                         NAME                                                   READY   UP-TO-DATE   AVAILABLE   AGE
cattle-fleet-local-system         deployment.apps/fleet-agent                            1/1     1            1           34s
cattle-fleet-system               deployment.apps/fleet-controller                       1/1     1            1           5m51s
cattle-fleet-system               deployment.apps/gitjob                                 1/1     1            1           5m51s
cattle-provisioning-capi-system   deployment.apps/capi-controller-manager                1/1     1            1           4m52s
cattle-system                     deployment.apps/rancher                                1/1     1            1           6m34s
cattle-system                     deployment.apps/rancher-webhook                        1/1     1            1           5m19s
cert-manager                      deployment.apps/cert-manager                           1/1     1            1           7m14s
cert-manager                      deployment.apps/cert-manager-cainjector                1/1     1            1           7m14s
cert-manager                      deployment.apps/cert-manager-webhook                   1/1     1            1           7m14s
endpoint-copier-operator          deployment.apps/endpoint-copier-operator               2/2     2            2           7m16s
kube-system                       deployment.apps/cilium-operator                        1/2     2            1           7m32s
kube-system                       deployment.apps/rke2-coredns-rke2-coredns              1/1     1            1           7m31s
kube-system                       deployment.apps/rke2-coredns-rke2-coredns-autoscaler   1/1     1            1           7m31s
kube-system                       deployment.apps/rke2-metrics-server                    1/1     1            1           7m15s
kube-system                       deployment.apps/rke2-snapshot-controller               1/1     1            1           7m14s
kube-system                       deployment.apps/rke2-snapshot-validation-webhook       1/1     1            1           7m15s
metallb-system                    deployment.apps/metallb-controller                     1/1     1            1           7m15s

NAMESPACE                         NAME                                                              DESIRED   CURRENT   READY   AGE
cattle-fleet-local-system         replicaset.apps/fleet-agent-68f4d5d5f7                            1         1         1       34s
cattle-fleet-system               replicaset.apps/fleet-controller-85564cc978                       1         1         1       5m51s
cattle-fleet-system               replicaset.apps/gitjob-9dc58fb5b                                  1         1         1       5m51s
cattle-provisioning-capi-system   replicaset.apps/capi-controller-manager-5c57b4b8f7                1         1         1       4m52s
cattle-system                     replicaset.apps/rancher-58575f9575                                1         1         1       6m34s
cattle-system                     replicaset.apps/rancher-webhook-5c6556f7ff                        1         1         1       5m19s
cert-manager                      replicaset.apps/cert-manager-6c69f9f796                           1         1         1       7m14s
cert-manager                      replicaset.apps/cert-manager-cainjector-584f44558c                1         1         1       7m14s
cert-manager                      replicaset.apps/cert-manager-webhook-76f9945d6f                   1         1         1       7m14s
endpoint-copier-operator          replicaset.apps/endpoint-copier-operator-58964b659b               2         2         2       7m16s
kube-system                       replicaset.apps/cilium-operator-558bbf6cfd                        2         2         1       7m32s
kube-system                       replicaset.apps/rke2-coredns-rke2-coredns-6c8d9bb6d               1         1         1       7m31s
kube-system                       replicaset.apps/rke2-coredns-rke2-coredns-autoscaler-55fb4bbbcf   1         1         1       7m31s
kube-system                       replicaset.apps/rke2-metrics-server-544c8c66fc                    1         1         1       7m15s
kube-system                       replicaset.apps/rke2-snapshot-controller-59cc9cd8f4               1         1         1       7m14s
kube-system                       replicaset.apps/rke2-snapshot-validation-webhook-54c5989b65       1         1         1       7m15s
metallb-system                    replicaset.apps/metallb-controller-5895d8446d                     1         1         1       7m15s

NAMESPACE     NAME                                                      COMPLETIONS   DURATION   AGE
kube-system   job.batch/helm-install-cert-manager                       1/1           85s        8m21s
kube-system   job.batch/helm-install-endpoint-copier-operator           1/1           59s        8m21s
kube-system   job.batch/helm-install-metallb                            1/1           60s        8m21s
kube-system   job.batch/helm-install-rancher                            1/1           100s       8m21s
kube-system   job.batch/helm-install-rke2-cilium                        1/1           44s        8m18s
kube-system   job.batch/helm-install-rke2-coredns                       1/1           45s        8m18s
kube-system   job.batch/helm-install-rke2-ingress-nginx                 1/1           76s        8m16s
kube-system   job.batch/helm-install-rke2-metrics-server                1/1           60s        8m16s
kube-system   job.batch/helm-install-rke2-snapshot-controller           1/1           61s        8m15s
kube-system   job.batch/helm-install-rke2-snapshot-controller-crd       1/1           60s        8m16s
kube-system   job.batch/helm-install-rke2-snapshot-validation-webhook   1/1           60s        8m14s</screen>
<para>And when we go to <literal><link
xl:href="https://192.168.100.50.sslip.io">https://192.168.100.50.sslip.io</link></literal>
and log in with the <literal>adminadminadmin</literal> password that we set
earlier, we are greeted with the Rancher dashboard:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width=""/>
</imageobject>
<textobject><phrase>air gapped rancher</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="neuvector-install">
<title>NeuVector Installation</title>
<para>Unlike the Rancher installation, the NeuVector installation does not require
any special handling in EIB. EIB will automatically air-gap every image
required by NeuVector.</para>
<para>We will create the definition file:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>We will also create a Helm values file for NeuVector:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (6/6, 20 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>Once a node using the built image is provisioned, we can verify the
NeuVector installation:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has
been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                           READY   STATUS    RESTARTS   AGE
pod/neuvector-controller-pod-bc74745cf-x9fsc   1/1     Running   0          13m
pod/neuvector-enforcer-pod-vzw7t               1/1     Running   0          13m

NAME                                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.240.25   &lt;none&gt;        443/TCP                         13m
service/neuvector-svc-controller          ClusterIP   None           &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   13m

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          13m

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           13m

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-bc74745cf   1         1         1       13m

NAME                                  SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-updater-pod   0 0 * * *   False     0        &lt;none&gt;          13m</screen>
</section>
<section xml:id="longhorn-install">
<title>Longhorn Installation</title>
<para>The <link
xl:href="https://longhorn.io/docs/1.6.1/deploy/install/airgap/">official
documentation</link> for Longhorn contains a
<literal>longhorn-images.txt</literal> file which lists all the images
required for an air-gapped installation.  We will be including them in our
definition file. Let’s create it:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 1.6.1
    repositories:
      - name: longhorn
        url: https://charts.longhorn.io
embeddedArtifactRegistry:
  images:
    - name: longhornio/csi-attacher:v4.4.2
    - name: longhornio/csi-provisioner:v3.6.2
    - name: longhornio/csi-resizer:v1.9.2
    - name: longhornio/csi-snapshotter:v6.3.2
    - name: longhornio/csi-node-driver-registrar:v2.9.2
    - name: longhornio/livenessprobe:v2.12.0
    - name: longhornio/backing-image-manager:v1.6.1
    - name: longhornio/longhorn-engine:v1.6.1
    - name: longhornio/longhorn-instance-manager:v1.6.1
    - name: longhornio/longhorn-manager:v1.6.1
    - name: longhornio/longhorn-share-manager:v1.6.1
    - name: longhornio/longhorn-ui:v1.6.1
    - name: longhornio/support-bundle-kit:v0.0.36</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (13/13, 20 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>Once a node using the built image is provisioned, we can verify the Longhorn
installation:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has
been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS      AGE
pod/csi-attacher-5c4bfdcf59-9hgvv                       1/1     Running   0             35s
pod/csi-attacher-5c4bfdcf59-dt6jl                       1/1     Running   0             35s
pod/csi-attacher-5c4bfdcf59-swpwq                       1/1     Running   0             35s
pod/csi-provisioner-667796df57-dfrzw                    1/1     Running   0             35s
pod/csi-provisioner-667796df57-tvsrt                    1/1     Running   0             35s
pod/csi-provisioner-667796df57-xszsx                    1/1     Running   0             35s
pod/csi-resizer-694f8f5f64-6khlb                        1/1     Running   0             35s
pod/csi-resizer-694f8f5f64-gnr45                        1/1     Running   0             35s
pod/csi-resizer-694f8f5f64-sbl4k                        1/1     Running   0             35s
pod/csi-snapshotter-959b69d4b-2k4v8                     1/1     Running   0             35s
pod/csi-snapshotter-959b69d4b-9d8wl                     1/1     Running   0             35s
pod/csi-snapshotter-959b69d4b-l2w95                     1/1     Running   0             35s
pod/engine-image-ei-5cefaf2b-cwd8f                      1/1     Running   0             43s
pod/instance-manager-f0d17f96bc92f3cc44787a2a347f6a98   1/1     Running   0             43s
pod/longhorn-csi-plugin-szv7t                           3/3     Running   0             35s
pod/longhorn-driver-deployer-9f4fc86-q8fz2              1/1     Running   0             83s
pod/longhorn-manager-zp66l                              1/1     Running   0             83s
pod/longhorn-ui-5f4b7bbf69-k645d                        1/1     Running   3 (65s ago)   83s
pod/longhorn-ui-5f4b7bbf69-t7xt4                        1/1     Running   3 (62s ago)   83s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.74.59    &lt;none&gt;        9502/TCP   83s
service/longhorn-backend              ClusterIP   10.43.45.206   &lt;none&gt;        9500/TCP   83s
service/longhorn-conversion-webhook   ClusterIP   10.43.83.108   &lt;none&gt;        9501/TCP   83s
service/longhorn-engine-manager       ClusterIP   None           &lt;none&gt;        &lt;none&gt;     83s
service/longhorn-frontend             ClusterIP   10.43.84.55    &lt;none&gt;        80/TCP     83s
service/longhorn-recovery-backend     ClusterIP   10.43.75.200   &lt;none&gt;        9503/TCP   83s
service/longhorn-replica-manager      ClusterIP   None           &lt;none&gt;        &lt;none&gt;     83s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-5cefaf2b   1         1         1       1            1           &lt;none&gt;          43s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          35s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          83s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           35s
deployment.apps/csi-provisioner            3/3     3            3           35s
deployment.apps/csi-resizer                3/3     3            3           35s
deployment.apps/csi-snapshotter            3/3     3            3           35s
deployment.apps/longhorn-driver-deployer   1/1     1            1           83s
deployment.apps/longhorn-ui                2/2     2            2           83s

NAME                                               DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-5c4bfdcf59            3         3         3       35s
replicaset.apps/csi-provisioner-667796df57         3         3         3       35s
replicaset.apps/csi-resizer-694f8f5f64             3         3         3       35s
replicaset.apps/csi-snapshotter-959b69d4b          3         3         3       35s
replicaset.apps/longhorn-driver-deployer-9f4fc86   1         1         1       83s
replicaset.apps/longhorn-ui-5f4b7bbf69             2         2         2       83s</screen>
</section>
<section xml:id="kubevirt-install">
<title>KubeVirt and CDI Installation</title>
<para>The Helm charts for both KubeVirt and CDI are only installing their
respective operators.  It is up to the operators to deploy the rest of the
systems which means we will have to include all necessary container images
in our definition file. Let’s create it:</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: kubevirt-chart
        repositoryName: suse-edge
        version: 0.2.4
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi-chart
        repositoryName: suse-edge
        version: 0.2.3
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-api:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-controller:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-handler:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1-150500.8.12.1</screen>
<para>Let’s build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>The output should be similar to the following:</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (13/13, 6 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>Once a node using the built image is provisioned, we can verify the
installation of both KubeVirt and CDI.</para>
<para>Verify KubeVirt:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has
been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/virt-api-7c45477984-z226r          1/1     Running   0          2m4s
pod/virt-controller-664d9986b5-8p8gm   1/1     Running   0          98s
pod/virt-controller-664d9986b5-v2n4h   1/1     Running   0          98s
pod/virt-handler-2fx8c                 1/1     Running   0          98s
pod/virt-operator-5cf69867dc-hz5s8     1/1     Running   0          2m30s
pod/virt-operator-5cf69867dc-kp266     1/1     Running   0          2m30s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.210.235   &lt;none&gt;        443/TCP   2m7s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m7s
service/virt-api                      ClusterIP   10.43.226.140   &lt;none&gt;        443/TCP   2m7s
service/virt-exportproxy              ClusterIP   10.43.213.201   &lt;none&gt;        443/TCP   2m7s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   98s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m4s
deployment.apps/virt-controller   2/2     2            2           98s
deployment.apps/virt-operator     2/2     2            2           2m30s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-7c45477984          1         1         1       2m4s
replicaset.apps/virt-controller-664d9986b5   2         2         2       98s
replicaset.apps/virt-operator-5cf69867dc     2         2         2       2m30s</screen>
<para>Verify CDI:</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>The output should be similar to the following, showing that everything has
been successfully deployed:</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-db465b888-mdsmm      1/1     Running   0          3m6s
pod/cdi-deployment-56c7d74995-vt9sw    1/1     Running   0          3m6s
pod/cdi-operator-55c74f4b86-gkt58      1/1     Running   0          3m10s
pod/cdi-uploadproxy-7d7b94b968-msg2h   1/1     Running   0          3m6s

NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.161.135   &lt;none&gt;        443/TCP    3m6s
service/cdi-prometheus-metrics   ClusterIP   10.43.161.159   &lt;none&gt;        8080/TCP   3m6s
service/cdi-uploadproxy          ClusterIP   10.43.25.136    &lt;none&gt;        443/TCP    3m6s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m6s
deployment.apps/cdi-deployment    1/1     1            1           3m6s
deployment.apps/cdi-operator      1/1     1            1           3m10s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m6s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-db465b888      1         1         1       3m6s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       3m6s
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       3m10s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       3m6s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>Troubleshooting</title>
<para>If you run into any issues while building the images or are looking to
further test and debug the process, please refer to the <link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.0/docs">upstream
documentation</link>.</para>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>Third-Party Integration</title>
<partintro>
<para>How to integrate third-party tools</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link xl:href="https://nats.io/">NATS</link> is a connective technology
built for the ever-increasingly hyper-connected world. It is a single
technology that enables applications to securely communicate across any
combination of cloud vendors, on-premises, edge, Web and mobile
devices. NATS consists of a family of open-source products that are tightly
integrated but can be deployed easily and independently. NATS is being used
globally by thousands of companies, spanning use cases including
microservices, edge computing, mobile and IoT, and can be used to augment or
replace traditional messaging.</para>
<section xml:id="id-architecture">
<title>Architecture</title>
<para>NATS is an infrastructure that allows data exchange between applications in
the form of messages.</para>
<section xml:id="id-nats-client-applications">
<title>NATS client applications</title>
<para>NATS client libraries can be used to allow the applications to publish,
subscribe, request and reply between different instances.  These
applications are generally referred to as <literal>client
applications</literal>.</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>NATS service infrastructure</title>
<para>The NATS services are provided by one or more NATS server processes that are
configured to interconnect with each other and provide a NATS service
infrastructure. The NATS service infrastructure can scale from a single NATS
server process running on an end device to a public global super-cluster of
many clusters spanning all major cloud providers and all regions of the
world.</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>Simple messaging design</title>
<para>NATS makes it easy for applications to communicate by sending and receiving
messages. These messages are addressed and identified by subject strings and
do not depend on network location.  Data is encoded and framed as a message
and sent by a publisher. The message is received, decoded and processed by
one or more subscribers.</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATS has a built-in distributed persistence system called JetStream.
JetStream was created to solve the problems identified with streaming in
technology today — complexity, fragility and a lack of
scalability. JetStream also solves the problem with the coupling of the
publisher and the subscriber (the subscribers need to be up and running to
receive the message when it is published).  More information about NATS
JetStream can be found <link
xl:href="https://docs.nats.io/nats-concepts/jetstream">here</link>.</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>Installation</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>Installing NATS on top of K3s</title>
<para>NATS is built for multiple architectures so it can easily be installed on
K3s. (<xref linkend="components-k3s"/>)</para>
<para>Let us create a values file to overwrite the default values of NATS.</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>Now let us install NATS via Helm:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>With the <literal>values.yaml</literal> file above, the following components
will be in the <literal>nats</literal> namespace:</para>
<orderedlist numeration="arabic">
<listitem>
<para>HA version of NATS Statefulset containing three containers: NATS server +
Config reloader and Metrics sidecars.</para>
</listitem>
<listitem>
<para>NATS box container, which comes with a set of <literal>NATS</literal>
utilities that can be used to verify the setup.</para>
</listitem>
<listitem>
<para>JetStream also leverages its Key-Value back-end that comes with
<literal>PVCs</literal> bounded to the pods.</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>Testing the setup</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>Create a subscription for the test subject:</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>Send a message to the test subject:</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>Cleaning up</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>NATS as a back-end for K3s</title>
<para>One component K3s leverages is <link
xl:href="https://github.com/k3s-io/kine">KINE</link>, which is a shim
enabling the replacement of etcd with alternate storage back-ends originally
targeting relational databases.  As JetStream provides a Key Value API, this
makes it possible to have NATS as a back-end for the K3s cluster.</para>
<para>There is an already merged PR which makes the built-in NATS in K3s
straightforward, but the change is still <link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">not
included</link> in the K3s releases.</para>
<para>For this reason, the K3s binary should be built manually.</para>
<para>In this tutorial, <link
xl:href="https://suse-edge.github.io/docs/quickstart/slemicro-utm-aarch64">SLE
Micro on OSX on Apple Silicon (UTM)</link> VM is used.</para>
<note>
<para>Run the commands below on the OSX PC.</para>
</note>
<section xml:id="id-building-k3s">
<title>Building K3s</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>The following command adds <literal>nats</literal> in the build tags to
enable the NATS built-in feature in K3s:</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>Replace &lt;node-ip&gt; with the actual IP of the node where the K3s will be
started:</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>Locally building K3s requires the buildx Docker CLI plugin.  It can be <link
xl:href="https://github.com/docker/buildx#manual-download">manually
installed</link> if <literal>$ make local</literal> fails.</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>Installing NATS CLI</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>Running NATS as K3s back-end</title>
<para>Let us <literal>ssh</literal> on the node and run the K3s with the
<literal>--datastore-endpoint</literal> flag pointing to
<literal>nats</literal>.</para>
<note>
<para>The command below starts K3s as a foreground process, so the logs can be
easily followed to see if there are any issues.  To not block the current
terminal, a <literal>&amp;</literal> flag could be added before the command
to start it as a background process.</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>For making the K3s server with the NATS back-end permanent on your
<literal>slemicro</literal> VM, the script below can be run, which creates a
<literal>systemd</literal> service with the needed configurations.</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>Troubleshooting</title>
<para>The following commands can be run on the node to verify that everything with
the stream works properly:</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-sle-micro">
<title>NVIDIA GPUs on SLE Micro</title>
<section xml:id="id-intro-2">
<title>Intro</title>
<para>This guide demonstrates how to implement host-level NVIDIA GPU support via
the pre-built <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">open-source
drivers</link> on SLE Micro 5.5. These are drivers that are baked into the
operating system rather than dynamically loaded by NVIDIA’s <link
xl:href="https://github.com/NVIDIA/gpu-operator">GPU Operator</link>. This
configuration is highly desirable for customers that want to pre-bake all
artifacts required for deployment into the image, and where the dynamic
selection of the driver version, that is, the user selecting the version of
the driver via Kubernetes, is not a requirement. This guide initially
explains how to deploy the additional components onto a system that has
already been pre-deployed, but follows with a section that describes how to
embed this configuration into the initial deployment via Edge Image
Builder. If you do not want to run through the basics and set things up
manually, skip right ahead to that section.</para>
<para>It is important to call out that the support for these drivers is provided
by both SUSE and NVIDIA in tight collaboration, where the driver is built
and shipped by SUSE as part of the package repositories. However, if you
have any concerns or questions about the combination in which you use the
drivers, ask your SUSE or NVIDIA account managers for further assistance. If
you plan to use <link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE), ensure that you are using an <link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">NVAIE
certified GPU</link>, which <emphasis>may</emphasis> require the use of
proprietary NVIDIA drivers. If you are unsure, speak with your NVIDIA
representative.</para>
<para>Further information about NVIDIA GPU operator integration is
<emphasis>not</emphasis> covered in this guide. While integrating the NVIDIA
GPU Operator for Kubernetes is not covered here, you can still follow most
of the steps in this guide to set up the underlying operating system and
simply enable the GPU operator to use the <emphasis>pre-installed</emphasis>
drivers via the <literal>driver.enabled=false</literal> flag in the NVIDIA
GPU Operator Helm chart, where it will simply pick up the installed drivers
on the host. More comprehensive instructions are available from NVIDIA <link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">here</link>.
SUSE recently also made a <link
xl:href="https://documentation.suse.com/trd/kubernetes/single-html/gs_rke2-slebci_nvidia-gpu-operator/">Technical
Reference Document</link> (TRD) available that discusses how to use the GPU
operator and the NVIDIA proprietary drivers, should this be a requirement
for your use case.</para>
</section>
<section xml:id="id-prerequisites-10">
<title>Prerequisites</title>
<para>If you are following this guide, it assumes that you have the following
already available:</para>
<itemizedlist>
<listitem>
<para>At least one host with SLE Micro 5.5 installed; this can be physical or
virtual.</para>
</listitem>
<listitem>
<para>Your hosts are attached to a subscription as this is required for package
access — an evaluation is available <link
xl:href="https://www.suse.com/download/sle-micro/">here</link>.</para>
</listitem>
<listitem>
<para>A <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">compatible
NVIDIA GPU</link> installed (or <emphasis>fully</emphasis> passed through to
the virtual machine in which SLE Micro is running).</para>
</listitem>
<listitem>
<para>Access to the root user — these instructions assume you are the root user,
and <emphasis>not</emphasis> escalating your privileges via
<literal>sudo</literal>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>Manual installation</title>
<para>In this section, you are going to install the NVIDIA drivers directly onto
the SLE Micro operating system as the NVIDIA open-driver is now part of the
core SLE Micro package repositories, which makes it as easy as installing
the required RPM packages. There is no compilation or downloading of
executable packages required. Below we walk through deploying the "G06"
generation of driver, which supports the latest GPUs (see <link
xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">here</link> for
further information), so select an appropriate driver generation for the
NVIDIA GPU that your system has. For modern GPUs, the "G06" driver is the
most common choice.</para>
<para>Before we begin, it is important to recognize that besides the NVIDIA
open-driver that SUSE ships as part of SLE Micro, you might also need
additional NVIDIA components for your setup. These could include OpenGL
libraries, CUDA toolkits, command-line utilities such as
<literal>nvidia-smi</literal>, and container-integration components such as
<literal>nvidia-container-toolkit</literal>. Many of these components are
not shipped by SUSE as they are proprietary NVIDIA software, or it makes no
sense for us to ship them instead of NVIDIA. Therefore, as part of the
instructions, we are going to configure additional repositories that give us
access to said components and walk through certain examples of how to use
these tools, resulting in a fully functional system. It is important to
distinguish between SUSE repositories and NVIDIA repositories, as
occasionally there can be a mismatch between the package versions that
NVIDIA makes available versus what SUSE has built. This usually arises when
SUSE makes a new version of the open-driver available, and it takes a couple
of days before the equivalent packages are made available in NVIDIA
repositories to match.</para>
<para>We recommend that you ensure that the driver version that you are selecting
is compatible with your GPU and meets any CUDA requirements that you may
have by checking:</para>
<itemizedlist>
<listitem>
<para>The <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">CUDA
release notes</link></para>
</listitem>
<listitem>
<para>The driver version that you plan on deploying has a matching version in the
<link xl:href="http://download.nvidia.com/suse/sle15sp5/x86_64/">NVIDIA
SLE15-SP5 repository</link> and ensuring that you have equivalent package
versions for the supporting components available</para>
</listitem>
</itemizedlist>
<tip>
<para>To find the NVIDIA open-driver versions, either run <literal>zypper se -s
nvidia-open-driver</literal> on the target machine <emphasis>or</emphasis>
search the SUSE Customer Center for the "nvidia-open-driver" in <link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Enterprise%20Micro&amp;version=5.5&amp;arch=x86_64">SLE
Micro 5.5 for x86_64</link>.</para>
<para>Here, you will see <emphasis>four</emphasis> versions available, with
<emphasis>545.29.06</emphasis> being the newest:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width=""/>
</imageobject>
<textobject><phrase>SUSE Customer Centre</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>When you have confirmed that an equivalent version is available in the
NVIDIA repos, you are ready to install the packages on the host operating
system. For this, we need to open up a
<literal>transactional-update</literal> session, which creates a new
read/write snapshot of the underlying operating system so we can make
changes to the immutable platform (for further instructions on
<literal>transactional-update</literal>, see <link
xl:href="https://documentation.suse.com/sle-micro/5.4/html/SLE-Micro-all/sec-transactional-udate.html">here</link>):</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>When you are in your <literal>transactional-update</literal> shell, add an
additional package repository from NVIDIA. This allows us to pull in
additional utilities, for example, <literal>nvidia-smi</literal>:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp5/ nvidia-sle15sp5-main
zypper --gpg-auto-import-keys refresh</screen>
<para>You can then install the driver and <literal>nvidia-compute-utils</literal>
for additional utilities. If you do not need the utilities, you can omit it,
but for testing purposes, it is worth installing at this stage:</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>If the installation fails, this might indicate a dependency mismatch between
the selected driver version and what NVIDIA ships in their
repositories. Refer to the previous section to verify that your versions
match. Attempt to install a different driver version. For example, if the
NVIDIA repositories have an earlier version, you can try specifying
<literal>nvidia-open-driver-G06-signed-kmp=545.29.06</literal> on your
install command to specify a version that aligns.</para>
</note>
<para>Next, if you are <emphasis>not</emphasis> using a supported GPU (remembering
that the list can be found <link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">here</link>),
you can see if the driver works by enabling support at the module level, but
your mileage may vary — skip this step if you are using a
<emphasis>supported</emphasis> GPU:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>Now that you have installed these packages, it is time to exit the
<literal>transactional-update</literal> session:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>Make sure that you have exited the <literal>transactional-update</literal>
session before proceeding.</para>
</note>
<para>Now that you have installed the drivers, it is time to reboot. As SLE Micro
is an immutable operating system, it needs to reboot into the new snapshot
that you created in a previous step. The drivers are only installed into
this new snapshot, hence it is not possible to load the drivers without
rebooting into this new snapshot, which happens automatically. Issue the
reboot command when you are ready:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>Once the system has rebooted successfully, log back in and use the
<literal>nvidia-smi</literal> tool to verify that the driver is loaded
successfully and that it can both access and enumerate your GPUs:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>The output of this command should show you something similar to the
following output, noting that in the example below, we have two GPUs:</para>
<screen language="shell" linenumbering="unnumbered">Wed Feb 28 12:31:06 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>This concludes the installation and verification process for the NVIDIA
drivers on your SLE Micro system.</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>Further validation of the manual installation</title>
<para>At this stage, all we have been able to verify is that, at the host level,
the NVIDIA device can be accessed and that the drivers are loading
successfully. However, if we want to be sure that it is functioning, a
simple test would be to validate that the GPU can take instructions from a
user-space application, ideally via a container, and through the CUDA
library, as that is typically what a real workload would use. For this, we
can make a further modification to the host OS by installing the
<literal>nvidia-container-toolkit</literal> (<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container Toolkit</link>). First, open another
<literal>transactional-update</literal> shell, noting that we could have
done this in a single transaction in the previous step, and see how to do
this fully automated in a later section:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>Next, install the <literal>nvidia-container-toolkit</literal> package from
the NVIDIA Container Toolkit repo:</para>
<itemizedlist>
<listitem>
<para>The <literal>nvidia-container-toolkit.repo</literal> below contains a stable
(<literal>nvidia-container-toolkit</literal>) and an experimental
(<literal>nvidia-container-toolkit-experimental</literal>) repository. The
stable repository is recommended for production use. The experimental
repository is disabled by default.</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>When you are ready, you can exit the <literal>transactional-update</literal>
shell:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>…​and reboot the machine into the new snapshot:</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>As before, you need to ensure that you have exited the
<literal>transactional-shell</literal> and rebooted the machine for your
changes to be enacted.</para>
</note>
<para>With the machine rebooted, you can verify that the system can successfully
enumerate the devices using the NVIDIA Container Toolkit. The output should
be verbose, with INFO and WARN messages, but no ERROR messages:</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>This ensures that any container started on the machine can employ NVIDIA GPU
devices that have been discovered. When ready, you can then run a
podman-based container. Doing this via <literal>podman</literal> gives us a
good way of validating access to the NVIDIA device from within a container,
which should give confidence for doing the same with Kubernetes at a later
stage. Give <literal>podman</literal> access to the labeled NVIDIA devices
that were taken care of by the previous command, based on <link
xl:href="https://registry.suse.com/bci/bci-base-15sp5/index.html">SLE
BCI</link>, and simply run the Bash command:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>You will now execute commands from within a temporary podman container. It
does not have access to your underlying system and is ephemeral, so whatever
we do here will not persist, and you should not be able to break anything on
the underlying host. As we are now in a container, we can install the
required CUDA libraries, again checking the correct CUDA version for your
driver <link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">here</link>,
although the previous output of <literal>nvidia-smi</literal> should show
the required CUDA version. In the example below, we are installing
<emphasis>CUDA 12.3</emphasis> and pulling many examples, demos and
development kits so you can fully validate the GPU:</para>
<screen language="shell" linenumbering="unnumbered">zypper ar http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-sle15-sp5
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>Once this has been installed successfully, do not exit the container. We
will run the <literal>deviceQuery</literal> CUDA example, which
comprehensively validates GPU access via CUDA, and from within the container
itself:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>If successful, you should see output that shows similar to the following,
noting the <literal>Result = PASS</literal> message at the end of the
command, and noting that in the output below, the system correctly
identifies two GPUs, whereas your environment may only have one:</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>From here, you can continue to run any other CUDA workload — use compilers
and any other aspect of the CUDA ecosystem to run further tests. When done,
you can exit from the container, noting that whatever you have installed in
there is ephemeral (so will be lost!), and has not impacted the underlying
operating system:</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Implementation with Kubernetes</title>
<para>Now that we have proven the installation and use of the NVIDIA open-driver
on SLE Micro, let us explore configuring Kubernetes on the same
machine. This guide does not walk you through deploying Kubernetes, but it
assumes that you have installed <link xl:href="https://k3s.io/">K3s</link>
or <link xl:href="https://docs.rke2.io/install/quickstart">RKE2</link> and
that your kubeconfig is configured accordingly, so that standard
<literal>kubectl</literal> commands can be executed as the superuser. We
assume that your node forms a single-node cluster, although the core steps
should be similar for multi-node clusters. First, ensure that your
<literal>kubectl</literal> access is working:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>This should show something similar to the following:</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.28.9+rke2r1</screen>
<para>What you should find is that your k3s/rke2 installation has detected the
NVIDIA Container Toolkit on the host and auto-configured the NVIDIA runtime
integration into <literal>containerd</literal> (the Container Runtime
Interface that k3s/rke2 use). Confirm this by checking the containerd
<literal>config.toml</literal> file:</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>This must show something akin to the following. The equivalent K3s location
is <literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>:</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>If these entries are not present, the detection might have failed. This
could be due to the machine or the Kubernetes services not being
restarted. Add these manually as above, if required.</para>
</note>
<para>Next, we need to configure the NVIDIA <literal>RuntimeClass</literal> as an
additional Kubernetes runtime to the default, ensuring that any user
requests for pods that need access to the GPU can use the NVIDIA Container
Toolkit to do so, via the <literal>nvidia-container-runtime</literal>, as
configured in the <literal>containerd</literal> configuration:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>The next step is to configure the <link
xl:href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA Device
Plugin</link>, which configures Kubernetes to leverage the NVIDIA GPUs as
resources within the cluster that can be used, working in combination with
the NVIDIA Container Toolkit. This tool initially detects all capabilities
on the underlying host, including GPUs, drivers and other capabilities (such
as GL) and then allows you to request GPU resources and consume them as part
of your applications.</para>
<para>First, you need to add and update the Helm repository for the NVIDIA Device
Plugin:</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>Now you can install the NVIDIA Device Plugin:</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>After a few minutes, you see a new pod running that will complete the
detection on your available nodes and tag them with the number of GPUs that
have been detected:</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>Now you are ready to create an NVIDIA pod that attempts to use this GPU. Let
us try with the CUDA Benchmark container:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>If all went well, you can look at the logs and see the benchmark
information:</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance)
	-numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
	-device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>Finally, if your applications require OpenGL, you can install the required
NVIDIA OpenGL libraries at the host level, and the NVIDIA Device Plugin and
NVIDIA Container Toolkit can make them available to containers. To do this,
install the package as follows:</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>You need to reboot to make this package available to your applications. The
NVIDIA Device Plugin should automatically redetect this via the NVIDIA
Container Toolkit.</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Bringing it together via Edge Image Builder</title>
<para>Okay, so you have demonstrated full functionality of your applications and
GPUs on SLE Micro and you now want to use <xref linkend="components-eib"/>
to provide it all together via a deployable/consumable ISO or RAW disk
image. This guide does not explain how to use Edge Image Builder, but it
provides the necessary configurations to build such image. Below you can
find an example of an image definition, along with the necessary Kubernetes
configuration files, to ensure that all the required components are deployed
out of the box. Here is the directory structure of the Edge Image Builder
directory for the example shown below:</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>Let us explore those files. First, here is a sample image definition for a
single-node cluster running K3s that deploys the utilities and OpenGL
packages, too (<literal>eib-config-iso.yaml</literal>):</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  arch: x86_64
  imageType: iso
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp5/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: &lt;snip&gt;
kubernetes:
  version: v1.28.9+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>This is just an example. You may need to customize it to fit your
requirements and expectations. Additionally, if using SLE Micro, you need to
provide your own <literal>sccRegistrationCode</literal> to resolve package
dependencies and pull the NVIDIA drivers.</para>
</note>
<para>Besides this, we need to add additional components, so they get loaded by
Kubernetes at boot time. The EIB directory needs a
<literal>kubernetes</literal> directory first, with subdirectories for the
configuration, Helm chart values and any additional manifests required:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>Let us now set up the (optional) Kubernetes configuration by choosing a CNI
(which defaults to Cilium if unselected) and enabling SELinux:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>Now ensure that the NVIDIA RuntimeClass is created on the Kubernetes
cluster:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>We use the built-in Helm Controller to deploy the NVIDIA Device Plugin
through Kubernetes itself.  Let’s provide the runtime class in the values
file for the chart:</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>We need to grab the NVIDIA Container Toolkit RPM public key before
proceeding:</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>All the required artifacts, including Kubernetes binary, container images,
Helm charts (and any referenced images), will be automatically air-gapped,
meaning that the systems at deploy time should require no Internet
connectivity by default. Now you need only to grab the SLE Micro ISO from
the <link xl:href="https://www.suse.com/download/sle-micro/">SUSE Downloads
Page</link> (and place it in the <literal>base-images</literal> directory),
and you can call the Edge Image Builder tool to generate the ISO for you. To
complete the example, here is the command that was used to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-config-iso.yaml</screen>
<para>For further instructions, please see the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md">documentation</link>
for Edge Image Builder.</para>
</section>
<section xml:id="id-resolving-issues">
<title>Resolving issues</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smi does not find the GPU</title>
<para>Check the kernel messages using <literal>dmesg</literal>. If this indicates
that it cannot allocate <literal>NvKMSKapDevice</literal>, apply the
unsupported GPU workaround:</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>NOTE</emphasis>: You will need to reload the kernel module, or
reboot, if you change the kernel module configuration in the above step for
it to take effect.</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="id-day-2-operations">
<title>Day 2 Operations</title>
<partintro>
<para>This section explains how administrators can handle different "Day Two"
operation tasks both on the management and on the downstream clusters.</para>
</partintro>
<chapter xml:id="day2-mgmt-cluster">
<title>Management Cluster</title>
<para>This section covers how to do various <literal>Day 2</literal> operations on
a <literal>management cluster</literal>.</para>
<section xml:id="id-rke2-upgrade">
<title>RKE2 upgrade</title>
<note>
<para>To ensure <emphasis role="strong">disaster recovery</emphasis>, we advise to
do a backup of the RKE2 cluster data. For information on how to do this,
check <link xl:href="https://docs.rke2.io/backup_restore">here</link>. The
default location for the <literal>rke2</literal> binary is
<literal>/opt/rke2/bin</literal>.</para>
</note>
<para>You can upgrade the RKE2 version using the RKE2 installation script as
follows:</para>
<screen language="bash" linenumbering="unnumbered">curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=vX.Y.Z+rke2rN sh -</screen>
<para>Remember to restart the <literal>rke2</literal> process after installing:</para>
<screen language="bash" linenumbering="unnumbered"># For server nodes:
systemctl restart rke2-server

# For agent nodes:
systemctl restart rke2-agent</screen>
<important>
<para>To avoid any unforseen upgrade problems, use the following node upgrade
order:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis>Server nodes</emphasis> - should be upgraded <emphasis
role="strong">one</emphasis> node at a time.</para>
</listitem>
<listitem>
<para><emphasis>Agent nodes</emphasis> - should be upgraded after <emphasis
role="strong">all</emphasis> server node upgrades have finished. Can be
upgraded in parallel.</para>
</listitem>
</orderedlist>
</important>
<para><emphasis>For further information, see the <link
xl:href="https://docs.rke2.io/upgrade/manual_upgrade#upgrade-rke2-using-the-installation-script">RKE2
upgrade documentation</link>.</emphasis></para>
</section>
<section xml:id="id-os-upgrade">
<title>OS upgrade</title>
<note>
<para>This section assumes that you have registered your system to <link
xl:href="https://scc.suse.com">https://scc.suse.com</link>.</para>
</note>
<para>SUSE regularly releases new <literal>SLE Micro</literal> package updates. To
retrieve the updated package versions SLE Micro uses
<literal>transactional-upgrade</literal>.</para>
<para><literal>transactional-upgrade</literal> provides an application and library
to update a Linux operating system in a transactional way, i.e. the update
will be performed in the background while the system continues running as it
is. Only after you <emphasis role="strong">reboot</emphasis> the system will
the update take effect. For further information, see the
<literal>transactional-update</literal> <link
xl:href="https://github.com/openSUSE/transactional-update">GitHub</link>
GitHub page.</para>
<formalpara>
<title>To update all packages in the system, execute:</title>
<para>
<screen language="bash" linenumbering="unnumbered">transactional-update</screen>
</para>
</formalpara>
<para>Since <emphasis role="strong">rebooting</emphasis> the node will result in
it being unavailable for some time, if you are running a multi-node cluster,
you can <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>
and <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link>
the node before the <emphasis role="strong">reboot</emphasis>.</para>
<formalpara>
<title>To cordon a node, execute:</title>
<para>
<screen language="bash" linenumbering="unnumbered">kubectl cordon &lt;node&gt;</screen>
</para>
</formalpara>
<para>This will result in the node being taken out of the default scheduling
mechanism, ensuring that no pods will be assigned to it by mistake.</para>
<formalpara>
<title>To drain a node, execute:</title>
<para>
<screen language="bash" linenumbering="unnumbered">kubectl drain &lt;node&gt;</screen>
</para>
</formalpara>
<para>This will ensure that all workloads on the node will be transferred to other
available nodes.</para>
<note>
<para>Depending on what workloads you are running on the node, you might also need
to provide additional flags (e.g. <literal>--delete-emptydir-data</literal>,
<literal>--ignore-daemonsets</literal>) to the command.</para>
</note>
<formalpara>
<title>Reboot node:</title>
<para>
<screen language="bash" linenumbering="unnumbered">sudo reboot</screen>
</para>
</formalpara>
<para>After a successful reboot, the packages on your node will be updated. The
only thing left is to bring the node back to the default scheduling
mechanism with the <link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/">uncordon</link>
command.</para>
<formalpara>
<title>Uncordon node:</title>
<para>
<screen language="bash" linenumbering="unnumbered">kubectl uncordon &lt;node&gt;</screen>
</para>
</formalpara>
<note>
<para>In case you want to revert the update, use the above steps with the
following <literal>transactional-update</literal> command:</para>
<screen language="bash" linenumbering="unnumbered">transactional-update rollback last</screen>
</note>
</section>
<section xml:id="id-helm-upgrade">
<title>Helm upgrade</title>
<note>
<para>This section assumes you have installed <literal>helm</literal> on your
system. For <literal>helm</literal> installation instructions, check <link
xl:href="https://helm.sh/docs/intro/install">here</link>.</para>
</note>
<para>This section covers how to upgrade both an EIB (<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade"/>) and non-EIB (<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade"/>) deployed helm chart.</para>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade">
<title>EIB deployed helm chart</title>
<para>EIB deploys helm charts defined in it’s image definition file (<xref
linkend="quickstart-eib-definition-file"/>) by using RKE2’s manifest <link
xl:href="https://docs.rke2.io/advanced#auto-deploying-manifests">auto-deploy</link>
functionality.</para>
<para>In order to upgrade a chart that is deployed in such a manner, you need to
upgrade the chart manifest file that EIB will create under the
<literal>/var/lib/rancher/rke2/server/manifests</literal> directory on your
<literal>initializer</literal> node.</para>
<note>
<para>To ensure disaster recovery, we advise that you always backup your chart
manifest file as well as follow any documentation related to disaster
recovery that your chart offers.</para>
</note>
<para>To upgrade the chart manifest file, follow these steps:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Locate the <literal>initializer</literal> node</para>
<itemizedlist>
<listitem>
<para>For <literal>multi-node clusters</literal> - in your EIB image definition
file, you should have specified the <literal>initializer: true</literal>
property for one of your nodes. If you have not specified this property, the
initializer node will be the first <emphasis role="strong">server</emphasis>
node in your node list.</para>
</listitem>
<listitem>
<para>For <literal>single-node clusters</literal> - the initializer is the
currently running node.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>SSH to the <literal>initializer</literal> node:</para>
<screen language="bash" linenumbering="unnumbered">ssh root@&lt;node_ip&gt;</screen>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/docs/helm/helm_pull/">Pull</link> the helm
chart:</para>
<itemizedlist>
<listitem>
<para>For helm charts hosted in a helm chart repository:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add &lt;chart_repo_name&gt; &lt;chart_repo_urls&gt;
helm pull &lt;chart_repo_name&gt;/&lt;chart_name&gt;

# Alternatively if you want to pull a specific verison
helm pull &lt;chart_repo_name&gt;/&lt;chart_name&gt; --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>For OCI-based helm charts:</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://&lt;chart_oci_url&gt;

# Alternatively if you want to pull a specific verison
helm pull oci://&lt;chart_oci_url&gt; --version=X.Y.Z</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Encode the pulled <literal>.tgz</literal> archive so that it can be passed
to a <literal>HelmChart</literal> CR config:</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 &lt;chart_name&gt;-X.Y.Z.tgz  &gt; &lt;chart_name&gt;-X.Y.Z.txt</screen>
</listitem>
<listitem>
<para>Make a copy of the chart manifest file that we will edit:</para>
<screen language="bash" linenumbering="unnumbered">cp /var/lib/rancher/rke2/server/manifests/&lt;chart_name&gt;.yaml ./&lt;chart_name&gt;.yaml</screen>
</listitem>
<listitem>
<para>Change the <literal>chartContent</literal> and <literal>version</literal>
configurations of the <literal>bar.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">sed -i -e "s|chartContent:.*|chartContent: $(&lt;chart-name-X.Y.Z.txt)|" -e "s|version:.*|version: X.Y.Z|" &lt;chart_name&gt;.yaml</screen>
<note>
<para>If you need to do any additional upgrade changes to the chart (e.g. adding
<emphasis role="strong">new</emphasis> custom chart values), you need to
manually edit the chart manifest file.</para>
</note>
</listitem>
<listitem>
<para>Replace the original chart manifest file:</para>
<screen language="bash" linenumbering="unnumbered">cp &lt;chart_name&gt;.yaml /var/lib/rancher/rke2/server/manifests/</screen>
</listitem>
</orderedlist>
<para>The above commands will trigger an upgrade of the helm chart. The upgrade
will be handled by the <link
xl:href="https://github.com/k3s-io/helm-controller#helm-controller">helm-controller</link>.</para>
<para>To track the helm chart upgrade you need to view the logs of the pod that
the <literal>helm-controller</literal> creates for the chart upgrade. Refer
to the Examples (<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade-examples"/>) section for
more information.</para>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade-examples">
<title>Examples</title>
<note>
<para>The examples in this section assume that you have already located and
connected to your <literal>initializer</literal> node.</para>
</note>
<para>This section offer examples on how to upgrade a:</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-rancher"/>) helm
chart</para>
</listitem>
<listitem>
<para>Metal3 (<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-metal3"/>) helm
chart</para>
</listitem>
</itemizedlist>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-rancher">
<title>Rancher upgrade</title>
<note>
<para>To ensure disaster recovery, we advise to do a Rancher backup. For
information on how to do this, check <link
xl:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-rancher">here</link>.</para>
</note>
<para>This example shows how to upgrade Rancher to the <literal>2.8.4</literal>
version.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Add the <literal>Rancher Prime</literal> Helm repository:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime</screen>
</listitem>
<listitem>
<para>Pull the latest <literal>Rancher Prime</literal> helm chart version:</para>
<screen language="bash" linenumbering="unnumbered">helm pull rancher-prime/rancher --version=2.8.4</screen>
</listitem>
<listitem>
<para>Encode <literal>.tgz</literal> archive so that it can be passed to a
<literal>HelmChart</literal> CR config:</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 rancher-2.8.4.tgz  &gt; rancher-2.8.4-encoded.txt</screen>
</listitem>
<listitem>
<para>Make a copy of the <literal>rancher.yaml</literal> file that we will edit:</para>
<screen language="bash" linenumbering="unnumbered">cp /var/lib/rancher/rke2/server/manifests/rancher.yaml ./rancher.yaml</screen>
</listitem>
<listitem>
<para>Change the <literal>chartContent</literal> and <literal>version</literal>
configurations of the <literal>rancher.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">sed -i -e "s|chartContent:.*|chartContent: $(&lt;rancher-2.8.4-encoded.txt)|" -e "s|version:.*|version: 2.8.4|" rancher.yaml</screen>
<note>
<para>If you need to do any additional upgrade changes to the chart (e.g. adding
<emphasis role="strong">new</emphasis> custom chart values), you need to
manually edit the <literal>rancher.yaml</literal> file.</para>
</note>
</listitem>
<listitem>
<para>Replace the original <literal>rancher.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">cp rancher.yaml /var/lib/rancher/rke2/server/manifests/</screen>
</listitem>
</orderedlist>
<para>To verify the update:</para>
<orderedlist numeration="arabic">
<listitem>
<para>List pods in <literal>default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n default

# Example output
NAME                              READY   STATUS      RESTARTS   AGE
helm-install-cert-manager-7v7nm   0/1     Completed   0          88m
helm-install-rancher-p99k5        0/1     Completed   0          3m21s</screen>
</listitem>
<listitem>
<para>Look at the logs of the <literal>helm-install-rancher-*</literal> pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;helm_install_rancher_pod&gt; -n default

# Example
kubectl logs helm-install-rancher-p99k5 -n default</screen>
</listitem>
<listitem>
<para>Verify <literal>Rancher</literal> pods are running:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n cattle-system

# Example output
NAME                               READY   STATUS      RESTARTS   AGE
helm-operation-mccvd               0/2     Completed   0          3m52s
helm-operation-np8kn               0/2     Completed   0          106s
helm-operation-q8lf7               0/2     Completed   0          2m53s
rancher-648d4fbc6c-qxfpj           1/1     Running     0          5m27s
rancher-648d4fbc6c-trdnf           1/1     Running     0          9m57s
rancher-648d4fbc6c-wvhbf           1/1     Running     0          9m57s
rancher-webhook-649dcc48b4-zqjs7   1/1     Running     0          100s</screen>
</listitem>
<listitem>
<para>Verify <literal>Rancher</literal> version upgrade:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get settings.management.cattle.io server-version

# Example output
NAME             VALUE
server-version   v2.8.4</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-metal3">
<title>Metal<superscript>3</superscript> upgrade</title>
<para>This example shows how to upgrade Metal<superscript>3</superscript> to the
<literal>0.7.1</literal> version.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Pull the latest <literal>Metal<superscript>3</superscript></literal> helm
chart version:</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://registry.suse.com/edge/metal3-chart --version 0.7.1</screen>
</listitem>
<listitem>
<para>Encode <literal>.tgz</literal> archive so that it can be passed to a
<literal>HelmChart</literal> CR config:</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 metal3-chart-0.7.1.tgz  &gt; metal3-chart-0.7.1-encoded.txt</screen>
</listitem>
<listitem>
<para>Make a copy of the <literal>Metal<superscript>3</superscript></literal>
manifest file that we will edit:</para>
<screen language="bash" linenumbering="unnumbered">cp /var/lib/rancher/rke2/server/manifests/metal3.yaml ./metal3.yaml</screen>
</listitem>
<listitem>
<para>Change the <literal>chartContent</literal> and <literal>version</literal>
configurations of the <literal>Metal<superscript>3</superscript></literal>
manifest file:</para>
<screen language="bash" linenumbering="unnumbered">sed -i -e "s|chartContent:.*|chartContent: $(&lt;metal3-chart-0.7.1-encoded.txt)|" -e "s|version:.*|version: 0.7.1|" metal3.yaml</screen>
<note>
<para>If you need to do any additional upgrade changes to the chart (e.g. adding
<emphasis role="strong">new</emphasis> custom chart values), you need to
manually edit the <literal>metal3.yaml</literal> file.</para>
</note>
</listitem>
<listitem>
<para>Replace the original <literal>Metal<superscript>3</superscript></literal>
manifest file:</para>
<screen language="bash" linenumbering="unnumbered">cp metal3.yaml /var/lib/rancher/rke2/server/manifests/</screen>
</listitem>
</orderedlist>
<para>To verify the update:</para>
<orderedlist numeration="arabic">
<listitem>
<para>List pods in <literal>default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n default

# Example output
NAME                              READY   STATUS      RESTARTS   AGE
helm-install-metal3-7p7bl         0/1     Completed   0          27s</screen>
</listitem>
<listitem>
<para>Look at the logs of the <literal>helm-install-rancher-*</literal> pod:</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;helm_install_rancher_pod&gt; -n default

# Example
kubectl logs helm-install-metal3-7p7bl -n default</screen>
</listitem>
<listitem>
<para>Verify <literal>Metal<superscript>3</superscript></literal> pods are
running:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n metal3-system

# Example output
NAME                                                     READY   STATUS    RESTARTS      AGE
baremetal-operator-controller-manager-785f99c884-9z87p   2/2     Running   2 (25m ago)   36m
metal3-metal3-ironic-96fb66cdd-lkss2                     4/4     Running   0             3m54s
metal3-metal3-mariadb-55fd44b648-q6zhk                   1/1     Running   0             36m</screen>
</listitem>
<listitem>
<para>Verify the <literal>HelmChart</literal> resource version is upgraded:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmchart metal3 -n default

# Example output
NAME     JOB                   CHART   TARGETNAMESPACE   VERSION   REPO   HELMVERSION   BOOTSTRAP
metal3   helm-install-metal3           metal3-system     0.7.1</screen>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade">
<title>Non-EIB deployed helm chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Get the values for the currently running helm chart <literal>.yaml</literal>
file and make any changes to them <emphasis role="strong">if
necessary</emphasis>:</para>
<screen language="bash" linenumbering="unnumbered">helm get values &lt;chart_name&gt; -n &lt;chart_namespace&gt; -o yaml &gt; &lt;chart_name&gt;-values.yaml</screen>
</listitem>
<listitem>
<para>Update the helm chart:</para>
<screen language="bash" linenumbering="unnumbered"># For charts using a chart repository
helm upgrade &lt;chart_name&gt; &lt;chart_repo_name&gt;/&lt;chart_name&gt; \
  --namespace &lt;chart_namespace&gt; \
  -f &lt;chart_name&gt;-values.yaml \
  --version=X.Y.Z

# For OCI based charts
helm upgrade &lt;chart_name&gt; oci://&lt;oci_registry_url&gt;/&lt;chart_name&gt; \
  --namespace &lt;chart_namespace&gt; \
  -f &lt;chart_name&gt;-values.yaml \
  --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>Verify the chart upgrade. Depending on the chart you may need to verify
different resources. For examples of chart upgrades, see the Examples (<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade-examples"/>) section.</para>
</listitem>
</orderedlist>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade-examples">
<title>Examples</title>
<para>This section offer examples on how to upgrade a:</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade-examples-rancher"/>) helm
chart</para>
</listitem>
<listitem>
<para>Metal3 (<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade-examples-metal3"/>) helm chart</para>
</listitem>
</itemizedlist>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade-examples-rancher">
<title>Rancher</title>
<note>
<para>To ensure disaster recovery, we advise to do a Rancher backup. For
information on how to do this, check <link
xl:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-rancher">here</link>.</para>
</note>
<para>This example shows how to upgrade Rancher to the <literal>2.8.4</literal>
version.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Get the values for the current Rancher release and print them to a
<literal>rancher-values.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">helm get values rancher -n cattle-system -o yaml &gt; rancher-values.yaml</screen>
</listitem>
<listitem>
<para>Update the helm chart:</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade rancher rancher-prime/rancher \
  --namespace cattle-system \
  -f rancher-values.yaml \
  --version=2.8.4</screen>
</listitem>
<listitem>
<para>Verify <literal>Rancher</literal> version upgrade:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get settings.management.cattle.io server-version

# Example output
NAME             VALUE
server-version   v2.8.4</screen>
</listitem>
</orderedlist>
<para><emphasis>For additional information on the Rancher helm chart upgrade,
check <link
xl:href="https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades">here</link>.</emphasis></para>
</section>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade-examples-metal3">
<title>Metal<superscript>3</superscript></title>
<para>This example shows how to upgrade Metal<superscript>3</superscript> to the
<literal>0.7.1</literal> version.</para>
<orderedlist numeration="arabic">
<listitem>
<para>Get the values for the current Rancher release and print them to a
<literal>rancher-values.yaml</literal> file:</para>
<screen language="bash" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml</screen>
</listitem>
<listitem>
<para>Update the helm chart:</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade metal3 oci://registry.suse.com/edge/metal3-chart \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.7.1</screen>
</listitem>
<listitem>
<para>Verify <literal>Metal<superscript>3</superscript></literal> pods are
running:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n metal3-system

# Example output
NAME                                                     READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-785f99c884-fvsx4   2/2     Running   0          12m
metal3-metal3-ironic-96fb66cdd-j9mgf                     4/4     Running   0          2m41s
metal3-metal3-mariadb-55fd44b648-7fmvk                   1/1     Running   0          12m</screen>
</listitem>
<listitem>
<para>Verify <literal>Metal<superscript>3</superscript></literal> helm release
version change:</para>
<screen language="bash" linenumbering="unnumbered">helm ls -n metal3-system

# Expected output
NAME  	NAMESPACE    	REVISION	UPDATED                                	STATUS  	CHART       	APP VERSION
metal3	metal3-system	2       	2024-06-17 12:43:06.774802846 +0000 UTC	deployed	metal3-0.7.1	1.16.0</screen>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-downstream-clusters">
<title>Downstream clusters</title>
<para>This section covers how to do various <literal>Day 2</literal> operations
for different parts of your downstream cluster using your
<literal>management cluster</literal>.</para>
<section xml:id="id-introduction">
<title>Introduction</title>
<para>This section is meant to be a <emphasis role="strong">starting
point</emphasis> for the <literal>Day 2</literal> operations
documentation. You can find the following information.</para>
<orderedlist numeration="arabic">
<listitem>
<para>The default components (<xref linkend="day2-downstream-components"/>) used
to achieve <literal>Day 2</literal> operations over multiple downstream
clusters.</para>
</listitem>
<listitem>
<para>Determining which <literal>Day 2</literal> resources should you use for your
specific use-case (<xref linkend="day2-determine-use-case"/>).</para>
</listitem>
<listitem>
<para>The suggested workflow sequence (<xref linkend="day2-upgrade-workflow"/>)
for <literal>Day 2</literal> operations.</para>
</listitem>
</orderedlist>
<section xml:id="day2-downstream-components">
<title>Components</title>
<para>Below you can find a description of the default components that should be
setup on either your <literal>management cluster</literal> or your
<literal>downstream clusters</literal> so that you can successfully perform
<literal>Day 2</literal> operations.</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<note>
<para>For use-cases where you want to utilise Fleet (<xref
linkend="components-fleet"/>) without Rancher, you can skip the Rancher
component all together.</para>
</note>
<para>Responsible for the management of your <literal>downstream
clusters</literal>. Should be deployed on your <literal>management
cluster</literal>.</para>
<para>For more information, see <xref linkend="components-rancher"/>.</para>
</section>
<section xml:id="id-fleet">
<title>Fleet</title>
<para>Responsible for multi-cluster resource deployment.</para>
<para>Typically offered by the <literal>Rancher</literal> component. For use-cases
where <literal>Rancher</literal> is not used, can be deployed as a
standalone component.</para>
<para>For more information on installing Fleet as a standalone component, see
Fleet’s <link xl:href="https://fleet.rancher.io/installation">Installation
Details</link>.</para>
<para>For more information regarding the Fleet component, see <xref
linkend="components-fleet"/>.</para>
<important>
<para>This documentation heavily relies on <literal>Fleet</literal> and more
specifically on the <literal>GitRepo</literal> and <literal>Bundle</literal>
resources (more on this in <xref linkend="day2-determine-use-case"/>) for
establishing a GitOps way of automating the deployment of resources related
to <literal>Day 2</literal> operations.</para>
<para>For use-cases, where a third party GitOps tool usage is desired, see:</para>
<orderedlist numeration="arabic">
<listitem>
<para>For <literal>OS package updates</literal> - <xref
linkend="os-pkg-suc-plan-deployment-third-party"/></para>
</listitem>
<listitem>
<para>For <literal>Kubernetes distribution upgrades</literal> - <xref
linkend="k8s-upgrade-suc-plan-deployment-third-party"/></para>
</listitem>
<listitem>
<para>For <literal>Helm chart upgrades</literal> - retrieve the chart version
supported by the desired Edge release from the <xref
linkend="release-notes"/> page and populate the chart version and URL in
your third party GitOps tool</para>
</listitem>
</orderedlist>
</important>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System-upgrade-controller (SUC)</title>
<para>The <emphasis role="strong">system-upgrade-controller (SUC)</emphasis> is
responsible for executing tasks on specified nodes based on configuration
data provided through a custom resource, called a
<literal>Plan</literal>. Should be located on each <literal>downstream
cluster</literal> that requires some sort of a <literal>Day 2</literal>
operation.</para>
<para>For more information regarding <emphasis role="strong">SUC</emphasis>, see
the upstream <link
xl:href="https://github.com/rancher/system-upgrade-controller">repository</link>.</para>
<para>For information on how to deploy <emphasis role="strong">SUC</emphasis> on
your downstream clusters, first determine your use-case (<xref
linkend="day2-determine-use-case"/>) and then refer to either <xref
linkend="day2-suc-dep-gitrepo"/>, or <xref linkend="day2-suc-dep-bundle"/>
for SUC deployment information.</para>
</section>
</section>
<section xml:id="day2-determine-use-case">
<title>Determine your use-case</title>
<para>As mentioned previously, resources related to <literal>Day 2</literal>
operations are propagated to downstream clusters using Fleet’s
<literal>GitRepo</literal> and <literal>Bundle</literal> resources.</para>
<para>Below you can find more information regarding what these resources do and
for which use-cases should they be used for <literal>Day 2</literal>
operations.</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para>A <literal>GitRepo</literal> is a Fleet (<xref linkend="components-fleet"/>)
resource that represents a Git repository from which
<literal>Fleet</literal> can create <literal>Bundles</literal>. Each
<literal>Bundle</literal> is created based on configuration paths defined
inside of the <literal>GitRepo</literal> resource. For more information, see
the <link xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>
documentation.</para>
<para>In terms of <literal>Day 2</literal> operations <literal>GitRepo</literal>
resources are normally used to deploy <literal>SUC</literal> or <literal>SUC
Plans</literal> on <emphasis role="strong">non air-gapped</emphasis>
environments that utilise a <emphasis>Fleet GitOps</emphasis> approach.</para>
<para>Alternatively, <literal>GitRepo</literal> resources can also be used to
deploy <literal>SUC</literal> or <literal>SUC Plans</literal> on <emphasis
role="strong">air-gapped</emphasis> environments, <emphasis role="strong">if
you mirror your repository setup through a local git server</emphasis>.</para>
</section>
<section xml:id="id-bundle">
<title>Bundle</title>
<para><literal>Bundles</literal> hold <emphasis role="strong">raw</emphasis>
Kubernetes resources that will be deployed on the targeted cluster. Usually
they are created from a <literal>GitRepo</literal> resource, but there are
use-cases where they can be deployed manually. For more information refer to
the <link xl:href="https://fleet.rancher.io/bundle-add">Bundle</link>
documentation.</para>
<para>In terms of <literal>Day 2</literal> operations <literal>Bundle</literal>
resources are normally used to deploy <literal>SUC</literal> or <literal>SUC
Plans</literal> on <emphasis role="strong">air-gapped</emphasis>
environments that do not use some form of <emphasis>local GitOps</emphasis>
procedure (e.g. a <emphasis role="strong">local git server</emphasis>).</para>
<para>Alternatively, if your use-case does not allow for a
<emphasis>GitOps</emphasis> workflow (e.g. using a Git repository),
<emphasis role="strong">Bundle</emphasis> resources could also be used to
deploy <literal>SUC</literal> or <literal>SUC Plans</literal> on <emphasis
role="strong">non air-gapped</emphasis> environments.</para>
</section>
</section>
<section xml:id="day2-upgrade-workflow">
<title>Day 2 workflow</title>
<para>The following is a <literal>Day 2</literal> workflow that should be followed
when upgrading a downstream cluster to a specific Edge release.</para>
<orderedlist numeration="arabic">
<listitem>
<para>OS package update (<xref linkend="day2-os-package-update"/>)</para>
</listitem>
<listitem>
<para>Kubernetes version upgrade (<xref linkend="day2-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helm chart upgrade (<xref linkend="day2-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-suc-deployment-guide">
<title>System upgrade controller deployment guide</title>
<para>The <emphasis role="strong">system-upgrade-controller (SUC)</emphasis> is
responsible for deploying resources on specific nodes of a cluster based on
configurations defined in a custom resource called a <emphasis
role="strong">Plan</emphasis>. For more information, see the <link
xl:href="https://github.com/rancher/system-upgrade-controller">upstream</link>
documentation.</para>
<note>
<para>This section focuses solely on deploying the
<literal>system-upgrade-controller</literal>. <emphasis
role="strong">Plan</emphasis> resources should be deployed from the
following documentations:</para>
<orderedlist numeration="arabic">
<listitem>
<para>OS package update (<xref linkend="day2-os-package-update"/>)</para>
</listitem>
<listitem>
<para>Kubernetes version upgrade (<xref linkend="day2-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helm chart upgrade (<xref linkend="day2-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</note>
<section xml:id="id-deployment-2">
<title>Deployment</title>
<note>
<para>This section assumes that you are going to use Fleet (<xref
linkend="components-fleet"/>) to orchestrate the <emphasis
role="strong">SUC</emphasis> deployment. Users using a third-party GitOps
workflow should see <xref linkend="day2-suc-third-party-gitops"/> for
information on what resources they need to setup in their workflow.</para>
</note>
<para>To determine the resource to use, refer to <xref
linkend="day2-determine-use-case"/>.</para>
<section xml:id="day2-suc-dep-gitrepo">
<title>SUC deployment using a GitRepo resource</title>
<para>This section covers how to create a <literal>GitRepo</literal> resource that
will ship the needed <literal>SUC Plans</literal> for a successful <emphasis
role="strong">SUC</emphasis> deployment to your <emphasis
role="strong">target</emphasis> downstream clusters.</para>
<para>The Edge team maintains a ready to use <literal>GitRepo</literal> resource
for <emphasis role="strong">SUC</emphasis> in each of our
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">releases</link>
under
<literal>gitrepos/day2/system-upgrade-controller-gitrepo.yaml</literal>.</para>
<important>
<para>If using the <literal>suse-edge/fleet-examples</literal> repository, make
sure you are using the resources from a dedicated <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag.</para>
</important>
<para><literal>GitRepo</literal> creation can be done in one of of the following
ways:</para>
<itemizedlist>
<listitem>
<para>Through the Rancher UI (<xref linkend="day2-suc-dep-gitrepo-rancher"/>)
(when <literal>Rancher</literal> is available)</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="day2-suc-dep-gitrepo-manual"/>) the
resources to your <literal>management cluster</literal></para>
</listitem>
</itemizedlist>
<para>Once created, <literal>Fleet</literal> will be responsible for picking up
the resource and deploying the <emphasis role="strong">SUC</emphasis>
resources to all your <emphasis role="strong">target</emphasis>
clusters. For information on how to track the deployment process, see <xref
linkend="monitor-suc-deployment"/>.</para>
<section xml:id="day2-suc-dep-gitrepo-rancher">
<title>GitRepo deployment - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Git Repos → Add Repository</emphasis></para>
</listitem>
</orderedlist>
<para>If you use the <literal>suse-edge/fleet-examples</literal> repository:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Repository URL</emphasis> - <literal><link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Watch → Revision</emphasis> - choose a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag for the <literal>suse-edge/fleet-examples</literal> repository that you
wish to use, e.g. <literal>release-3.0.1</literal>.</para>
</listitem>
<listitem>
<para>Under <emphasis role="strong">Paths</emphasis> add the path to the <emphasis
role="strong">system-upgrade-controller</emphasis> as seen in the release
tag - <literal>fleets/day2/system-upgrade-controller</literal></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Next</emphasis> to move to the <emphasis
role="strong">target</emphasis> configuration section</para>
</listitem>
<listitem>
<para><emphasis role="strong">Only select clusters for which you wish to deploy
the <literal>system-upgrade-controller</literal>.</emphasis> When you are
satisfied with your configurations, click <emphasis
role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
<para>Alternatively, if you decide to use your own repository to host these files,
you would need to provide your repo data above.</para>
</section>
<section xml:id="day2-suc-dep-gitrepo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Choose the desired Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to deploy the <emphasis role="strong">SUC</emphasis>
<literal>GitRepo</literal> from (referenced below as
<literal>${REVISION}</literal>).</para>
</listitem>
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o system-upgrade-controller-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/{REVISION}/gitrepos/day2/system-upgrade-controller-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configurations, under
<literal>spec.targets</literal> specify your desired target list. By
default, the <literal>GitRepo</literal> resources from the
<literal>suse-edge/fleet-examples</literal> are <emphasis
role="strong">NOT</emphasis> mapped to any down stream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters, change the default <literal>GitRepo</literal>
<emphasis role="strong">target</emphasis> to:</para>
<screen language="bash" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resource to your
<literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under
the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo system-upgrade-controller -n fleet-default

# Example output
NAME                        REPO                                               COMMIT       BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.0.1   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-suc-dep-bundle">
<title>SUC deployment using a Bundle resource</title>
<para>This section covers how to create a <literal>Bundle</literal> resource that
will ship the needed <literal>SUC Plans</literal> for a successful <emphasis
role="strong">SUC</emphasis> deployment to your <emphasis
role="strong">target</emphasis> downstream clusters.</para>
<para>The Edge team maintains a ready to use <literal>Bundle</literal> resources
for <emphasis role="strong">SUC</emphasis> in each of our
<literal>suse-edge/fleet-examples</literal> <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">releases</link>
under
<literal>bundles/day2/system-upgrade-controller/controller-bundle.yaml</literal>.</para>
<important>
<para>If using the <literal>suse-edge/fleet-examples</literal> repository, make
sure you are using the resources from a dedicated <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag.</para>
</important>
<para><literal>Bundle</literal> creation can be done in one of of the following
ways:</para>
<itemizedlist>
<listitem>
<para>Through the Rancher UI (<xref linkend="day2-suc-dep-bundle-rancher"/>) (when
<literal>Rancher</literal> is available)</para>
</listitem>
<listitem>
<para>By manually deploying (<xref linkend="day2-suc-dep-bundle-manual"/>) the
resources to your <literal>management cluster</literal></para>
</listitem>
</itemizedlist>
<para>Once created, <literal>Fleet</literal> will be responsible for pickuping the
resource and deploying the <emphasis role="strong">SUC</emphasis> resources
to all your <emphasis role="strong">target</emphasis> clusters. For
information on how to track the deployment process, see <xref
linkend="monitor-suc-deployment"/>.</para>
<section xml:id="day2-suc-dep-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<itemizedlist>
<listitem>
<para>By manually copying the file content to the <emphasis role="strong">Create
from YAML</emphasis> page. File content can be retrieved from this url -
<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller/controller-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller/controller-bundle.yaml</link>.
Where <literal>${REVISION}</literal> is the Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you desire (e.g. <literal>release-3.0.1</literal>).</para>
</listitem>
<listitem>
<para>By cloning the <literal>suse-edge/fleet-examples</literal> repository to the
desired <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag and selecting the <emphasis role="strong">Read from File</emphasis>
option in the <emphasis role="strong">Create from YAML</emphasis> page. From
there, navigate to <literal>bundles/day2/system-upgrade-controller</literal>
directory and select <literal>controller-bundle.yaml</literal>. This will
auto-populate the <emphasis role="strong">Create from YAML</emphasis> page
with the Bundle content.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>To match all downstream clusters change the default Bundle
<literal>.spec.targets</literal> to:</para>
<screen language="bash" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>For a more granular downstream cluster mappings, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-suc-dep-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Choose the desired Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to deploy the <emphasis role="strong">SUC</emphasis>
<literal>Bundle</literal> from (referenced below as
<literal>${REVISION}</literal>).</para>
</listitem>
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o controller-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller/controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> <emphasis role="strong">target</emphasis>
configurations, under <literal>spec.targets</literal> provide your desired
target list. By default the <literal>Bundle</literal> resources from the
<literal>suse-edge/fleet-examples</literal> are <emphasis
role="strong">NOT</emphasis> mapped to any down stream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>Bundle</literal> <emphasis
role="strong">target</emphasis> to:</para>
<screen language="bash" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resource to your
<literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f controller-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under
the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles system-upgrade-controller -n fleet-default

# Example output
NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-suc-third-party-gitops">
<title>Deploying system-upgrade-controller when using a third-party GitOps workflow</title>
<para>To deploy the <literal>system-upgrade-controller</literal> using a
third-party GitOps tool, depending on the tool, you might need information
for the <literal>system-upgrade-controller</literal> Helm chart or
Kubernetes resoruces, or both.</para>
<para>Choose a specific Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
from which you wish to use the <emphasis role="strong">SUC</emphasis> from.</para>
<para>From there, the <emphasis role="strong">SUC</emphasis> Helm chart data can
be found under the <literal>helm</literal> configuration section of the
<literal>fleets/day2/system-upgrade-controller/fleet.ymal</literal> file.</para>
<para>The <emphasis role="strong">SUC</emphasis> Kubernetes resources can be found
under the <emphasis role="strong">SUC</emphasis> <literal>Bundle</literal>
configuration under <literal>.spec.resources.content</literal>. The location
for the bundle is
<literal>bundles/day2/system-upgrade-controller/controller-bundle.yaml</literal>.</para>
<para>Use the above mentioned resoruces to populate the data that your third-party
GitOps workflow needs in order to deploy <emphasis
role="strong">SUC</emphasis>.</para>
</section>
</section>
<section xml:id="id-monitor-suc-resources-using-rancher">
<title>Monitor SUC resources using Rancher</title>
<para>This section covers how to monitor the lifecycle of the <emphasis
role="strong">SUC</emphasis> deployment and any deployed <emphasis
role="strong">SUC</emphasis> Plans using the Rancher UI.</para>
<section xml:id="monitor-suc-deployment">
<title>Monitor SUC deployment</title>
<para>To check the <emphasis role="strong">SUC</emphasis> pod logs for a specific
cluster:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ →
&lt;your-cluster-name&gt;</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Workloads → Pods</emphasis></para>
</listitem>
<listitem>
<para>Under the namespace drop down menu select the
<literal>cattle-system</literal> namespace</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-1.png"
width=""/> </imageobject>
<textobject><phrase>day2 monitor suc deployment 1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>In the Pod filter bar, write the <emphasis role="strong">SUC</emphasis> name
- <literal>system-upgrade-controller</literal></para>
</listitem>
<listitem>
<para>On the right of the pod select <emphasis role="strong">⋮ → View
Logs</emphasis></para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-2.png"
width=""/> </imageobject>
<textobject><phrase>day2 monitor suc deployment 2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>The <emphasis role="strong">SUC</emphasis> logs should looks something
similar to:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-3.png"
width=""/> </imageobject>
<textobject><phrase>day2 monitor suc deployment 3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
<section xml:id="monitor-suc-plans">
<title>Monitor SUC Plans</title>
<important>
<para>The <emphasis role="strong">SUC Plan</emphasis> Pods are kept alive for
<emphasis role="strong">15</emphasis> minutes. After that they are removed
by the corresponding Job that created them. To have access to the <emphasis
role="strong">SUC Plan</emphasis> Pod logs, you should enable logging for
your cluster. For information on how to do this in Rancher, see <link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/logging">Rancher
Integration with Logging Services</link>.</para>
</important>
<para>To check <emphasis role="strong">Pod</emphasis> logs for the specific
<emphasis role="strong">SUC</emphasis> plan:</para>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ →
&lt;your-cluster-name&gt;</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Workloads → Pods</emphasis></para>
</listitem>
<listitem>
<para>Under the namespace drop down menu select the
<literal>cattle-system</literal> namespace</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-1.png"
width=""/> </imageobject>
<textobject><phrase>day2 monitor suc deployment 1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>In the Pod filter bar, write the name for your <emphasis role="strong">SUC
Plan</emphasis> Pod. The name will be in the following template format:
<literal>apply-&lt;plan_name&gt;-on-&lt;node_name&gt;</literal></para>
<figure>
<title>Example Kubernetes upgrade plan pods</title>
<mediaobject>
<imageobject> <imagedata fileref="day2-k8s-plan-monitor.png" width=""/>
</imageobject>
<textobject><phrase>day2 k8s plan monitor</phrase></textobject>
</mediaobject></figure>
<para>Note how in <emphasis>Figure 1</emphasis>, we have one Pod in <emphasis
role="strong">Completed</emphasis> and one in <emphasis
role="strong">Unknown</emphasis> state. This is expected and has happened
due to the Kubernetes version upgrade on the node.</para>
<figure>
<title>Example OS pacakge update plan pods</title>
<mediaobject>
<imageobject> <imagedata fileref="day2-os-pkg-plan-monitor.png" width=""/>
</imageobject>
<textobject><phrase>day2 os pkg plan monitor</phrase></textobject>
</mediaobject></figure>
<figure>
<title>Example of upgrade plan pods for EIB deployed Helm charts on an HA cluster</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_chart_upgrade_plan_monitor.png"
width=""/> </imageobject>
<textobject><phrase>day2 chart upgrade plan monitor</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>Select the pod that you want to review the logs of and navigate to <emphasis
role="strong">⋮ → View Logs</emphasis></para>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="day2-os-package-update">
<title>OS package update</title>
<section xml:id="id-components">
<title>Components</title>
<para>This section covers the custom components that the <literal>OS package
update</literal> process uses over the default <literal>Day 2</literal>
components (<xref linkend="day2-downstream-components"/>).</para>
<section xml:id="id-edge-update-service">
<title>edge-update.service</title>
<para>Systemd service responsible for performing the <literal>OS package
update</literal>. Uses the <link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>
command to perform a <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Distribution_upgrade">distribution
upgrade</link> (<literal>dup</literal>).</para>
<note>
<para>If you wish to use a <link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">normal
upgrade</link> method, create a <literal>edge-update.conf</literal> file
under <literal>/etc/edge/</literal> on each node. Inside this file, add the
<literal>UPDATE_METHOD=up</literal> variable.</para>
</note>
<para>Shipped through a <emphasis role="strong">SUC plan</emphasis>, which should
be located on each <emphasis role="strong">downstream cluster</emphasis>
that is in need of a OS package update.</para>
</section>
</section>
<section xml:id="id-requirements">
<title>Requirements</title>
<para><emphasis>General:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC registered machine</emphasis> - All downstream
cluster nodes should be registered to <literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>. This
is needed so that the <literal>edge-update.service</literal> can
successfully connect to the needed OS RPM repositories.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Make sure that SUC Plan tolerations match node
tolerations</emphasis> - If your Kubernetes cluster nodes have custom
<emphasis role="strong">taints</emphasis>, make sure to add <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</link>
for those taints in the <emphasis role="strong">SUC Plans</emphasis>. By
default <emphasis role="strong">SUC Plans</emphasis> have tolerations only
for <emphasis role="strong">control-plane</emphasis> nodes. Default
tolerations include:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Any additional tolerations must be added under the
<literal>.spec.tolerations</literal> section of each Plan. <emphasis
role="strong">SUC Plans</emphasis> related to the OS package update can be
found in the <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
repository under
<literal>fleets/day2/system-upgrade-controller-plans/os-pkg-update</literal>.
<emphasis role="strong">Make sure you use the Plans from a valid repository
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag.</emphasis></para>
<para>An example of defining custom tolerations for the <emphasis
role="strong">control-plane</emphasis> SUC Plan, would look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-pkg-plan-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>Air-gapped:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Mirror SUSE RPM repositories</emphasis> - OS RPM
repositories should be locally mirrored so that
<literal>edge-update.service</literal> can have access to them. This can be
achieved using <link xl:href="https://github.com/SUSE/rmt">RMT</link>.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-update-procedure">
<title>Update procedure</title>
<note>
<para>This section assumes you will be deploying the <literal>OS package
update</literal> <emphasis role="strong">SUC Plan</emphasis> using Fleet
(<xref linkend="components-fleet"/>). If you intend to deploy the <emphasis
role="strong">SUC Plan</emphasis> using a different approach, refer to <xref
linkend="os-pkg-suc-plan-deployment-third-party"/>.</para>
</note>
<para>The <literal>OS package update procedure</literal> revolves around deploying
<emphasis role="strong">SUC Plans</emphasis> to downstream clusters. These
plans then hold information about how and on which nodes to deploy the
<literal>edge-update.service</literal> systemd.service. For information
regarding the structure of a <emphasis role="strong">SUC Plan</emphasis>,
refer to the <link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">upstream</link>
documentation.</para>
<para><literal>OS package update</literal> SUC Plans are shipped in the following
ways:</para>
<itemizedlist>
<listitem>
<para>Through a <literal>GitRepo</literal> resources - <xref
linkend="os-pkg-suc-plan-deployment-git-repo"/></para>
</listitem>
<listitem>
<para>Through a <literal>Bundle</literal> resource - <xref
linkend="os-pkg-suc-plan-deployment-bundle"/></para>
</listitem>
</itemizedlist>
<para>To determine which resource you should use, refer to <xref
linkend="day2-determine-use-case"/>.</para>
<para>For a full overview of what happens during the <emphasis>update
procedure</emphasis>, refer to the <xref linkend="os-pkg-update-overview"/>
section.</para>
<section xml:id="os-pkg-update-overview">
<title>Overview</title>
<para>This section aims to describe the full workflow that the <emphasis
role="strong"><emphasis>OS package update process</emphasis></emphasis> goes
throught from start to finish.</para>
<figure>
<title>OS package update workflow</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_os_pkg_update_diagram.png" width=""/>
</imageobject>
<textobject><phrase>day2 os pkg update diagram</phrase></textobject>
</mediaobject></figure>
<para>OS pacakge update steps:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Based on his use-case, the user determines whether to use a <emphasis
role="strong">GitRepo</emphasis> or a <emphasis
role="strong">Bundle</emphasis> resource for the deployment of the
<literal>OS package update SUC Plans</literal> to the desired downstream
clusters. For information on how to map a <emphasis
role="strong">GitRepo/Bundle</emphasis> to a specific set of downstream
clusters, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link>.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>If you are unsure whether you should use a <emphasis
role="strong">GitRepo</emphasis> or a <emphasis
role="strong">Bundle</emphasis> resource for the <emphasis role="strong">SUC
Plan</emphasis> deployment, refer to <xref
linkend="day2-determine-use-case"/>.</para>
</listitem>
<listitem>
<para>For <emphasis role="strong">GitRepo/Bundle</emphasis> configuration options,
refer to <xref linkend="os-pkg-suc-plan-deployment-git-repo"/> or <xref
linkend="os-pkg-suc-plan-deployment-bundle"/>.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>The user deploys the configured <emphasis
role="strong">GitRepo/Bundle</emphasis> resource to the
<literal>fleet-default</literal> namespace in his <literal>management
cluster</literal>. This is done either <emphasis
role="strong">manually</emphasis> or thorugh the <emphasis
role="strong">Rancher UI</emphasis> if such is available.</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>) constantly monitors the
<literal>fleet-default</literal> namespace and immediately detects the newly
deployed <emphasis role="strong">GitRepo/Bundle</emphasis> resource. For
more information regarding what namespaces does Fleet monitor, refer to
Fleet’s <link
xl:href="https://fleet.rancher.io/namespaces">Namespaces</link>
documentation.</para>
</listitem>
<listitem>
<para>If the user has deployed a <emphasis role="strong">GitRepo</emphasis>
resource, <literal>Fleet</literal> will reconcile the <emphasis
role="strong">GitRepo</emphasis> and based on its <emphasis
role="strong">paths</emphasis> and <emphasis
role="strong">fleet.yaml</emphasis> configurations it will deploy a
<emphasis role="strong">Bundle</emphasis> resource in the
<literal>fleet-default</literal> namespace. For more information, refer to
Fleet’s <link xl:href="https://fleet.rancher.io/gitrepo-content">GitRepo
Contents</link> documentation.</para>
</listitem>
<listitem>
<para><literal>Fleet</literal> then proceeds to deploy the <literal>Kubernetes
resources</literal> from this <emphasis role="strong">Bundle</emphasis> to
all the targeted <literal>downstream clusters</literal>. In the context of
<literal>OS package updates</literal>, Fleet deploys the following resources
from the <emphasis role="strong">Bundle</emphasis>:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>os-pkg-plan-agent</literal> <emphasis role="strong">SUC
Plan</emphasis> - instructs <emphasis role="strong">SUC</emphasis> on how to
do a package update on cluster <emphasis
role="strong"><emphasis>agent</emphasis></emphasis> nodes. Will <emphasis
role="strong">not</emphasis> be interpreted if the cluster consists only
from <emphasis>control-plane</emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>os-pkg-plan-control-plane</literal> <emphasis role="strong">SUC
Plan</emphasis> - instructs <emphasis role="strong">SUC</emphasis> on how to
do a package update on cluster <emphasis
role="strong"><emphasis>control-plane</emphasis></emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>os-pkg-update</literal> <emphasis role="strong">Secret</emphasis> -
referenced in each <emphasis role="strong">SUC Plan</emphasis>; ships an
<literal>update.sh</literal> script responsible for creating the
<literal>edge-update.service</literal> <emphasis
role="strong"><emphasis>sustemd.service</emphasis></emphasis> which will do
the actual package update.</para>
<note>
<para>The above resources will be deployed in the <literal>cattle-system</literal>
namespace of each downstream cluster.</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>On the downstream cluster, <emphasis role="strong">SUC</emphasis> picks up
the newly deployed <emphasis role="strong">SUC Plans</emphasis> and deploys
an <emphasis role="strong"><emphasis>Update Pod</emphasis></emphasis> on
each node that matches the <emphasis role="strong">node selector</emphasis>
defined in the <emphasis role="strong">SUC Plan</emphasis>. For information
how to monitor the <emphasis role="strong">SUC Plan Pod</emphasis>, refer to
<xref linkend="monitor-suc-plans"/>.</para>
</listitem>
<listitem>
<para>The <emphasis role="strong">Update Pod</emphasis> (deployed on each node)
<emphasis role="strong">mounts</emphasis> the
<literal>os-pkg-update</literal> Secret and <emphasis
role="strong">executes</emphasis> the <literal>update.sh</literal> script
that the Secret ships.</para>
</listitem>
<listitem>
<para>The <literal>update.sh</literal> proceeds to do the following:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Create the <literal>edge-update.service</literal> - the service created will
be of type <emphasis role="strong">oneshot</emphasis> and will adopt the
following workflow:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Update all package versions on the node OS, by executing:</para>
<screen language="bash" linenumbering="unnumbered">transactional-update cleanup dup</screen>
</listitem>
<listitem>
<para>After a successful <literal>transactional-update</literal>, shedule a system
<emphasis role="strong">reboot</emphasis> so that the package version
updates can take effect</para>
<note>
<para>System reboot will be scheduled for <emphasis role="strong">1
minute</emphasis> after a successful <literal>transactional-update</literal>
execution.</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Start the <literal>edge-update.service</literal> and wait for it to complete</para>
</listitem>
<listitem>
<para>Cleanup the <literal>edge-update.service</literal> - after the <emphasis
role="strong"><emphasis>systemd.service</emphasis></emphasis> has done its
job, it is removed from the system in order to ensure that no accidental
executions/reboots happen in the future.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>The OS package update procedure finishes with the <emphasis
role="strong"><emphasis>system reboot</emphasis></emphasis>. After the
reboot all OS package versions should be updated to their respective latest
version as seen in the available OS RPM repositories.</para>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment">
<title>OS package update - SUC Plan deployment</title>
<para>This section describes how to orchestrate the deployment of <emphasis
role="strong">SUC Plans</emphasis> related OS package updates using Fleet’s
<emphasis role="strong">GitRepo</emphasis> and <emphasis
role="strong">Bundle</emphasis> resources.</para>
<section xml:id="os-pkg-suc-plan-deployment-git-repo">
<title>SUC Plan deployment - GitRepo resource</title>
<para>A <emphasis role="strong">GitRepo</emphasis> resource, that ships the needed
<literal>OS package update</literal> <emphasis role="strong">SUC
Plans</emphasis>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref
linkend="os-pkg-suc-plan-deployment-git-repo-rancher"/> (when
<literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref
linkend="os-pkg-suc-plan-deployment-git-repo-manual"/>) the resource to your
<literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the OS package update process of the nodes of your
targeted cluster, refer to the <xref linkend="monitor-suc-plans"/>
documentation.</para>
<section xml:id="os-pkg-suc-plan-deployment-git-repo-rancher">
<title>GitRepo creation - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Git Repos → Add Repository</emphasis></para>
</listitem>
</orderedlist>
<para>If you use the <literal>suse-edge/fleet-examples</literal> repository:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Repository URL</emphasis> - <literal><link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Watch → Revision</emphasis> - choose a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag for the <literal>suse-edge/fleet-examples</literal> repository that you
wish to use</para>
</listitem>
<listitem>
<para>Under <emphasis role="strong">Paths</emphasis> add the path to the OS
package update Fleets that you wish to use -
<literal>fleets/day2/system-upgrade-controller-plans/os-pkg-update</literal></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Next</emphasis> to move to the <emphasis
role="strong">target</emphasis> configuration section. <emphasis
role="strong">Only select clusters whose node’s packages you wish to
upgrade</emphasis></para>
</listitem>
<listitem>
<para><emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
<para>Alternatively, if you decide to use your own repository to host these files,
you would need to provide your repo data above.</para>
</section>
<section xml:id="os-pkg-suc-plan-deployment-git-repo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Choose the desired Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to apply the OS <emphasis role="strong">SUC update
Plans</emphasis> from (referenced below as <literal>${REVISION}</literal>).</para>
</listitem>
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-pkg-update-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/os-pkg-update-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configuration, under
<literal>spec.targets</literal> specify your desired target list. By default
the <literal>GitRepo</literal> resources from the
<literal>suse-edge/fleet-examples</literal> are <emphasis
role="strong">NOT</emphasis> mapped to any down stream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>GitRepo</literal>
<emphasis role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resources to your
<literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-pkg-update-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under
the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-pkg-update -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-pkg-update   https://github.com/suse-edge/fleet-examples.git   release-3.0.1  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment-bundle">
<title>SUC Plan deployment - Bundle resource</title>
<para>A <emphasis role="strong">Bundle</emphasis> resource, that ships the needed
<literal>OS package update</literal> <emphasis role="strong">SUC
Plans</emphasis>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref
linkend="os-pkg-suc-plan-deployment-bundle-rancher"/> (when
<literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref
linkend="os-pkg-suc-plan-deployment-bundle-manual"/>) the resource to your
<literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the OS package update process of the nodes of your
targeted cluster, refer to the <xref linkend="monitor-suc-plans"/>
documentation.</para>
<section xml:id="os-pkg-suc-plan-deployment-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, click <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>By manually copying the <emphasis role="strong">Bundle</emphasis> content to
the <emphasis role="strong">Create from YAML</emphasis> page. Content can be
retrieved from <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml</link>,
where <literal>${REVISION}</literal> is the Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
that you are using</para>
</listitem>
<listitem>
<para>By cloning the <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
repository to the desired <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag and selecting the <emphasis role="strong">Read from File</emphasis>
option in the <emphasis role="strong">Create from YAML</emphasis> page. From
there, navigate to
<literal>bundles/day2/system-upgrade-controller-plans/os-pkg-update</literal>
directory and select <literal>pkg-update-bundle.yaml</literal>. This will
auto-populate the <emphasis role="strong">Create from YAML</emphasis> page
with the Bundle content.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>To match all downstream clusters change the default Bundle
<literal>.spec.targets</literal> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>For a more granular downstream cluster mappings, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="os-pkg-suc-plan-deployment-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Choose the desired Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to apply the OS package update <emphasis role="strong">SUC
Plans</emphasis> from (referenced below as <literal>${REVISION}</literal>).</para>
</listitem>
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resource:</para>
<screen language="bash" linenumbering="unnumbered">curl -o pkg-update-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml</screen>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> <emphasis role="strong">target</emphasis>
configurations, under <literal>spec.targets</literal> provide your desired
target list. By default the <literal>Bundle</literal> resources from the
<literal>suse-edge/fleet-examples</literal> are <emphasis
role="strong">NOT</emphasis> mapped to any down stream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>Bundle</literal> <emphasis
role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resources to your
<literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f pkg-update-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under
the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles os-pkg-update -n fleet-default

# Example output
NAME            BUNDLEDEPLOYMENTS-READY   STATUS
os-pkg-update   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment-third-party">
<title>SUC Plan deployment - third-party GitOps workflow</title>
<para>There might be use-cases where users would like to incorporate the OS
package update <emphasis role="strong">SUC Plans</emphasis> to their own
third-party GitOps workflow (e.g. <literal>Flux</literal>).</para>
<para>To get the OS package update resources that you need, first determine the
Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag of the <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
repository that you would like to use.</para>
<para>After that, resources can be found at
<literal>fleets/day2/system-upgrade-controller-plans/os-pkg-update</literal>,
where:</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> -
<literal>system-upgrade-controller</literal> Plan resource for <emphasis
role="strong">control-plane</emphasis> nodes</para>
</listitem>
<listitem>
<para><literal>plan-agent.yaml</literal> -
<literal>system-upgrade-controller</literal> Plan resource for <emphasis
role="strong">agent</emphasis> nodes</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> - secret that ships a script that creates the
<literal>edge-update.service</literal> <link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link></para>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the
<literal>system-upgrade-controller</literal> and should be deployed on each
downstream cluster that you wish to upgrade. For information on how to
deploy the <literal>system-upgrade-controller</literal>, see <xref
linkend="day2-suc-third-party-gitops"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the
<emphasis role="strong">SUC Plans</emphasis> for OS package update, it can
be beneficial to take a look at the overview (<xref
linkend="os-pkg-update-overview"/>) of the update procedure using
<literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="day2-k8s-upgrade">
<title>Kubernetes version upgrade</title>
<important>
<para>This section covers Kubernetes upgrades for downstream clusters that have
<emphasis role="strong">NOT</emphasis> been created through a Rancher (<xref
linkend="components-rancher"/>) instance. For information on how to upgrade
the Kubernetes version of <literal>Rancher</literal> created clusters, see
<link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes</link>.</para>
</important>
<section xml:id="id-components-2">
<title>Components</title>
<para>This section covers the custom components that the <literal>Kubernetes
upgrade</literal> process uses over the default <literal>Day 2</literal>
components (<xref linkend="day2-downstream-components"/>).</para>
<section xml:id="id-rke2-upgrade-2">
<title>rke2-upgrade</title>
<para>Image responsible for upgrading the RKE2 version of a specific node.</para>
<para>Shipped through a Pod created by <emphasis role="strong">SUC</emphasis>
based on a <emphasis role="strong">SUC Plan</emphasis>. The Plan should be
located on each <emphasis role="strong">downstream cluster</emphasis> that
is in need of a RKE2 upgrade.</para>
<para>For more information regarding how the <literal>rke2-upgrade</literal> image
performs the upgrade, see the <link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">upstream</link>
documentation.</para>
</section>
<section xml:id="id-k3s-upgrade">
<title>k3s-upgrade</title>
<para>Image responsible for upgrading the K3s version of a specific node.</para>
<para>Shipped through a Pod created by <emphasis role="strong">SUC</emphasis>
based on a <emphasis role="strong">SUC Plan</emphasis>. The Plan should be
located on each <emphasis role="strong">downstream cluster</emphasis> that
is in need of a K3s upgrade.</para>
<para>For more information regarding how the <literal>k3s-upgrade</literal> image
performs the upgrade, see the <link
xl:href="https://github.com/k3s-io/k3s-upgrade">upstream</link>
documentation.</para>
</section>
</section>
<section xml:id="id-requirements-2">
<title>Requirements</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Backup your Kubernetes distribution:</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para>For <emphasis role="strong">imported RKE2 clusters</emphasis>, see the <link
xl:href="https://docs.rke2.io/backup_restore">RKE2 Backup and Restore</link>
documentation.</para>
</listitem>
<listitem>
<para>For <emphasis role="strong">imported K3s clusters</emphasis>, see the <link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3s Backup and
Restore</link> documentation.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Make sure that SUC Plan tolerations match node
tolerations</emphasis> - If your Kubernetes cluster nodes have custom
<emphasis role="strong">taints</emphasis>, make sure to add <link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">tolerations</link>
for those taints in the <emphasis role="strong">SUC Plans</emphasis>. By
default <emphasis role="strong">SUC Plans</emphasis> have tolerations only
for <emphasis role="strong">control-plane</emphasis> nodes. Default
tolerations include:</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>Any additional tolerations must be added under the
<literal>.spec.tolerations</literal> section of each Plan. <emphasis
role="strong">SUC Plans</emphasis> related to the Kubernetes version upgrade
can be found in the <link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>
repository under:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Make sure you use the Plans from a valid repository
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag.</emphasis></para>
<para>An example of defining custom tolerations for the RKE2 <emphasis
role="strong">control-plane</emphasis> SUC Plan, would look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-plan-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade-procedure">
<title>Upgrade procedure</title>
<note>
<para>This section assumes you will be deploying <emphasis role="strong">SUC
Plans</emphasis> using Fleet (<xref linkend="components-fleet"/>). If you
intend to deploy the <emphasis role="strong">SUC Plan</emphasis> using a
different approach, refer to <xref
linkend="k8s-upgrade-suc-plan-deployment-third-party"/>.</para>
</note>
<para>The <literal>Kubernetes version upgrade procedure</literal> revolves around
deploying <emphasis role="strong">SUC Plans</emphasis> to downstream
clusters. These plans hold information that instructs the <emphasis
role="strong">SUC</emphasis> on which nodes to create Pods which run the
<literal>rke2/k3s-upgrade</literal> images. For information regarding the
structure of a <emphasis role="strong">SUC Plan</emphasis>, refer to the
<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">upstream</link>
documentation.</para>
<para><literal>Kubernetes upgrade</literal> Plans are shipped in the following
ways:</para>
<itemizedlist>
<listitem>
<para>Through a <literal>GitRepo</literal> resources - <xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo"/></para>
</listitem>
<listitem>
<para>Through a <literal>Bundle</literal> resource - <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle"/></para>
</listitem>
</itemizedlist>
<para>To determine which resource you should use, refer to <xref
linkend="day2-determine-use-case"/>.</para>
<para>For a full overview of what happens during the <emphasis>update
procedure</emphasis>, refer to the <xref
linkend="k8s-version-upgrade-overview"/> section.</para>
<section xml:id="k8s-version-upgrade-overview">
<title>Overview</title>
<para>This section aims to describe the full workflow that the <emphasis
role="strong"><emphasis>Kubernetes version upgrade
process</emphasis></emphasis> goes throught from start to finish.</para>
<figure>
<title>Kubernetes version upgrade workflow</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_k8s_version_upgrade_diagram.png"
width=""/> </imageobject>
<textobject><phrase>day2 k8s version upgrade diagram</phrase></textobject>
</mediaobject></figure>
<para>Kubernetes version upgrade steps:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Based on his use-case, the user determines whether to use a <emphasis
role="strong">GitRepo</emphasis> or a <emphasis
role="strong">Bundle</emphasis> resource for the deployment of the
<literal>Kubernetes upgrade SUC Plans</literal> to the desired downstream
clusters. For information on how to map a <emphasis
role="strong">GitRepo/Bundle</emphasis> to a specific set of downstream
clusters, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link>.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>If you are unsure whether you should use a <emphasis
role="strong">GitRepo</emphasis> or a <emphasis
role="strong">Bundle</emphasis> resource for the <emphasis role="strong">SUC
Plan</emphasis> deployment, refer to <xref
linkend="day2-determine-use-case"/>.</para>
</listitem>
<listitem>
<para>For <emphasis role="strong">GitRepo/Bundle</emphasis> configuration options,
refer to <xref linkend="k8s-upgrade-suc-plan-deployment-git-repo"/> or <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle"/>.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>The user deploys the configured <emphasis
role="strong">GitRepo/Bundle</emphasis> resource to the
<literal>fleet-default</literal> namespace in his <literal>management
cluster</literal>. This is done either <emphasis
role="strong">manually</emphasis> or thorugh the <emphasis
role="strong">Rancher UI</emphasis> if such is available.</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>) constantly monitors the
<literal>fleet-default</literal> namespace and immediately detects the newly
deployed <emphasis role="strong">GitRepo/Bundle</emphasis> resource. For
more information regarding what namespaces does Fleet monitor, refer to
Fleet’s <link
xl:href="https://fleet.rancher.io/namespaces">Namespaces</link>
documentation.</para>
</listitem>
<listitem>
<para>If the user has deployed a <emphasis role="strong">GitRepo</emphasis>
resource, <literal>Fleet</literal> will reconcile the <emphasis
role="strong">GitRepo</emphasis> and based on its <emphasis
role="strong">paths</emphasis> and <emphasis
role="strong">fleet.yaml</emphasis> configurations it will deploy a
<emphasis role="strong">Bundle</emphasis> resource in the
<literal>fleet-default</literal> namespace. For more information, refer to
Fleet’s <link xl:href="https://fleet.rancher.io/gitrepo-content">GitRepo
Contents</link> documentation.</para>
</listitem>
<listitem>
<para><literal>Fleet</literal> then proceeds to deploy the <literal>Kubernetes
resources</literal> from this <emphasis role="strong">Bundle</emphasis> to
all the targeted <literal>downstream clusters</literal>. In the context of
the <literal>Kubernetes version upgrade</literal>, Fleet deploys the
following resources from the <emphasis role="strong">Bundle</emphasis>
(depending on the Kubernetes distrubution):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>rke2-plan-agent</literal>/<literal>k3s-plan-agent</literal> -
instructs <emphasis role="strong">SUC</emphasis> on how to do a Kubernetes
upgrade on cluster <emphasis
role="strong"><emphasis>agent</emphasis></emphasis> nodes. Will <emphasis
role="strong">not</emphasis> be interpreted if the cluster consists only
from <emphasis>control-plane</emphasis> nodes.</para>
</listitem>
<listitem>
<para><literal>rke2-plan-control-plane</literal>/<literal>k3s-plan-control-plane</literal>
- instructs <emphasis role="strong">SUC</emphasis> on how to do a Kubernetes
upgrade on cluster <emphasis
role="strong"><emphasis>control-plane</emphasis></emphasis> nodes.</para>
<note>
<para>The above <emphasis role="strong">SUC Plans</emphasis> will be deployed in
the <literal>cattle-system</literal> namespace of each downstream cluster.</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>On the downstream cluster, <emphasis role="strong">SUC</emphasis> picks up
the newly deployed <emphasis role="strong">SUC Plans</emphasis> and deploys
an <emphasis role="strong"><emphasis>Update Pod</emphasis></emphasis> on
each node that matches the <emphasis role="strong">node selector</emphasis>
defined in the <emphasis role="strong">SUC Plan</emphasis>. For information
how to monitor the <emphasis role="strong">SUC Plan Pod</emphasis>, refer to
<xref linkend="monitor-suc-plans"/>.</para>
</listitem>
<listitem>
<para>Depending on which <emphasis role="strong">SUC Plans</emphasis> you have
deployed, the <emphasis role="strong">Update Pod</emphasis> will run either
a <link
xl:href="https://hub.docker.com/r/rancher/rke2-upgrade/tags">rke2-upgrade</link>
or a <link
xl:href="https://hub.docker.com/r/rancher/k3s-upgrade/tags">k3s-upgrade</link>
image and will execute the following workflow on <emphasis
role="strong">each</emphasis> cluster node:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">Cordon</link>
cluster node - to ensure that no pods are scheduled accidentally on this
node while it is being upgraded, we mark it as
<literal>unschedulable</literal>.</para>
</listitem>
<listitem>
<para>Replace the <literal>rke2/k3s</literal> binary that is installed on the node
OS with the binary shipped by the
<literal>rke2-upgrade/k3s-upgrade</literal> image that the Pod is currently
running.</para>
</listitem>
<listitem>
<para>Kill the <literal>rke2/k3s</literal> process that is running on the node OS
- this instructs the <emphasis role="strong">supervisor</emphasis> to
automatically restart the <literal>rke2/k3s</literal> process using the new
version.</para>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/">Uncordon</link>
cluster node - after the successful Kubernetes distribution upgrade, the
node is again marked as <literal>scheduable</literal>.</para>
<note>
<para>For further information regarding how the <literal>rke2-upgrade</literal>
and <literal>k3s-upgrade</literal> images work, see the <link
xl:href="https://github.com/rancher/rke2-upgrade">rke2-upgrade</link> and
<link xl:href="https://github.com/k3s-io/k3s-upgrade">k3s-upgrade</link>
upstream projects.</para>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>With the above steps executed, the Kubernetes version of each cluster node
should have been upgraded to the desired Edge compatible <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>.</para>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment">
<title>Kubernetes version upgrade - SUC Plan deployment</title>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo">
<title>SUC Plan deployment - GitRepo resource</title>
<para>A <emphasis role="strong">GitRepo</emphasis> resource, that ships the needed
<literal>Kubernetes upgrade</literal> <emphasis role="strong">SUC
Plans</emphasis>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo-rancher"/> (when
<literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo-manual"/>) the resource to
your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the Kubernetes upgrade process of the nodes of
your targeted cluster, refer to the <xref linkend="monitor-suc-plans"/>
documentation.</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo-rancher">
<title>GitRepo creation - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Git Repos → Add Repository</emphasis></para>
</listitem>
</orderedlist>
<para>If you use the <literal>suse-edge/fleet-examples</literal> repository:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Repository URL</emphasis> - <literal><link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">Watch → Revision</emphasis> - choose a <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag for the <literal>suse-edge/fleet-examples</literal> repository that you
wish to use</para>
</listitem>
<listitem>
<para>Under <emphasis role="strong">Paths</emphasis> add the path to the
Kubernetes distribution upgrade Fleets as seen in the release tag:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>For RKE2 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>For K3s -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Next</emphasis> to move to the <emphasis
role="strong">target</emphasis> configuration section. <emphasis
role="strong">Only select clusters for which you wish to upgrade the desired
Kubernetes distribution</emphasis></para>
</listitem>
<listitem>
<para><emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
<para>Alternatively, if you decide to use your own repository to host these files,
you would need to provide your repo data above.</para>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo-manual">
<title>GitRepo creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Choose the desired Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to apply the Kubernetes <emphasis role="strong">SUC
upgrade Plans</emphasis> from (referenced below as
<literal>${REVISION}</literal>).</para>
</listitem>
<listitem>
<para>Pull the <emphasis role="strong">GitRepo</emphasis> resource:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edit the <emphasis role="strong">GitRepo</emphasis> configuration, under
<literal>spec.targets</literal> specify your desired target list. By default
the <literal>GitRepo</literal> resources from the
<literal>suse-edge/fleet-examples</literal> are <emphasis
role="strong">NOT</emphasis> mapped to any down stream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>GitRepo</literal>
<emphasis role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">GitRepo</emphasis> resources to your
<literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">GitRepo</emphasis> resource under
the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   release-3.0.1   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   release-3.0.1   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle">
<title>SUC Plan deployment - Bundle resource</title>
<para>A <emphasis role="strong">Bundle</emphasis> resource, that ships the needed
<literal>Kubernetes upgrade</literal> <emphasis role="strong">SUC
Plans</emphasis>, can be deployed in one of the following ways:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Through the <literal>Rancher UI</literal> - <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle-rancher"/> (when
<literal>Rancher</literal> is available).</para>
</listitem>
<listitem>
<para>By manually deploying (<xref
linkend="k8s-upgrade-suc-plan-deployment-bundle-manual"/>) the resource to
your <literal>management cluster</literal>.</para>
</listitem>
</orderedlist>
<para>Once deployed, to monitor the Kubernetes upgrade process of the nodes of
your targeted cluster, refer to the <xref linkend="monitor-suc-plans"/>
documentation.</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle-rancher">
<title>Bundle creation - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>In the upper left corner, click <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Advanced</emphasis> &gt; <emphasis
role="strong">Bundles</emphasis></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Create from YAML</emphasis></para>
</listitem>
<listitem>
<para>From here you can create the Bundle in one of the following ways:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>By manually copying the <emphasis role="strong">Bundle</emphasis> content to
the <emphasis role="strong">Create from YAML</emphasis> page. Content can be
retrieved:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>For RKE2 - <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</link></para>
</listitem>
<listitem>
<para>For K3s - <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</link></para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>By cloning the <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
repository to the desired <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag and selecting the <emphasis role="strong">Read from File</emphasis>
option in the <emphasis role="strong">Create from YAML</emphasis> page. From
there, navigate to the bundle that you need
(<literal>/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>
for RKE2 and
<literal>/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>
for K3s). This will auto-populate the <emphasis role="strong">Create from
YAML</emphasis> page with the Bundle content</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Change the <emphasis role="strong">target</emphasis> clusters for the
<literal>Bundle</literal>:</para>
<itemizedlist>
<listitem>
<para>To match all downstream clusters change the default Bundle
<literal>.spec.targets</literal> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>For a more granular downstream cluster mappings, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link>.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle-manual">
<title>Bundle creation - manual</title>
<orderedlist numeration="arabic">
<listitem>
<para>Choose the desired Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to apply the Kubernetes <emphasis role="strong">SUC
upgrade Plans</emphasis> from (referenced below as
<literal>${REVISION}</literal>).</para>
</listitem>
<listitem>
<para>Pull the <emphasis role="strong">Bundle</emphasis> resources:</para>
<itemizedlist>
<listitem>
<para>For <emphasis role="strong">RKE2</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>For <emphasis role="strong">K3s</emphasis> clusters:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Edit the <literal>Bundle</literal> <emphasis role="strong">target</emphasis>
configurations, under <literal>spec.targets</literal> provide your desired
target list. By default the <literal>Bundle</literal> resources from the
<literal>suse-edge/fleet-examples</literal> are <emphasis
role="strong">NOT</emphasis> mapped to any down stream clusters.</para>
<itemizedlist>
<listitem>
<para>To match all clusters change the default <literal>Bundle</literal> <emphasis
role="strong">target</emphasis> to:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>Alternatively, if you want a more granular cluster selection see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Apply the <emphasis role="strong">Bundle</emphasis> resources to your
<literal>management cluster</literal>:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para>View the created <emphasis role="strong">Bundle</emphasis> resource under
the <literal>fleet-default</literal> namespace:</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-third-party">
<title>SUC Plan deployment - third-party GitOps workflow</title>
<para>There might be use-cases where users would like to incorporate the
Kubernetes upgrade resources to their own third-party GitOps workflow
(e.g. <literal>Flux</literal>).</para>
<para>To get the upgrade resources that you need, first determine the he Edge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag of the <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
repository that you would like to use.</para>
<para>After that, the resources can be found at:</para>
<itemizedlist>
<listitem>
<para>For a RKE2 cluster upgrade:</para>
<itemizedlist>
<listitem>
<para>For <literal>control-plane</literal> nodes -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>For <literal>agent</literal> nodes -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>For a K3s cluster upgrade:</para>
<itemizedlist>
<listitem>
<para>For <literal>control-plane</literal> nodes -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para>For <literal>agent</literal> nodes -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the
<literal>system-upgrade-controller</literal> and should be deployed on each
downstream cluster that you wish to upgrade. For information on how to
deploy the <literal>system-upgrade-controller</literal>, see <xref
linkend="day2-suc-third-party-gitops"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the
<emphasis role="strong">SUC Plans</emphasis> for Kubernetes version upgrade,
it can be beneficial to take a look at the overview (<xref
linkend="k8s-version-upgrade-overview"/>) of the update procedure using
<literal>Fleet</literal>.</para>
</section>
</section>
</section>
<section xml:id="day2-helm-upgrade">
<title>Helm chart upgrade</title>
<note>
<para>The below sections focus on using <literal>Fleet</literal> functionalities
to achieve a Helm chart update.</para>
<para>Users adopting a third-party GitOps workflow, should take the configurations
for their desired helm chart from its <literal>fleet.yaml</literal> located
at
<literal>fleets/day2/chart-templates/&lt;chart-name&gt;</literal>. <emphasis
role="strong">Make sure you are retrieving the chart data from a valid "Day
2" Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>.</emphasis></para>
</note>
<section xml:id="id-components-3">
<title>Components</title>
<para>Apart from the default <literal>Day 2</literal> components (<xref
linkend="day2-downstream-components"/>), no other custom components are
needed for this operation.</para>
</section>
<section xml:id="id-preparation-for-air-gapped-environments">
<title>Preparation for air-gapped environments</title>
<section xml:id="id-ensure-that-you-have-access-to-your-helm-charts-upgrade-fleet-yaml-file">
<title>Ensure that you have access to your Helm chart’s upgrade
<literal>fleet.yaml</literal> file</title>
<para>Host the needed resources on a local git server that is accessible by your
<literal>management cluster</literal>.</para>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Find the required assets for your Edge release version</title>
<orderedlist numeration="arabic">
<listitem>
<para>Go to the Day 2 <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
page and find the Edge 3.X.Y release that you want to upgrade your chart to
and click <emphasis role="strong">Assets</emphasis>.</para>
</listitem>
<listitem>
<para>From the release’s <emphasis role="strong">Assets</emphasis> section,
download the following files, which are required for an air-gapped upgrade
of a SUSE supported helm chart:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Release File</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">Description</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>This script pulls the images in the
<literal>edge-release-images.txt</literal> file and saves them to a
'.tar.gz' archive that can then be used in your air-gapped environment.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>This script pulls the SUSE OCI chart artefacts in the
<literal>edge-release-helm-oci-artefacts.txt</literal> file and creates a
'.tar.gz' archive of a directory containing all other chart OCI archives.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>This script loads the images in the '.tar.gz' archive generated by
<literal>edge-save-images.sh</literal>, retags them and pushes them to your
private registry.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>This script takes a directory containing '.tgz' SUSE OCI charts and loads
all OCI charts to your private registry. The directory is retrieved from the
'.tar.gz' archive that the <literal>edge-save-oci-artefacts.sh</literal>
script has generated.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>This file contains a list of OCI artefacts for the SUSE Edge release Helm
charts.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>This file contains a list of images needed by the Edge release Helm charts.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-suse-edge-release-images-archive">
<title>Create the SUSE Edge release images archive</title>
<para><emphasis>On a machine with internet access:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Make <literal>edge-save-images.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para>Use <literal>edge-save-images.sh</literal> script to create a
<emphasis>Docker</emphasis> importable '.tar.gz' archive:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>This will create a ready to load <literal>edge-images.tar.gz</literal>
(unless you have specified the <literal>-i|--images</literal> option)
archive with the needed images.</para>
</listitem>
<listitem>
<para>Copy this archive to your <emphasis role="strong">air-gapped</emphasis>
machine</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-a-suse-edge-helm-chart-oci-images-archive">
<title>Create a SUSE Edge Helm chart OCI images archive</title>
<para><emphasis>On a machine with internet access:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Make <literal>edge-save-oci-artefacts.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Use <literal>edge-save-oci-artefacts.sh</literal> script to create a
'.tar.gz' archive of all SUSE Edge Helm chart OCI images:</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>This will create a <literal>oci-artefacts.tar.gz</literal> archive
containing all SUSE Edge Helm chart OCI images</para>
</listitem>
<listitem>
<para>Copy this archive to your <emphasis role="strong">air-gapped</emphasis>
machine</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-suse-edge-release-images-to-your-air-gapped-machine">
<title>Load SUSE Edge release images to your air-gapped machine</title>
<para><emphasis>On your air-gapped machine:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Log into your private registry (if required):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Make <literal>edge-load-images.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para>Use <literal>edge-load-images.sh</literal> to load the images from the
<emphasis role="strong">copied</emphasis>
<literal>edge-images.tar.gz</literal> archive, retag them and push them to
your private registry:</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-suse-edge-helm-chart-oci-images-to-your-air-gapped-machine">
<title>Load SUSE Edge Helm chart OCI images to your air-gapped machine</title>
<para><emphasis>On your air-gapped machine:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>Log into your private registry (if required):</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para>Make <literal>edge-load-oci-artefacts.sh</literal> executable:</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>Untar the copied <literal>oci-artefacts.tar.gz</literal> archive:</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>This will produce a directory with the naming template
<literal>edge-release-oci-tgz-&lt;date&gt;</literal></para>
</listitem>
<listitem>
<para>Pass this directory to the <literal>edge-load-oci-artefacts.sh</literal>
script to load the SUSE Edge helm chart OCI images to your private registry:</para>
<note>
<para>This script assumes the <literal>helm</literal> CLI has been pre-installed
on your environment. For Helm installation instructions, see <link
xl:href="https://helm.sh/docs/intro/install/">Installing Helm</link>.</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-registry-mirrors-pointing-to-your-private-registry-for-your-kubernetes-distribution">
<title>Create registry mirrors pointing to your private registry for your
Kubernetes distribution</title>
<para>For RKE2, see <link
xl:href="https://docs.rke2.io/install/containerd_registry_configuration">Containerd
Registry Configuration</link></para>
<para>For K3s, see <link
xl:href="https://docs.k3s.io/installation/registry-mirror">Embedded Registry
Mirror</link></para>
</section>
</section>
<section xml:id="id-upgrade-procedure-2">
<title>Upgrade procedure</title>
<note>
<para>The below upgrade procedure utilises Rancher’s Fleet (<xref
linkend="components-fleet"/>) funtionality. Users using a third-party GitOps
workflow should retrieve the chart versions supported by each Edge release
from the <xref linkend="release-notes"/> and populate these versions to
their third-party GitOps workflow.</para>
</note>
<para>This section focuses on the following Helm upgrade procedure use-cases:</para>
<orderedlist numeration="arabic">
<listitem>
<para>I have a new cluster and would like to deploy and manage a SUSE Helm chart
(<xref linkend="day2-helm-upgrade-new-cluster"/>)</para>
</listitem>
<listitem>
<para>I would like to upgrade a Fleet managed Helm chart (<xref
linkend="day2-helm-upgrade-fleet-managed-chart"/>)</para>
</listitem>
<listitem>
<para>I would like to upgrade an EIB created Helm chart (<xref
linkend="day2-helm-upgrade-eib-chart"/>)</para>
</listitem>
</orderedlist>
<important>
<para>Manually deployed Helm charts cannot be reliably upgraded. We suggest to
redeploy the helm chart using the <xref
linkend="day2-helm-upgrade-new-cluster"/> method.</para>
</important>
<section xml:id="day2-helm-upgrade-new-cluster">
<title>I have a new cluster and would like to deploy and manage a SUSE Helm chart</title>
<para>For users that want to manage their Helm chart lifecycle through Fleet.</para>
<section xml:id="id-prepare-your-fleet-resources">
<title>Prepare your Fleet resources</title>
<orderedlist numeration="arabic">
<listitem>
<para>Acquire the Chart’s Fleet resources from the Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag that you wish to use</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>From the selected Edge release tag revision, navigate to the Helm chart
fleet - <literal>fleets/day2/chart-templates/&lt;chart&gt;</literal></para>
</listitem>
<listitem>
<para>Copy the chart Fleet directory to the Git repository that you will be using
for your GitOps workflow</para>
</listitem>
<listitem>
<para><emphasis role="strong">Optionally</emphasis>, if the Helm chart requires
configurations to its <emphasis role="strong">values</emphasis>, edit the
<literal>.helm.values</literal> configuration inside the
<literal>fleet.yaml</literal> file of the copied directory</para>
</listitem>
<listitem>
<para><emphasis role="strong">Optionally</emphasis>, there may be use-cases where
you need to add additional resources to your chart’s fleet so that it can
better fit your environment. For information on how to enhance your Fleet
directory, see <link xl:href="https://fleet.rancher.io/gitrepo-content">Git
Repository Contents</link></para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>An <emphasis role="strong">example</emphasis> for the
<literal>longhorn</literal> helm chart would look like:</para>
<itemizedlist>
<listitem>
<para>User Git repository strucutre:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
└── longhorn
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet.yaml</literal> content populated with user
<literal>longhorn</literal> data:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.longhorn.io"
  version: "1.6.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>These are just example values that are used to illustrate custom
configurations over the <literal>longhorn</literal> chart. They should
<emphasis role="strong">NOT</emphasis> be treated as deployment guidelines
for the <literal>longhorn</literal> chart.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-create-the-gitrepo">
<title>Create the GitRepo</title>
<para>After populating your repository with the chart’s Fleet resources, you must
create a <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>
resource. This resource will hold information on how to access your chart’s
Fleet resources and to which clusters it needs to apply those resources.</para>
<para>The <literal>GitRepo</literal> resource can be created through the Rancher
UI, or by manually deploying the resource to the <literal>management
cluster</literal>.</para>
<para>For information on how to create and deploy the GitRepo resource <emphasis
role="strong">manually</emphasis>, see <link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a
Deployment</link>.</para>
<para>To create a <literal>GitRepo</literal> resource through the <emphasis
role="strong">Rancher UI</emphasis>, see <link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI</link>.</para>
<para><emphasis>Example <emphasis role="strong">longhorn</emphasis>
<literal>GitRepo</literal> resource for <emphasis
role="strong">manual</emphasis> deployment:</emphasis></para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: &lt;user_repository_tag&gt;
  #
  # If using a branch
  # branch: &lt;user_repository_branch&gt;
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  repo: &lt;user_repository_url&gt;
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="id-managing-the-deployed-helm-chart">
<title>Managing the deployed Helm chart</title>
<para>Once deployed with Fleet, for Helm chart upgrades, see <xref
linkend="day2-helm-upgrade-fleet-managed-chart"/>.</para>
</section>
</section>
<section xml:id="day2-helm-upgrade-fleet-managed-chart">
<title>I would like to upgrade a Fleet managed Helm chart</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine the version to which you need to upgrade your chart so that it is
compatible with an Edge 3.X.Y release. Helm chart version per Edge release
can be viewed from the <xref linkend="release-notes"/>.</para>
</listitem>
<listitem>
<para>In your Fleet monitored Git repository, edit the Helm chart’s
<literal>fleet.yaml</literal> file with the correct chart <emphasis
role="strong">version</emphasis> and <emphasis
role="strong">repository</emphasis> from the <xref
linkend="release-notes"/>.</para>
</listitem>
<listitem>
<para>After commiting and pushing the changes to your repository, this will
trigger an upgrade of the desired Helm chart</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-helm-upgrade-eib-chart">
<title>I would like to upgrade an EIB created Helm chart</title>
<note>
<para>This section assumes that you have deployed the system-upgrade-controller
(SUC) beforehand, if you have not done so, or are unsure why you need it,
see the default Day 2 components (<xref
linkend="day2-downstream-components"/>) list.</para>
</note>
<para>EIB deploys Helm charts by utilizing the auto-deploy manifests functionality
of <link
xl:href="https://docs.rke2.io/advanced#auto-deploying-manifests">rke2</link>/<link
xl:href="https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons">k3s</link>.
It creates a <link
xl:href="https://github.com/k3s-io/helm-controller#helm-controller">HelmChart</link>
resource definition manifest unter the
<literal>/var/lib/rancher/&lt;rke2/k3s&gt;/server/manifests</literal>
location of the initialiser node and lets <literal>rke2/k3s</literal> pick
it up and auto-deploy it in the cluster.</para>
<para>From a <literal>Day 2</literal> point of view this would mean that any
upgrades of the Helm chart need to happen by editing the
<literal>HelmChart</literal> manifest file of the specific chart. To
automate this process for multiple clusters, this section uses <emphasis
role="strong">SUC Plans</emphasis>.</para>
<para>Below you can find information on:</para>
<itemizedlist>
<listitem>
<para>The general overview (<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>) of the helm chart upgrade
workflow.</para>
</listitem>
<listitem>
<para>The necessary upgrade steps (<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>) needed for a
successful helm chart upgrade.</para>
</listitem>
<listitem>
<para>An example (<xref linkend="day2-helm-upgrade-eib-chart-example"/>)
showcasing a <link xl:href="https://longhorn.io">Longhorn</link> chart
upgrade using the explained method.</para>
</listitem>
<listitem>
<para>How to use the upgrade process with a different GitOps tool (<xref
linkend="day2-helm-upgrade-eib-chart-third-party"/>).</para>
</listitem>
</itemizedlist>
<section xml:id="day2-helm-upgrade-eib-chart-overview">
<title>Overview</title>
<para>This section is meant to give a high overview of the workflow that the user
goes through in order to upgrade one or multiple Helm charts. For a detailed
explanation of the steps needed for a Helm chart upgrade, see <xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>.</para>
<figure>
<title>Helm chart upgrade workflow</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_diagram.png"
width=""/> </imageobject>
<textobject><phrase>day2 helm chart upgrade diagram</phrase></textobject>
</mediaobject></figure>
<orderedlist numeration="arabic">
<listitem>
<para>The workflow begins with the user <link
xl:href="https://helm.sh/docs/helm/helm_pull/">pulling</link> the new Helm
chart archive(s) that he wishes to upgrade his chart(s) to.</para>
</listitem>
<listitem>
<para>The archive(s) should then be <emphasis>encoded</emphasis> and passed as
configuration to the <literal>eib-chart-upgrade-user-data.yaml</literal>
file that is located under the fleet directory for the related SUC
Plan. This is further explained in the upgrade steps (<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>) section.</para>
</listitem>
<listitem>
<para>The user then proceeds to configure and deploy a <literal>GitRepo</literal>
resource that will ship all the needed resources (SUC Plan, secrets, etc.)
to the downstream clusters.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>The resource is deployed on the <literal>management cluster</literal> under
the <literal>fleet-default</literal> namespace.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>) detects the deployed resource and
deploys all the configured resources to the specified downstream
clusters. Deployed resources include:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>The <literal>eib-chart-upgrade</literal> SUC Plan that will be used by SUC
to create an <emphasis role="strong">Upgrade Pod</emphasis> on each node.</para>
</listitem>
<listitem>
<para>The <literal>eib-chart-upgrade-script</literal> Secret that ships the
<literal>upgrade script</literal> that the <emphasis role="strong">Upgrade
Pod</emphasis> will use to upgrade the <literal>HelmChart</literal>
manifests on the initialiser node.</para>
</listitem>
<listitem>
<para>The <literal>eib-chart-upgrade-user-data</literal> Secret that ships the
chart data that the <literal>upgrade script</literal> will use in order to
understand which chart manifests it needs to upgrade.</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Once the <literal>eib-chart-upgrade</literal> SUC Plan has been deployed,
the SUC picks it up and creates a Job which deploys the <emphasis
role="strong">Upgrade Pod</emphasis>.</para>
</listitem>
<listitem>
<para>Once deployed, the <emphasis role="strong">Upgrade Pod</emphasis> mounts the
<literal>eib-chart-upgrade-script</literal> and
<literal>eib-chart-upgrade-user-data</literal> Secrets and executes the
<literal>upgrade script</literal> that is shipped by the
<literal>eib-chart-upgrade-script</literal> Secret.</para>
</listitem>
<listitem>
<para>The <literal>upgrade script</literal> does the following:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Determine whether the Pod that the script is running on has been deployed on
the <literal>initialiser</literal> node. The <literal>initialiser</literal>
node is the node that is hosting the <literal>HelmChart</literal>
manifests. For a single-node cluster it is the single control-plane
node. For HA clusters it is the node that you have marked as
<literal>initializer</literal> when creating the cluster in EIB. If you have
not specified the <literal>initializer</literal> property, then the first
node from the <literal>nodes</literal> list is marked as
<literal>initializer</literal>. For more information, see the <link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">upstream</link>
documentation for EIB.</para>
<note>
<para>If the <literal>upgrade script</literal> is running on a non-initialiser
node, it immediately finishes its execution and does not go through the
steps below.</para>
</note>
</listitem>
<listitem>
<para>Backup the manifests that will be edited in order to ensure disaster
recover.</para>
<note>
<para>By default backups of the manifests are stored under the
<literal>/tmp/eib-helm-chart-upgrade-&lt;date&gt;</literal> directory. If
you wish to use a custom location you can pass the
<literal>MANIFEST_BACKUP_DIR</literal> enviroment variable to the Helm chart
upgrade SUC Plan (example in the Plan).</para>
</note>
</listitem>
<listitem>
<para>Edit the <literal>HelmChart</literal> manifests. As of this version, the
following properties are changed in order to trigger a chart upgrade:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>The content of the <literal>chartContent</literal> property is replaced with
the encoded archive provided in the
<literal>eib-chart-upgrade-user-data</literal> Secret.</para>
</listitem>
<listitem>
<para>The value of the <literal>version</literal> property is replaced with the
version provided in the <literal>eib-chart-upgrade-user-data</literal>
Secret.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>After the successful execution of the <literal>upgrade script</literal>, the
Helm integration for <link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link> will pickup the change and
automatically trigger an upgrade on the Helm chart.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-upgrade-steps">
<title>Upgrade Steps</title>
<orderedlist numeration="arabic">
<listitem>
<para>Determine an Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">relase
tag</link> from which you wish to copy the Helm chart upgrade logic.</para>
</listitem>
<listitem>
<para>Copy the
<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>
fleet to the repository that your Fleet will be using to do GitOps from.</para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/docs/helm/helm_pull/">Pull</link> the Helm
chart archive that you wish to upgrade to:</para>
<screen language="bash" linenumbering="unnumbered">helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>Encode the chart archive that you pulled:</para>
<screen language="bash" linenumbering="unnumbered"># Encode the archive and disable line wrapping
base64 -w 0 &lt;chart-archive&gt;.tgz</screen>
</listitem>
<listitem>
<para>Configure the <literal>eib-chart-upgrade-user-data.yaml</literal> secret
located under the <literal>eib-chart-upgrade</literal> fleet that you copied
from step (2):</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>The secret ships a file called
<literal>chart_upgrade_data.txt</literal>. This file holds the chart upgrade
data that the <literal>upgrade script</literal> will use to know which
charts need to be upgraded. The file expects one-line per chart entries in
the following format <emphasis
role="strong">"&lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;"</emphasis>:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para><literal>name</literal> - is the name of the helm chart as seen in the
<literal>kubernetes.helm.charts.name[]</literal> property of the EIB
definition file.</para>
</listitem>
<listitem>
<para><literal>version</literal> - should hold the new version of the Helm
chart. During the upgrade this value will be used to replace the old
<literal>version</literal> value of the <literal>HelmChart</literal>
manifest.</para>
</listitem>
<listitem>
<para><literal>base64_encoded_archive</literal> - pass the output of the
<literal>base64 -w 0 &lt;chart-archive&gt;.tgz</literal> here. During
upgrade this value will be used to replace the old
<literal>chartContent</literal> value of the <literal>HelmChart</literal>
manifest.</para>
<note>
<para>The <emphasis
role="strong">&lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;</emphasis>
line should be removed from the file before you start adding your data. It
serves as an example of where and how you should configure your chart data.</para>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Configure a <literal>GitRepo</literal> resource that will be shipping your
chart upgrade <literal>fleet</literal>. For more information on what a
<literal>GitRepo</literal> is, see <link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo Resource</link>.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Configure <literal>GitRepo</literal> through the Rancher UI:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>In the upper left corner, <emphasis role="strong">☰ → Continuous
Delivery</emphasis></para>
</listitem>
<listitem>
<para>Go to <emphasis role="strong">Git Repos → Add Repository</emphasis></para>
</listitem>
<listitem>
<para>Here pass your <emphasis role="strong">repository data</emphasis> and
<emphasis role="strong">path</emphasis> to your chart <literal>upgrade
fleet</literal></para>
</listitem>
<listitem>
<para>Select <emphasis role="strong">Next</emphasis> and specify the <emphasis
role="strong">target</emphasis> clusters of which you want to upgrade the
configured charts</para>
</listitem>
<listitem>
<para><emphasis role="strong">Create</emphasis></para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>If <literal>Rancher</literal> is not available on your setup, you can
configure a <literal>GitRepo</literal> manually on your <literal>management
cluster</literal>:</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>Populate the following template with your data:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: CHANGE_ME
  namespace: fleet-default
spec:
  # if running from a tag
  # revision: CHANGE_ME
  # if running from a branch
  # branch: CHANGE_ME
  paths:
  # path to your chart upgrade fleet relative to your repository
  - CHANGE_ME
  # your repository URL
  repo: CHANGE_ME
  targets:
  # Select target clusters
  - clusterSelector: CHANGE_ME
  # To match all clusters:
  # - clusterSelector: {}</screen>
<para>For more information on how to setup and deploy a <literal>GitRepo</literal>
resource, see <link xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo
Resource</link> and <link
xl:href="https://fleet.rancher.io/gitrepo-add">Create a GitRepo
Resource</link>.</para>
<para>For information on how to match <emphasis role="strong">taget</emphasis>
clusters on a more granular level, see <link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters</link>.</para>
</listitem>
<listitem>
<para>Deploy the configured <literal>GitRepo</literal> resource to the
<literal>fleet-default</literal> namespace of the <literal>management
cluster</literal>.</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Executing this steps should result in a successfully created
<literal>GitRepo</literal> resource. It will then be picked up by Fleet and
a Bundle will be created. This Bunlde will hold the <emphasis
role="strong">raw</emphasis> Kubernetes resources that the
<literal>GitRepo</literal> has configured under its fleet directory.</para>
<para>Fleet will then deploy all the Kubernetes resources from the Bundle to the
specified downstream clusters. One of this resources will be a SUC Plan that
will trigger the chart upgrade. For a full list of the resoruces that will
be deployed and the workflow of the upgrade process, refer to the overview
(<xref linkend="day2-helm-upgrade-eib-chart-overview"/>) section.</para>
<para>To track the upgrade process itself, refer to the Monitor SUC Plans (<xref
linkend="monitor-suc-plans"/>) section.</para>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-example">
<title>Example</title>
<para>The following section serves to provide a real life example to the <xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/> section.</para>
<para>I have the following two EIB deployed clusters:</para>
<itemizedlist>
<listitem>
<para><literal>longhorn-single-k3s</literal> - single node K3s cluster</para>
</listitem>
<listitem>
<para><literal>longhorn-ha-rke2</literal> - HA RKE2 cluster</para>
</listitem>
</itemizedlist>
<para>Both clusters are running <link
xl:href="https://longhorn.io">Longhorn</link> and have been deployed through
EIB, using the following image definition <emphasis>snippet</emphasis>:</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  # HA RKE2 cluster specific snippet
  # nodes:
  # - hostname: cp1rke2.example.com
  #   initializer: true
  #   type: server
  # - hostname: cp2rke2.example.com
  #   type: server
  # - hostname: cp3rke2.example.com
  #   type: server
  # - hostname: agent1rke2.example.com
  #   type: agent
  # - hostname: agent2rke2.example.com
  #   type: agent
  # version depending on the distribution
  version: v1.28.9+k3s1/v1.28.9+rke2r1
  helm:
    charts:
    - name: longhorn
      repositoryName: longhorn
      targetNamespace: longhorn-system
      createNamespace: true
      version: 1.5.5
    repositories:
    - name: longhorn
      url: https://charts.longhorn.io
...</screen>
<figure>
<title>longhorn-single-k3s installed Longhorn version</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_k3s_old.png" width=""/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example k3s old</phrase></textobject>
</mediaobject></figure>
<figure>
<title>longhorn-ha-rke2 installed Longhorn version</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_rke2_old.png" width=""/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example rke2 old</phrase></textobject>
</mediaobject></figure>
<para>The problem with this is that currently
<literal>longhorn-single-k3s</literal> and
<literal>longhorn-ha-rke2</literal> are running with a Longhorn version that
is not compatible with any Edge release.</para>
<para>We need to upgrade the chart on both clusters to a Edge supported Longhorn
version.</para>
<para>To do this we follow these steps:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Determine the Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">relase
tag</link> from which we want to take the upgrade logic. For example, this
example will use the <literal>release-3.0.1</literal> release tag for which
the supported Longhorn version is <literal>1.6.1</literal>.</para>
</listitem>
<listitem>
<para>Clone the <literal>release-3.0.1</literal> release tag and copy the
<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>
directory to our own repository.</para>
<para>For simplicity this section works from a branch of the
<literal>suse-edge/fleet-examples</literal> repository, so the directory
structure is the same, but you can place the
<literal>eib-chart-upgrade</literal> fleet anywhere in your repository.</para>
<formalpara>
<title>Directory structure example</title>
<para>
<screen language="bash" linenumbering="unnumbered">.
...
|-- fleets
|   `-- day2
|       `-- system-upgrade-controller-plans
|           `-- eib-chart-upgrade
|               |-- eib-chart-upgrade-script.yaml
|               |-- eib-chart-upgrade-user-data.yaml
|               |-- fleet.yaml
|               `-- plan.yaml
...</screen>
</para>
</formalpara>
</listitem>
<listitem>
<para>Add the Longhorn chart repository:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add longhorn https://charts.longhorn.io</screen>
</listitem>
<listitem>
<para>Pull the Longhorn chart version <literal>1.6.1</literal>:</para>
<screen language="bash" linenumbering="unnumbered">helm pull longhorn/longhorn --version 1.6.1</screen>
<para>This will pull the longhorn as an archvie named
<literal>longhorn-1.6.1.tgz</literal>.</para>
</listitem>
<listitem>
<para>Encode the Longhorn archive:</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 longhorn-1.6.1.tgz</screen>
<para>This will output a long one-line base64 encoded string of the archive.</para>
</listitem>
<listitem>
<para>Now we have all the needed data to configure the
<literal>eib-chart-upgrade-user-data.yaml</literal> file. The file
configuration should look like this:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: eib-chart-upgrade-user-data
type: Opaque
stringData:
  # &lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;
  chart_upgrade_data.txt: |
    longhorn|1.6.1|H4sIFAAAAAAA/ykAK2FIUjBjSE02THk5NWIzV...</screen>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>longhorn</literal> is the name of the chart as seen in my EIB
definition file</para>
</listitem>
<listitem>
<para><literal>1.6.1</literal> is the version to which I want to upgrade the
<literal>version</literal> property of the Longhorn
<literal>HelmChart</literal> manifest</para>
</listitem>
<listitem>
<para><literal>H4sIFAAAAAAA/ykAK2FIUjBjSE02THk5NWIzV…​</literal> is a snippet of
the encoded Longhorn <literal>1.6.1</literal> archive. <emphasis
role="strong">A snippet has been added here for better readibility. You
should always provide the full base64 encoded archive string
here.</emphasis></para>
<note>
<para>This example shows configuration for a single chart upgrade, but if your
use-case requires to upgrade multiple charts on multiple clusters, you can
append the additional chart data as seen below:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: eib-chart-upgrade-user-data
type: Opaque
stringData:
  # &lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;
  chart_upgrade_data.txt: |
    chartA|0.0.0|&lt;chartA_base64_archive&gt;
    chartB|0.0.0|&lt;chartB_base64_archive&gt;
    chartC|0.0.0|&lt;chartC_base64_archive&gt;
    ...</screen>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>We also decided that we do not want to keep manifest backups at
<literal>/tmp</literal>, so the following configuration was added to the
<literal>plan.yaml</literal> file:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: eib-chart-upgrade
spec:
  ...
  upgrade:
    ...
    # For when you want to backup your chart
    # manifest data under a specific directory
    #
    envs:
    - name: MANIFEST_BACKUP_DIR
      value: "/root"</screen>
<para>This will ensure that manifest backups will be saved under the
<literal>/root</literal> directory instead of <literal>/tmp</literal>.</para>
</listitem>
<listitem>
<para>Now that we have made all the needed configurations, what is left is to
create the <literal>GitRepo</literal> resource. This example creates the
<literal>GitRepo</literal> resource through the <literal>Rancher
UI</literal>.</para>
</listitem>
<listitem>
<para>Following the steps described in the Upgrade Steps (<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>), we:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Named the <literal>GitRepo</literal> "longhorn-upgrade".</para>
</listitem>
<listitem>
<para>Passed the URL to the repository that will be used - <link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></para>
</listitem>
<listitem>
<para>Passed the branch of the repository - "doc-example"</para>
</listitem>
<listitem>
<para>Passed the path to the <literal>eib-chart-upgrade</literal> fleet as seen in
the repo -
<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal></para>
</listitem>
<listitem>
<para>Selected the target clusters and created the resource</para>
<figure>
<title>Successfully deployed SUC and longhorn GitRepos</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_gitrepo.png" width=""/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example gitrepo</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Now we need to monitor the upgrade procedures on the clusters:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Check the status of the <emphasis role="strong">Upgrade Pods</emphasis>,
following the directions from the SUC plan monitor (<xref
linkend="monitor-suc-plans"/>) section.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>A successfully completed <emphasis role="strong">Upgrade Pod</emphasis> that
has been working on an <literal>intialiser</literal> node should hold logs
similar to:</para>
<figure>
<title>Upgrade Pod running on an initialiser node</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_initialiser_logs.png" width=""/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example initialiser logs</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>A successfully completed <emphasis role="strong">Upgrade Pod</emphasis> that
has been working on a <literal>non-initialiser</literal> node should hold
logs similar to:</para>
<figure>
<title>Upgrade Pod running on a non-initialiser node</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_non_initialiser_logs.png"
width=""/> </imageobject>
<textobject><phrase>day2 helm chart upgrade example non initialiser logs</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>After a successful <emphasis role="strong">Upgrade Pod</emphasis>
completion, we would also need to wait and monitor for the pods that wil lbe
created by the helm controller. These pods will do the actual upgrade based
on the file chagnes that the <emphasis role="strong">Upgrade Pod</emphasis>
has done to the <literal>HelmChart</literal> manifest file.</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>In your cluster, go to <emphasis role="strong">Workloads → Pods</emphasis>
and search for a pod that contains the <literal>longhorn</literal> string in
the <literal>default</literal> namespace. This should produce a pod with the
naming template <literal>helm-install-longhorn-*</literal>, view the logs of
this pod.</para>
<figure>
<title>Successfully completed helm-install pod</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_helm_install.png" width=""/>
</imageobject>
<textobject><phrase>day2 helm chart upgrade example helm install</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>The logs should be similar to:</para>
<figure>
<title>Successfully completed helm-install pod logs</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_successfully_upgraded_pod.png"
width=""/> </imageobject>
<textobject><phrase>day2 helm chart upgrade example successfully upgraded pod</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>Now that we have ensured that everything has completed succesfully, we need
to verify the version change:</para>
<orderedlist numeration="arabic">
<listitem>
<para>On the clusters we need to go to <emphasis role="strong">More Resources →
Helm → HelmCharts</emphasis> and in the <literal>default</literal> namespace
search for the <literal>longhorn</literal> HelmChart resource:</para>
<figure>
<title>longhorn-single-k3s upgraded Longhorn version</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_k3s_longhorn_upgrade.png"
width=""/> </imageobject>
<textobject><phrase>day2 helm chart upgrade example k3s longhorn upgrade</phrase></textobject>
</mediaobject></figure>
<figure>
<title>longhorn-ha-rke2 upgraded Longhorn version</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_rke2_longhorn_upgrade.png"
width=""/> </imageobject>
<textobject><phrase>day2 helm chart upgrade example rke2 longhorn upgrade</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>This ensures that the <literal>Longhorn</literal> helm chart has been
successfully upgraded and concludes this example.</para>
<para>If for some reason we would like to revert to the previous chart version of
Longhorn, the previous Longhorn manifest will be located under
<literal>/root/longhorn.yaml</literal> on the initialiser node. This is
true, because we have specified the <literal>MANIFEST_BACKUP_DIR</literal>
in the SUC Plan.</para>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-third-party">
<title>Helm chart upgrade using a third-party GitOps tool</title>
<para>There might be use-cases where users would like to use this upgrade
procedure with a GitOps workflow other than Fleet
(e.g. <literal>Flux</literal>).</para>
<para>To get the resources related to EIB deployed Helm chart upgrades you need to
first determine the Edge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">release</link>
tag of the <link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>
repository that you would like to use.</para>
<para>After that, resources can be found at
<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>,
where:</para>
<itemizedlist>
<listitem>
<para><literal>plan.yaml</literal> - system-upgrade-controller Plan related to the
upgrade procedure.</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-script.yaml</literal> - Secret holding the
<literal>upgrade script</literal> that is responsible for editing and
upgrade the <literal>HelmChart</literal> manifest files.</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-user-data.yaml</literal> - Secret holding a file
that is utilised by the <literal>upgrade scritp</literal>; populated by the
user with relevat chart upgrade data beforehand.</para>
</listitem>
</itemizedlist>
<important>
<para>These <literal>Plan</literal> resources are interpreted by the
<literal>system-upgrade-controller</literal> and should be deployed on each
downstream cluster that holds charts in need of an upgrade. For information
on how to deploy the <literal>system-upgrade-controller</literal>, see <xref
linkend="day2-suc-third-party-gitops"/>.</para>
</important>
<para>To better understand how your GitOps workflow can be used to deploy the
<emphasis role="strong">SUC Plans</emphasis> for the upgrade process, it can
be beneficial to take a look at the overview (<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>) of the process using
<literal>Fleet</literal>.</para>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-product-documentation">
<title>Product Documentation</title>
<partintro>
<para>Find the ATIP documentation here</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Adaptive Telco Infrastructure Platform (ATIP)</title>
<para>SUSE Adaptive Telco Infrastructure Platform (<literal>ATIP</literal>) is a
Telco-optimized edge computing platform that enables telecom companies to
innovate and accelerate the modernization of their networks.</para>
<para>ATIP is a complete Telco cloud stack for hosting CNFs such as 5G Packet Core
and Cloud RAN.</para>
<itemizedlist>
<listitem>
<para>Automates zero-touch rollout and lifecycle management of complex edge stack
configurations at Telco scale.</para>
</listitem>
<listitem>
<para>Continuously assures quality on Telco-grade hardware, using Telco-specific
configurations and workloads.</para>
</listitem>
<listitem>
<para>Consists of components that are purpose-built for the edge and hence have
smaller footprint and higher performance per Watt.</para>
</listitem>
<listitem>
<para>Maintains a flexible platform strategy with vendor-neutral APIs and 100%
open source.</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>Concept &amp; Architecture</title>
<para>SUSE ATIP is a platform designed for hosting modern, cloud native, Telco
applications at scale from core to edge.</para>
<para>This page explains the architecture and components used in ATIP. Knowledge
of this helps deploy and use ATIP.</para>
<section xml:id="id-atip-architecture">
<title>ATIP Architecture</title>
<para>The following diagram shows the high-level architecture of ATIP:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.png" width=""/>
</imageobject>
<textobject><phrase>product atip architecture1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-4">
<title>Components</title>
<para>There are two different blocks, the management stack and the runtime stack:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management stack</emphasis>: This is the part of
ATIP that is used to manage the provision and lifecycle of the runtime
stacks. It includes the following components:</para>
<itemizedlist>
<listitem>
<para>Multi-cluster management in public and private cloud environments with
Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Bare-metal support with Metal3 (<xref linkend="components-metal3"/>),
MetalLB (<xref linkend="components-metallb"/>) and <literal>CAPI</literal>
(Cluster API) infrastructure providers</para>
</listitem>
<listitem>
<para>Comprehensive tenant isolation and <literal>IDP</literal> (Identity
Provider) integrations</para>
</listitem>
<listitem>
<para>Large marketplace of third-party integrations and extensions</para>
</listitem>
<listitem>
<para>Vendor-neutral API and rich ecosystem of providers</para>
</listitem>
<listitem>
<para>Control the SLE Micro transactional updates</para>
</listitem>
<listitem>
<para>GitOps Engine for managing the lifecycle of the clusters using Git
repositories with Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Runtime stack</emphasis>: This is the part of ATIP
that is used to run the workloads.</para>
<itemizedlist>
<listitem>
<para>Kubernetes with secure and lightweight distributions like K3s (<xref
linkend="components-k3s"/>) and RKE2 (<xref linkend="components-rke2"/>)
(<literal>RKE2</literal> is hardened, certified and optimized for government
use and regulated industries).</para>
</listitem>
<listitem>
<para>NeuVector (<xref linkend="components-neuvector"/>) to enable security
features like image vulnerability scanning, deep packet inspection and
automatic intra-cluster traffic control.</para>
</listitem>
<listitem>
<para>Block Storage with Longhorn (<xref linkend="components-longhorn"/>) to
enable a simple and easy way to use a cloud native storage solution.</para>
</listitem>
<listitem>
<para>Optimized Operating System with SLE Micro (<xref
linkend="components-slmicro"/>) to enable a secure, lightweight and
immutable (transactional file system) OS for running containers. SLE Micro
is available on <literal>aarch64</literal> and <literal>x86_64</literal>
architectures, and it also supports <literal>Real-Time Kernel</literal> for
Telco and edge use cases.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>Example deployment flows</title>
<para>The following are high-level examples of workflows to understand the
relationship between the management and the runtime components.</para>
<para>Direct network provisioning is the workflow that enables the deployment of a
new downstream cluster with all the components preconfigured and ready to
run workloads with no manual intervention.</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>Example 1: Deploying a new management cluster with all components installed</title>
<para>Using the Edge Image Builder (<xref linkend="components-eib"/>) to create a
new <literal>ISO</literal> image with the management stack included. You can
then use this <literal>ISO</literal> image to install a new management
cluster on VMs or bare metal.</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png" width=""/>
</imageobject>
<textobject><phrase>product atip architecture2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>For more information about how to deploy a new management cluster, see the
ATIP Management Cluster guide (<xref linkend="atip-management-cluster"/>).</para>
</note>
<note>
<para>For more information about how to use the Edge Image Builder, see the Edge
Image Builder guide (<xref linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>Example 2: Deploying a single-node downstream cluster with Telco profiles to
enable it to run Telco workloads</title>
<para>Once we have the management cluster up and running, we can use it to deploy
a single-node downstream cluster with all Telco capabilities enabled and
configured using the directed network provisioning workflow.</para>
<para>The following diagram shows the high-level workflow to deploy it:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png" width=""/>
</imageobject>
<textobject><phrase>product atip architecture3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>For more information about how to deploy a downstream cluster, see the ATIP
Automated Provisioning guide. (<xref
linkend="atip-automated-provisioning"/>)</para>
</note>
<note>
<para>For more information about Telco features, see the ATIP Telco Features
guide. (<xref linkend="atip-features"/>)</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>Example 3: Deploying a high availability downstream cluster using MetalLB as
a Load Balancer</title>
<para>Once we have the management cluster up and running, we can use it to deploy
a high availability downstream cluster with <literal>MetalLB</literal> as a
load balancer using the directed network provisioning workflow.</para>
<para>The following diagram shows the high-level workflow to deploy it:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png" width=""/>
</imageobject>
<textobject><phrase>product atip architecture4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>For more information about how to deploy a downstream cluster, see the ATIP
Automated Provisioning guide. (<xref
linkend="atip-automated-provisioning"/>)</para>
</note>
<note>
<para>For more information about <literal>MetalLB</literal>, see here: (<xref
linkend="components-metallb"/>)</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>Requirements &amp; Assumptions</title>
<section xml:id="id-hardware">
<title>Hardware</title>
<para>The hardware requirements for the ATIP nodes are based on the following
components:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management cluster</emphasis>: The management
cluster contains components like <literal>SLE Micro</literal>,
<literal>RKE2</literal>, <literal>Rancher Prime</literal>,
<literal>Metal<superscript>3</superscript></literal>, and it is used to
manage several downstream clusters. Depending on the number of downstream
clusters to be managed, the hardware requirements for the server could vary.</para>
<itemizedlist>
<listitem>
<para>Minimum requirements for the server (<literal>VM</literal> or <literal>Bare
Metal</literal>) are:</para>
<itemizedlist>
<listitem>
<para>RAM: 8 GB Minimum (we recommend at least 16 GB)</para>
</listitem>
<listitem>
<para>CPU: 2 Minimum (we recommend at least 4 CPU)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Downstream clusters</emphasis>: The downstream
clusters are the clusters deployed on the ATIP nodes to run Telco
workloads. Specific requirements are needed to enable certain Telco
capabilities like <literal>SR-IOV</literal>, <literal>CPU Performance
Optimization</literal>, etc.</para>
<itemizedlist>
<listitem>
<para>SR-IOV: to attach VFs (Virtual Functions) in pass-through mode to CNFs/VNFs,
the NIC must support SR-IOV and VT-d/AMD-Vi be enabled in the BIOS.</para>
</listitem>
<listitem>
<para>CPU Processors: To run specific Telco workloads, the CPU Processor model
should be adapted to enable most of the features available in this reference
table (<xref linkend="atip-features"/>).</para>
</listitem>
<listitem>
<para>Firmware requirements for installing with virtual media:</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Server Hardware</para></entry>
<entry align="left" valign="top"><para>BMC Model</para></entry>
<entry align="left" valign="top"><para>Management</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Dell hardware</para></entry>
<entry align="left" valign="top"><para>15th Generation</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Supermicro hardware</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>HPE hardware</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="id-network">
<title>Network</title>
<para>As a reference for the network architecture, the following diagram shows a
typical network architecture for a Telco environment:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirement1.png" width=""/>
</imageobject>
<textobject><phrase>product atip requirement1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>The network architecture is based on the following components:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Management network</emphasis>: This network is used
for the management of the ATIP nodes. It is used for the out-of-band
management. Usually, this network is also connected to a separate management
switch, but it can be connected to the same service switch using VLANs to
isolate the traffic.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Control-plane network</emphasis>: This network is
used for the communication between the ATIP nodes and the services that are
running on them. This network is also used for the communication between the
ATIP nodes and the external services, like the <literal>DHCP</literal> or
<literal>DNS</literal> servers. In some cases, for connected environments,
the switch/router can handle traffic through the Internet.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Other networks</emphasis>: In some cases, the ATIP
nodes could be connected to other networks for specific customer purposes.</para>
</listitem>
</itemizedlist>
<note>
<para>To use the directed network provisioning workflow, the management cluster
must have network connectivity to the downstream cluster server Baseboard
Management Controller (BMC) so that host preparation and provisioning can be
automated.</para>
</note>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>Services (DHCP, DNS, etc.)</title>
<para>Some external services like <literal>DHCP</literal>, <literal>DNS</literal>,
etc. could be required depending on the kind of environment where they are
deployed:</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">Connected environment</emphasis>: In this case, the
ATIP nodes will be connected to the Internet (via routing L3 protocols) and
the external services will be provided by the customer.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Disconnected / air-gap environment</emphasis>: In
this case, the ATIP nodes will not have Internet IP connectivity and
additional services will be required to locally mirror content required by
the ATIP directed network provisioning workflow.</para>
</listitem>
<listitem>
<para><emphasis role="strong">File server</emphasis>: A file server is used to
store the ISO images to be provisioned on the ATIP nodes during the directed
network provisioning workflow. The
<literal>metal<superscript>3</superscript></literal> Helm chart can deploy a
media server to store the ISO images — check the following section (<xref
linkend="metal3-media-server"/>), but it is also possible to use an existing
local webserver.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-rebootmgr">
<title>Disabling rebootmgr</title>
<para><literal>rebootmgr</literal> is a service which allows to configure a
strategy for reboot when the system has pending updates.  For Telco
workloads, it is really important to disable or configure properly the
<literal>rebootmgr</literal> service to avoid the reboot of the nodes in
case of updates scheduled by the system, to avoid any impact on the services
running on the nodes.</para>
<note>
<para>For more information about <literal>rebootmgr</literal>, see <link
xl:href="https://github.com/SUSE/rebootmgr">rebootmgr GitHub
repository</link>.</para>
</note>
<para>Verify the strategy being used by running:</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>and you could disable it by running:</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>or using the <literal>rebootmgrctl</literal> command:</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para>This configuration to set the <literal>rebootmgr</literal> strategy can be
automated using the directed network provisioning workflow. For more
information, check the ATIP Automated Provisioning documentation (<xref
linkend="atip-automated-provisioning"/>).</para>
</note>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>Setting up the management cluster</title>
<section xml:id="id-introduction-2">
<title>Introduction</title>
<para>The management cluster is the part of ATIP that is used to manage the
provision and lifecycle of the runtime stacks.  From a technical point of
view, the management cluster contains the following components:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Enterprise Micro</literal> as the OS. Depending on the
use case, some configurations like networking, storage, users and kernel
arguments can be customized.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> as the Kubernetes cluster. Depending on the use
case, it can be configured to use specific CNI plugins, such as
<literal>Multus</literal>, <literal>Cilium</literal>, etc.</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> as the management platform to manage the
lifecycle of the clusters.</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal> as the component to
manage the lifecycle of the bare metal nodes.</para>
</listitem>
<listitem>
<para><literal>CAPI</literal> as the component to manage the lifecycle of the
Kubernetes clusters (downstream clusters). With ATIP, also the <literal>RKE2
CAPI Provider</literal> is used to manage the lifecycle of the RKE2 clusters
(downstream clusters).</para>
</listitem>
</itemizedlist>
<para>With all components mentioned above, the management cluster can manage the
lifecycle of downstream clusters, using a declarative approach to manage the
infrastructure and applications.</para>
<note>
<para>For more information about <literal>SUSE Linux Enterprise Micro</literal>,
see: SLE Micro (<xref linkend="components-slmicro"/>)</para>
<para>For more information about <literal>RKE2</literal>, see: RKE2 (<xref
linkend="components-rke2"/>)</para>
<para>For more information about <literal>Rancher</literal>, see: Rancher (<xref
linkend="components-rancher"/>)</para>
<para>For more information about
<literal>Metal<superscript>3</superscript></literal>, see: Metal3 (<xref
linkend="components-metal3"/>)</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>Steps to set up the management cluster</title>
<para>The following steps are necessary to set up the management cluster (using a
single node):</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png" width=""/>
</imageobject>
<textobject><phrase>product atip mgmtcluster1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>There are three main steps to set up the management cluster using a
declarative approach:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Image preparation for connected environments (<xref
linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>: The first
step is to prepare the manifests and files with all the necessary
configurations to be used in connected environments.</para>
<itemizedlist>
<listitem>
<para>Directory structure for connected environments (<xref
linkend="mgmt-cluster-directory-structure"/>): This step creates a directory
structure to be used by Edge Image Builder to store the configuration files
and the image itself.</para>
</listitem>
<listitem>
<para>Management cluster definition file (<xref
linkend="mgmt-cluster-image-definition-file"/>): The
<literal>mgmt-cluster.yaml</literal> file is the main definition file for
the management cluster. It contains the following information about the
image to be created:</para>
<itemizedlist>
<listitem>
<para>Image Information: The information related to the image to be created using
the base image.</para>
</listitem>
<listitem>
<para>Operating system: The operating system configurations to be used in the
image.</para>
</listitem>
<listitem>
<para>Kubernetes: Helm charts and repositories, kubernetes version, network
configuration, and the nodes to be used in the cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Custom folder (<xref linkend="mgmt-cluster-custom-folder"/>): The
<literal>custom</literal> folder contains the configuration files and
scripts to be used by Edge Image Builder to deploy a fully functional
management cluster.</para>
<itemizedlist>
<listitem>
<para>Files: Contains the configuration files to be used by the management
cluster.</para>
</listitem>
<listitem>
<para>Scripts: Contains the scripts to be used by the management cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Kubernetes folder (<xref linkend="mgmt-cluster-kubernetes-folder"/>): The
<literal>kubernetes</literal> folder contains the configuration files to be
used by the management cluster.</para>
<itemizedlist>
<listitem>
<para>Manifests: Contains the manifests to be used by the management cluster.</para>
</listitem>
<listitem>
<para>Helm: Contains the Helm charts to be used by the management cluster.</para>
</listitem>
<listitem>
<para>Config: Contains the configuration files to be used by the management
cluster.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Network folder (<xref linkend="mgmt-cluster-network-folder"/>): The
<literal>network</literal> folder contains the network configuration files
to be used by the management cluster nodes.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Image preparation for air-gap environments (<xref
linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>: The step is
to show the differences to prepare the manifests and files to be used in an
air-gap scenario.</para>
<itemizedlist>
<listitem>
<para>Directory structure for air-gap environments (<xref
linkend="mgmt-cluster-directory-structure-airgap"/>): The directory
structure must be modified to include the resources needed to run the
management cluster in an air-gap environment.</para>
</listitem>
<listitem>
<para>Modifications in the definition file (<xref
linkend="mgmt-cluster-image-definition-file-airgap"/>): The
<literal>mgmt-cluster.yaml</literal> file must be modified to include the
<literal>embeddedArtifactRegistry</literal> section with the
<literal>images</literal> field set to all container images to be included
into the EIB output image.</para>
</listitem>
<listitem>
<para>Modifications in the custom folder (<xref
linkend="mgmt-cluster-custom-folder-airgap"/>): The
<literal>custom</literal> folder must be modified to include the resources
needed to run the management cluster in an air-gap environment.</para>
<itemizedlist>
<listitem>
<para>Register script: The <literal>custom/scripts/99-register.sh</literal> script
must be removed when you use an air-gap environment.</para>
</listitem>
<listitem>
<para>Air-gap resources: The
<literal>custom/files/airgap-resources.tar.gz</literal> file must be
included in the <literal>custom/files</literal> folder with all the
resources needed to run the management cluster in an air-gap environment.</para>
</listitem>
<listitem>
<para>Scripts: The <literal>custom/scripts/99-mgmt-setup.sh</literal> script must
be modified to extract and copy the
<literal>airgap-resources.tar.gz</literal> file to the final location. The
<literal>custom/files/metal3.sh</literal> script must be modified to use the
local resources included in the <literal>airgap-resources.tar.gz</literal>
file instead of downloading them from the internet.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">Image creation (<xref
linkend="mgmt-cluster-image-creation"/>)</emphasis>: This step covers the
creation of the image using the Edge Image Builder tool (for both, connected
and air-gap scenarios). Check the prerequisites (<xref
linkend="components-eib"/>) to run the Edge Image Builder tool on your
system.</para>
</listitem>
<listitem>
<para><emphasis role="strong">Management Cluster Provision (<xref
linkend="mgmt-cluster-provision"/>)</emphasis>: This step covers the
provisioning of the management cluster using the image created in the
previous step (for both, connected and air-gap scenarios). This step can be
done using a laptop, server, VM or any other x86_64 system with a USB port.</para>
</listitem>
</orderedlist>
<note>
<para>For more information about Edge Image Builder, see Edge Image Builder (<xref
linkend="components-eib"/>) and Edge Image Builder Quick Start (<xref
linkend="quickstart-eib"/>).</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>Image preparation for connected environments</title>
<para>Using Edge Image Builder to create the image for the management cluster, a
lot of configurations can be customized, but in this document, we cover the
minimal configurations necessary to set up the management cluster.  Edge
Image Builder is typically run from inside a container so, if you do not
already have a way to run containers, we need to start by installing a
container runtime such as <link xl:href="https://podman.io">Podman</link> or
<link xl:href="https://rancherdesktop.io">Rancher Desktop</link>. For this
guide, we assume you already have a container runtime available.</para>
<para>Also, as a prerequisite to deploy a highly available management cluster, you
need to reserve three IPs in your network: - <literal>apiVIP</literal> for
the API VIP Address (used to access the Kubernetes API server).  -
<literal>ingressVIP</literal> for the Ingress VIP Address (consumed, for
example, by the Rancher UI).  - <literal>metal3VIP</literal> for the Metal3
VIP Address.</para>
<section xml:id="mgmt-cluster-directory-structure">
<title>Directory structure</title>
<para>When running EIB, a directory is mounted from the host, so the first thing
to do is to create a directory structure to be used by EIB to store the
configuration files and the image itself.  This directory has the following
structure:</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>The image
<literal>SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso</literal>
must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE
Customer Center</link> or the <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE Download
page</link>, and it must be located under the <literal>base-images</literal>
folder.</para>
<para>You should check the SHA256 checksum of the image to ensure it has not been
tampered with. The checksum can be found in the same location where the
image was downloaded.</para>
<para>An example of the directory structure can be found in the <link
xl:href="https://github.com/suse-edge/atip">SUSE Edge GitHub repository
under the "telco-examples" folder</link>.</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>Management cluster definition file</title>
<para>The <literal>mgmt-cluster.yaml</literal> file is the main definition file
for the management cluster. It contains the following information:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.14.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.7.1
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
      - hostname: mgmt-cluster-node1
        initializer: true
        type: server
#     - hostname: mgmt-cluster-node2
#       initializer: true
#       type: server
#     - hostname: mgmt-cluster-node3
#       initializer: true
#       type: server</screen>
<para>To explain the fields and values in the <literal>mgmt-cluster.yaml</literal>
definition file, we have divided it into the following sections.</para>
<itemizedlist>
<listitem>
<para>Image section (definition file):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>where the <literal>baseImage</literal> is the original image you downloaded
from the SUSE Customer Center or the SUSE Download
page. <literal>outputImageName</literal> is the name of the new image that
will be used to provision the management cluster.</para>
<itemizedlist>
<listitem>
<para>Operating system section (definition file):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<para>where the <literal>installDevice</literal> is the device to be used to
install the operating system, the <literal>username</literal> and
<literal>encryptedPassword</literal> are the credentials to be used to
access the system, the <literal>packageList</literal> is the list of
packages to be installed (<literal>jq</literal> is required internally
during the installation process), and the
<literal>sccRegistrationCode</literal> is the registration code used to get
the packages and dependencies at build time and can be obtained from the
SUSE Customer Center.  The encrypted password can be generated using the
<literal>openssl</literal> command as follows:</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>This outputs something similar to:</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Kubernetes section (definition file):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.14.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.7.1
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
      - hostname: mgmt-cluster1
        initializer: true
        type: server
#      - hostname: mgmt-cluster2
#        type: server
#      - hostname: mgmt-cluster3
#        type: server</screen>
<para>where <literal>version</literal> is the version of Kubernetes to be
installed. In our case, we are using an RKE2 cluster, so the version must be
minor less than 1.29 to be compatible with <literal>Rancher</literal> (for
example, <literal>v1.28.9+rke2r1</literal>).</para>
<para>The <literal>helm</literal> section contains the list of Helm charts to be
installed, the repositories to be used, and the version configuration for
all of them.</para>
<para>The <literal>network</literal> section contains the configuration for the
network, like the <literal>apiHost</literal> and <literal>apiVIP</literal>
to be used by the <literal>RKE2</literal> component.  The
<literal>apiVIP</literal> should be an IP address that is not used in the
network and should not be part of the DHCP pool (in case we use DHCP). Also,
when we use the <literal>apiVIP</literal> in a multi-node cluster, it is
used to access the Kubernetes API server.  The <literal>apiHost</literal> is
the name resolution to <literal>apiVIP</literal> to be used by the
<literal>RKE2</literal> component.</para>
<para>The <literal>nodes</literal> section contains the list of nodes to be used
in the cluster. The <literal>nodes</literal> section contains the list of
nodes to be used in the cluster. In this example, a single-node cluster is
being used, but it can be extended to a multi-node cluster by adding more
nodes to the list (by uncommenting the lines).</para>
<note>
<para>The names of the nodes must be unique in the cluster, and the
<literal>initializer</literal> field mustbe set to <literal>true</literal>
for the first node in the list.  The names of the nodes must be the same as
the host names defined in the <literal>network</literal> section matching
directly with the file name in the <literal>network</literal> section.</para>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Custom folder</title>
<para>The <literal>custom</literal> folder contains the following subfolders:</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para>The <literal>custom/files</literal> folder contains the configuration files
to be used by the management cluster.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts</literal> folder contains the scripts to be used
by the management cluster.</para>
</listitem>
</itemizedlist>
<para>The <literal>custom/files</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>: contains the configuration parameters
about the <literal>Metal<superscript>3</superscript></literal> version to be
used, as well as the <literal>Rancher</literal> and
<literal>MetalLB</literal> basic parameters. Only modify this file if you
want to change the versions of the components or the namespaces to be used.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"
export METAL3_CLUSTERCTLVERSION="1.6.2"
export METAL3_CAPICOREVERSION="1.6.2"
export METAL3_CAPIMETAL3VERSION="1.6.0"
export METAL3_CAPIRKE2VERSION="0.2.6"
export METAL3_CAPIPROVIDER="rke2"
export METAL3_CAPISYSTEMNAMESPACE="capi-system"
export METAL3_RKE2BOOTSTRAPNAMESPACE="rke2-bootstrap-system"
export METAL3_CAPM3NAMESPACE="capm3-system"
export METAL3_RKE2CONTROLPLANENAMESPACE="rke2-control-plane-system"
export METAL3_CAPI_IMAGES="registry.suse.com/edge"
# Or registry.opensuse.org/isv/suse/edge/clusterapi/containerfile/suse for the upstream ones

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>: contains the configuration for the
<literal>Metal<superscript>3</superscript></literal> component to be used
(no modifications needed). In future versions, this script will be replaced
to use instead <literal>Rancher Turtles</literal> to make it easy.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic-bmo -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
	EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
	EOF
fi

# If clusterctl is not installed, install it
if ! command -v clusterctl &gt; /dev/null 2&gt;&amp;1; then
  LINUXARCH=$(uname -m)
  case $(uname -m) in
    "x86_64")
      export GOARCH="amd64" ;;
    "aarch64")
      export GOARCH="arm64" ;;
    "*")
      echo "Arch not found, asumming amd64"
      export GOARCH="amd64" ;;
  esac

  # Clusterctl bin
  # Maybe just use the binary from hauler if available
  curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v${METAL3_CLUSTERCTLVERSION}/clusterctl-linux-${GOARCH} -o /usr/local/bin/clusterctl
  chmod +x /usr/local/bin/clusterctl
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Deploy CAPI
if [ $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -o name | wc -l) -lt 1 ]; then

  # https://github.com/rancher-sandbox/cluster-api-provider-rke2#setting-up-clusterctl
  mkdir -p ~/.cluster-api
  cat &lt;&lt;-EOF &gt; ~/.cluster-api/clusterctl.yaml
	images:
	  all:
	    repository: ${METAL3_CAPI_IMAGES}
	EOF

  # Try this command 3 times just in case, stolen from https://stackoverflow.com/a/33354419
  if ! (r=3; while ! clusterctl init \
    --core "cluster-api:v${METAL3_CAPICOREVERSION}"\
    --infrastructure "metal3:v${METAL3_CAPIMETAL3VERSION}"\
    --bootstrap "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}"\
    --control-plane "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}" ; do
            ((--r))||exit
            echo "Something went wrong, let's wait 10 seconds and retry"
            sleep 10;done) ; then
      echo "clusterctl failed"
      exit 1
  fi

  # Wait for capi-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPISYSTEMNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -l cluster.x-k8s.io/provider=cluster-api -o name) --timeout=10s; do sleep 2 ; done

  # Wait for capm3-controller-manager, there are two pods, the ipam and the capm3 one, just wait for the first one
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPM3NAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPM3NAMESPACE} -l cluster.x-k8s.io/provider=infrastructure-metal3 -o name | head -n1 ) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-bootstrap-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} -l cluster.x-k8s.io/provider=bootstrap-rke2 -o name) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-control-plane-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2CONTROLPLANENAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2CONTROLPLANENAMESPACE} -l cluster.x-k8s.io/provider=control-plane-rke2 -o name) --timeout=10s; do sleep 2 ; done

fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>: contains the configuration for the
<literal>Rancher</literal> component to be used (no modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>: contains the configuration to
create the systemd service to run the scripts during the first boot (no
modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single on
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>The <literal>custom/scripts</literal> folder contains the following files:</para>
<itemizedlist>
<listitem>
<para><literal>99-alias.sh</literal> script: contains the alias to be used by the
management cluster to load the kubeconfig file at first boot (no
modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para><literal>99-mgmt-setup.sh</literal> script: contains the configuration to
copy the scripts during the first boot (no modifications needed).</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para><literal>99-register.sh</literal> script: contains the configuration to
register the system using the SCC registration code. The
<literal>${SCC_ACCOUNT_EMAIL}</literal> and
<literal>${SCC_REGISTRATION_CODE}</literal> have to be set properly to
register the system with your account.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
	zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Kubernetes folder</title>
<para>The <literal>kubernetes</literal> folder contains the following subfolders:</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para>The <literal>kubernetes/config</literal> folder contains the following
files:</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>: By default, the <literal>CNI</literal>
plug-in installed by default is <literal>Cilium</literal>, so you do not
need to create this folder and file. Just in case you need to customize the
<literal>CNI</literal> plug-in, you can use the
<literal>server.yaml</literal> file under the
<literal>kubernetes/config</literal> folder. It contains the following
information:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</listitem>
</itemizedlist>
<note>
<para>This is an optional file to define certain Kubernetes customization, like
the CNI plug-ins to be used or many options you can check in the <link
xl:href="https://docs.rke2.io/install/configuration">official
documentation</link>.</para>
</note>
<para>The <literal>kubernetes/manifests</literal> folder contains the following
files:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>: contains the configuration to
create the <literal>Ingress</literal> service for the management cluster (no
modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>: contains the configuration to
create the <literal>NeuVector</literal> namespace (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>: contains the configuration to create
the <literal>L2Advertisement</literal> for the <literal>MetalLB</literal>
component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>: contains the configuration to create
the <literal>IPAddressPool</literal> for the
<literal>rke2-ingress-nginx</literal> component. The
<literal>${INGRESS_VIP}</literal> has to be set properly to define the IP
address reserved to be used by the <literal>rke2-ingress-nginx</literal>
component.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para>The <literal>kubernetes/helm/values</literal> folder contains the following
files:</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>: contains the configuration to create the
<literal>Rancher</literal> component. The <literal>${INGRESS_VIP}</literal>
must be set properly to define the IP address to be consumed by the
<literal>Rancher</literal> component. The URL to access the
<literal>Rancher</literal> component will be <literal><link
xl:href="https://rancher-${INGRESS_VIP}.sslip.io">https://rancher-${INGRESS_VIP}.sslip.io</link></literal>.</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global.cattle.psp.enabled: "false"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>: contains the configuration to create the
<literal>NeuVector</literal> component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>: contains the configuration to create the
<literal>Metal<superscript>3</superscript></literal> component. The
<literal>${METAL3_VIP}</literal> must be set properly to define the IP
address to be consumed by the
<literal>Metal<superscript>3</superscript></literal> component.</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>The Media Server is an optional feature included in
Metal<superscript>3</superscript> (by default is disabled). To use the
Metal3 feature, you need to configure it on the previous manifest.  To use
the Metal<superscript>3</superscript> media server, specify the following
variable:</para>
<itemizedlist>
<listitem>
<para>add the <literal>enable_metal3_media_server</literal> to
<literal>true</literal> to enable the media server feature in the global
section.</para>
</listitem>
<listitem>
<para>include the following configuration about the media server where
${MEDIA_VOLUME_PATH} is the path to the media volume in the media (e.g
<literal>/home/metal3/bmh-image-cache</literal>)</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>An external media server can be used to store the images, and in the case
you want to use it with TLS, you will need to modify the following
configurations:</para>
<itemizedlist>
<listitem>
<para>set to <literal>true</literal> the <literal>additionalTrustedCAs</literal>
in the previous <literal>metal3.yaml</literal> file to enable the additional
trusted CAs from the external media server.</para>
</listitem>
<listitem>
<para>include the following secret configuration in the folder
<literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal> to store
the CA certificate of the external media server.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para>The <literal>additional_ca_cert</literal> is the base64-encoded CA
certificate of the external media server. You can use the following command
to encode the certificate and generate the secret doing manually:</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>: contains the configuration to create
the <literal>Cert-Manager</literal> component (no modifications needed).</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: "true"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>Networking folder</title>
<para>The <literal>network</literal> folder contains as many files as nodes in the
management cluster. In our case, we have only one node, so we have only one
file called <literal>mgmt-cluster-node1.yaml</literal>.  The name of the
file must match the host name defined in the
<literal>mgmt-cluster.yaml</literal> definition file into the network/node
section described above.</para>
<para>If you need to customize the networking configuration, for example, to use a
specific static IP address (DHCP-less scenario), you can use the
<literal>mgmt-cluster-node1.yaml</literal> file under the
<literal>network</literal> folder. It contains the following information:</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: The gateway IP address.</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: The DNS server IP address.</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: The MAC address of the network interface.</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: The IP address of the management
cluster.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>If you want to use DHCP to get the IP address, you can use the following
configuration (the <literal>MAC</literal> address must be set properly using
the <literal>${MGMT_MAC}</literal> variable):</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
## interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>Depending on the number of nodes in the management cluster, you can create
more files like <literal>mgmt-cluster-node2.yaml</literal>,
<literal>mgmt-cluster-node3.yaml</literal>, etc. to configure the rest of
the nodes.</para>
</listitem>
<listitem>
<para>The <literal>routes</literal> section is used to define the routing table
for the management cluster.</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>Image preparation for air-gap environments</title>
<para>This section describes how to prepare the image for air-gap environments
showing only the differences from the previous sections. The following
changes to the previous section (Image preparation for connected
environments (<xref linkend="mgmt-cluster-image-preparation-connected"/>))
are required to prepare the image for air-gap environments:</para>
<itemizedlist>
<listitem>
<para>The <literal>mgmt-cluster.yaml</literal> file must be modified to include
the <literal>embeddedArtifactRegistry</literal> section with the
<literal>images</literal> field set to all container images to be included
into the EIB output image.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts/99-register.sh</literal> script must be removed
when use an air-gap environment.</para>
</listitem>
<listitem>
<para>The <literal>custom/files/airgap-resources.tar.gz</literal> file must be
included in the <literal>custom/files</literal> folder with all the
resources needed to run the management cluster in an air-gap environment.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts/99-mgmt-setup.sh</literal> script must be
modified to extract and copy the <literal>airgap-resources.tar.gz</literal>
file to the final location.</para>
</listitem>
<listitem>
<para>The <literal>custom/files/metal3.sh</literal> script must be modified to use
the local resources included in the
<literal>airgap-resources.tar.gz</literal> file instead of downloading them
from the internet.</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure-airgap">
<title>Directory structure for air-gap environments</title>
<para>The directory structure for air-gap environments is the same as for
connected environments, with the differences explained as follows:</para>
<screen language="console" linenumbering="unnumbered">eib
|-- base-images
|   |-- SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
|-- custom
|   |-- files
|   |   |-- airgap-resources.tar.gz
|   |   |-- basic-setup.sh
|   |   |-- metal3.sh
|   |   |-- mgmt-stack-setup.service
|   |   |-- rancher.sh
|   |-- scripts
|       |-- 99-alias.sh
|       |-- 99-mgmt-setup.sh
|-- kubernetes
|   |-- config
|   |   |-- server.yaml
|   |-- helm
|   |   |-- values
|   |       |-- certmanager.yaml
|   |       |-- metal3.yaml
|   |       |-- neuvector.yaml
|   |       |-- rancher.yaml
|   |-- manifests
|       |-- neuvector-namespace.yaml
|-- mgmt-cluster.yaml
|-- network
    |-- mgmt-cluster-network.yaml</screen>
<note>
<para>The image
<literal>SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso</literal>
must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE
Customer Center</link> or the <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE Download
page</link>, and it must be located under the <literal>base-images</literal>
folder before starting with the process.</para>
<para>You should check the SHA256 checksum of the image to ensure it has not been
tampered with. The checksum can be found in the same location where the
image was downloaded.</para>
<para>An example of the directory structure can be found in the <link
xl:href="https://github.com/suse-edge/atip">SUSE Edge GitHub repository
under the "telco-examples" folder</link>.</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>Modifications in the definition file</title>
<para>The <literal>mgmt-cluster.yaml</literal> file must be modified to include
the <literal>embeddedArtifactRegistry</literal> section with the
<literal>images</literal> field set to all container images to be included
into the EIB output image. The <literal>images</literal> field must contain
the list of all container images to be included in the output image. The
following is an example of the <literal>mgmt-cluster.yaml</literal> file
with the <literal>embeddedArtifactRegistry</literal> section included:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.14.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.7.1
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
      - hostname: mgmt-cluster-node1
        initializer: true
        type: server
#     - hostname: mgmt-cluster-node2
#       initializer: true
#       type: server
#     - hostname: mgmt-cluster-node3
#       initializer: true
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v4.0.2
    - name: registry.rancher.com/rancher/calico-cni:v3.27.0-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.0.13
    - name: registry.rancher.com/rancher/coreos-kube-state-metrics:v1.9.7
    - name: registry.rancher.com/rancher/coreos-prometheus-config-reloader:v0.38.1
    - name: registry.rancher.com/rancher/coreos-prometheus-operator:v0.38.1
    - name: registry.rancher.com/rancher/flannel-cni:v0.3.0-rancher9
    - name: registry.rancher.com/rancher/fleet-agent:v0.9.4
    - name: registry.rancher.com/rancher/fleet:v0.9.4
    - name: registry.rancher.com/rancher/gitjob:v0.9.7
    - name: registry.rancher.com/rancher/grafana-grafana:7.1.5
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240410
    - name: registry.rancher.com/rancher/hardened-calico:v3.27.3-build20240423
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.10-build20240124
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.4.1-build20240325
    - name: registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240305
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.22.28-build20240125
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.9-k3s1-build20240418
    - name: registry.rancher.com/rancher/hardened-flannel:v0.25.1-build20240423
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240401
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.28.9-rke2r1-build20240416
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.0.2-build20240208
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.14.1-build20230926
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.6.3-build20240208
    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1
    - name: registry.rancher.com/rancher/istio-kubectl:1.5.10
    - name: registry.rancher.com/rancher/jimmidyson-configmap-reload:v0.3.0
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.8.3-build20240228
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.7
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.1
    - name: registry.rancher.com/rancher/kubectl:v1.28.7
    - name: registry.rancher.com/rancher/library-nginx:1.19.2-alpine
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.26
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher112
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.4.4
    - name: registry.rancher.com/rancher/nginx-ingress-controller:nginx-1.9.6-rancher1
    - name: registry.rancher.com/rancher/pause:3.6
    - name: registry.rancher.com/rancher/prom-alertmanager:v0.21.0
    - name: registry.rancher.com/rancher/prom-node-exporter:v1.0.1
    - name: registry.rancher.com/rancher/prom-prometheus:v2.18.2
    - name: registry.rancher.com/rancher/prometheus-auth:v0.2.2
    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.0-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.0-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.8.4
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v3.0.1
    - name: registry.rancher.com/rancher/rancher-webhook:v0.4.5
    - name: registry.rancher.com/rancher/rancher:v2.8.4
    - name: registry.rancher.com/rancher/rke-tools:v0.1.96
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.29.3-build20240412
    - name: registry.rancher.com/rancher/rke2-runtime:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.2.15
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.6-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.1
    - name: registry.rancher.com/rancher/ui-plugin-catalog:1.3.0
    - name: registry.rancher.com/rancher/ui-plugin-operator:v0.1.1
    - name: registry.rancher.com/rancher/webhook-receiver:v0.2.5
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.4.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v3.6.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.9.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v6.3.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.9.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.12.0
    - name: registry.rancher.com/rancher/mirrored-longhornio-backing-image-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.36
    - name: registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:v0.2.6
    - name: registry.suse.com/edge/cluster-api-provider-rke2-controlplane:v0.2.6
    - name: registry.suse.com/edge/cluster-api-controller:v1.6.2
    - name: registry.suse.com/edge/cluster-api-provider-metal3:v1.6.0
    - name: registry.suse.com/edge/ip-address-manager:v1.6.0</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>Modifications in the custom folder</title>
<itemizedlist>
<listitem>
<para>The <literal>custom/scripts/99-register.sh</literal> script must be removed
when using an air-gap environment. As you can see in the directory
structure, the <literal>99-register.sh</literal> script is not included in
the <literal>custom/scripts</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts/99-mgmt-setup.sh</literal> script must be
modified to extract and copy the <literal>airgap-resources.tar.gz</literal>
file to the final location. The following is an example of the
<literal>99-mgmt-setup.sh</literal> script with the modifications to extract
and copy the <literal>airgap-resources.tar.gz</literal> file:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service

# Extract the airgap resources
tar zxf airgap-resources.tar.gz

# Copy the clusterctl binary to the final location
cp airgap-resources/clusterctl /opt/mgmt/bin/ &amp;&amp; chmod +x /opt/mgmt/bin/clusterctl

# Copy the clusterctl.yaml and override
mkdir -p /root/cluster-api
cp -r airgap-resources/clusterctl.yaml airgap-resources/overrides /root/cluster-api/</screen>
</listitem>
<listitem>
<para>The <literal>custom/files/metal3.sh</literal> script must be modified to use
the local resources included in the
<literal>airgap-resources.tar.gz</literal> file instead of downloading them
from the internet. The following is an example of the
<literal>metal3.sh</literal> script with the modifications to use the local
resources:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Deploy CAPI
if [ $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -o name | wc -l) -lt 1 ]; then

  # Try this command 3 times just in case, stolen from https://stackoverflow.com/a/33354419
  if ! (r=3; while ! /opt/mgmt/bin/clusterctl init \
    --core "cluster-api:v${METAL3_CAPICOREVERSION}"\
    --infrastructure "metal3:v${METAL3_CAPIMETAL3VERSION}"\
    --bootstrap "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}"\
    --control-plane "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}"\
    --config /root/cluster-api/clusterctl.yaml ; do
            ((--r))||exit
            echo "Something went wrong, let's wait 10 seconds and retry"
            sleep 10;done) ; then
      echo "clusterctl failed"
      exit 1
  fi

  # Wait for capi-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPISYSTEMNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -l cluster.x-k8s.io/provider=cluster-api -o name) --timeout=10s; do sleep 2 ; done

  # Wait for capm3-controller-manager, there are two pods, the ipam and the capm3 one, just wait for the first one
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPM3NAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPM3NAMESPACE} -l cluster.x-k8s.io/provider=infrastructure-metal3 -o name | head -n1 ) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-bootstrap-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} -l cluster.x-k8s.io/provider=bootstrap-rke2 -o name) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-control-plane-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2CONTROLPLANENAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2CONTROLPLANENAMESPACE} -l cluster.x-k8s.io/provider=control-plane-rke2 -o name) --timeout=10s; do sleep 2 ; done

fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para>The <literal>custom/files/airgap-resources.tar.gz</literal> file must be
included in the <literal>custom/files</literal> folder with all the
resources needed to run the management cluster in an air-gap
environment. This file must be prepared manually downloading all resources
and compressing them into this single file. The
<literal>airgap-resources.tar.gz</literal> file contains the following
resources:</para>
<screen language="console" linenumbering="unnumbered">|-- clusterctl
|-- clusterctl.yaml
|-- overrides
    |-- bootstrap-rke2
    |   |-- v0.2.6
    |       |-- bootstrap-components.yaml
    |       |-- metadata.yaml
    |-- cluster-api
    |   |-- v1.6.2
    |       |-- core-components.yaml
    |       |-- metadata.yaml
    |-- control-plane-rke2
    |   |-- v0.2.6
    |       |-- control-plane-components.yaml
    |       |-- metadata.yaml
    |-- infrastructure-metal3
        |-- v1.6.0
            |-- cluster-template.yaml
            |-- infrastructure-components.yaml
            |-- metadata.yaml</screen>
</listitem>
</itemizedlist>
<para>The <literal>clusterctl.yaml</literal> file contains the configuration to
specify the images location and the overrides to be used by the
<literal>clusterctl</literal> tool. The <literal>overrides</literal> folder
contains <literal>yaml</literal> file manifests to be used instead of
downloading them from the internet.</para>
<screen language="yaml" linenumbering="unnumbered">providers:
  # override a pre-defined provider
  - name: "cluster-api"
    url: "/root/cluster-api/overrides/cluster-api/v1.6.2/core-components.yaml"
    type: "CoreProvider"
  - name: "metal3"
    url: "/root/cluster-api/overrides/infrastructure-metal3/v1.6.0/infrastructure-components.yaml"
    type: "InfrastructureProvider"
  - name: "rke2"
    url: "/root/cluster-api/overrides/bootstrap-rke2/v0.2.6/bootstrap-components.yaml"
    type: "BootstrapProvider"
  - name: "rke2"
    url: "/root/cluster-api/overrides/control-plane-rke2/v0.2.6/control-plane-components.yaml"
    type: "ControlPlaneProvider"
images:
  all:
    repository: registry.suse.com/edge</screen>
<para>The <literal>clusterctl</literal> and the rest of the files included in the
<literal>overrides</literal> folder can be downloaded using the following
curls commands:</para>
<screen language="shell" linenumbering="unnumbered"># clusterctl binary
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/1.6.2/clusterctl-linux-${GOARCH} -o /usr/local/bin/clusterct

# boostrap-components (boostrap-rke2)
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/bootstrap-components.yaml
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/metadata.yaml

# control-plane-components (control-plane-rke2)
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/control-plane-components.yaml
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/metadata.yaml

# cluster-api components
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.2/core-components.yaml
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.2/metadata.yaml

# infrastructure-components (infrastructure-metal3)
curl -L https://github.com/metal3-io/cluster-api-provider-metal3/releases/download/v1.6.0/infrastructure-components.yaml
curl -L https://github.com/metal3-io/cluster-api-provider-metal3/releases/download/v1.6.0/metadata.yaml</screen>
<note>
<para>If you want to use different versions of the components, you can change the
version in the URL to download the specific version of the components.</para>
</note>
<para>With the previous resources downloaded, you can compress them into a single
file using the following command:</para>
<screen language="shell" linenumbering="unnumbered">tar -czvf airgap-resources.tar.gz clusterctl clusterctl.yaml overrides</screen>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>Image creation</title>
<para>Once the directory structure is prepared following the previous sections
(for both, connected and air-gap scenarios), run the following command to
build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>This creates the ISO output image file that, in our case, based on the image
definition described above, is
<literal>eib-mgmt-cluster-image.iso</literal>.</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>Provision the management cluster</title>
<para>The previous image contains all components explained above, and it can be
used to provision the management cluster using a virtual machine or a
bare-metal server (using the virtual-media feature).</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>Telco features configuration</title>
<para>This section documents and explains the configuration of Telco-specific
features on ATIP-deployed clusters.</para>
<para>The directed network provisioning deployment method is used, as described in
the ATIP Automated Provision (<xref linkend="atip-automated-provisioning"/>)
section.</para>
<para>The following topics are covered in this section:</para>
<itemizedlist>
<listitem>
<para>Kernel image for real time (<xref linkend="kernel-image-for-real-time"/>):
Kernel image to be used by the real-time kernel.</para>
</listitem>
<listitem>
<para>CPU tuned configuration (<xref linkend="cpu-tuned-configuration"/>): Tuned
configuration to be used by the real-time kernel.</para>
</listitem>
<listitem>
<para>CNI configuration (<xref linkend="cni-configuration"/>): CNI configuration
to be used by the Kubernetes cluster.</para>
</listitem>
<listitem>
<para>SR-IOV configuration (<xref linkend="sriov"/>): SR-IOV configuration to be
used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>DPDK configuration (<xref linkend="dpdk"/>): DPDK configuration to be used
by the system.</para>
</listitem>
<listitem>
<para>vRAN acceleration card (<xref linkend="acceleration"/>): Acceleration card
configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Huge pages (<xref linkend="huge-pages"/>): Huge pages configuration to be
used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>CPU pinning configuration (<xref linkend="cpu-pinning-configuration"/>): CPU
pinning configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>NUMA-aware scheduling configuration (<xref
linkend="numa-aware-scheduling"/>): NUMA-aware scheduling configuration to
be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Metal LB configuration (<xref linkend="metal-lb-configuration"/>): Metal LB
configuration to be used by the Kubernetes workloads.</para>
</listitem>
<listitem>
<para>Private registry configuration (<xref linkend="private-registry"/>): Private
registry configuration to be used by the Kubernetes workloads.</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>Kernel image for real time</title>
<para>The real-time kernel image is not necessarily better than a standard
kernel.  It is a different kernel tuned to a specific use case. The
real-time kernel is tuned for lower latency at the cost of throughput. The
real-time kernel is not recommended for general purpose use, but in our
case, this is the recommended kernel for Telco Workloads where latency is a
key factor.</para>
<para>There are four top features:</para>
<itemizedlist>
<listitem>
<para>Deterministic execution:</para>
<para>Get greater predictability — ensure critical business processes complete in
time, every time and deliver high-quality service, even under heavy system
loads. By shielding key system resources for high-priority processes, you
can ensure greater predictability for time-sensitive applications.</para>
</listitem>
<listitem>
<para>Low jitter:</para>
<para>The low jitter built upon the highly deterministic technology helps to keep
applications synchronized with the real world. This helps services that need
ongoing and repeated calculation.</para>
</listitem>
<listitem>
<para>Priority inheritance:</para>
<para>Priority inheritance refers to the ability of a lower priority process to
assume a higher priority when there is a higher priority process that
requires the lower priority process to finish before it can accomplish its
task. SUSE Linux Enterprise Real Time solves these priority inversion
problems for mission-critical processes.</para>
</listitem>
<listitem>
<para>Thread interrupts:</para>
<para>Processes running in interrupt mode in a general-purpose operating system
are not preemptible. With SUSE Linux Enterprise Real Time, these interrupts
have been encapsulated by kernel threads, which are interruptible, and allow
the hard and soft interrupts to be preempted by user-defined higher priority
processes.</para>
<para>In our case, if you have installed a real-time image like <literal>SLE Micro
RT</literal>, kernel real time is already installed. From the <link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>, you can
download the real-time kernel image.</para>
<note>
<para>For more information about the real-time kernel, visit <link
xl:href="https://www.suse.com/products/realtime/">SUSE Real Time</link>.</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="cpu-tuned-configuration">
<title>CPU tuned configuration</title>
<para>The CPU Tuned configuration allows the possibility to isolate the CPU cores
to be used by the real-time kernel. It is important to prevent the OS from
using the same cores as the real-time kernel, because the OS could use the
cores and increase the latency in the real-time kernel.</para>
<para>To enable and configure this feature, the first thing is to create a profile
for the CPU cores we want to isolate. In this case, we are isolating the
cores <literal>1-30</literal> and <literal>33-62</literal>.</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>Then we need to modify the GRUB option to isolate CPU cores and other
important parameters for CPU usage.  The following options are important to
be customized with your current hardware specifications:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Isolate the cores 1-30 and 33-62</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to skew the timer interrupts across the
isolated CPUs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to run the timer tick on a single CPU when the
system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>kernel boot parameter is the current main interface to configure full
dynticks along with CPU Isolation.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to run the RCU callbacks on a single CPU when
the system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>kthread_cpus</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to run the kthreads on a single CPU when the
system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>This option allows the kernel to run the interrupts on a single CPU when the
system is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>processor.max_cstate</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>This option prevents the CPU from dropping into a sleep state when idle</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_idle.max_cstate</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>This option disables the intel_idle driver and allows acpi_idle to be used</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>With the values shown above, we are isolating 60 cores, and we are using
four cores for the OS.</para>
<para>The following commands modify the GRUB configuration and apply the changes
mentioned above to be present on the next boot:</para>
<para>Edit the <literal>/etc/default/grub</literal> file and add the parameters
mentioned above:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>Update the GRUB configuration:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>To validate that the parameters are applied after the reboot, the following
command can be used to check the kernel command line:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
</section>
<section xml:id="cni-configuration">
<title>CNI Configuration</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal> is the default CNI plug-in for ATIP.  To enable
Cilium on RKE2 cluster as the default plug-in, the following configurations
are required in the <literal>/etc/rancher/rke2/config.yaml</literal> file:</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>This can also be specified with command-line arguments, that is,
<literal>--cni=cilium</literal> into the server line in
<literal>/etc/systemd/system/rke2-server</literal> file.</para>
<para>To use the <literal>SR-IOV</literal> network operator described in the next
section (<xref linkend="option2-sriov-helm"/>), use
<literal>Multus</literal> with another CNI plug-in, like
<literal>Cilium</literal> or <literal>Calico</literal>, as a secondary
plug-in.</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
<note>
<para>For more information about CNI plug-ins, visit <link
xl:href="https://docs.rke2.io/install/network_options">Network
Options</link>.</para>
</note>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOV allows a device, such as a network adapter, to separate access to its
resources among various <literal>PCIe</literal> hardware functions.  There
are different ways to deploy <literal>SR-IOV</literal>, and here, we show
two different options:</para>
<itemizedlist>
<listitem>
<para>Option 1: using the <literal>SR-IOV</literal> CNI device plug-ins and a
config map to configure it properly.</para>
</listitem>
<listitem>
<para>Option 2 (recommended): using the <literal>SR-IOV</literal> Helm chart from
Rancher Prime to make this deployment easy.</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">Option 1 - Installation of SR-IOV CNI device
plug-ins and a config map to configure it properly</emphasis></para>
<itemizedlist>
<listitem>
<para>Prepare the config map for the device plug-in</para>
</listitem>
</itemizedlist>
<para>Get the information to fill the config map from the <literal>lspci</literal>
command:</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>The config map consists of a <literal>JSON</literal> file that describes
devices using filters to discover, and creates groups for the interfaces.
The key is understanding filters and groups. The filters are used to
discover the devices and the groups are used to create the interfaces.</para>
<para>It could be possible to set filters:</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (Accelerator card)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (driver)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (physical interface name)</para>
</listitem>
</itemizedlist>
<para>It could be possible to also set filters to match more complex interface
syntax, for example:</para>
<itemizedlist>
<listitem>
<para>pfNames: <literal>["eth1#1,2,3,4,5,6"]</literal> or
<literal>[eth1#1-6]</literal> (physical interface name)</para>
</listitem>
</itemizedlist>
<para>Related to the groups, we could create a group for the
<literal>FEC</literal> card and another group for the
<literal>Intel</literal> card, even creating a prefix depending on our use
case:</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>There are a lot of combinations to discover and create the resource group to
allocate some <literal>VFs</literal> to the pods.</para>
<note>
<para>For more information about the filters and groups, visit <link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sr-iov
network device plug-in</link>.</para>
</note>
<para>After setting the filters and groups to match the interfaces depending on
the hardware and the use case, the following config map shows an example to
be used:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para>Prepare the <literal>daemonset</literal> file to deploy the device plug-in.</para>
</listitem>
</itemizedlist>
<para>The device plug-in supports several architectures (<literal>arm</literal>,
<literal>amd</literal>, <literal>ppc64le</literal>), so the same file can be
used for different architectures deploying several
<literal>daemonset</literal> for each architecture.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.5.1-build20231009-amd64
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>After applying the config map and the <literal>daemonset</literal>, the
device plug-in will be deployed and the interfaces will be discovered and
available for the pods.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Check the interfaces discovered and available in the nodes to be used by the
pods:</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para>The <literal>FEC</literal> is <literal>intel.com/intel_fec_5g</literal> and
the value is 1.</para>
</listitem>
<listitem>
<para>The <literal>VF</literal> is <literal>intel.com/intel_sriov_odu</literal> or
<literal>intel.com/intel_sriov_oru</literal> if you deploy it with a device
plug-in and the config map without Helm charts.</para>
</listitem>
</itemizedlist>
<important>
<para>If there are no interfaces here, it makes little sense to continue because
the interface will not be available for pods. Review the config map and
filters to solve the issue first.</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">Option 2 (recommended) - Installation using Rancher
using Helm chart for SR-IOV CNI and device plug-ins</emphasis></para>
<itemizedlist>
<listitem>
<para>Get Helm if not present:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>Install SR-IOV.</para>
</listitem>
</itemizedlist>
<para>This part could be done in two ways, using the <literal>CLI</literal> or
using the <literal>Rancher UI</literal>.</para>
<variablelist>
<varlistentry>
<term>Install Operator from CLI</term>
<listitem>
<screen>helm repo add suse-edge https://suse-edge.github.io/charts
helm install sriov-crd suse-edge/sriov-crd -n sriov-network-operator
helm install install sriov-network-operator suse-edge/sriov-network-operator -n sriov-network-operator</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Install Operator from Rancher UI</term>
<listitem>
<para>Once your cluster is installed, and you have access to the <literal>Rancher
UI</literal>, you can install the <literal>SR-IOV Operator</literal> from
the <literal>Rancher UI</literal> from the apps tab:</para>
</listitem>
</varlistentry>
</variablelist>
<note>
<para>Make sure you select the right namespace to install the operator, for
example, <literal>sriov-network-operator</literal>.</para>
</note>
<para>+ image::features_sriov.png[sriov.png]</para>
<itemizedlist>
<listitem>
<para>Check the deployed resources crd and pods:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>Check the label in the nodes.</para>
</listitem>
</itemizedlist>
<para>With all resources running, the label appears automatically in your node:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para>Review the <literal>daemonset</literal> to see the new
<literal>sriov-network-config-daemon</literal> and
<literal>sriov-rancher-nfd-worker</literal> as active and ready:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>In a few minutes (can take up to 10 min to be updated), the nodes are
detected and configured with the <literal>SR-IOV</literal> capabilities:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>Check the interfaces detected.</para>
</listitem>
</itemizedlist>
<para>The interfaces discovered should be the PCI address of the network
device. Check this information with the <literal>lspci</literal> command in
the host.</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>If your interface is not detected here, ensure that it is present in the
next config map:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>If your device is not there, edit the config map, adding the right values to
be discovered (should be necessary to restart the
<literal>sriov-network-config-daemon</literal> daemonset).</para>
</note>
<itemizedlist>
<listitem>
<para>Create the <literal>NetworkNode Policy</literal> to configure the
<literal>VFs</literal>.</para>
</listitem>
</itemizedlist>
<para>Some <literal>VFs</literal> (<literal>numVfs</literal>) from the device
(<literal>rootDevices</literal>) will be created, and it will be configured
with the driver <literal>deviceType</literal> and the
<literal>MTU</literal>:</para>
<note>
<para>The <literal>resourceName</literal> field must not contain any special
characters and must be unique across the cluster.  The example uses the
<literal>deviceType: vfio-pci</literal> because <literal>dpdk</literal> will
be used in combination with <literal>sr-iov</literal>. If you don’t use
<literal>dpdk</literal>, the deviceType should be <literal>deviceType:
netdevice</literal> (default value).</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>Validate configurations:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>Create the sr-iov network (optional, just in case a different network is
needed):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>Check the network created:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.3.1", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal> (Data Plane Development Kit) is a set of libraries
and drivers for fast packet processing. It is used to accelerate packet
processing workloads running on a wide variety of CPU architectures.  The
DPDK includes data plane libraries and optimized network interface
controller (<literal>NIC</literal>) drivers for the following:</para>
<orderedlist numeration="arabic">
<listitem>
<para>A queue manager implements lockless queues.</para>
</listitem>
<listitem>
<para>A buffer manager pre-allocates fixed size buffers.</para>
</listitem>
<listitem>
<para>A memory manager allocates pools of objects in memory and uses a ring to
store free objects; ensures that objects are spread equally on all
<literal>DRAM</literal> channels.</para>
</listitem>
<listitem>
<para>Poll mode drivers (<literal>PMD</literal>) are designed to work without
asynchronous notifications, reducing overhead.</para>
</listitem>
<listitem>
<para>A packet framework as a set of libraries that are helpers to develop packet
processing.</para>
</listitem>
</orderedlist>
<para>The following steps will show how to enable <literal>DPDK</literal> and how
to create <literal>VFs</literal> from the <literal>NICs</literal> to be used
by the <literal>DPDK</literal> interfaces:</para>
<itemizedlist>
<listitem>
<para>Install the <literal>DPDK</literal> package:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk22 dpdk22-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Kernel parameters:</para>
</listitem>
</itemizedlist>
<para>To use DPDK, employ some drivers to enable certain parameters in the kernel:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>This option enables the use of the <literal>vfio</literal> driver for the
DPDK interfaces.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>This option enables the use of <literal>vfio</literal> for
<literal>VFs</literal>.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>To enable the parameters, add them to the
<literal>/etc/default/grub</literal> file:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>Update the GRUB configuration and reboot the system to apply the changes:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para>Load <literal>vfio-pci</literal> kernel module and enable
<literal>SR-IOV</literal> on the <literal>NICs</literal>:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Create some virtual functions (<literal>VFs</literal>) from the
<literal>NICs</literal>.</para>
</listitem>
</itemizedlist>
<para>To create for <literal>VFs</literal>, for example, for two different
<literal>NICs</literal>, the following commands are required:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>Bind the new VFs with the <literal>vfio-pci</literal> driver:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>Review the configuration is correctly applied:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>vRAN acceleration (<literal>Intel ACC100/ACC200</literal>)</title>
<para>As communications service providers move from 4 G to 5 G networks, many are
adopting virtualized radio access network (<literal>vRAN</literal>)
architectures for higher channel capacity and easier deployment of
edge-based services and applications. vRAN solutions are ideally located to
deliver low-latency services with the flexibility to increase or decrease
capacity based on the volume of real-time traffic and demand on the network.</para>
<para>One of the most compute-intensive 4 G and 5 G workloads is RAN layer 1
(<literal>L1</literal>) <literal>FEC</literal>, which resolves data
transmission errors over unreliable or noisy communication
channels. <literal>FEC</literal> technology detects and corrects a limited
number of errors in 4 G or 5 G data, eliminating the need for
retransmission. Since the <literal>FEC</literal> acceleration transaction
does not contain cell state information, it can be easily virtualized,
enabling pooling benefits and easy cell migration.</para>
<itemizedlist>
<listitem>
<para>Kernel parameters</para>
</listitem>
</itemizedlist>
<para>To enable the <literal>vRAN</literal> acceleration, we need to enable the
following kernel parameters (if not present yet):</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>This option enables the use of vfio for the DPDK interfaces.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>This option enables the use of vfio for VFs.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modify the GRUB file <literal>/etc/default/grub</literal> to add them to the
kernel command line:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>Update the GRUB configuration and reboot the system to apply the changes:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>To verify that the parameters are applied after the reboot, check the
command line:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Load vfio-pci kernel modules to enable the <literal>vRAN</literal>
acceleration:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>Get interface information Acc100:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>Bind the physical interface (<literal>PF</literal>) with
<literal>vfio-pci</literal> driver:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>Create the virtual functions (<literal>VFs</literal>) from the physical
interface (<literal>PF</literal>).</para>
</listitem>
</itemizedlist>
<para>Create 2 <literal>VFs</literal> from the <literal>PF</literal> and bind with
<literal>vfio-pci</literal> following the next steps:</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>Configure acc100 with the proposed configuration file:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>Check the new VFs created from the FEC PF:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>Huge pages</title>
<para>When a process uses <literal>RAM</literal>, the <literal>CPU</literal> marks
it as used by that process. For efficiency, the <literal>CPU</literal>
allocates <literal>RAM</literal> in chunks <literal>4K</literal> bytes is
the default value on many platforms. Those chunks are named pages. Pages can
be swapped to disk, etc.</para>
<para>Since the process address space is virtual, the <literal>CPU</literal> and
the operating system need to remember which pages belong to which process,
and where each page is stored. The greater the number of pages, the longer
the search for memory mapping. When a process uses <literal>1 GB</literal>
of memory, that is 262144 entries to look up (<literal>1 GB</literal> /
<literal>4 K</literal>). If a page table entry consumes 8 bytes, that is
<literal>2 MB</literal> (262144 * 8) to look up.</para>
<para>Most current <literal>CPU</literal> architectures support
larger-than-default pages, which give the <literal>CPU/OS</literal> fewer
entries to look up.</para>
<itemizedlist>
<listitem>
<para>Kernel parameters</para>
</listitem>
</itemizedlist>
<para>To enable the huge pages, we should add the next kernel parameters:</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">parameter</entry>
<entry align="left" valign="top">value</entry>
<entry align="left" valign="top">description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>This option allows to set the size of huge pages to 1 G</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>This is the number of huge pages defined before</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>This is the default value to get the huge pages</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Modify the GRUB file <literal>/etc/default/grub</literal> to add them to the
kernel command line:</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>Update the GRUB configuration and reboot the system to apply the changes:</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>To validate that the parameters are applied after the reboot, you can check
the command line:</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Using huge pages</para>
</listitem>
</itemizedlist>
<para>To use the huge pages, we need to mount them:</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Deploy a Kubernetes workload, creating the resources and the volumes:</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-configuration">
<title>CPU pinning configuration</title>
<itemizedlist>
<listitem>
<para>Requirements</para>
<orderedlist numeration="arabic">
<listitem>
<para>Must have the <literal>CPU</literal> tuned to the performance profile
covered in this section (<xref linkend="cpu-tuned-configuration"/>).</para>
</listitem>
<listitem>
<para>Must have the <literal>RKE2</literal> cluster kubelet configured with the
CPU management arguments adding the following block (as an example) to the
<literal>/etc/rancher/rke2/config.yaml</literal> file:</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager=true"
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=1"
- "system-reserved=cpu=1"</screen>
<itemizedlist>
<listitem>
<para>Using CPU pinning on Kubernetes</para>
</listitem>
</itemizedlist>
<para>There are three ways to use that feature using the <literal>Static
Policy</literal> defined in kubelet depending on the requests and limits you
define on your workload:</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>BestEffort</literal> QoS Class: If you do not define any request or
limit for <literal>CPU</literal>, the pod is scheduled on the first
<literal>CPU</literal> available on the system.</para>
<para>An example of using the <literal>BestEffort</literal> QoS Class could be:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para><literal>Burstable</literal> QoS Class: If you define a request for CPU,
which is not equal to the limits, or there is no CPU request.</para>
<para>Examples of using the <literal>Burstable</literal> QoS Class could be:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>or</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para><literal>Guaranteed</literal> QoS Class: If you define a request for CPU,
which is equal to the limits.</para>
<para>An example of using the <literal>Guaranteed</literal> QoS Class could be:</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="numa-aware-scheduling">
<title>NUMA-aware scheduling</title>
<para>Non-Uniform Memory Access or Non-Uniform Memory Architecture
(<literal>NUMA</literal>) is a physical memory design used in
<literal>SMP</literal> (multiprocessors) architecture, where the memory
access time depends on the memory location relative to a processor. Under
<literal>NUMA</literal>, a processor can access its own local memory faster
than non-local memory, that is, memory local to another processor or memory
shared between processors.</para>
<section xml:id="id-identifying-numa-nodes">
<title>Identifying NUMA nodes</title>
<para>To identify the <literal>NUMA</literal> nodes, on your system use the
following command:</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>For this example, we have only one <literal>NUMA</literal> node showing 64
<literal>CPUs</literal>.</para>
<para><literal>NUMA</literal> needs to be enabled in the
<literal>BIOS</literal>. If <literal>dmesg</literal> does not have records
of NUMA initialization during the bootup, then
<literal>NUMA</literal>-related messages in the kernel ring buffer might
have been overwritten.</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>Metal LB</title>
<para><literal>MetalLB</literal> is a load-balancer implementation for bare-metal
Kubernetes clusters, using standard routing protocols like
<literal>L2</literal> and <literal>BGP</literal> as advertisement
protocols. It is a network load balancer that can be used to expose services
in a Kubernetes cluster to the outside world due to the need to use
Kubernetes Services type <literal>LoadBalancer</literal> with bare metal.</para>
<para>To enable <literal>MetalLB</literal> in the <literal>RKE2</literal> cluster,
the following steps are required:</para>
<itemizedlist>
<listitem>
<para>Install <literal>MetalLB</literal> using the following command:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  repo: https://metallb.github.io/metallb/
  chart: metallb
  targetNamespace: metallb-system
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  repo: https://suse-edge.github.io/endpoint-copier-operator
  chart: endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
EOF</screen>
<itemizedlist>
<listitem>
<para>Create the <literal>IpAddressPool</literal> and the
<literal>L2advertisement</literal> configuration:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para>Create the endpoint service to expose the <literal>VIP</literal>:</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para>Check the <literal>VIP</literal> is created and the
<literal>MetalLB</literal> pods are running:</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>Private registry configuration</title>
<para><literal>Containerd</literal> can be configured to connect to private
registries and use them to pull private images on each node.</para>
<para>Upon startup, <literal>RKE2</literal> checks if a
<literal>registries.yaml</literal> file exists at
<literal>/etc/rancher/rke2/</literal> and instructs
<literal>containerd</literal> to use any registries defined in the file. If
you wish to use a private registry, create this file as root on each node
that will use the registry.</para>
<para>To add the private registry, create the file
<literal>/etc/rancher/rke2/registries.yaml</literal> with the following
content:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>or without authentication:</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>For the registry changes to take effect, you need to either configure this
file before starting RKE2 on the node, or restart RKE2 on each configured
node.</para>
<note>
<para>For more information about this, please check <link
xl:href="https://docs.rke2.io/install/containerd_registry_configuration#registries-configuration-file">containerd
registry configuration rke2</link>.</para>
</note>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>Fully automated directed network provisioning</title>
<section xml:id="id-introduction-3">
<title>Introduction</title>
<para>Direct network provisioning is a feature that allows you to automate the
provisioning of downstream clusters. This feature is useful when you have
many downstream clusters to provision, and you want to automate the process.</para>
<para>A management cluster (<xref linkend="atip-management-cluster"/>) automates
deployment of the following components:</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Enterprise Micro RT</literal> as the OS. Depending on
the use case, configurations like networking, storage, users and kernel
arguments can be customized.</para>
</listitem>
<listitem>
<para><literal>RKE2</literal> as the Kubernetes cluster. The default
<literal>CNI</literal> plug-in is <literal>Cilium</literal>. Depending on
the use case, certain <literal>CNI</literal> plug-ins can be used, such as
<literal>Cilium+Multus</literal>.</para>
</listitem>
<listitem>
<para><literal>Longhorn</literal> as the storage solution.</para>
</listitem>
<listitem>
<para><literal>NeuVector</literal> as the security solution.</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal> can be used as the load balancer for highly
available multi-node clusters.</para>
</listitem>
</itemizedlist>
<note>
<para>For more information about <literal>SUSE Linux Enterprise Micro</literal>,
see <xref linkend="components-slmicro"/> For more information about
<literal>RKE2</literal>, see <xref linkend="components-rke2"/> For more
information about <literal>Longhorn</literal>, see <xref
linkend="components-longhorn"/> For more information about
<literal>NeuVector</literal>, see <xref linkend="components-neuvector"/></para>
</note>
<para>The following sections describe the different directed network provisioning
workflows and some additional features that can be added to the provisioning
process:</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="eib-edge-image-connected">
<title>Prepare downstream cluster image for connected scenarios</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) is used to prepare a
modified SLEMicro base image which is provisioned on downstream cluster
hosts.</para>
<para>Much of the configuration via Edge Image Builder is possible, but in this
guide, we cover the minimal configurations necessary to set up the
downstream cluster.</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>Prerequisites for connected scenarios</title>
<itemizedlist>
<listitem>
<para>A container runtime such as <link xl:href="https://podman.io">Podman</link>
or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link> is
required to run Edge Image Builder.</para>
</listitem>
<listitem>
<para>The base image <literal>SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw</literal>
must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE
Customer Center</link> or the <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE Download
page</link>.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>Image configuration for connected scenarios</title>
<para>When running Edge Image Builder, a directory is mounted from the host, so it
is necessary to create a directory structure to store the configuration
files used to define the target image.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal> is the image definition
file, see <xref linkend="quickstart-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The base image when downloaded is <literal>xz</literal> compressed, which
must be uncompressed with <literal>unxz</literal> and copied/moved under the
<literal>base-images</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>network</literal> folder is optional, see <xref
linkend="add-network-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The custom/scripts directory contains scripts to be run on first-boot;
currently a <literal>01-fix-growfs.sh</literal> script is required to resize
the OS root partition on deployment</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>Downstream cluster image definition file</title>
<para>The <literal>downstream-cluster-config.yaml</literal> file is the main
configuration file for the downstream cluster image. The following is a
minimal example for deployment via Metal<superscript>3</superscript>:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</screen>
<para><literal>${ROOT_PASSWORD}</literal> is the encrypted password for the root
user, which can be useful for test/debugging.  It can be generated with the
<literal>openssl passwd -6 PASSWORD</literal> command</para>
<para>For the production environments, it is recommended to use the SSH keys that
can be added to the users block replacing the <literal>${USERKEY1}</literal>
with the real SSH keys.</para>
<note>
<para><literal>net.ifnames=1</literal> enables <link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link></para>
<para>This matches the default configuration for the metal3 chart, but the setting
must match the configured chart <literal>predictableNicNames</literal>
value.</para>
<para>Also note <literal>ignition.platform.id=openstack</literal> is mandatory,
without this argument SLEMicro configuration via ignition will fail in the
Metal<superscript>3</superscript> automated flow.</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Growfs script</title>
<para>Currently, a custom script
(<literal>custom/scripts/01-fix-growfs.sh</literal>) is required to grow the
file system to match the disk size on first-boot after provisioning. The
<literal>01-fix-growfs.sh</literal> script contains the following
information:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>Add your own custom scripts to be executed during the provisioning process
using the same approach.  For more information, see <xref
linkend="quickstart-eib"/>.</para>
<para>The bug related to this workaround is <link
xl:href="https://bugzilla.suse.com/show_bug.cgi?id=1217430">https://bugzilla.suse.com/show_bug.cgi?id=1217430</link></para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>Additional configuration for Telco workloads</title>
<para>To enable Telco features like <literal>dpdk</literal>,
<literal>sr-iov</literal> or <literal>FEC</literal>, additional packages may
be required as shown in the following example.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${user1Key1}
  packages:
    packageList:
      - jq
      - dpdk22
      - dpdk22-tools
      - libdpdk-23
      - pf-bb-config
    additionalRepos:
      - url: https://download.opensuse.org/repositories/isv:/SUSE:/Edge:/Telco/SLEMicro5.5/
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<para>Where <literal>${SCC_REGISTRATION_CODE}</literal> is the registration code
copied from <link xl:href="https://scc.suse.com/">SUSE Customer
Center</link>, and the package list contains the minimum packages to be used
for the Telco profiles.  To use the <literal>pf-bb-config</literal> package
(to enable the <literal>FEC</literal> feature and binding with drivers), the
<literal>additionalRepos</literal> block must be included to add the
<literal>SUSE Edge Telco</literal> repository.</para>
</section>
<section xml:id="add-network-eib">
<title>Additional script for Advanced Network Configuration</title>
<para>If you need to configure static IPs or more advanced networking scenarios as
described in <xref linkend="advanced-network-configuration"/>, the following
additional configuration is required.</para>
<para>In the <literal>network</literal> folder, create the following
<literal>configure-network.sh</literal> file - this consumes configuration
drive data on first-boot, and configures the host networking using the <link
xl:href="https://github.com/suse-edge/nm-configurator">NM Configurator
tool</link>.</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>Image creation</title>
<para>Once the directory structure is prepared following the previous sections,
run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>This creates the output ISO image file named
<literal>eibimage-slemicro55rt-telco.raw</literal>, based on the definition
described above.</para>
<para>The output image must then be made available via a webserver, either the
media-server container enabled via the Management Cluster Documentation
(<xref linkend="metal3-media-server"/>)  or some other locally accessible
server.  In the examples below, we refer to this server as
<literal>imagecache.local:8080</literal></para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>Prepare downstream cluster image for air-gap scenarios</title>
<para>Edge Image Builder (<xref linkend="components-eib"/>) is used to prepare a
modified SLEMicro base image which is provisioned on downstream cluster
hosts.</para>
<para>Much of the configuration is possible with Edge Image Builder, but in this
guide, we cover the minimal configurations necessary to set up the
downstream cluster for air-gap scenarios.</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>Prerequisites for air-gap scenarios</title>
<itemizedlist>
<listitem>
<para>A container runtime such as <link xl:href="https://podman.io">Podman</link>
or <link xl:href="https://rancherdesktop.io">Rancher Desktop</link> is
required to run Edge Image Builder.</para>
</listitem>
<listitem>
<para>The base image <literal>SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw</literal>
must be downloaded from the <link xl:href="https://scc.suse.com/">SUSE
Customer Center</link> or the <link
xl:href="https://www.suse.com/download/sle-micro/">SUSE Download
page</link>.</para>
</listitem>
<listitem>
<para>If you want to use SR-IOV or any other workload which require a container
image, a local private registry must be deployed and already configured
(with/without TLS and/or authentication). This registry will be used to
store the images and the helm chart OCI images.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>Image configuration for air-gap scenarios</title>
<para>When running Edge Image Builder, a directory is mounted from the host, so it
is necessary to create a directory structure to store the configuration
files used to define the target image.</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal> is the image
definition file, see <xref linkend="quickstart-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The base image when downloaded is <literal>xz</literal> compressed, which
must be uncompressed with <literal>unxz</literal> and copied/moved under the
<literal>base-images</literal> folder.</para>
</listitem>
<listitem>
<para>The <literal>network</literal> folder is optional, see <xref
linkend="add-network-eib"/> for more details.</para>
</listitem>
<listitem>
<para>The <literal>custom/scripts</literal> directory contains scripts to be run
on first-boot; currently a <literal>01-fix-growfs.sh</literal> script is
required to resize the OS root partition on deployment. For air-gap
scenarios, a script <literal>02-airgap.sh</literal> is required to copy the
images to the right place during the image creation process.</para>
</listitem>
<listitem>
<para>The <literal>custom/files</literal> directory contains the
<literal>rke2</literal> and the <literal>cni</literal> images to be copied
to the image during the image creation process.</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>Downstream cluster image definition file</title>
<para>The <literal>downstream-cluster-airgap-config.yaml</literal> file is the
main configuration file for the downstream cluster image and the content has
been described in the previous section (<xref
linkend="add-telco-feature-eib"/>).</para>
</section>
<section xml:id="id-growfs-script-2">
<title>Growfs script</title>
<para>Currently, a custom script
(<literal>custom/scripts/01-fix-growfs.sh</literal>) is required to grow the
file system to match the disk size on first-boot after provisioning. The
<literal>01-fix-growfs.sh</literal> script contains the following
information:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>Air-gap script</title>
<para>The following script (<literal>custom/scripts/02-airgap.sh</literal>) is
required to copy the images to the right place during the image creation
process:</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>Custom files for air-gap scenarios</title>
<para>The <literal>custom/files</literal> directory contains the
<literal>rke2</literal> and the <literal>cni</literal> images to be copied
to the image during the image creation process.  To easily generate the
images, prepare them locally using following <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-rke2-images.sh">script</link>
and the list of images <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-release-rke2-images.txt">here</link>
to generate the artifacts required to be included in
<literal>custom/files</literal>.  Also, you can download the latest
<literal>rke2-install</literal> script from <link
xl:href="https://get.rke2.io/">here</link>.</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>After downloading the images, the directory structure should look like this:</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>Preload your private registry with images required for air-gap scenarios and
SR-IOV (optional)</title>
<para>If you want to use SR-IOV in your air-gap scenario or any other workload
images, you must preload your local private registry with the images
following the next steps:</para>
<itemizedlist>
<listitem>
<para>Download, extract, and push the helm-chart OCI images to the private
registry</para>
</listitem>
<listitem>
<para>Download, extract, and push the rest of images required to the private
registry</para>
</listitem>
</itemizedlist>
<para>The following scripts can be used to download, extract, and push the images
to the private registry. We will show an example to preload the SR-IOV
images, but you can also use the same approach to preload any other custom
images:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Preload with helm-chart OCI images for SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>You must create a list with the helm-chart OCI images required:</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:1.2.2
edge/sriov-crd-chart:1.2.2
EOF</screen>
</listitem>
<listitem>
<para>Generate a local tarball file using the following <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-oci-artefacts.sh">script</link>
and the list created above:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/sriov-network-operator-chart:1.2.2
Pulled: registry.suse.com/edge/sriov-crd-chart:1.2.2
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-1.2.2.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-1.2.2.tgz</screen>
</listitem>
<listitem>
<para>Upload your tarball file to your private registry
(e.g. <literal>myregistry:5000</literal>) using the following <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-load-oci-artefacts.sh">script</link>
to preload your registry with the helm chart OCI images downloaded in the
previous step:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Preload with the rest of the images required for SR-IOV:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>In this case, we must include the `sr-iov container images for telco
workloads (e.g. as a reference, you could get them from <link
xl:href="https://github.com/suse-edge/charts/blob/release-3.0/charts/sriov-network-operator/1.2.2%2Bup0.1.0/values.yaml">helm-chart
values</link>)</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.2.0-build20240327
rancher/rancher/hardened-sriov-network-config-daemon:v1.2.0-build20240327
rancher/hardened-sriov-cni:v1.2.0-build20240327
rancher/hardened-ib-sriov-cni:v1.2.0-build20240327
rancher/hardened-sriov-network-device-plugin:v1.2.0-build20240327
rancher/hardened-sriov-network-resources-injector:v1.2.0-build20240327
rancher/hardened-sriov-network-webhook:v1.2.0-build20240327
EOF</screen>
</listitem>
<listitem>
<para>Using the following <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-images.sh">script</link>
and the list created above, you must generate locally the tarball file with
the images required:</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -al ./edge-release-images.txt -s registry.suse.com
Pulled: registry.suse.com/rancher/hardened-sriov-network-operator:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/rancher/hardened-sriov-network-config-daemon:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-cni:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.2.0-build20240327
a edge-release-images-tgz-20240705
a edge-release-images-tgz-20240705/hardened-sriov-network-operator-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-config-daemon-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-cni-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-ib-sriov-cni-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-device-plugin-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-resources-injector-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-webhook-v1.2.0-build20240327.tar.gz</screen>
</listitem>
<listitem>
<para>Upload your tarball file to your private registry
(e.g. <literal>myregistry:5000</literal>) using the following <link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-load-images.sh">script</link>
to preload your private registry with the images downloaded in the previous
step:</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>Image creation for air-gap scenarios</title>
<para>Once the directory structure is prepared following the previous sections,
run the following command to build the image:</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>This creates the output ISO image file named
<literal>eibimage-slemicro55rt-telco.raw</literal>, based on the definition
described above.</para>
<para>The output image must then be made available via a webserver, either the
media-server container enabled via the Management Cluster Documentation
(<xref linkend="metal3-media-server"/>)  or some other locally accessible
server.  In the examples below, we refer to this server as
<literal>imagecache.local:8080</literal>.</para>
</section>
</section>
<section xml:id="single-node">
<title>Downstream cluster provisioning with Direct network provisioning
(single-node)</title>
<para>This section describes the workflow used to automate the provisioning of a
single-node downstream cluster using directed network provisioning.  This is
the simplest way to automate the provisioning of a downstream cluster.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal>, as described in the
previous section (<xref linkend="eib-edge-image-connected"/>), with the
minimal configuration to set up the downstream cluster has to be located in
the management cluster exactly on the path you configured on this section
(<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>The management server created and available to be used on the following
sections. For more information, refer to the Management Cluster section
<xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Workflow</emphasis></para>
<para>The following diagram shows the workflow used to automate the provisioning
of a single-node downstream cluster using directed network provisioning:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png" width=""/>
</imageobject>
<textobject><phrase>atip automated singlenode1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>There are two different steps to automate the provisioning of a single-node
downstream cluster using directed network provisioning:</para>
<orderedlist numeration="arabic">
<listitem>
<para>Enroll the bare-metal host to make it available for the provisioning
process.</para>
</listitem>
<listitem>
<para>Provision the bare-metal host to install and configure the operating system
and the Kubernetes cluster.</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">Enroll the bare-metal host</emphasis></para>
<para>The first step is to enroll the new bare-metal host in the management
cluster to make it available to be provisioned.  To do that, the following
file (<literal>bmh-example.yaml</literal>) has to be created in the
management cluster, to specify the <literal>BMC</literal> credentials to be
used and the <literal>BaremetalHost</literal> object to be enrolled:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>where:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal> — The user name for the
<literal>BMC</literal> of the new bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal> — The password for the
<literal>BMC</literal> of the new bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal> — The <literal>MAC</literal> address of the
new bare-metal host to be used.</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal> — The <literal>URL</literal> for the
bare-metal host <literal>BMC</literal> (for example,
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>).
To learn more about the different options available depending on your
hardware provider, check the following <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">link</link>.</para>
</listitem>
</itemizedlist>
<para>Once the file is created, the following command has to be executed in the
management cluster to start enrolling the new bare-metal host in the
management cluster:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>The new bare-metal host object will be enrolled, changing its state from
registering to inspecting and available. The changes can be checked using
the following command:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para>The <literal>BaremetalHost</literal> object is in the
<literal>registering</literal> state until the <literal>BMC</literal>
credentials are validated. Once the credentials are validated, the
<literal>BaremetalHost</literal> object changes its state to
<literal>inspecting</literal>, and this step could take some time depending
on the hardware (up to 20 minutes). During the inspecting phase, the
hardware information is retrieved and the Kubernetes object is
updated. Check the information using the following command: <literal>kubectl
get bmh -o yaml</literal>.</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">Provision step</emphasis></para>
<para>Once the bare-metal host is enrolled and available, the next step is to
provision the bare-metal host to install and configure the operating system
and the Kubernetes cluster.  To do that, the following file
(<literal>capi-provisioning-example.yaml</literal>) has to be created in the
management-cluster with the following information (the
<literal>capi-provisioning-example.yaml</literal> can be generated by
joining the following blocks).</para>
<note>
<para>Only values between <literal>$\{…​\}</literal> must be replaced with the
real values.</para>
</note>
<para>The following block is the cluster definition, where the networking can be
configured using the <literal>pods</literal> and the
<literal>services</literal> blocks. Also, it contains the references to the
control plane and the infrastructure (using the
<literal>Metal<superscript>3</superscript></literal> provider) objects to be
used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<para>The <literal>Metal3Cluster</literal> object specifies the control-plane
endpoint (replacing the <literal>${DOWNSTREAM_CONTROL_PLANE_IP}</literal>)
to be configured and the <literal>noCloudProvider</literal> because a
bare-metal node is used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IP}
    port: 6443
  noCloudProvider: true</screen>
<para>The <literal>RKE2ControlPlane</literal> object specifies the control-plane
configuration to be used and the <literal>Metal3MachineTemplate</literal>
object specifies the control-plane image to be used.  Also, it contains the
information about the number of replicas to be used (in this case, one) and
the <literal>CNI</literal> plug-in to be used (in this case,
<literal>Cilium</literal>).  The agentConfig block contains the
<literal>Ignition</literal> format to be used and the
<literal>additionalUserData</literal> to be used to configure the
<literal>RKE2</literal> node with information like a systemd named
<literal>rke2-preinstall.service</literal> to replace automatically the
<literal>BAREMETALHOST_UUID</literal> and <literal>node-name</literal>
during the provisioning process using the Ironic information.  The last
block of information contains the Kubernetes version to be
used. <literal>${RKE2_VERSION}</literal> is the version of
<literal>RKE2</literal> to be used replacing this value (for example,
<literal>v1.28.9+rke2r1</literal>).</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>The <literal>Metal3MachineTemplate</literal> object specifies the following
information:</para>
<itemizedlist>
<listitem>
<para>The <literal>dataTemplate</literal> to be used as a reference to the
template.</para>
</listitem>
<listitem>
<para>The <literal>hostSelector</literal> to be used matching with the label
created during the enrollment process.</para>
</listitem>
<listitem>
<para>The <literal>image</literal> to be used as a reference to the image
generated using <literal>EIB</literal> on the previous section (<xref
linkend="eib-edge-image-connected"/>), and the <literal>checksum</literal>
and <literal>checksumType</literal> to be used to validate the image.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</screen>
<para>The <literal>Metal3DataTemplate</literal> object specifies the
<literal>metaData</literal> for the downstream cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Once the file is created by joining the previous blocks, the following
command must be executed in the management cluster to start provisioning the
new bare-metal host:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>Downstream cluster provisioning with Direct network provisioning
(multi-node)</title>
<para>This section describes the workflow used to automate the provisioning of a
multi-node downstream cluster using directed network provisioning and
<literal>MetalLB</literal> as a load-balancer strategy.  This is the
simplest way to automate the provisioning of a downstream cluster. The
following diagram shows the workflow used to automate the provisioning of a
multi-node downstream cluster using directed network provisioning and
<literal>MetalLB</literal>.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal>, as described in the
previous section (<xref linkend="eib-edge-image-connected"/>), with the
minimal configuration to set up the downstream cluster has to be located in
the management cluster exactly on the path you configured on this section
(<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>The management server created and available to be used on the following
sections. For more information, refer to the Management Cluster section:
<xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Workflow</emphasis></para>
<para>The following diagram shows the workflow used to automate the provisioning
of a multi-node downstream cluster using directed network provisioning:</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png" width=""/>
</imageobject>
<textobject><phrase>atip automate multinode1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>Enroll the three bare-metal hosts to make them available for the
provisioning process.</para>
</listitem>
<listitem>
<para>Provision the three bare-metal hosts to install and configure the operating
system and the Kubernetes cluster using <literal>MetalLB</literal>.</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">Enroll the bare-metal hosts</emphasis></para>
<para>The first step is to enroll the three bare-metal hosts in the management
cluster to make them available to be provisioned.  To do that, the following
files (<literal>bmh-example-node1.yaml</literal>,
<literal>bmh-example-node2.yaml</literal> and
<literal>bmh-example-node3.yaml</literal>) must be created in the management
cluster, to specify the <literal>BMC</literal> credentials to be used and
the <literal>BaremetalHost</literal> object to be enrolled in the management
cluster.</para>
<note>
<itemizedlist>
<listitem>
<para>Only the values between <literal>$\{…​\}</literal> have to be replaced with
the real values.</para>
</listitem>
<listitem>
<para>We will walk you through the process for only one host. The same steps apply
to the other two nodes.</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>Where:</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal> — The username for the BMC of the
first bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal> — The password for the BMC of the
first bare-metal host.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal> — The MAC address of the first
bare-metal host to be used.</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal> — The URL for the first bare-metal
host BMC (for example,
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>).
To learn more about the different options available depending on your
hardware provider, check the following <link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">link</link>.</para>
</listitem>
</itemizedlist>
<para>Once the file is created, the following command must be executed in the
management cluster to start enrolling the bare-metal hosts in the management
cluster:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>The new bare-metal host objects are enrolled, changing their state from
registering to inspecting and available. The changes can be checked using
the following command:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para>The <literal>BaremetalHost</literal> object is in the
<literal>registering</literal> state until the <literal>BMC</literal>
credentials are validated. Once the credentials are validated, the
<literal>BaremetalHost</literal> object changes its state to
<literal>inspecting</literal>, and this step could take some time depending
on the hardware (up to 20 minutes). During the inspecting phase, the
hardware information is retrieved and the Kubernetes object is
updated. Check the information using the following command: <literal>kubectl
get bmh -o yaml</literal>.</para>
</note>
<para><emphasis role="strong">Provision step</emphasis></para>
<para>Once the three bar metal hosts are enrolled and available, the next step is
to provision the bare-metal hosts to install and configure the operating
system and the Kubernetes cluster, creating a load balancer to manage them.
To do that, the following file
(<literal>capi-provisioning-example.yaml</literal>) must be created in the
management cluster with the following information (the
`capi-provisioning-example.yaml can be generated by joining the following
blocks).</para>
<note>
<itemizedlist>
<listitem>
<para>Only values between <literal>$\{…​\}</literal> must be replaced with the
real values.</para>
</listitem>
<listitem>
<para>The <literal>VIP</literal> address is a reserved IP address that is not
assigned to any node and is used to configure the load balancer.</para>
</listitem>
</itemizedlist>
</note>
<para>Below is the cluster definition, where the cluster network can be configured
using the <literal>pods</literal> and the <literal>services</literal>
blocks. Also, it contains the references to the control plane and the
infrastructure (using the
<literal>Metal<superscript>3</superscript></literal> provider) objects to be
used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<para>The <literal>Metal3Cluster</literal> object specifies the control-plane
endpoint that uses the <literal>VIP</literal> address already reserved
(replacing the <literal>${DOWNSTREAM_VIP_ADDRESS}</literal>) to be
configured and the <literal>noCloudProvider</literal> because the three
bare-metal nodes are used.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS}
    port: 6443
  noCloudProvider: true</screen>
<para>The <literal>RKE2ControlPlane</literal> object specifies the control-plane
configuration to be used, and the <literal>Metal3MachineTemplate</literal>
object specifies the control-plane image to be used.</para>
<itemizedlist>
<listitem>
<para>The number of replicas to be used (in this case, three).</para>
</listitem>
<listitem>
<para>The advertisement mode to be used by the Load Balancer
(<literal>address</literal> uses the L2 implementation), as well as the
address to be used (replacing the <literal>${EDGE_VIP_ADDRESS}</literal>
with the <literal>VIP</literal> address).</para>
</listitem>
<listitem>
<para>The <literal>serverConfig</literal> with the <literal>CNI</literal> plug-in
to be used (in this case, <literal>Cilium</literal>), and the
<literal>tlsSan</literal> to be used to configure the <literal>VIP</literal>
address.</para>
</listitem>
<listitem>
<para>The agentConfig block contains the <literal>Ignition</literal> format to be
used and the <literal>additionalUserData</literal> to be used to configure
the <literal>RKE2</literal> node with information like:</para>
<itemizedlist>
<listitem>
<para>The systemd service named <literal>rke2-preinstall.service</literal> to
replace automatically the <literal>BAREMETALHOST_UUID</literal> and
<literal>node-name</literal> during the provisioning process using the
Ironic information.</para>
</listitem>
<listitem>
<para>The <literal>storage</literal> block which contains the Helm charts to be
used to install the <literal>MetalLB</literal> and the
<literal>endpoint-copier-operator</literal>.</para>
</listitem>
<listitem>
<para>The <literal>metalLB</literal> custom resource file with the
<literal>IPaddressPool</literal> and the <literal>L2Advertisement</literal>
to be used (replacing <literal>${EDGE_VIP_ADDRESS}</literal> with the
<literal>VIP</literal> address).</para>
</listitem>
<listitem>
<para>The <literal>endpoint-svc.yaml</literal> file to be used to configure the
<literal>kubernetes-vip</literal> service to be used by the
<literal>MetalLB</literal> to manage the <literal>VIP</literal> address.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>The last block of information contains the Kubernetes version to be
used. The <literal>${RKE2_VERSION}</literal> is the version of
<literal>RKE2</literal> to be used replacing this value (for example,
<literal>v1.28.9+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  registrationMethod: "address"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS}
      - https://${EDGE_VIP_ADDRESS}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/endpoint-copier-operator-chart
                    targetNamespace: endpoint-copier-operator
                    version: 0.2.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/metallb-chart
                    targetNamespace: metallb-system
                    version: 0.14.3
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS}/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "Node-multinode-cluster"</screen>
<para>The <literal>Metal3MachineTemplate</literal> object specifies the following
information:</para>
<itemizedlist>
<listitem>
<para>The <literal>dataTemplate</literal> to be used as a reference to the
template.</para>
</listitem>
<listitem>
<para>The <literal>hostSelector</literal> to be used matching with the label
created during the enrollment process.</para>
</listitem>
<listitem>
<para>The <literal>image</literal> to be used as a reference to the image
generated using <literal>EIB</literal> on the previous section (<xref
linkend="eib-edge-image-connected"/>), and <literal>checksum</literal> and
<literal>checksumType</literal> to be used to validate the image.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</screen>
<para>The <literal>Metal3DataTemplate</literal> object specifies the
<literal>metaData</literal> for the downstream cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>Once the file is created by joining the previous blocks, the following
command has to be executed in the management cluster to start provisioning
the new three bar metal hosts:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>Advanced Network Configuration</title>
<para>The directed network provisioning workflow allows downstream clusters
network configurations such as static IPs, bonding, VLAN’s, etc.</para>
<para>The following sections describe the additional steps required to enable
provisioning downstream clusters using advanced network configuration.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal> has to include the network
folder and the script following this section (<xref
linkend="add-network-eib"/>).</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuration</emphasis></para>
<para>Use the following two sections as the base to enroll and provision the
hosts:</para>
<itemizedlist>
<listitem>
<para>Downstream cluster provisioning with Direct network provisioning
(single-node) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Downstream cluster provisioning with Direct network provisioning
(multi-node) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>The changes required to enable the advanced network configuration are the
following:</para>
<itemizedlist>
<listitem>
<para>Enrollment step: The following new example file with a secret containing the
information about the <literal>networkData</literal> to be used to
configure, for example, the static <literal>IPs</literal> and
<literal>VLAN</literal> for the downstream cluster</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>This file contains the <literal>networkData</literal> in a
<literal>nmstate</literal> format to be used to configure the advance
network configuration (for example, <literal>static IPs</literal> and
<literal>VLAN</literal>) for the downstream cluster.  As you can see, the
example shows the configuration to enable the interface with static IPs, as
well as the configuration to enable the VLAN using the base interface.  Any
other <literal>nmstate</literal> example could be defined to be used to
configure the network for the downstream cluster to adapt to the specific
requirements, where the following variables have to be replaced with real
values:</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE1_INTERFACE}</literal> — The control-plane interface
to be used for the edge cluster (for example, <literal>eth0</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_IP}</literal> — The IP address to be used as an
endpoint for the edge cluster (must match with the kubeapi-server endpoint).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_PREFIX}</literal> — The CIDR to be used for the
edge cluster (for example, <literal>24</literal> if you want
<literal>/24</literal> or <literal>255.255.255.0</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_GATEWAY}</literal> — The gateway to be used for the
edge cluster (for example, <literal>192.168.100.1</literal>).</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_MAC}</literal> — The MAC address to be used for the
control-plane interface (for example, <literal>00:0c:29:3e:3e:3e</literal>).</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal> — The DNS to be used for the edge cluster
(for example, <literal>192.168.100.2</literal>).</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal> — The VLAN ID to be used for the edge cluster
(for example, <literal>100</literal>).</para>
</listitem>
</itemizedlist>
<para>Also, the reference to that secret using
<literal>preprovisioningNetworkDataName</literal> is needed in the
<literal>BaremetalHost</literal> object at the end of the file to be
enrolled in the management cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<para>If you need to deploy a multi-node cluster, the same process must be done
for the other nodes.</para>
</note>
<itemizedlist>
<listitem>
<para>Provision step: The block of information related to the network data has to
be removed because the platform includes the network data configuration into
the secret <literal>controlplane-0-networkdata</literal>.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<note>
<para>The <literal>Metal3DataTemplate</literal>, <literal>networkData</literal>
and <literal>Metal3 IPAM</literal> are currently not supported; only the
configuration via static secrets is fully supported.</para>
</note>
</section>
<section xml:id="add-telco">
<title>Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)</title>
<para>The directed network provisioning workflow allows to automate the Telco
features to be used in the downstream clusters to run Telco workloads on top
of those servers.</para>
<para><emphasis role="strong">Requirements</emphasis></para>
<itemizedlist>
<listitem>
<para>The image generated using <literal>EIB</literal> has to include the specific
Telco packages following this section (<xref
linkend="add-telco-feature-eib"/>).</para>
</listitem>
<listitem>
<para>The image generated using <literal>EIB</literal>, as described in the
previous section (<xref linkend="eib-edge-image-connected"/>), has to be
located in the management cluster exactly on the path you configured on this
section (<xref linkend="metal3-media-server"/>).</para>
</listitem>
<listitem>
<para>The management server created and available to be used on the following
sections. For more information, refer to the Management Cluster section:
<xref linkend="atip-management-cluster"/>.</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">Configuration</emphasis></para>
<para>Use the following two sections as the base to enroll and provision the
hosts:</para>
<itemizedlist>
<listitem>
<para>Downstream cluster provisioning with Direct network provisioning
(single-node) (<xref linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>Downstream cluster provisioning with Direct network provisioning
(multi-node) (<xref linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>The Telco features covered in this section are the following:</para>
<itemizedlist>
<listitem>
<para>DPDK and VFs creation</para>
</listitem>
<listitem>
<para>SR-IOV and VFs allocation to be used by the workloads</para>
</listitem>
<listitem>
<para>CPU isolation and performance tuning</para>
</listitem>
<listitem>
<para>Huge pages configuration</para>
</listitem>
<listitem>
<para>Kernel parameters tuning</para>
</listitem>
</itemizedlist>
<note>
<para>For more information about the Telco features, see <xref
linkend="atip-features"/>.</para>
</note>
<para>The changes required to enable the Telco features shown above are all inside
the <literal>RKE2ControlPlane</literal> block in the provision file
<literal>capi-provisioning-example.yaml</literal>. The rest of the
information inside the file
<literal>capi-provisioning-example.yaml</literal> is the same as the
information provided in the provisioning section (<xref
linkend="single-node-provision"/>).</para>
<para>To make the process clear, the changes required on that block
(<literal>RKE2ControlPlane</literal>) to enable the Telco features are the
following:</para>
<itemizedlist>
<listitem>
<para>The <literal>preRKE2Commands</literal> to be used to execute the commands
before the <literal>RKE2</literal> installation process. In this case, use
the <literal>modprobe</literal> command to enable the
<literal>vfio-pci</literal> and the <literal>SR-IOV</literal> kernel
modules.</para>
</listitem>
<listitem>
<para>The ignition file
<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>
to be used to define the interfaces, drivers and the number of
<literal>VFs</literal> to be created and exposed to the workloads.</para>
<itemizedlist>
<listitem>
<para>The values inside the config map <literal>sriov-custom-auto-config</literal>
are the only values to be replaced with real values.</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal> — The resource name to be used for the
first <literal>PF</literal> interface (for example,
<literal>sriov-resource-du1</literal>). It is added to the prefix
<literal>rancher.io</literal> to be used as a label to be used by the
workloads (for example, <literal>rancher.io/sriov-resource-du1</literal>).</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal> — The name of the first
<literal>PF</literal> interface to be used (for example,
<literal>eth0</literal>).</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal> — The name of the first physical function
<literal>PF</literal> to be used. Generate more complex filters using this
(for example, <literal>eth0#2-5</literal>).</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal> — The driver name to be used for the
first <literal>VF</literal> interface (for example,
<literal>vfio-pci</literal>).</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal> — The number of <literal>VFs</literal> to be
created for the first <literal>PF</literal> interface (for example,
<literal>8</literal>).</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>The <literal>/var/sriov-auto-filler.sh</literal> to be used as a translator
between the high-level config map
<literal>sriov-custom-auto-config</literal> and the
<literal>sriovnetworknodepolicy</literal> which contains the low-level
hardware information. This script has been created to abstract the user from
the complexity to know in advance the hardware information. No changes are
required in this file, but it should be present if we need to enable
<literal>sr-iov</literal> and create <literal>VFs</literal>.</para>
</listitem>
<listitem>
<para>The kernel arguments to be used to enable the following features:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Parameter</para></entry>
<entry align="left" valign="top"><para>Value</para></entry>
<entry align="left" valign="top"><para>Description</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Isolate the cores 1-30 and 33-62.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to skew the timer interrupts across the isolated CPUs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the timer tick on a single CPU when the system is
idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>kernel boot parameter is the current main interface to configure full
dynticks along with CPU Isolation.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30,33-62</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the RCU callbacks on a single CPU when the system
is idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>kthread_cpus</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the kthreads on a single CPU when the system is
idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0,31,32,63</para></entry>
<entry align="left" valign="top"><para>Allows the kernel to run the interrupts on a single CPU when the system is
idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>processor.max_cstate</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>Prevents the CPU from dropping into a sleep state when idle.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_idle.max_cstate</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>Disables the intel_idle driver and allows acpi_idle to be used.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>Allows to use vfio for the dpdk interfaces.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>Enables the use of vfio for VFs.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Allows to set the size of huge pages to 1 G.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>Number of huge pages defined before.</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Default value to enable huge pages.</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>The following systemd services are used to enable the following:</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> to replace automatically the
<literal>BAREMETALHOST_UUID</literal> and <literal>node-name</literal>
during the provisioning process using the Ironic information.</para>
</listitem>
<listitem>
<para><literal>cpu-performance.service</literal> to enable the CPU performance
tuning. The <literal>${CPU_FREQUENCY}</literal> has to be replaced with the
real values (for example, <literal>2500000</literal> to set the CPU
frequency to <literal>2.5GHz</literal>).</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> to enable the isolation cores of
the <literal>CPU</literal> (for example, <literal>1-30,33-62</literal>).</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> to install the
<literal>sriov</literal> Helm chart, wait until custom resources are created
and run the <literal>/var/sriov-auto-filler.sh</literal> to replace the
values in the config map <literal>sriov-custom-auto-config</literal> and
create the <literal>sriovnetworknodepolicy</literal> to be used by the
workloads.</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>The <literal>${RKE2_VERSION}</literal> is the version of
<literal>RKE2</literal> to be used replacing this value (for example,
<literal>v1.28.9+rke2r1</literal>).</para>
</listitem>
</itemizedlist>
<para>With all these changes mentioned, the <literal>RKE2ControlPlane</literal>
block in the <literal>capi-provisioning-example.yaml</literal> will look
like the following:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/sriov-crd-chart
                    targetNamespace: sriov-network-operator
                    version: 1.2.2
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/sriov-network-operator-chart
                    targetNamespace: sriov-network-operator
                    version: 1.2.2
                    createNamespace: true
            - path: /var/sriov-auto-filler.sh
              overwrite: true
              contents:
                inline: |
                  #!/bin/bash
                  cat &lt;&lt;- EOF &gt; /var/sriov-networkpolicy-template.yaml
                  apiVersion: sriovnetwork.openshift.io/v1
                  kind: SriovNetworkNodePolicy
                  metadata:
                    name: atip-RESOURCENAME
                    namespace: sriov-network-operator
                  spec:
                    nodeSelector:
                      feature.node.kubernetes.io/network-sriov.capable: "true"
                    resourceName: RESOURCENAME
                    deviceType: DRIVER
                    numVfs: NUMVF
                    mtu: 1500
                    nicSelector:
                      pfNames: ["PFNAMES"]
                      deviceID: "DEVICEID"
                      vendor: "VENDOR"
                      rootDevices:
                        - PCIADDRESS
                  EOF

                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export KUBECTL=/var/lib/rancher/rke2/bin/kubectl
                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r '.items[].status.syncStatus') != "Succeeded" ]; do sleep 1; done
                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm sriov-custom-auto-config -n kube-system -ojson | jq -r '.data."config.json"')
                  jq -c '.[]' &lt;&lt;&lt; $input | while read i; do
                    interface=$(echo $i | jq -r '.interface')
                    pfname=$(echo $i | jq -r '.pfname')
                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.pciAddress")
                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.vendor")
                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.deviceID")
                    resourceName=$(echo $i | jq -r '.resourceName')
                    driver=$(echo $i | jq -r '.driver')
                    sed -e "s/RESOURCENAME/$resourceName/g" \
                        -e "s/DRIVER/$driver/g" \
                        -e "s/PFNAMES/$pfname/g" \
                        -e "s/VENDOR/$vendor/g" \
                        -e "s/DEVICEID/$deviceid/g" \
                        -e "s/PCIADDRESS/$pciaddress/g" \
                        -e "s/NUMVF/$(echo $i | jq -r '.numVFsToCreate')/g" /var/sriov-networkpolicy-template.yaml &gt; /var/lib/rancher/rke2/server/manifests/$resourceName.yaml
                  done
              mode: 0755
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - intel_pstate=passive
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - iommu=pt
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - kthread_cpus=${NON-ISOLATED_CPU_CORES}
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nohz=on
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-performance.service
              enabled: true
              contents: |
                [Unit]
                Description=CPU perfomance
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "cpupower frequency-set -g performance; cpupower frequency-set -u ${CPU_FREQUENCY}; cpupower frequency-set -d ${CPU_FREQUENCY}"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/var/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>Once the file is created by joining the previous blocks, the following
command must be executed in the management cluster to start provisioning the
new downstream cluster using the Telco features:</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>Private registry</title>
<para>It is possible to configure a private registry as a mirror for images used
by workloads.</para>
<para>To do this we create the secret containing the information about the private
registry to be used by the downstream cluster.</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para>The <literal>tls.crt</literal>, <literal>tls.key</literal> and
<literal>ca.crt</literal> are the certificates to be used to authenticate
the private registry. The <literal>username</literal> and
<literal>password</literal> are the credentials to be used to authenticate
the private registry.</para>
<note>
<para>The <literal>tls.crt</literal>, <literal>tls.key</literal>,
<literal>ca.crt</literal> , <literal>username</literal> and
<literal>password</literal> have to be encoded in base64 format before to be
used in the secret.</para>
</note>
<para>With all these changes mentioned, the <literal>RKE2ControlPlane</literal>
block in the <literal>capi-provisioning-example.yaml</literal> will look
like the following:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>Where the <literal>registry.example.com</literal> is the example name of the
private registry to be used by the downstream cluster, and it should be
replaced with the real values.</para>
</section>
<section xml:id="airgap-deployment">
<title>Downstream cluster provisioning in air-gapped scenarios</title>
<para>The directed network provisioning workflow allows to automate the
provisioning of downstream clusters in air-gapped scenarios.</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>Requirements for air-gapped scenarios</title>
<orderedlist numeration="arabic">
<listitem>
<para>The <literal>raw</literal> image generated using <literal>EIB</literal> must
include the specific container images (helm-chart OCI and container images)
required to run the downstream cluster in an air-gapped scenario. For more
information, refer to this section (<xref
linkend="eib-edge-image-airgap"/>).</para>
</listitem>
<listitem>
<para>In case of using SR-IOV or any other custom workload, the images required to
run the workloads must be preloaded in your private registry following the
preload private registry section (<xref
linkend="preload-private-registry"/>).</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>Enroll the bare metal hosts in air-gap scenarios</title>
<para>The process to enroll the bare metal hosts in the management cluster is the
same as described in the previous section (<xref
linkend="enroll-bare-metal-host"/>).</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>Provision the downstream cluster in air-gap scenarios</title>
<para>There are some important changes required to provision the downstream
cluster in air-gapped scenarios:</para>
<orderedlist numeration="arabic">
<listitem>
<para>The <literal>RKE2ControlPlane</literal> block in the
<literal>capi-provisioning-example.yaml</literal> file must include the
<literal>spec.agentConfig.airGapped: true</literal> directive.</para>
</listitem>
<listitem>
<para>The private registry configuration must be included in the
<literal>RKE2ControlPlane</literal> block in the
<literal>capi-provisioning-airgap-example.yaml</literal> file following the
private registry section (<xref linkend="atip-private-registry"/>).</para>
</listitem>
<listitem>
<para>If you are using SR-IOV or any other <literal>AdditionalUserData</literal>
configuration (combustion script) which requires the helm-chart
installation, you must modify the content to reference the private registry
instead of using the public registry.</para>
</listitem>
</orderedlist>
<para>The following example shows the SR-IOV configuration in the
<literal>AdditionalUserData</literal> block in the
<literal>capi-provisioning-airgap-example.yaml</literal> file with the
modifications required to reference the private registry</para>
<itemizedlist>
<listitem>
<para>Private Registry secrets references</para>
</listitem>
<listitem>
<para>Helm-Chart definition using the private registry instead of the public OCI
images.</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: cilium
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: ${SRIOV_CRD_VERSION}
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: ${SRIOV_OPERATOR_VERSION}
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/sriov-auto-filler.sh
              overwrite: true
              contents:
                inline: |
                  #!/bin/bash
                  cat &lt;&lt;- EOF &gt; /var/sriov-networkpolicy-template.yaml
                  apiVersion: sriovnetwork.openshift.io/v1
                  kind: SriovNetworkNodePolicy
                  metadata:
                    name: atip-RESOURCENAME
                    namespace: sriov-network-operator
                  spec:
                    nodeSelector:
                      feature.node.kubernetes.io/network-sriov.capable: "true"
                    resourceName: RESOURCENAME
                    deviceType: DRIVER
                    numVfs: NUMVF
                    mtu: 1500
                    nicSelector:
                      pfNames: ["PFNAMES"]
                      deviceID: "DEVICEID"
                      vendor: "VENDOR"
                      rootDevices:
                        - PCIADDRESS
                  EOF

                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export KUBECTL=/var/lib/rancher/rke2/bin/kubectl
                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r '.items[].status.syncStatus') != "Succeeded" ]; do sleep 1; done
                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm sriov-custom-auto-config -n sriov-network-operator -ojson | jq -r '.data."config.json"')
                  jq -c '.[]' &lt;&lt;&lt; $input | while read i; do
                    interface=$(echo $i | jq -r '.interface')
                    pfname=$(echo $i | jq -r '.pfname')
                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.pciAddress")
                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.vendor")
                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.deviceID")
                    resourceName=$(echo $i | jq -r '.resourceName')
                    driver=$(echo $i | jq -r '.driver')
                    sed -e "s/RESOURCENAME/$resourceName/g" \
                        -e "s/DRIVER/$driver/g" \
                        -e "s/PFNAMES/$pfname/g" \
                        -e "s/VENDOR/$vendor/g" \
                        -e "s/DEVICEID/$deviceid/g" \
                        -e "s/PCIADDRESS/$pciaddress/g" \
                        -e "s/NUMVF/$(echo $i | jq -r '.numVFsToCreate')/g" /var/sriov-networkpolicy-template.yaml &gt; /var/lib/rancher/rke2/server/manifests/$resourceName.yaml
                  done
              mode: 0755
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - intel_pstate=passive
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - iommu=pt
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - kthread_cpus=${NON-ISOLATED_CPU_CORES}
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nohz=on
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/var/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>Lifecycle actions</title>
<para>This section covers the lifecycle management actions of deployed ATIP
clusters.</para>
<section xml:id="id-management-cluster-upgrades">
<title>Management cluster upgrades</title>
<para>The upgrade of the management cluster involves several components. For a
list of the general components that require an upgrade, see the <literal>Day
2</literal> management cluster (<xref linkend="day2-mgmt-cluster"/>)
documentation.</para>
<para>The upgrade procedure for comoponents specific to this setup can be seen
below.</para>
<para><emphasis role="strong">Upgrading
Metal<superscript>3</superscript></emphasis></para>
<para>To upgrade <literal>Metal<superscript>3</superscript></literal>, use the
following command to update the Helm repository cache and fetch the latest
chart to install <literal>Metal<superscript>3</superscript></literal> from
the Helm chart repository:</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>After that, the easy way to upgrade is to export your current configurations
to a file, and then upgrade the
<literal>Metal<superscript>3</superscript></literal> version using that
previous file.  If any change is required in the new version, the file can
be edited before the upgrade.</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.7.1</screen>
</section>
<section xml:id="id-downstream-cluster-upgrades">
<title>Downstream cluster upgrades</title>
<para>Upgrading downstream clusters involves updating several components. The
following sections cover the upgrade process for each of the components.</para>
<para><emphasis role="strong">Upgrading the operating system</emphasis></para>
<para>For this process, check the following <link
xl:href="atip-automated-provisioning.xml#eib-edge-image-connected">reference</link>
to build the new image with a new operating system version.  With this new
image generated by <literal>EIB</literal>, the next provision phase uses the
new operating version provided.  In the following step, the new image is
used to upgrade the nodes.</para>
<para><emphasis role="strong">Upgrading the RKE2 cluster</emphasis></para>
<para>The changes required to upgrade the <literal>RKE2</literal> cluster using
the automated workflow are the following:</para>
<itemizedlist>
<listitem>
<para>Change the block <literal>RKE2ControlPlane</literal> in the
<literal>capi-provisioning-example.yaml</literal> shown in the following
<link
xl:href="atip-automated-provisioning.xml#single-node-provision">section</link>:</para>
<itemizedlist>
<listitem>
<para>Add the rollout strategy in the spec file.</para>
</listitem>
<listitem>
<para>Change the version of the <literal>RKE2</literal> cluster to the new version
replacing <literal>${RKE2_NEW_VERSION}</literal>.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_NEW_VERSION}
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>Change the block <literal>Metal3MachineTemplate</literal> in the
<literal>capi-provisioning-example.yaml</literal> shown in the following
<link
xl:href="atip-automated-provisioning.xml#single-node-provision">section</link>:</para>
<itemizedlist>
<listitem>
<para>Change the image name and checksum to the new version generated in the
previous step.</para>
</listitem>
<listitem>
<para>Add the directive <literal>nodeReuse</literal> to <literal>true</literal> to
avoid creating a new node.</para>
</listitem>
<listitem>
<para>Add the directive <literal>automatedCleaningMode</literal> to
<literal>metadata</literal> to enable the automated cleaning for the node.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>After making these changes, the
<literal>capi-provisioning-example.yaml</literal> file can be applied to the
cluster using the following command:</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-appendix">
<title>Appendix</title>
<chapter xml:id="id-release-notes">
<title>Release Notes</title>
<section xml:id="release-notes">
<title>Abstract</title>
<para>SUSE Edge 3.0 is a tightly integrated and comprehensively validated
end-to-end solution for addressing the unique challenges of the deployment
of infrastructure and cloud-native applications at the edge. Its driving
focus is to provide an opinionated, yet highly flexible, highly scalable,
and secure platform that spans initial deployment image building, node
provisioning and onboarding, application deployment, observability, and
lifecycle management.</para>
<para>The solution is designed with the notion that there is no
"one-size-fits-all" edge platform due to our customers’ widely varying
requirements and expectations. Edge deployments push us to solve, and
continually evolve, some of the most challenging problems, including massive
scalability, restricted network availability, physical space constraints,
new security threats and attack vectors, variations in hardware architecture
and system resources, the requirement to deploy and interface with legacy
infrastructure and applications, and customer solutions that have extended
lifespans.</para>
<para>SUSE Edge is built on best-of-breed open source software from the ground up,
consistent with both our 30-year history in delivering secure, stable, and
certified SUSE Linux platforms and our experience in providing highly
scalable and feature-rich Kubernetes management with our Rancher
portfolio. SUSE Edge builds on-top of these capabilities to deliver
functionality that can address a wide number of market segments, including
retail, medical, transportation, logistics, telecommunications, smart
manufacturing, and Industrial IoT.</para>
<note>
<para>SUSE Adaptive Telco Infrastructure Platform (ATIP) is a derivative (or
downstream product) of SUSE Edge, with additional optimizations and
components that enable the platform to address the requirements found in
telecommunications use-cases. Unless explicitly stated, all of the release
notes are applicable for both SUSE Edge 3.0, and SUSE ATIP 3.0.</para>
</note>
</section>
<section xml:id="id-about">
<title>About</title>
<para>These Release Notes are, unless explicitly specified and explained,
identical across all architectures, and the most recent version, along with
the release notes of all other SUSE products are always available online at
<link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>.</para>
<para>Entries are only listed once, but they can be referenced in several places
if they are important and belong to more than one section. Release notes
usually only list changes that happened between two subsequent
releases. Certain important entries from the release notes of previous
product versions may be repeated. To make these entries easier to identify,
they contain a note to that effect.</para>
<para>However, repeated entries are provided as a courtesy only. Therefore, if you
are skipping one or releases, check the release notes of the skipped
releases also. If you are only reading the release notes of the current
release, you could miss important changes that may affect system behavior.</para>
</section>
<section xml:id="id-release-3-0-0">
<title>Release 3.0.0</title>
<para>Availability Date: 26th April 2024</para>
<para>Summary: SUSE Edge 3.0.0 is the first release in the SUSE Edge 3.0
portfolio.</para>
<section xml:id="id-new-features">
<title>New Features</title>
<itemizedlist>
<listitem>
<para>Not Applicable - this is the first release shipped in 3.0.z.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>Bug &amp; Security Fixes</title>
<itemizedlist>
<listitem>
<para>Not Applicable - this is the first release shipped in 3.0.z.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-components-versions">
<title>Components Versions</title>
<para>The following table describes the individual components that make up the
3.0.0 release, including the version, the Helm chart version (if
applicable), and where the released artifact can be pulled from in binary
format. Please follow the associated documentation for usage and deployment
examples.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Name</para></entry>
<entry align="left" valign="top"><para>Version</para></entry>
<entry align="left" valign="top"><para>Helm Chart Version</para></entry>
<entry align="left" valign="top"><para>Artifact Location (URL/Image)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SLE Micro</para></entry>
<entry align="left" valign="top"><para>5.5 (latest)</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SLE Micro Download
Page</link><?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso (sha256
4f672a4a0f8ec421e7c25797def05598037c56b7f306283566a9f921bdce904a)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-SelfInstall-GM2.install.iso (sha256
527a5a7cdbf11e3e6238e386533755257676ad8b4c80be3b159d0904cb637678)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz (sha256
13243a737ca219bad6a7aa41fa747c06e8b825fef10a756cf4d575f4493ed68b)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw.xz (sha256
6c2af94e7ac785c8f6a276032c8e6a4b493c294e6cd72809c75089522f01bc93)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Manager</para></entry>
<entry align="left" valign="top"><para>4.3.11</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE Manager
Download Page</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.28.8</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.28.8%2Bk3s1">Upstream
K3s Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.28.8</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.28.8%2Brke2r1">Upstream
RKE2 Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.8.3</para></entry>
<entry align="left" valign="top"><para>2.8.3</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rancher/releases/download/v2.8.3/rancher-images.txt">Rancher
2.8.3 Images</link><?asciidoc-br?> <link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher Prime Helm
Repo</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Longhorn</para></entry>
<entry align="left" valign="top"><para>1.6.1</para></entry>
<entry align="left" valign="top"><para>103.3.0</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn-images.txt">Longhorn
1.6.1 Images</link><?asciidoc-br?> <link
xl:href="https://charts.longhorn.io">Longhorn Helm Repo</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.2.3</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.2.3">NMConfigurator
Upstream Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NeuVector</para></entry>
<entry align="left" valign="top"><para>5.3.0</para></entry>
<entry align="left" valign="top"><para>103.0.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/mirrored-neuvector-controller:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-manager:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.0<?asciidoc-br?>
registry.suse.com/rancher
mirrored-neuvector-registry-adapter:0.1.1-s1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-scanner:latest<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-updater:latest</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Cluster API (CAPI)</para></entry>
<entry align="left" valign="top"><para>1.6.2</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cluster-api-controller:1.6.2<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-metal3:1.6.0<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:0.2.6</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>1.16.0</para></entry>
<entry align="left" valign="top"><para>0.6.5</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metal3-chart:0.6.5<?asciidoc-br?>
registry.suse.com/edge/baremetal-operator:0.5.1<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-controlplane:0.2.6<?asciidoc-br?>
registry.suse.com/edge/ip-address-manager:1.6.0<?asciidoc-br?>
registry.suse.com/edge/ironic:23.0.1.2<?asciidoc-br?>
registry.suse.com/edge/ironic-ipa-downloader:1.3.1<?asciidoc-br?>
registry.suse.com/edge/kube-rbac-proxy:v0.14.2<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metallb-chart:0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-controller:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-speaker:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/frr-k8s:v0.0.8</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.4.3</para></entry>
<entry align="left" valign="top"><para>103.1.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.4.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.4.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.4.3</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.0.1</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/edge-image-builder:1.0.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.1.1</para></entry>
<entry align="left" valign="top"><para>0.2.4</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-chart:0.2.4<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-operator:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-api:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-controller:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-handler:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt Dashboard Extension</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-dashboard-extension-chart:1.0.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.58.0</para></entry>
<entry align="left" valign="top"><para>0.2.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cdi-chart:0.2.3<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-operator:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/endpoint-copier-operator:v0.2.0<?asciidoc-br?>
registry.suse.com/edge/endpoint-copier-operator-chart:0.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (Tech Preview)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/akri-chart:0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-dashboard-extension-chart:1.0.0<?asciidoc-br?>
registry.suse.com/edge/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-webhook-configuration:v0.12.20</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<note>
<para>SUSE Edge z-stream releases are tightly integrated and thoroughly tested as
a versioned stack. Upgrade of any individual components to a different
versions to those listed above is likely to result in system downtime. While
it’s possible to run Edge clusters in untested configurations, it is not
recommended, and it may take longer to provide resolution through the
support channels.</para>
</note>
</section>
</section>
<section xml:id="id-release-3-0-1">
<title>Release 3.0.1</title>
<para>Availability Date: 14th June 2024</para>
<para>Summary: SUSE Edge 3.0.1 is the first z-stream release in the SUSE Edge 3.0
portfolio.</para>
<section xml:id="id-new-features-2">
<title>New Features</title>
<itemizedlist>
<listitem>
<para>Elemental and EIB now support node reset for unmanaged hosts</para>
</listitem>
<listitem>
<para>SR-IOV Network Operator chart is now included</para>
</listitem>
<listitem>
<para>The Metal<superscript>3</superscript> chart now supports providing
additional trusted CA certificates</para>
</listitem>
<listitem>
<para>NM Configurator now supports applying unified configurations without any MAC
specification</para>
</listitem>
<listitem>
<para>Added <literal>version</literal> subcommand to EIB; the version will also
automatically be included in each image built by EIB</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes-2">
<title>Bug &amp; Security Fixes</title>
<itemizedlist>
<listitem>
<para>EIB now automatically sets the execute bit on custom scripts: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/429">SUSE
Edge issue #429</link></para>
</listitem>
<listitem>
<para>EIB now supports disks which are &gt;512 byte sector size: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/447">SUSE
Edge issue #447</link></para>
</listitem>
<listitem>
<para>Enhance EIB’s ability to detect container images in Helm charts: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/442">SUSE
Edge issue #442</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-components-versions-2">
<title>Components Versions</title>
<para>The following table describes the individual components that make up the
3.0.1 release, including the version, the Helm chart version (if
applicable), and where the released artifact can be pulled from in binary
format. Please follow the associated documentation for usage and deployment
examples.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>Name</para></entry>
<entry align="left" valign="top"><para>Version</para></entry>
<entry align="left" valign="top"><para>Helm Chart Version</para></entry>
<entry align="left" valign="top"><para>Artifact Location (URL/Image)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SLE Micro</para></entry>
<entry align="left" valign="top"><para>5.5 (latest)</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SLE Micro Download
Page</link><?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso (sha256
4f672a4a0f8ec421e7c25797def05598037c56b7f306283566a9f921bdce904a)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-SelfInstall-GM2.install.iso (sha256
527a5a7cdbf11e3e6238e386533755257676ad8b4c80be3b159d0904cb637678)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz (sha256
13243a737ca219bad6a7aa41fa747c06e8b825fef10a756cf4d575f4493ed68b)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw.xz (sha256
6c2af94e7ac785c8f6a276032c8e6a4b493c294e6cd72809c75089522f01bc93)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Manager</para></entry>
<entry align="left" valign="top"><para>4.3.11</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE Manager
Download Page</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">K3s</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.28.9</emphasis></para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.28.9%2Bk3s1">Upstream
K3s Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">RKE2</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.28.9</emphasis></para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.28.9%2Brke2r1">Upstream
RKE2 Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Rancher Prime</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.8.4</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.8.4</emphasis></para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rancher/releases/download/v2.8.4/rancher-images.txt">Rancher
2.8.4 Images</link><?asciidoc-br?> <link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher Prime Helm
Repo</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Longhorn</para></entry>
<entry align="left" valign="top"><para>1.6.1</para></entry>
<entry align="left" valign="top"><para>103.3.0</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn-images.txt">Longhorn
1.6.1 Images</link><?asciidoc-br?> <link
xl:href="https://charts.longhorn.io">Longhorn Helm Repo</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">NM Configurator</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.3.0</emphasis></para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.0">NMConfigurator
Upstream Release</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NeuVector</para></entry>
<entry align="left" valign="top"><para>5.3.0</para></entry>
<entry align="left" valign="top"><para>103.0.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/mirrored-neuvector-controller:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-manager:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.0<?asciidoc-br?>
registry.suse.com/rancher
mirrored-neuvector-registry-adapter:0.1.1-s1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-scanner:latest<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-updater:latest</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Cluster API (CAPI)</para></entry>
<entry align="left" valign="top"><para>1.6.2</para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cluster-api-controller:1.6.2<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-metal3:1.6.0<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:0.2.6</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Metal<superscript>3</superscript></emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.16.0</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.7.1</emphasis></para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metal3-chart:0.7.1<?asciidoc-br?>
registry.suse.com/edge/baremetal-operator:0.5.1<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-controlplane:0.2.6<?asciidoc-br?>
registry.suse.com/edge/ip-address-manager:1.6.0<?asciidoc-br?>
registry.suse.com/edge/ironic:23.0.2.1<?asciidoc-br?>
registry.suse.com/edge/ironic-ipa-downloader:1.3.2<?asciidoc-br?>
registry.suse.com/edge/kube-rbac-proxy:v0.14.2 +.1
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metallb-chart:0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-controller:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-speaker:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/frr-k8s:v0.0.8</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Elemental</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.4.4</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">103.1.0</emphasis></para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.4.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.4.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.4.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Edge Image Builder</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.0.2</emphasis></para></entry>
<entry align="left" valign="top"><para>N/A</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/edge-image-builder:1.0.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.1.1</para></entry>
<entry align="left" valign="top"><para>0.2.4</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-chart:0.2.4<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-operator:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-api:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-controller:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-handler:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt Dashboard Extension</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-dashboard-extension-chart:1.0.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.58.0</para></entry>
<entry align="left" valign="top"><para>0.2.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cdi-chart:0.2.3<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-operator:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/endpoint-copier-operator:v0.2.0<?asciidoc-br?>
registry.suse.com/edge/endpoint-copier-operator-chart:0.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (Tech Preview)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/akri-chart:0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-dashboard-extension-chart:1.0.0<?asciidoc-br?>
registry.suse.com/edge/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">SR-IOV Network Operator</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.2.2</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.2.2+up0.1.0</emphasis></para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/sriov-network-operator-chart:1.2.2<?asciidoc-br?>
registry.suse.com/edge/sriov-crd-chart:1.2.2</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-components-verification">
<title>Components Verification</title>
<para>The components mentioned above may be verified using the Software Bill Of
Materials (SBOM) data - for example using <literal>cosign</literal> as
outlined below:</para>
<para>Download the SUSE Edge Container public key from the <link
xl:href="https://www.suse.com/support/security/keys/">SUSE Signing Keys
source</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>Verify the container image hash, for example using <literal>crane</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/baremetal-operator:0.5.1
sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782</screen>
<para>Verify with <literal>cosign</literal>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 &gt; /dev/null
#
Verification for registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The claims were present in the transparency log
  - The signatures were integrated into the transparency log when the certificate was valid
  - The signatures were verified against the specified public key</screen>
<para>Extract SBOM data as described at the <link
xl:href="https://www.suse.com/support/security/sbom/">upstream
documentation</link>:</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>Upgrade Steps</title>
<para>Refer to the Day 2 Documentation for details around how to upgrade to a new
z-stream release.</para>
</section>
<section xml:id="id-known-limitations">
<title>Known Limitations</title>
<para>Unless otherwise stated these apply to the 3.0.0 release and all subsequent
z-stream versions.</para>
<itemizedlist>
<listitem>
<para>Akri is released for the first time as a Technology Preview offering, and is
not subject to the standard scope of support.</para>
</listitem>
<listitem>
<para>Rancher UI Extensions used in SUSE Edge cannot currently be deployed via the
Rancher Marketplace and must be deployed manually. <link
xl:href="https://github.com/rancher/rancher/issues/29105">Rancher issue
#29105</link></para>
</listitem>
<listitem>
<para>If you’re using NVIDIA GPU’s, SELinux cannot be enabled at the containerd
layer due to a missing SELinux policy. <link
xl:href="https://bugzilla.suse.com/show_bug.cgi?id=1222725">Bugzilla
#1222725</link></para>
</listitem>
<listitem>
<para>If deploying with Metal<superscript>3</superscript> and Cluster API (CAPI),
clusters aren’t automatically imported into Rancher post-installation. It
will be addressed in future releases.</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-product-support-lifecycle">
<title>Product Support Lifecycle</title>
<para>SUSE Edge is backed by award-winning support from SUSE, an established
technology leader with a proven history of delivering enterprise-quality
support services. For more information, see <link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>
and the Support Policy page at <link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>.
If you have any questions about raising a support case, how SUSE classifies
severity levels, or the scope of support, please see the Technical Support
Handbook at <link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>.</para>
<para>At the time of publication, each minor version of SUSE Edge, e.g. "3.0" is
supported for 12-months of production support, with an initial 6-months of
"full support", followed by 6-months of "maintenance support". In the "full
support" coverage period, SUSE may introduce new features (that do not break
existing functionality), introduce bug fixes, and deliver security
patches. During the "maintenance support" window, only critical security and
bug fixes will be introduced, with other fixes delivered at our discretion.</para>
<para>Unless explicitly stated, all components listed are considered Generally
Available (GA), and are covered by SUSE’s standard scope of support. Some
components may be listed as "Technology Preview", where SUSE is providing
customers with access to early pre-GA features and functionality for
evaluation, but are not subject to the standard support policies and are not
recommended for production use-cases. SUSE very much welcomes feedback and
suggestions on the improvements that can be made to Technology Preview
components, but SUSE reserves the right to deprecate a Technology Preview
feature before it becomes Generally Available if it doesn’t meet the needs
of our customers or doesn’t reach a state of maturity that we require.</para>
<para>Please note that SUSE must occasionally deprecate features or change API
specifications. Reasons for feature deprecation or API change could include
a feature being updated or replaced by a new implementation, a new feature
set, upstream technology is no longer available, or the upstream community
has introduced incompatible changes. It is not intended that this will ever
happen within a given minor release (x.z), and so all z-stream releases will
maintain API compatibility and feature functionality. SUSE will endeavor to
provide deprecation warnings with plenty of notice within the release notes,
along with workarounds, suggestions, and mitigations to minimize service
disruption.</para>
<para>The SUSE Edge team also welcomes community feedback, where issues can be
raised within the respective code repository within <link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>.</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>Obtaining source code</title>
<para>This SUSE product includes materials licensed to SUSE under the GNU General
Public License (GPL) and various other open source licenses. The GPL
requires SUSE to provide the source code that corresponds to the
GPL-licensed material, and SUSE conforms to all other open-source license
requirements. As such, SUSE makes all source code available, and can
generally be found in the SUSE Edge GitHub repository (<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>),
the SUSE Rancher GitHub repository (<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)
for dependent components, and specifically for SLE Micro, the source code is
available for download at <link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>
on "Medium 2".</para>
</section>
<section xml:id="id-legal-notices">
<title>Legal notices</title>
<para>SUSE makes no representations or warranties with regard to the contents or
use of this documentation, and specifically disclaims any express or implied
warranties of merchantability or fitness for any particular
purpose. Further, SUSE reserves the right to revise this publication and to
make changes to its content, at any time, without the obligation to notify
any person or entity of such revisions or changes.</para>
<para>Further, SUSE makes no representations or warranties with regard to any
software, and specifically disclaims any express or implied warranties of
merchantability or fitness for any particular purpose. Further, SUSE
reserves the right to make changes to any and all parts of SUSE software, at
any time, without any obligation to notify any person or entity of such
changes.</para>
<para>Any products or technical information provided under this Agreement may be
subject to U.S. export controls and the trade laws of other countries. You
agree to comply with all export control regulations and to obtain any
required licenses or classifications to export, re-export, or import
deliverables. You agree not to export or re-export to entities on the
current U.S. export exclusion lists or to any embargoed or terrorist
countries as specified in U.S. export laws. You agree to not use
deliverables for prohibited nuclear, missile, or chemical/biological
weaponry end uses. Refer to <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>
for more information on exporting SUSE software. SUSE assumes no
responsibility for your failure to obtain any necessary export approvals.</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>This release notes document is licensed under a Creative Commons
Attribution-NoDerivatives 4.0 International License (CC-BY-ND-4.0). You
should have received a copy of the license along with this document. If not,
see <link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>.</para>
<para>SUSE has intellectual property rights relating to technology embodied in the
product that is described in this document. In particular, and without
limitation, these intellectual property rights may include one or more of
the U.S. patents listed at <link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>
and one or more additional patents or pending patent applications in the
U.S. and other countries.</para>
<para>For SUSE trademarks, see the SUSE Trademark and Service Mark list (<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>).
All third-party trademarks are the property of their respective owners. For
SUSE brand information and usage requirements, please see the guidelines
published at <link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link>.</para>
</section>
</chapter>
</part>
</book>
