<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | BMC automated deployments with Metal3</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="BMC automated deployments with Metal3"/>
<meta name="description" content="Metal3 is a CNCF project which provides bare-metal infrastructure management capabilities for Kubernetes."/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 1. BMC automated deployments with Metal3"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="BMC automated deployments with Metal3"/>
<meta property="og:description" content="Metal3 is a CNCF project which provides bare-metal infrastr…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="BMC automated deployments with Metal3"/>
<meta name="twitter:description" content="Metal3 is a CNCF project which provides bare-metal infrastr…"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    

    "headline": "BMC automated deployments with Metal3",
  
    "description": "BMC automated deployments with Metal3",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="id-quick-starts.html" title="Part I. Quick Starts"/><link rel="next" href="quickstart-elemental.html" title="Chapter 2. Remote host onboarding with Elemental"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-quick-starts.html">Quick Starts</a><span> / </span><a class="crumb" href="quickstart-metal3.html">BMC automated deployments with Metal3</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge 3.4 Documentation</span></a></li><li class="active"><a href="id-quick-starts.html" class="has-children you-are-here"><span class="title-number">I </span><span class="title-name">Quick Starts</span></a><ol><li><a href="quickstart-metal3.html" class=" you-are-here"><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone clusters with Edge Image Builder</span></a></li><li><a href="quickstart-suma.html" class=" "><span class="title-number">4 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-components.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">5 </span><span class="title-name">Rancher</span></a></li><li><a href="components-rancher-dashboard-extensions.html" class=" "><span class="title-number">6 </span><span class="title-name">Rancher Dashboard Extensions</span></a></li><li><a href="components-rancher-turtles.html" class=" "><span class="title-number">7 </span><span class="title-name">Rancher Turtles</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">8 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">9 </span><span class="title-name">SUSE Linux Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">10 </span><span class="title-name">Metal<sup>3</sup></span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">11 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-nmc.html" class=" "><span class="title-number">12 </span><span class="title-name">Edge Networking</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">13 </span><span class="title-name">Elemental</span></a></li><li><a href="components-akri.html" class=" "><span class="title-number">14 </span><span class="title-name">Akri</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">15 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">16 </span><span class="title-name">RKE2</span></a></li><li><a href="components-suse-storage.html" class=" "><span class="title-number">17 </span><span class="title-name">SUSE Storage</span></a></li><li><a href="components-suse-security.html" class=" "><span class="title-number">18 </span><span class="title-name">SUSE Security</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">19 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-eco.html" class=" "><span class="title-number">20 </span><span class="title-name">Endpoint Copier Operator</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">21 </span><span class="title-name">Edge Virtualization</span></a></li><li><a href="components-system-upgrade-controller.html" class=" "><span class="title-number">22 </span><span class="title-name">System Upgrade Controller</span></a></li><li><a href="components-upgrade-controller.html" class=" "><span class="title-number">23 </span><span class="title-name">Upgrade Controller</span></a></li><li><a href="components-suma.html" class=" "><span class="title-number">24 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How-To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">25 </span><span class="title-name">MetalLB on K3s (using Layer 2 Mode)</span></a></li><li><a href="guides-metallb-k3s-l3.html" class=" "><span class="title-number">26 </span><span class="title-name">MetalLB on K3s (using Layer 3 Mode)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">27 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li><li><a href="id-air-gapped-deployments-with-edge-image-builder.html" class=" "><span class="title-number">28 </span><span class="title-name">Air-gapped deployments with Edge Image Builder</span></a></li><li><a href="guides-kiwi-builder-images.html" class=" "><span class="title-number">29 </span><span class="title-name">Building Updated SUSE Linux Micro Images with Kiwi</span></a></li><li><a href="guides-clusterclass-example.html" class=" "><span class="title-number">30 </span><span class="title-name">Using clusterclass to deploy downstream clusters</span></a></li></ol></li><li><a href="tips-and-tricks.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Tips and Tricks</span></a><ol><li><a href="tips-edge-image-builder.html" class=" "><span class="title-number">31 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="tips-elemental.html" class=" "><span class="title-number">32 </span><span class="title-name">Elemental</span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">V </span><span class="title-name">Third-Party Integration</span></a><ol><li><a href="integrations-nats.html" class=" "><span class="title-number">33 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-suse-linux-micro.html" class=" "><span class="title-number">34 </span><span class="title-name">NVIDIA GPUs on SUSE Linux Micro</span></a></li></ol></li><li><a href="day-2-operations.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Day 2 Operations</span></a><ol><li><a href="day2-migration.html" class=" "><span class="title-number">35 </span><span class="title-name">Edge 3.4 migration</span></a></li><li><a href="day2-mgmt-cluster.html" class=" "><span class="title-number">36 </span><span class="title-name">Management Cluster</span></a></li><li><a href="day2-downstream-clusters.html" class=" "><span class="title-number">37 </span><span class="title-name">Downstream clusters</span></a></li></ol></li><li><a href="id-suse-telco-cloud-documentation.html" class="has-children "><span class="title-number">VII </span><span class="title-name">SUSE Telco Cloud Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">38 </span><span class="title-name">SUSE Telco Cloud</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">39 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">40 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" "><span class="title-number">41 </span><span class="title-name">Setting up the management cluster</span></a></li><li><a href="atip-features.html" class=" "><span class="title-number">42 </span><span class="title-name">Telco features configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" "><span class="title-number">43 </span><span class="title-name">Fully automated directed network provisioning</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">44 </span><span class="title-name">Lifecycle actions</span></a></li></ol></li><li><a href="id-troubleshooting-3.html" class="has-children "><span class="title-number">VIII </span><span class="title-name">Troubleshooting</span></a><ol><li><a href="general-troubleshooting-principles.html" class=" "><span class="title-number">45 </span><span class="title-name">General Troubleshooting Principles</span></a></li><li><a href="troubleshooting-kiwi.html" class=" "><span class="title-number">46 </span><span class="title-name">Troubleshooting Kiwi</span></a></li><li><a href="troubleshooting-edge-image-builder.html" class=" "><span class="title-number">47 </span><span class="title-name">Troubleshooting Edge Image Builder (EIB)</span></a></li><li><a href="troubleshooting-edge-networking.html" class=" "><span class="title-number">48 </span><span class="title-name">Troubleshooting Edge Networking (NMC)</span></a></li><li><a href="troubleshooting-phone-home-scenarios.html" class=" "><span class="title-number">49 </span><span class="title-name">Troubleshooting Phone-Home scenarios</span></a></li><li><a href="troubleshooting-directed-network-provisioning.html" class=" "><span class="title-number">50 </span><span class="title-name">Troubleshooting Directed-network provisioning</span></a></li><li><a href="troubleshooting-other-components.html" class=" "><span class="title-number">51 </span><span class="title-name">Troubleshooting Other components</span></a></li><li><a href="collecting-diagnostics-for-support.html" class=" "><span class="title-number">52 </span><span class="title-name">Collecting Diagnostics for Support</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">IX </span><span class="title-name">Appendix</span></a><ol><li><a href="id-release-notes.html" class=" "><span class="title-number">53 </span><span class="title-name">Release Notes</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="quickstart-metal3" data-id-title="BMC automated deployments with Metal3"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Metal<sup>3</sup> is a <a class="link" href="https://metal3.io/" target="_blank">CNCF project</a> which provides bare-metal infrastructure
management capabilities for Kubernetes.</p><p>Metal<sup>3</sup> provides Kubernetes-native resources to manage the lifecycle of bare-metal servers
which support management via out-of-band protocols such as <a class="link" href="https://www.dmtf.org/standards/redfish" target="_blank">Redfish</a>.</p><p>It also has mature support for <a class="link" href="https://cluster-api.sigs.k8s.io/" target="_blank">Cluster API (CAPI)</a> which enables management
of infrastructure resources across multiple infrastructure providers via broadly adopted vendor-neutral APIs.</p><section class="sect1" id="id-why-use-this-method" data-id-title="Why use this method"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.1 </span><span class="title-name">Why use this method</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-why-use-this-method">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This method is useful for scenarios where the target hardware supports out-of-band management, and a fully automated
infrastructure management flow is desired.</p><p>A management cluster is configured to provide declarative APIs that enable inventory and state management of downstream
cluster bare-metal servers, including automated inspection, cleaning and provisioning/deprovisioning.</p></section><section class="sect1" id="id-high-level-architecture-2" data-id-title="High-level architecture"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.2 </span><span class="title-name">High-level architecture</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-high-level-architecture-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="informalfigure"><div class="mediaobject"><a href="images/quickstart-metal3-architecture.svg"><img src="images/quickstart-metal3-architecture.svg" width="100%" alt="quickstart metal3 architecture" title="quickstart metal3 architecture"/></a></div></div></section><section class="sect1" id="id-prerequisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.3 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-prerequisites">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>There are some specific constraints related to the downstream cluster server hardware and networking:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Management cluster</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Must have network connectivity to the target server management/BMC API</p></li><li class="listitem"><p>Must have network connectivity to the target server control plane network</p></li><li class="listitem"><p>For multi-node management clusters, an additional reserved IP address is required</p></li></ul></div></li><li class="listitem"><p>Hosts to be controlled</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Must support out-of-band management via Redfish, iDRAC or iLO interfaces</p></li><li class="listitem"><p>Must support deployment via virtual media (PXE is not currently supported)</p></li><li class="listitem"><p>Must have network connectivity to the management cluster for access to the Metal<sup>3</sup> provisioning APIs</p></li></ul></div></li></ul></div><p>Some tools are required, these can be installed either on the management cluster, or on a host which can access it.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><a class="link" href="https://kubernetes.io/docs/reference/kubectl/kubectl/" target="_blank">Kubectl</a>, <a class="link" href="https://helm.sh" target="_blank">Helm</a> and <a class="link" href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl" target="_blank">Clusterctl</a></p></li><li class="listitem"><p>A container runtime such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a></p></li></ul></div><p>The <code class="literal">SL-Micro.x86_64-6.1-Base-GM.raw</code> OS image file must be downloaded from the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a> or the <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">SUSE Download page</a>.</p></section><section class="sect1" id="id-deployment" data-id-title="Deployment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.4 </span><span class="title-name">Deployment</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-deployment">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-setup-management-cluster" data-id-title="Setup Management Cluster"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.1 </span><span class="title-name">Setup Management Cluster</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-setup-management-cluster">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The basic steps to install a management cluster and use Metal<sup>3</sup> are:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install an RKE2 management cluster</p></li><li class="listitem"><p>Install Rancher</p></li><li class="listitem"><p>Install a storage provider (optional)</p></li><li class="listitem"><p>Install the Metal<sup>3</sup> dependencies</p></li><li class="listitem"><p>Install CAPI dependencies via Rancher Turtles</p></li><li class="listitem"><p>Build a SLEMicro OS image for downstream cluster hosts</p></li><li class="listitem"><p>Register BareMetalHost CRs to define the bare-metal inventory</p></li><li class="listitem"><p>Create a downstream cluster by defining CAPI resources</p></li></ol></div><p>This guide assumes an existing RKE2 cluster and Rancher (including cert-manager) has been installed, for example by using Edge Image Builder (<a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a>).</p><div id="id-1.3.3.8.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>The steps here can also be fully automated as described in the Management Cluster Documentation (<a class="xref" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster">Chapter 41, <em>Setting up the management cluster</em></a>).</p></div></section><section class="sect2" id="id-installing-metal3-dependencies" data-id-title="Installing Metal3 dependencies"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.2 </span><span class="title-name">Installing Metal<sup>3</sup> dependencies</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-installing-metal3-dependencies">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>If not already installed as part of the Rancher installation, cert-manager must be installed and running.</p><p>A persistent storage provider must be installed. SUSE Storage is recommended but <code class="literal">local-path-provisioner</code> can also be used for
dev/PoC environments. The instructions below assume a StorageClass has been
<a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/" target="_blank">marked as default</a>,
otherwise additional configuration for the Metal<sup>3</sup> chart is required.</p><p>An additional IP is required, which is managed by <a class="link" href="https://metallb.universe.tf/" target="_blank">MetalLB</a> to provide a
consistent endpoint for the Metal<sup>3</sup> management services.
This IP must be part of the control plane subnet and reserved for static configuration (not part of any DHCP pool).</p><div id="id-1.3.3.8.3.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>If the management cluster is a single node, the requirement for an additional floating IP managed via MetalLB can be avoided, see <a class="xref" href="quickstart-metal3.html#id-single-node-configuration" title="1.7.1. Single-node configuration">Section 1.7.1, “Single-node configuration”</a></p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>First, we install MetalLB:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
  metallb oci://registry.suse.com/edge/charts/metallb \
  --namespace metallb-system \
  --create-namespace</pre></div></li><li class="listitem"><p>Then we define an <code class="literal">IPAddressPool</code> and <code class="literal">L2Advertisement</code> using the reserved IP, defined as <code class="literal">STATIC_IRONIC_IP</code> below:</p><div class="verbatim-wrap highlight bash"><pre class="screen">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</pre></div><div class="verbatim-wrap highlight bash"><pre class="screen">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</pre></div></li><li class="listitem"><p>Now Metal<sup>3</sup> can be installed:</p><div class="verbatim-wrap highlight bash"><pre class="screen">helm install \
  metal3 oci://registry.suse.com/edge/charts/metal3 \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="$STATIC_IRONIC_IP"</pre></div></li><li class="listitem"><p>It can take around two minutes for the init container to run on this deployment, so ensure the pods are all running before proceeding:</p><div class="verbatim-wrap"><pre class="screen">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</pre></div></li></ol></div><div id="id-1.3.3.8.3.7" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>Do not proceed to the following steps until all pods in the <code class="literal">metal3-system</code> namespace are running.</p></div></section><section class="sect2" id="id-installing-cluster-api-dependencies" data-id-title="Installing cluster API dependencies"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.3 </span><span class="title-name">Installing cluster API dependencies</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-installing-cluster-api-dependencies">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Cluster API dependencies are managed via the Rancher Turtles Helm chart:</p><div class="verbatim-wrap highlight bash"><pre class="screen">cat &gt; values.yaml &lt;&lt;EOF
rancherTurtles:
  features:
    embedded-capi:
      disabled: true
    rancher-webhook:
      cleanup: true
EOF

helm install \
  rancher-turtles oci://registry.suse.com/edge/charts/rancher-turtles \
  --namespace rancher-turtles-system \
  --create-namespace \
  -f values.yaml</pre></div><p>After some time, the controller pods should be running in the <code class="literal">capi-system</code>, <code class="literal">capm3-system</code>, <code class="literal">rke2-bootstrap-system</code> and <code class="literal">rke2-control-plane-system</code> namespaces.</p></section><section class="sect2" id="id-prepare-downstream-cluster-image" data-id-title="Prepare downstream cluster image"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.4 </span><span class="title-name">Prepare downstream cluster image</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-prepare-downstream-cluster-image">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Kiwi (<a class="xref" href="guides-kiwi-builder-images.html" title="Chapter 29. Building Updated SUSE Linux Micro Images with Kiwi">Chapter 29, <em>Building Updated SUSE Linux Micro Images with Kiwi</em></a>) and Edge Image Builder (<a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a>) are used to prepare a modified SLEMicro base image which is provisioned on downstream cluster hosts.</p><p>In this guide, we cover the minimal configuration necessary to deploy the downstream cluster.</p><section class="sect3" id="id-image-configuration" data-id-title="Image configuration"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.4.4.1 </span><span class="title-name">Image configuration</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-image-configuration">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div id="id-1.3.3.8.5.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Please follow <a class="xref" href="guides-kiwi-builder-images.html" title="Chapter 29. Building Updated SUSE Linux Micro Images with Kiwi">Chapter 29, <em>Building Updated SUSE Linux Micro Images with Kiwi</em></a> first to build a fresh image as the first step required to create clusters.</p></div><p>When running Edge Image Builder, a directory is mounted from the host, so it is necessary to create a directory structure to store the configuration files used to define the target image.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">downstream-cluster-config.yaml</code> is the image definition file, see <a class="xref" href="quickstart-eib.html" title="Chapter 3. Standalone clusters with Edge Image Builder">Chapter 3, <em>Standalone clusters with Edge Image Builder</em></a> for more details.</p></li><li class="listitem"><p>The base image when downloaded is <code class="literal">xz</code> compressed, which must be uncompressed with <code class="literal">unxz</code> and copied/moved under the <code class="literal">base-images</code> folder.</p></li><li class="listitem"><p>The <code class="literal">network</code> folder is optional, see <a class="xref" href="quickstart-metal3.html#metal3-add-network-eib" title="1.4.5.1.1. Additional script for static network configuration">Section 1.4.5.1.1, “Additional script for static network configuration”</a> for more details.</p></li><li class="listitem"><p>The custom/scripts directory contains scripts to be run on first-boot; currently a <code class="literal">01-fix-growfs.sh</code> script is required to resize the OS root partition on deployment</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">├── downstream-cluster-config.yaml
├── base-images/
│   └ SL-Micro.x86_64-6.1-Base-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</pre></div><section class="sect4" id="id-downstream-cluster-image-definition-file" data-id-title="Downstream cluster image definition file"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">1.4.4.1.1 </span><span class="title-name">Downstream cluster image definition file</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-downstream-cluster-image-definition-file">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">downstream-cluster-config.yaml</code> file is the main configuration file for the downstream cluster image. The following is a minimal example for deployment via Metal<sup>3</sup>:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  imageType: raw
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      forceWait: true
      pools:
        - 2.suse.pool.ntp.org
      servers:
        - 10.0.0.1
        - 10.0.0.2
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
      - transactional-update.timer
      - transactional-update-cleanup.timer
  users:
    - username: root
      encryptedPassword: $ROOT_PASSWORD
      sshKeys:
      - $USERKEY1
  packages:
    packageList:
      - jq
  sccRegistrationCode: $SCC_REGISTRATION_CODE</pre></div><p>Where <code class="literal">$SCC_REGISTRATION_CODE</code> is the registration code copied from <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a>, and the package list contains <code class="literal">jq</code> w
hich is required.</p><p><code class="literal">$ROOT_PASSWORD</code> is the encrypted password for the root user, which can be useful for test/debugging.  It can be generated with the <code class="literal">openssl passwd -6 PASSWORD</code> command</p><p>For the production environments, it is recommended to use the SSH keys that can be added to the users block replacing the <code class="literal">$USERKEY1</code> with the real SSH keys.</p><div id="id-1.3.3.8.5.4.6.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p><code class="literal">net.ifnames=1</code> enables <a class="link" href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html" target="_blank">Predictable Network Interface Naming</a></p><p>This matches the default configuration for the Metal<sup>3</sup> chart, but the setting must match the configured chart <code class="literal">predictableNicNames</code> value.</p><p>Also note that <code class="literal">ignition.platform.id=openstack</code> is mandatory - without this argument SUSE Linux Micro configuration via ignition will fail in the Metal<sup>3</sup> automated flow.</p><p>The <code class="literal">time</code> section is optional but it is highly recommended to be configured to avoid potential issues with certificates and clock skew. The values provided in this example are for illustrative purposes only. Please adjust them to fit your specific requirements.</p></div></section><section class="sect4" id="growfs-script" data-id-title="Growfs script"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">1.4.4.1.2 </span><span class="title-name">Growfs script</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#growfs-script">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Currently, a custom script (<code class="literal">custom/scripts/01-fix-growfs.sh</code>) is required to grow the file system to match the disk size on first-boot after provisioning. The <code class="literal">01-fix-growfs.sh</code> script contains the following information:</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</pre></div><div id="id-1.3.3.8.5.4.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Add your own custom scripts to be executed during the provisioning process using the same approach.
For more information, see <a class="xref" href="quickstart-eib.html" title="Chapter 3. Standalone clusters with Edge Image Builder">Chapter 3, <em>Standalone clusters with Edge Image Builder</em></a>.</p></div></section></section><section class="sect3" id="id-image-creation" data-id-title="Image creation"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.4.4.2 </span><span class="title-name">Image creation</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-image-creation">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Once the directory structure is prepared following the previous sections, run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file downstream-cluster-config.yaml</pre></div><p>This creates the output image file named <code class="literal">SLE-Micro-eib-output.raw</code>, based on the definition described above.</p><p>The output image must then be made available via a webserver, either the media-server container enabled via the Metal3 chart (<a class="xref" href="atip-management-cluster.html#metal3-media-server" title="Note">Note</a>)
or some other locally accessible server.  In the examples below, we refer to this server as <code class="literal">imagecache.local:8080</code></p><div id="id-1.3.3.8.5.5.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>When deploying EIB images to downstream clusters, it is required also to include the sha256 sum of the image on the <code class="literal">Metal3MachineTemplate</code> object.
It can be generated as:</p><div class="verbatim-wrap"><pre class="screen">sha256sum &lt;image_file&gt; &gt; &lt;image_file&gt;.sha256
# On this example:
sha256sum SLE-Micro-eib-output.raw &gt; SLE-Micro-eib-output.raw.sha256</pre></div></div></section></section><section class="sect2" id="id-adding-baremetalhost-inventory" data-id-title="Adding BareMetalHost inventory"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.5 </span><span class="title-name">Adding BareMetalHost inventory</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-adding-baremetalhost-inventory">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Registering bare-metal servers for automated deployment requires creating two resources: a Secret storing
BMC access credentials and a Metal<sup>3</sup> BareMetalHost resource defining the BMC connection and other details:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  architecture: x86_64
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</pre></div><p>Note the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The Secret username/password must be base64 encoded. Note this should not include any trailing newlines (for example, use <code class="literal">echo -n</code>, not just <code class="literal">echo</code>!)</p></li><li class="listitem"><p>The <code class="literal">cluster-role</code> label may be set now or later on cluster creation. In the example below, we expect <code class="literal">control-plane</code> or <code class="literal">worker</code></p></li><li class="listitem"><p><code class="literal">bootMACAddress</code> must be a valid MAC that matches the control plane NIC of the host</p></li><li class="listitem"><p>The <code class="literal">bmc</code> address is the connection to the BMC management API, the following are supported:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">redfish-virtualmedia://&lt;IP ADDRESS&gt;/redfish/v1/Systems/&lt;SYSTEM ID&gt;</code>: Redfish virtual media, for example, SuperMicro</p></li><li class="listitem"><p><code class="literal">idrac-virtualmedia://&lt;IP ADDRESS&gt;/redfish/v1/Systems/System.Embedded.1</code>: Dell iDRAC</p></li></ul></div></li><li class="listitem"><p>See the <a class="link" href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md" target="_blank">Upstream API docs</a> for more details on the BareMetalHost API</p></li></ul></div><section class="sect3" id="id-configuring-static-ips" data-id-title="Configuring Static IPs"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.4.5.1 </span><span class="title-name">Configuring Static IPs</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-configuring-static-ips">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The BareMetalHost example above assumes DHCP provides the controlplane network configuration, but for scenarios where manual configuration
is needed such as static IPs it is possible to provide additional configuration, as described below.</p><section class="sect4" id="metal3-add-network-eib" data-id-title="Additional script for static network configuration"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">1.4.5.1.1 </span><span class="title-name">Additional script for static network configuration</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#metal3-add-network-eib">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When creating the base image with Edge Image Builder, in the <code class="literal">network</code> folder, create the following <code class="literal">configure-network.sh</code> file.</p><p>This consumes configuration drive data on first-boot, and configures the host networking using the <a class="link" href="https://github.com/suse-edge/nm-configurator" target="_blank">NM Configurator tool</a>.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')
echo "${DESIRED_HOSTNAME}" &gt; /etc/hostname

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/_all.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</pre></div></section><section class="sect4" id="id-additional-secret-with-host-network-configuration" data-id-title="Additional secret with host network configuration"><div class="titlepage"><div><div><div class="title-container"><h5 class="title"><span class="title-number-name"><span class="title-number">1.4.5.1.2 </span><span class="title-name">Additional secret with host network configuration</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-additional-secret-with-host-network-configuration">#</a></h5><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>An additional secret containing data in the <a class="link" href="https://nmstate.io/" target="_blank">nmstate</a> format supported by NM Configurator (<a class="xref" href="components-nmc.html" title="Chapter 12. Edge Networking">Chapter 12, <em>Edge Networking</em></a>) can be defined for each host.</p><p>The secret is then referenced in the <code class="literal">BareMetalHost</code> resource via the <code class="literal">preprovisioningNetworkDataName</code> spec field.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</pre></div><div id="id-1.3.3.8.6.6.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>In some circumstances the MAC address may be omitted. See <a class="xref" href="components-nmc.html#networking-unified" title="12.5.8. Unified node configurations">Section 12.5.8, “Unified node configurations”</a> for additional details.</p></div></section></section><section class="sect3" id="id-baremetalhost-preparation" data-id-title="BareMetalHost preparation"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">1.4.5.2 </span><span class="title-name">BareMetalHost preparation</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-baremetalhost-preparation">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>After creating the BareMetalHost resource and associated secrets as described above, a host preparation workflow is triggered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>A ramdisk image is booted by virtualmedia attachment to the target host BMC</p></li><li class="listitem"><p>The ramdisk inspects hardware details, and prepares the host for provisioning (for example by cleaning disks of previous data)</p></li><li class="listitem"><p>On completion of this process, hardware details in the BareMetalHost <code class="literal">status.hardware</code> field are updated and can be verified</p></li></ul></div><p>This process can take several minutes, but when completed you should see the BareMetalHost state become <code class="literal">available</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</pre></div></section></section><section class="sect2" id="id-creating-downstream-clusters" data-id-title="Creating downstream clusters"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.6 </span><span class="title-name">Creating downstream clusters</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-creating-downstream-clusters">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>We now create Cluster API resources which define the downstream cluster, and Machine resources which will cause the BareMetalHost resources to
be provisioned, then bootstrapped to form an RKE2 cluster.</p></section><section class="sect2" id="id-control-plane-deployment" data-id-title="Control plane deployment"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.7 </span><span class="title-name">Control plane deployment</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-control-plane-deployment">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>To deploy the controlplane we define a yaml manifest similar to the one below, which contains the following resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Cluster resource defines the cluster name, networks, and type of controlplane/infrastructure provider (in this case RKE2/Metal3)</p></li><li class="listitem"><p>Metal3Cluster defines the controlplane endpoint (host IP for single-node, LoadBalancer endpoint for multi-node, this example assumes single-node)</p></li><li class="listitem"><p>RKE2ControlPlane defines the RKE2 version and any additional configuration needed during cluster bootstrapping</p></li><li class="listitem"><p>Metal3MachineTemplate defines the OS Image to be applied to the BareMetalHost resources, and the hostSelector defines which BareMetalHosts to consume</p></li><li class="listitem"><p>Metal3DataTemplate defines additional metaData to be passed to the BareMetalHost (note networkData is not currently supported in the Edge solution)</p></li></ul></div><div id="id-1.3.3.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For simplicity this example assumes a single-node control plane where the BareMetalHost is configured with an IP of <code class="literal">192.168.125.200</code>. For more advanced multi-node examples, please see <a class="xref" href="atip-automated-provisioning.html" title="Chapter 43. Fully automated directed network provisioning">Chapter 43, <em>Fully automated directed network provisioning</em></a>.</p></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  version: v1.33.3+rke2r1
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>Once adapted to your environment, you can apply the example via <code class="literal">kubectl</code> and then monitor the cluster status via <code class="literal">clusterctl</code>.</p><div class="verbatim-wrap highlight bash"><pre class="screen">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</pre></div></section><section class="sect2" id="id-workercompute-deployment" data-id-title="Worker/Compute deployment"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.8 </span><span class="title-name">Worker/Compute deployment</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-workercompute-deployment">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Similar to the control plane deployment, we define a YAML manifest which contains the following resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>MachineDeployment defines the number of replicas (hosts) and the bootstrap/infrastructure provider (in this case RKE2/Metal3)</p></li><li class="listitem"><p>RKE2ConfigTemplate describes the RKE2 version and first-boot configuration for agent host bootstrapping</p></li><li class="listitem"><p>Metal3MachineTemplate defines the OS Image to be applied to the BareMetalHost resources, and the host selector defines which BareMetalHosts to consume</p></li><li class="listitem"><p>Metal3DataTemplate defines additional metadata to be passed to the BareMetalHost (note that <code class="literal">networkData</code> is not currently supported)</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.33.3+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.33.3+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</pre></div><p>When the example above has been copied and adapted to suit your environment, it can be applied via <code class="literal">kubectl</code> then the cluster status can be monitored with <code class="literal">clusterctl</code></p><div class="verbatim-wrap highlight bash"><pre class="screen">% kubectl apply -f rke2-agent.yaml

# Wait for the worker nodes to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</pre></div></section><section class="sect2" id="id-cluster-deprovisioning" data-id-title="Cluster deprovisioning"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.4.9 </span><span class="title-name">Cluster deprovisioning</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-cluster-deprovisioning">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The downstream cluster may be deprovisioned by deleting the resources applied in the creation steps above:</p><div class="verbatim-wrap highlight bash"><pre class="screen">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</pre></div><p>This triggers deprovisioning of the BareMetalHost resources, which may take several minutes, after which they should be in available state again:</p><div class="verbatim-wrap highlight bash"><pre class="screen">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</pre></div></section></section><section class="sect1" id="id-known-issues" data-id-title="Known issues"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.5 </span><span class="title-name">Known issues</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-known-issues">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The upstream <a class="link" href="https://github.com/metal3-io/ip-address-manager" target="_blank">IP Address Management controller</a> is currently not supported, because it’s not yet compatible with our choice of network configuration tooling and first-boot toolchain in SLEMicro.</p></li><li class="listitem"><p>Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are not currently supported.</p></li><li class="listitem"><p>Only deployment via redfish-virtualmedia is currently supported.</p></li><li class="listitem"><p>It is possible to observe a network device name misalignment between the ironic python agent (IPA) and the target operating system (SL Micro 6.0/6.1), especially when trying to configure predictable names for the devices.</p></li></ul></div><p>This happens because the kernel of the ironic python agent (IPA) is not currently aligned with the kernel of the target operating system (SL Micro 6.0/6.1), therefore there’s a misalignment in the network drivers that allows the IPA to discover network devices in a different naming pattern than SL Micro expects.</p><p>There are two different approaches to be used as a workaround in the meantime:
* Create two different secrets with the network configuration, one to be used with the IPA using the device names as IPA will discover and use it as <code class="literal">preprovisioningNetworkDataName</code> on the <code class="literal">BareMetalHost</code> definition and another secret with the device names as SL Micro will discover and use it as <code class="literal">networkData.name</code> on the <code class="literal">BareMetalHost</code> definition.
* Use the UUIDs to reference other interfaces on the generated nmconnection files instead.
More details in the <a class="link" href="..tips/metal3.adoc" target="_blank">tips and tricks</a> section.</p></section><section class="sect1" id="id-planned-changes" data-id-title="Planned changes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.6 </span><span class="title-name">Planned changes</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-planned-changes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Enable support of the IPAM resources and configuration via networkData fields</p></li></ul></div></section><section class="sect1" id="id-additional-resources" data-id-title="Additional resources"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">1.7 </span><span class="title-name">Additional resources</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-additional-resources">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The SUSE Telco Cloud Documentation (<a class="xref" href="atip.html" title="Chapter 38. SUSE Telco Cloud">Chapter 38, <em>SUSE Telco Cloud</em></a>) has examples of more advanced usage of Metal<sup>3</sup> for telco use-cases.</p><section class="sect2" id="id-single-node-configuration" data-id-title="Single-node configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.7.1 </span><span class="title-name">Single-node configuration</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-single-node-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>For test/PoC environments where the management cluster is a single node, it is possible to avoid the requirement for an additional floating IP managed via MetalLB.</p><p>In this mode, the endpoint for the management cluster APIs is the IP of the management cluster, therefore it should be reserved when using DHCP
or statically configured to ensure the management cluster IP does not change - referred to as <code class="literal">&lt;MANAGEMENT_CLUSTER_IP&gt;</code> below.</p><p>To enable this scenario, the Metal<sup>3</sup> chart values required are as follows:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</pre></div></section><section class="sect2" id="disabling-tls-for-virtualmedia-iso-attachment" data-id-title="Disabling TLS for virtualmedia ISO attachment"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.7.2 </span><span class="title-name">Disabling TLS for virtualmedia ISO attachment</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#disabling-tls-for-virtualmedia-iso-attachment">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Some server vendors verify the SSL connection when attaching virtual-media ISO images to the BMC, which can cause a problem because the generated
certificates for the Metal<sup>3</sup> deployment are self-signed, to work around this issue it’s possible to disable TLS only for the virtualmedia disk attachment
with Metal<sup>3</sup> chart values as follows:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  enable_vmedia_tls: false</pre></div><p>An alternative solution is to configure the BMCs with the CA cert - in this case you can read the certificates from the cluster using <code class="literal">kubectl</code>:</p><div class="verbatim-wrap highlight bash"><pre class="screen">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</pre></div><p>The certificate can then be configured on the server BMC console, although the process for that is vendor specific (and not possible for all
vendors, in which case the <code class="literal">enable_vmedia_tls</code> flag may be required).</p></section><section class="sect2" id="id-storage-configuration" data-id-title="Storage configuration"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">1.7.3 </span><span class="title-name">Storage configuration</span></span> <a title="Permalink" class="permalink" href="quickstart-metal3.html#id-storage-configuration">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>For test/PoC environments where the management cluster is a single node, no persistent storage is required, but for production use-cases it
is recommended to install SUSE Storage (Longhorn) on the management cluster so that images related to Metal<sup>3</sup> can be persisted during a pod
restart/reschedule.</p><p>To enable this persistent storage, the Metal<sup>3</sup> chart values required are as follows:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">metal3-ironic:
  persistence:
    ironic:
      size: "5Gi"</pre></div><p>The SUSE Telco Cloud Management Cluster Documentation (<a class="xref" href="atip-management-cluster.html" title="Chapter 41. Setting up the management cluster">Chapter 41, <em>Setting up the management cluster</em></a>) has more details on how to configure a management cluster
with persistent storage.</p></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="id-quick-starts.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part I </span>Quick Starts</span></a> </div><div><a class="pagination-link next" href="quickstart-elemental.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 2 </span>Remote host onboarding with Elemental</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="quickstart-metal3.html#id-why-use-this-method"><span class="title-number">1.1 </span><span class="title-name">Why use this method</span></a></span></li><li><span class="section"><a href="quickstart-metal3.html#id-high-level-architecture-2"><span class="title-number">1.2 </span><span class="title-name">High-level architecture</span></a></span></li><li><span class="section"><a href="quickstart-metal3.html#id-prerequisites"><span class="title-number">1.3 </span><span class="title-name">Prerequisites</span></a></span></li><li><span class="section"><a href="quickstart-metal3.html#id-deployment"><span class="title-number">1.4 </span><span class="title-name">Deployment</span></a></span></li><li><span class="section"><a href="quickstart-metal3.html#id-known-issues"><span class="title-number">1.5 </span><span class="title-name">Known issues</span></a></span></li><li><span class="section"><a href="quickstart-metal3.html#id-planned-changes"><span class="title-number">1.6 </span><span class="title-name">Planned changes</span></a></span></li><li><span class="section"><a href="quickstart-metal3.html#id-additional-resources"><span class="title-number">1.7 </span><span class="title-name">Additional resources</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2025</span></div></div></footer></body></html>