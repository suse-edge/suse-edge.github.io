<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | NVIDIA GPUs on SUSE Linux Micro</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="NVIDIA GPUs on SUSE Linux Micro"/>
<meta name="description" content="This guide demonstrates how to implement host-level NVIDIA GPU support via the pre-built open-source drivers on SUSE Linux Micro 6.1. These are drive…"/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 34. NVIDIA GPUs on SUSE Linux Micro"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="NVIDIA GPUs on SUSE Linux Micro"/>
<meta property="og:description" content="This guide demonstrates how to implement host-level NVIDIA …"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NVIDIA GPUs on SUSE Linux Micro"/>
<meta name="twitter:description" content="This guide demonstrates how to implement host-level NVIDIA …"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    

    "headline": "NVIDIA GPUs on SUSE Linux Micro",
  
    "description": "NVIDIA GPUs on SUSE Linux Micro",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="integrations-nats.html" title="Chapter 33. NATS"/><link rel="next" href="day-2-operations.html" title="Part VI. Day 2 Operations"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-third-party-integration.html">Third-Party Integration</a><span> / </span><a class="crumb" href="id-nvidia-gpus-on-suse-linux-micro.html">NVIDIA GPUs on SUSE Linux Micro</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge 3.4 Documentation</span></a></li><li><a href="id-quick-starts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quick Starts</span></a><ol><li><a href="quickstart-metal3.html" class=" "><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone clusters with Edge Image Builder</span></a></li><li><a href="quickstart-suma.html" class=" "><span class="title-number">4 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-components.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">5 </span><span class="title-name">Rancher</span></a></li><li><a href="components-rancher-dashboard-extensions.html" class=" "><span class="title-number">6 </span><span class="title-name">Rancher Dashboard Extensions</span></a></li><li><a href="components-rancher-turtles.html" class=" "><span class="title-number">7 </span><span class="title-name">Rancher Turtles</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">8 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">9 </span><span class="title-name">SUSE Linux Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">10 </span><span class="title-name">Metal<sup>3</sup></span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">11 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-nmc.html" class=" "><span class="title-number">12 </span><span class="title-name">Edge Networking</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">13 </span><span class="title-name">Elemental</span></a></li><li><a href="components-akri.html" class=" "><span class="title-number">14 </span><span class="title-name">Akri</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">15 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">16 </span><span class="title-name">RKE2</span></a></li><li><a href="components-suse-storage.html" class=" "><span class="title-number">17 </span><span class="title-name">SUSE Storage</span></a></li><li><a href="components-suse-security.html" class=" "><span class="title-number">18 </span><span class="title-name">SUSE Security</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">19 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-eco.html" class=" "><span class="title-number">20 </span><span class="title-name">Endpoint Copier Operator</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">21 </span><span class="title-name">Edge Virtualization</span></a></li><li><a href="components-system-upgrade-controller.html" class=" "><span class="title-number">22 </span><span class="title-name">System Upgrade Controller</span></a></li><li><a href="components-upgrade-controller.html" class=" "><span class="title-number">23 </span><span class="title-name">Upgrade Controller</span></a></li><li><a href="components-suma.html" class=" "><span class="title-number">24 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How-To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">25 </span><span class="title-name">MetalLB on K3s (using Layer 2 Mode)</span></a></li><li><a href="guides-metallb-k3s-l3.html" class=" "><span class="title-number">26 </span><span class="title-name">MetalLB on K3s (using Layer 3 Mode)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">27 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li><li><a href="id-air-gapped-deployments-with-edge-image-builder.html" class=" "><span class="title-number">28 </span><span class="title-name">Air-gapped deployments with Edge Image Builder</span></a></li><li><a href="guides-kiwi-builder-images.html" class=" "><span class="title-number">29 </span><span class="title-name">Building Updated SUSE Linux Micro Images with Kiwi</span></a></li><li><a href="guides-clusterclass-example.html" class=" "><span class="title-number">30 </span><span class="title-name">Using clusterclass to deploy downstream clusters</span></a></li></ol></li><li><a href="tips-and-tricks.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Tips and Tricks</span></a><ol><li><a href="tips-edge-image-builder.html" class=" "><span class="title-number">31 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="tips-elemental.html" class=" "><span class="title-number">32 </span><span class="title-name">Elemental</span></a></li></ol></li><li class="active"><a href="id-third-party-integration.html" class="has-children you-are-here"><span class="title-number">V </span><span class="title-name">Third-Party Integration</span></a><ol><li><a href="integrations-nats.html" class=" "><span class="title-number">33 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-suse-linux-micro.html" class=" you-are-here"><span class="title-number">34 </span><span class="title-name">NVIDIA GPUs on SUSE Linux Micro</span></a></li></ol></li><li><a href="day-2-operations.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Day 2 Operations</span></a><ol><li><a href="day2-migration.html" class=" "><span class="title-number">35 </span><span class="title-name">Edge 3.3 migration</span></a></li><li><a href="day2-mgmt-cluster.html" class=" "><span class="title-number">36 </span><span class="title-name">Management Cluster</span></a></li><li><a href="day2-downstream-clusters.html" class=" "><span class="title-number">37 </span><span class="title-name">Downstream clusters</span></a></li></ol></li><li><a href="id-suse-telco-cloud-documentation.html" class="has-children "><span class="title-number">VII </span><span class="title-name">SUSE Telco Cloud Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">38 </span><span class="title-name">SUSE Telco Cloud</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">39 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">40 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" "><span class="title-number">41 </span><span class="title-name">Setting up the management cluster</span></a></li><li><a href="atip-features.html" class=" "><span class="title-number">42 </span><span class="title-name">Telco features configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" "><span class="title-number">43 </span><span class="title-name">Fully automated directed network provisioning</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">44 </span><span class="title-name">Lifecycle actions</span></a></li></ol></li><li><a href="id-troubleshooting-3.html" class="has-children "><span class="title-number">VIII </span><span class="title-name">Troubleshooting</span></a><ol><li><a href="general-troubleshooting-principles.html" class=" "><span class="title-number">45 </span><span class="title-name">General Troubleshooting Principles</span></a></li><li><a href="troubleshooting-kiwi.html" class=" "><span class="title-number">46 </span><span class="title-name">Troubleshooting Kiwi</span></a></li><li><a href="troubleshooting-edge-image-builder.html" class=" "><span class="title-number">47 </span><span class="title-name">Troubleshooting Edge Image Builder (EIB)</span></a></li><li><a href="troubleshooting-edge-networking.html" class=" "><span class="title-number">48 </span><span class="title-name">Troubleshooting Edge Networking (NMC)</span></a></li><li><a href="troubleshooting-phone-home-scenarios.html" class=" "><span class="title-number">49 </span><span class="title-name">Troubleshooting Phone-Home scenarios</span></a></li><li><a href="troubleshooting-directed-network-provisioning.html" class=" "><span class="title-number">50 </span><span class="title-name">Troubleshooting Directed-network provisioning</span></a></li><li><a href="troubleshooting-other-components.html" class=" "><span class="title-number">51 </span><span class="title-name">Troubleshooting Other components</span></a></li><li><a href="collecting-diagnostics-for-support.html" class=" "><span class="title-number">52 </span><span class="title-name">Collecting Diagnostics for Support</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">IX </span><span class="title-name">Appendix</span></a><ol><li><a href="id-release-notes.html" class=" "><span class="title-number">53 </span><span class="title-name">Release Notes</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="id-nvidia-gpus-on-suse-linux-micro" data-id-title="NVIDIA GPUs on SUSE Linux Micro"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">34 </span><span class="title-name">NVIDIA GPUs on SUSE Linux Micro</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="id-intro-2" data-id-title="Intro"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.1 </span><span class="title-name">Intro</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-intro-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This guide demonstrates how to implement host-level NVIDIA GPU support via the pre-built <a class="link" href="https://github.com/NVIDIA/open-gpu-kernel-modules" target="_blank">open-source drivers</a> on SUSE Linux Micro 6.1. These are drivers that are baked into the operating system rather than dynamically loaded by NVIDIA’s <a class="link" href="https://github.com/NVIDIA/gpu-operator" target="_blank">GPU Operator</a>. This configuration is highly desirable for customers that want to pre-bake all artifacts required for deployment into the image, and where the dynamic selection of the driver version, that is, the user selecting the version of the driver via Kubernetes, is not a requirement. This guide initially explains how to deploy the additional components onto a system that has already been pre-deployed, but follows with a section that describes how to embed this configuration into the initial deployment via Edge Image Builder. If you do not want to run through the basics and set things up manually, skip right ahead to that section.</p><p>It is important to call out that the support for these drivers is provided by both SUSE and NVIDIA in tight collaboration, where the driver is built and shipped by SUSE as part of the package repositories. However, if you have any concerns or questions about the combination in which you use the drivers, ask your SUSE or NVIDIA account managers for further assistance. If you plan to use <a class="link" href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/" target="_blank">NVIDIA AI Enterprise</a> (NVAIE), ensure that you are using an <a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems" target="_blank">NVAIE certified GPU</a>, which <span class="emphasis"><em>may</em></span> require the use of proprietary NVIDIA drivers. If you are unsure, speak with your NVIDIA representative.</p><p>Further information about NVIDIA GPU operator integration is <span class="emphasis"><em>not</em></span> covered in this guide. While integrating the NVIDIA GPU Operator for Kubernetes is not covered here, you can still follow most of the steps in this guide to set up the underlying operating system and simply enable the GPU operator to use the <span class="emphasis"><em>pre-installed</em></span> drivers via the <code class="literal">driver.enabled=false</code> flag in the NVIDIA GPU Operator Helm chart, where it will simply pick up the installed drivers on the host. More comprehensive instructions are available from NVIDIA <a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options" target="_blank">here</a>.</p></section><section class="sect1" id="id-prerequisites-12" data-id-title="Prerequisites"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.2 </span><span class="title-name">Prerequisites</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-prerequisites-12">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>If you are following this guide, it assumes that you have the following already available:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>At least one host with SUSE Linux Micro 6.1 installed; this can be physical or virtual.</p></li><li class="listitem"><p>Your hosts are attached to a subscription as this is required for package access — an evaluation is available <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">here</a>.</p></li><li class="listitem"><p>A <a class="link" href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus" target="_blank">compatible NVIDIA GPU</a> installed (or <span class="emphasis"><em>fully</em></span> passed through to the virtual machine in which SUSE Linux Micro is running).</p></li><li class="listitem"><p>Access to the root user — these instructions assume you are the root user, and <span class="emphasis"><em>not</em></span> escalating your privileges via <code class="literal">sudo</code>.</p></li></ul></div></section><section class="sect1" id="id-manual-installation" data-id-title="Manual installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.3 </span><span class="title-name">Manual installation</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-manual-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>In this section, you are going to install the NVIDIA drivers directly onto the SUSE Linux Micro operating system as the NVIDIA open-driver is now part of the core SUSE Linux Micro package repositories, which makes it as easy as installing the required RPM packages. There is no compilation or downloading of executable packages required. Below we walk through deploying the "G06" generation of driver, which supports the latest GPUs (see <a class="link" href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install" target="_blank">here</a> for further information), so select an appropriate driver generation for the NVIDIA GPU that your system has. For modern GPUs, the "G06" driver is the most common choice.</p><p>Before we begin, it is important to recognize that besides the NVIDIA open-driver that SUSE ships as part of SUSE Linux Micro, you might also need additional NVIDIA components for your setup. These could include OpenGL libraries, CUDA toolkits, command-line utilities such as <code class="literal">nvidia-smi</code>, and container-integration components such as <code class="literal">nvidia-container-toolkit</code>. Many of these components are not shipped by SUSE as they are proprietary NVIDIA software, or it makes no sense for us to ship them instead of NVIDIA. Therefore, as part of the instructions, we are going to configure additional repositories that give us access to said components and walk through certain examples of how to use these tools, resulting in a fully functional system. It is important to distinguish between SUSE repositories and NVIDIA repositories, as occasionally there can be a mismatch between the package versions that NVIDIA makes available versus what SUSE has built. This usually arises when SUSE makes a new version of the open-driver available, and it takes a couple of days before the equivalent packages are made available in NVIDIA repositories to match.</p><p>We recommend that you ensure that the driver version that you are selecting is compatible with your GPU and meets any CUDA requirements that you may have by checking:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <a class="link" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/" target="_blank">CUDA release notes</a></p></li><li class="listitem"><p>The driver version that you plan on deploying has a matching version in the <a class="link" href="https://download.nvidia.com/suse/sle15sp6/x86_64/" target="_blank">NVIDIA repository</a> and ensuring that you have equivalent package versions for the supporting components available</p></li></ul></div><div id="id-1.7.4.4.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>To find the NVIDIA open-driver versions, either run <code class="literal">zypper se -s nvidia-open-driver</code> on the target machine <span class="emphasis"><em>or</em></span> search the SUSE Customer Center for the "nvidia-open-driver" in <a class="link" href="https://scc.suse.com/packages?name=SUSE%20Linux%20Micro&amp;version=6.1&amp;arch=x86_64" target="_blank">SUSE Linux Micro 6.1 for AMD64/Intel 64</a>.</p><div class="informalfigure"><div class="mediaobject"><a href="images/scc-packages-nvidia.png"><img src="images/scc-packages-nvidia.png" width="100%" alt="SUSE Customer Centre" title="SUSE Customer Centre"/></a></div></div></div><p>When you have confirmed that an equivalent version is available in the NVIDIA repos, you are ready to install the packages on the host operating system. For this, we need to open up a <code class="literal">transactional-update</code> session, which creates a new read/write snapshot of the underlying operating system so we can make changes to the immutable platform (for further instructions on <code class="literal">transactional-update</code>, see <a class="link" href="https://documentation.suse.com/sle-micro/6.1/html/Micro-transactional-updates/transactional-updates.html" target="_blank">here</a>):</p><div class="verbatim-wrap"><pre class="screen">transactional-update shell</pre></div><p>When you are in your <code class="literal">transactional-update</code> shell, add an additional package repository from NVIDIA. This allows us to pull in additional utilities, for example, <code class="literal">nvidia-smi</code>:</p><div class="verbatim-wrap"><pre class="screen">zypper ar https://download.nvidia.com/suse/sle15sp6/ nvidia-suse-main
zypper --gpg-auto-import-keys refresh</pre></div><p>You can then install the driver and <code class="literal">nvidia-compute-utils</code> for additional utilities. If you do not need the utilities, you can omit it, but for testing purposes, it is worth installing at this stage:</p><div class="verbatim-wrap"><pre class="screen">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</pre></div><div id="id-1.7.4.4.13" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>If the installation fails, this might indicate a dependency mismatch between the selected driver version and what NVIDIA ships in their repositories. Refer to the previous section to verify that your versions match. Attempt to install a different driver version. For example, if the NVIDIA repositories have an earlier version, you can try specifying <code class="literal">nvidia-open-driver-G06-signed-kmp=550.54.14</code> on your install command to specify a version that aligns.</p></div><p>Next, if you are <span class="emphasis"><em>not</em></span> using a supported GPU (remembering that the list can be found <a class="link" href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus" target="_blank">here</a>), you can see if the driver works by enabling support at the module level, but your mileage may vary — skip this step if you are using a <span class="emphasis"><em>supported</em></span> GPU:</p><div class="verbatim-wrap"><pre class="screen">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</pre></div><p>Now that you have installed these packages, it is time to exit the <code class="literal">transactional-update</code> session:</p><div class="verbatim-wrap"><pre class="screen">exit</pre></div><div id="id-1.7.4.4.18" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Make sure that you have exited the <code class="literal">transactional-update</code> session before proceeding.</p></div><p>Now that you have installed the drivers, it is time to reboot. As SUSE Linux Micro is an immutable operating system, it needs to reboot into the new snapshot that you created in a previous step. The drivers are only installed into this new snapshot, hence it is not possible to load the drivers without rebooting into this new snapshot, which happens automatically. Issue the reboot command when you are ready:</p><div class="verbatim-wrap"><pre class="screen">reboot</pre></div><p>Once the system has rebooted successfully, log back in and use the <code class="literal">nvidia-smi</code> tool to verify that the driver is loaded successfully and that it can both access and enumerate your GPUs:</p><div class="verbatim-wrap"><pre class="screen">nvidia-smi</pre></div><p>The output of this command should show you something similar to the following output, noting that in the example below, we have two GPUs:</p><div class="verbatim-wrap"><pre class="screen">+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</pre></div><p>This concludes the installation and verification process for the NVIDIA drivers on your SUSE Linux Micro system.</p></section><section class="sect1" id="id-further-validation-of-the-manual-installation" data-id-title="Further validation of the manual installation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.4 </span><span class="title-name">Further validation of the manual installation</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-further-validation-of-the-manual-installation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>At this stage, all we have been able to verify is that, at the host level, the NVIDIA device can be accessed and that the drivers are loading successfully. However, if we want to be sure that it is functioning, a simple test would be to validate that the GPU can take instructions from a user-space application, ideally via a container, and through the CUDA library, as that is typically what a real workload would use. For this, we can make a further modification to the host OS by installing the <code class="literal">nvidia-container-toolkit</code> (<a class="link" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper" target="_blank">NVIDIA Container Toolkit</a>). First, open another <code class="literal">transactional-update</code> shell, noting that we could have done this in a single transaction in the previous step, and see how to do this fully automated in a later section:</p><div class="verbatim-wrap"><pre class="screen">transactional-update shell</pre></div><p>Next, install the <code class="literal">nvidia-container-toolkit</code> package from the NVIDIA Container Toolkit repo:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">nvidia-container-toolkit.repo</code> below contains a stable (<code class="literal">nvidia-container-toolkit</code>) and an experimental (<code class="literal">nvidia-container-toolkit-experimental</code>) repository. The stable repository is recommended for production use. The experimental repository is disabled by default.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</pre></div><p>When you are ready, you can exit the <code class="literal">transactional-update</code> shell:</p><div class="verbatim-wrap"><pre class="screen">exit</pre></div><p>…​and reboot the machine into the new snapshot:</p><div class="verbatim-wrap"><pre class="screen">reboot</pre></div><div id="id-1.7.4.5.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>As before, you need to ensure that you have exited the <code class="literal">transactional-shell</code> and rebooted the machine for your changes to be enacted.</p></div><p>With the machine rebooted, you can verify that the system can successfully enumerate the devices using the NVIDIA Container Toolkit. The output should be verbose, with INFO and WARN messages, but no ERROR messages:</p><div class="verbatim-wrap"><pre class="screen">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</pre></div><p>This ensures that any container started on the machine can employ NVIDIA GPU devices that have been discovered. When ready, you can then run a podman-based container. Doing this via <code class="literal">podman</code> gives us a good way of validating access to the NVIDIA device from within a container, which should give confidence for doing the same with Kubernetes at a later stage. Give <code class="literal">podman</code> access to the labeled NVIDIA devices that were taken care of by the previous command, based on <a class="link" href="https://registry.suse.com/repositories/bci-bci-base-15sp6" target="_blank">SLE BCI</a>, and simply run the Bash command:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</pre></div><p>You will now execute commands from within a temporary podman container. It does not have access to your underlying system and is ephemeral, so whatever we do here will not persist, and you should not be able to break anything on the underlying host. As we are now in a container, we can install the required CUDA libraries, again checking the correct CUDA version for your driver <a class="link" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/" target="_blank">here</a>, although the previous output of <code class="literal">nvidia-smi</code> should show the required CUDA version. In the example below, we are installing <span class="emphasis"><em>CUDA 12.3</em></span> and pulling many examples, demos and development kits so you can fully validate the GPU:</p><div class="verbatim-wrap"><pre class="screen">zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-suse
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</pre></div><p>Once this has been installed successfully, do not exit the container. We will run the <code class="literal">deviceQuery</code> CUDA example, which comprehensively validates GPU access via CUDA, and from within the container itself:</p><div class="verbatim-wrap"><pre class="screen">/usr/local/cuda-12/extras/demo_suite/deviceQuery</pre></div><p>If successful, you should see output that shows similar to the following, noting the <code class="literal">Result = PASS</code> message at the end of the command, and noting that in the output below, the system correctly identifies two GPUs, whereas your environment may only have one:</p><div class="verbatim-wrap"><pre class="screen">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</pre></div><p>From here, you can continue to run any other CUDA workload — use compilers and any other aspect of the CUDA ecosystem to run further tests. When done, you can exit from the container, noting that whatever you have installed in there is ephemeral (so will be lost!), and has not impacted the underlying operating system:</p><div class="verbatim-wrap"><pre class="screen">exit</pre></div></section><section class="sect1" id="id-implementation-with-kubernetes" data-id-title="Implementation with Kubernetes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.5 </span><span class="title-name">Implementation with Kubernetes</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-implementation-with-kubernetes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Now that we have proven the installation and use of the NVIDIA open-driver on SUSE Linux Micro, let us explore configuring Kubernetes on the same machine. This guide does not walk you through deploying Kubernetes, but it assumes that you have installed <a class="link" href="https://k3s.io/" target="_blank">K3s</a> or <a class="link" href="https://docs.rke2.io/install/quickstart" target="_blank">RKE2</a> and that your kubeconfig is configured accordingly, so that standard <code class="literal">kubectl</code> commands can be executed as the superuser. We assume that your node forms a single-node cluster, although the core steps should be similar for multi-node clusters. First, ensure that your <code class="literal">kubectl</code> access is working:</p><div class="verbatim-wrap"><pre class="screen">kubectl get nodes</pre></div><p>This should show something similar to the following:</p><div class="verbatim-wrap"><pre class="screen">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.33.3+rke2r1</pre></div><p>What you should find is that your k3s/rke2 installation has detected the NVIDIA Container Toolkit on the host and auto-configured the NVIDIA runtime integration into <code class="literal">containerd</code> (the Container Runtime Interface that k3s/rke2 use). Confirm this by checking the containerd <code class="literal">config.toml</code> file:</p><div class="verbatim-wrap"><pre class="screen">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</pre></div><p>This must show something akin to the following. The equivalent K3s location is <code class="literal">/var/lib/rancher/k3s/agent/etc/containerd/config.toml</code>:</p><div class="verbatim-wrap"><pre class="screen">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</pre></div><div id="id-1.7.4.6.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>If these entries are not present, the detection might have failed. This could be due to the machine or the Kubernetes services not being restarted. Add these manually as above, if required.</p></div><p>Next, we need to configure the NVIDIA <code class="literal">RuntimeClass</code> as an additional Kubernetes runtime to the default, ensuring that any user requests for pods that need access to the GPU can use the NVIDIA Container Toolkit to do so, via the <code class="literal">nvidia-container-runtime</code>, as configured in the <code class="literal">containerd</code> configuration:</p><div class="verbatim-wrap"><pre class="screen">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</pre></div><p>The next step is to configure the <a class="link" href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank">NVIDIA Device Plugin</a>, which configures Kubernetes to leverage the NVIDIA GPUs as resources within the cluster that can be used, working in combination with the NVIDIA Container Toolkit. This tool initially detects all capabilities on the underlying host, including GPUs, drivers and other capabilities (such as GL) and then allows you to request GPU resources and consume them as part of your applications.</p><p>First, you need to add and update the Helm repository for the NVIDIA Device Plugin:</p><div class="verbatim-wrap"><pre class="screen">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</pre></div><p>Now you can install the NVIDIA Device Plugin:</p><div class="verbatim-wrap"><pre class="screen">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</pre></div><p>After a few minutes, you see a new pod running that will complete the detection on your available nodes and tag them with the number of GPUs that have been detected:</p><div class="verbatim-wrap"><pre class="screen">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</pre></div><p>Now you are ready to create an NVIDIA pod that attempts to use this GPU. Let us try with the CUDA Benchmark container:</p><div class="verbatim-wrap"><pre class="screen">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</pre></div><p>If all went well, you can look at the logs and see the benchmark information:</p><div class="verbatim-wrap"><pre class="screen">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance)
	-numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
	-device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</pre></div><p>Finally, if your applications require OpenGL, you can install the required NVIDIA OpenGL libraries at the host level, and the NVIDIA Device Plugin and NVIDIA Container Toolkit can make them available to containers. To do this, install the package as follows:</p><div class="verbatim-wrap"><pre class="screen">transactional-update pkg install nvidia-gl-G06</pre></div><div id="id-1.7.4.6.26" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>You need to reboot to make this package available to your applications. The NVIDIA Device Plugin should automatically redetect this via the NVIDIA Container Toolkit.</p></div></section><section class="sect1" id="id-bringing-it-together-via-edge-image-builder" data-id-title="Bringing it together via Edge Image Builder"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.6 </span><span class="title-name">Bringing it together via Edge Image Builder</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-bringing-it-together-via-edge-image-builder">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Okay, so you have demonstrated full functionality of your applications and GPUs on SUSE Linux Micro and you now want to use <a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a> to provide it all together via a deployable/consumable ISO or RAW disk image. This guide does not explain how to use Edge Image Builder, but it provides the necessary configurations to build such image. Below you can find an example of an image definition, along with the necessary Kubernetes configuration files, to ensure that all the required components are deployed out of the box. Here is the directory structure of the Edge Image Builder directory for the example shown below:</p><div class="verbatim-wrap"><pre class="screen">.
├── base-images
│   └── SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</pre></div><p>Let us explore those files. First, here is a sample image definition for a single-node cluster running K3s that deploys the utilities and OpenGL packages, too (<code class="literal">eib-config-iso.yaml</code>):</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  arch: x86_64
  imageType: iso
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp6/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: [snip]
kubernetes:
  version: v1.33.3+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</pre></div><div id="id-1.7.4.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>This is just an example. You may need to customize it to fit your requirements and expectations. Additionally, if using SUSE Linux Micro, you need to provide your own <code class="literal">sccRegistrationCode</code> to resolve package dependencies and pull the NVIDIA drivers.</p></div><p>Besides this, we need to add additional components, so they get loaded by Kubernetes at boot time. The EIB directory needs a <code class="literal">kubernetes</code> directory first, with subdirectories for the configuration, Helm chart values and any additional manifests required:</p><div class="verbatim-wrap"><pre class="screen">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</pre></div><p>Let us now set up the (optional) Kubernetes configuration by choosing a CNI (which defaults to Cilium if unselected) and enabling SELinux:</p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</pre></div><p>Now ensure that the NVIDIA RuntimeClass is created on the Kubernetes cluster:</p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</pre></div><p>We use the built-in Helm Controller to deploy the NVIDIA Device Plugin through Kubernetes itself.
Let’s provide the runtime class in the values file for the chart:</p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</pre></div><p>We need to grab the NVIDIA Container Toolkit RPM public key before proceeding:</p><div class="verbatim-wrap"><pre class="screen">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</pre></div><p>All the required artifacts, including Kubernetes binary, container images, Helm charts (and any referenced images), will be automatically air-gapped, meaning that the systems at deploy time should require no Internet connectivity by default. Now you need only to grab the SUSE Linux Micro ISO from the <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">SUSE Downloads Page</a> (and place it in the <code class="literal">base-images</code> directory), and you can call the Edge Image Builder tool to generate the ISO for you. To complete the example, here is the command that was used to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
build --definition-file eib-config-iso.yaml</pre></div><p>For further instructions, please see the <a class="link" href="https://github.com/suse-edge/edge-image-builder/blob/release-1.3/docs/building-images.md" target="_blank">documentation</a> for Edge Image Builder.</p></section><section class="sect1" id="id-resolving-issues" data-id-title="Resolving issues"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">34.7 </span><span class="title-name">Resolving issues</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-resolving-issues">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect2" id="id-nvidia-smi-does-not-find-the-gpu" data-id-title="nvidia-smi does not find the GPU"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">34.7.1 </span><span class="title-name">nvidia-smi does not find the GPU</span></span> <a title="Permalink" class="permalink" href="id-nvidia-gpus-on-suse-linux-micro.html#id-nvidia-smi-does-not-find-the-gpu">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Check the kernel messages using <code class="literal">dmesg</code>. If this indicates that it cannot allocate <code class="literal">NvKMSKapDevice</code>, apply the unsupported GPU workaround:</p><div class="verbatim-wrap"><pre class="screen">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</pre></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>NOTE</em></span>: You will need to reload the kernel module, or reboot, if you change the kernel module configuration in the above step for it to take effect.</p></blockquote></div></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="integrations-nats.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 33 </span>NATS</span></a> </div><div><a class="pagination-link next" href="day-2-operations.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Part VI </span>Day 2 Operations</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-intro-2"><span class="title-number">34.1 </span><span class="title-name">Intro</span></a></span></li><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-prerequisites-12"><span class="title-number">34.2 </span><span class="title-name">Prerequisites</span></a></span></li><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-manual-installation"><span class="title-number">34.3 </span><span class="title-name">Manual installation</span></a></span></li><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-further-validation-of-the-manual-installation"><span class="title-number">34.4 </span><span class="title-name">Further validation of the manual installation</span></a></span></li><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-implementation-with-kubernetes"><span class="title-number">34.5 </span><span class="title-name">Implementation with Kubernetes</span></a></span></li><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-bringing-it-together-via-edge-image-builder"><span class="title-number">34.6 </span><span class="title-name">Bringing it together via Edge Image Builder</span></a></span></li><li><span class="section"><a href="id-nvidia-gpus-on-suse-linux-micro.html#id-resolving-issues"><span class="title-number">34.7 </span><span class="title-name">Resolving issues</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2025</span></div></div></footer></body></html>