{"searchDocs":[{"title":"Edge Computing Reference Architecture with Rancher and Linkerd","type":0,"sectionRef":"#","url":"/blog/Linkerd","content":"","keywords":"","version":null},{"title":"Introducing the architecture​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#introducing-the-architecture","content":" Before we get into the &quot;how,&quot; let's introduce the edge computing stack and examine why these tools work so well together for an edge computing scenario. If you are running Rancher, we recommend combining Rancher Prime, Buoyant's Linkerd, RKE2, and K3s (for an overview of what each piece does, please refer to the table below).    Why this stack for edge computing? First and foremost, they integrate seamlessly, which will save you a lot of headaches. Additionally, Linkerd and RKE2 significantly improve security, while Linkerd provides an additional layer of reliability. This stack is lightweight and resource efficient, making it ideal for resource-constrained environments. And lastly, all these tools focus on operational simplicity. And when it comes to edge computing, that is incredibly important. With multiple disparate devices, you need a unified way to operate them all.  Project Name\tWhat it is\tWhy for the edge?Buoyant’s Linkerd\tOpen-source, security-first service mesh for Kubernetes\tProvides security, reliability, and observability without any code changes. Is ultra-lightweight and easy to install with a small runtime footprint (this is key in edge computing where managing communication must be efficient) Rancher Prime\tOpen-source multi-cluster Kubernetes orchestration platform\tFlexible and compatible with any CNCF Kubernetes distribution, including K3s and RKE2, Rancher Prime proactively monitors cluster health and performance. RKE2\tCNCF-certified Kubernetes distribution optimized for air-gapped, offline, or edge environments deployed at the core or near the edge.\tFully CNCF-certified, RKE2 improves security and simplicity of your Kubernetes deployment. It is designed to be secure, reliable, and lightweight, ideal for general-purpose computing and near-edge use cases. K3s\tCNCF-certified ultra-lightweight Kubernetes distribution providing the best choice for clusters running at the edge.\tIdeal for edge applications, allowing for simple deployment and management while still fully CNCF-certified. It is ultra-lightweight and optimized for resource-constrained environments and functions even in remote and disconnected areas.  Security, reliability, and observability are all critical concerns for edge computing, and it’s therefore important to choose an architecture that helps, rather than hinders, accomplishing these goals. An effective architecture will be simple to deploy and operate, using technologies in ways that play to their strengths, as described above. With Rancher and Linkerd, we can adopt an extremely simple architecture that nevertheless brings an enormous amount of functionality to the table:    Here, our instruments (on the left of the diagram) are connected to IoT “gateway” systems running Linux. By deploying k3s clusters with Linkerd all the way out on the edge gateways, we can use Linkerd’s secure multicluster capabilities to extend the secure service mesh all the way from the central cluster (shown on the right, running RKE) to the edge gateways themselves.  These tools all integrate seamlessly, providing a secure, reliable, observable edge platform that is lightweight and resource efficient. Now let's explore why we believe these technologies are a perfect match for the edge.  ","version":null,"tagName":"h2"},{"title":"Why Rancher and Buoyant's Linkerd?​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#why-rancher-and-buoyants-linkerd","content":" ","version":null,"tagName":"h2"},{"title":"Seamless Integration​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#seamless-integration","content":" One of the most significant advantages of combining Rancher Prime, Linkerd, RKE2, and K3s is their compatibility. These tools are designed to work hand-in-hand, providing a seamless experience. Rancher provides the overarching platform to manage your Kubernetes clusters, including RKE2, K3s, EKS, AKS, GKE, etc. And Linkerd easily integrates with any Kubernetes distribution, adding a service mesh layer to your Rancher-managed clusters.  ","version":null,"tagName":"h3"},{"title":"Reliability and Robustness​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#reliability-and-robustness","content":" Linkerd adds a layer of reliability and robustness to your Kubernetes clusters by providing traffic splitting, retries, and timeouts capabilities for your applications. With its fault-tolerance feature, Linkerd ensures your applications continue running smoothly, even in the event of a network failure.  RKE2 takes Kubernetes security to the next level. It includes several enhancements like CIS benchmark compliance, security by default, and defense in depth. These features, along with Linkerd's ability to automate mutual TLS for all service-to-service communication, provide a secure environment for your Edge computing needs.  ","version":null,"tagName":"h3"},{"title":"Lightweight and Resource Efficient​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#lightweight-and-resource-efficient","content":" Edge environments often have limited resources. K3s is designed explicitly for such situations. It is a fully compliant Kubernetes distribution with a significantly smaller footprint, consuming less than half the memory of a typical Kubernetes installation. This lightweight nature extends to Linkerd as well, which maintains a small runtime footprint, making it an ideal service mesh for Edge environments.  ","version":null,"tagName":"h3"},{"title":"Comprehensive Observability​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#comprehensive-observability","content":" With numerous devices and applications running in various locations, clearly understanding their performance and issues is vital. The Rancher Prime, Linkerd, RKE2, and K3s stack addresses that by providing comprehensive observability capabilities.  Rancher enables users to monitor and manage clusters from a unified interface, regardless of where they are deployed. With built-in monitoring and alerting tools, users get detailed insight into cluster health, allowing for quick identification and resolution of potential issues. Linkerd provides deep real-time data into your applications' performance and includes features such as request volume, success rates, and latency distributions for all meshed services. Users get a more granular level of observability into microservices communication, which is crucial in Edge computing scenarios where the network is notorious for being unstable and latency-sensitive. Linkerd also automatically adds mTLS to all service-to-service communication, adding security with no code changes, which is particularly valuable in Edge computing.  ","version":null,"tagName":"h3"},{"title":"Operational Simplicity​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#operational-simplicity","content":" When it comes to Edge computing, operational simplicity is key. Edge environments involve managing numerous devices, often spread across multiple geographical locations, making traditional management methods impractical. Rancher simplifies Kubernetes cluster management with an intuitive user interface and robust API. Rancher allows users to manage all their Kubernetes clusters from a single control point, whether in the cloud, data center, or Edge environments. This unified approach simplifies operations and reduces the complexity of managing multiple clusters.  The Linkerd service mesh requires minimal configuration and comes with zero-config features such as load balancing, retries, timeouts, and more. With no time-consuming setup, developers have more time to build business logic. That being said, edge devices will require some initial setup work and configuration. Due to their resource limitations, these devices typically require deployment optimization to ensure they run efficiently.  Linkerd Architecture:    The Linkerd architecture is fairly simple. The Linkerd control plane manages Linkerd’s operation; it also has a CLI that the user can use to configure, and examine, the mesh. Application Pods that are part of the mesh have the ultra-fast, ultra-lightweight Linkerd proxy, purpose-built in Rust, injected into them as a sidecar container. Once the proxy is injected, all application communication goes through the proxy, which manages mTLS, workload authentication and authorization, retries, circuit breaking, metrics collection, and much more. Having these critical features uniformly applied across the entire application at the platform level eliminates the need to change the application itself, meaning that the application developers are free to focus on the business needs of the application rather than on the complexities of the network.  ","version":null,"tagName":"h3"},{"title":"Edge computing use case examples​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#edge-computing-use-case-examples","content":" Industries such as manufacturing, healthcare, transportation, retail, and energy are all increasingly taking advantage of edge computing to optimize their operations. Let's look at some examples. But keep in mind that the stack is vertical agnostic. The role of Rancher, Linkerd, K3s, and RKE2 is always the same. The examples below put them in industry-specific context.  Industry Use Case\tRetail Industry - Point of Sale (POS) Systems\tManufacturing - Predictive Maintenance\tHealthcare - Remote Patient Monitoring\tTransportation - Fleet Management\tSumming it upSpecific challenge\tIn retail environments the people setting up and maintaining the physical devices are more likely to be store managers than technicians. This leads to fragile physical systems.\tPredictive maintenance is often critical. Manufacturing equipment sensors send data to the central system that predicts potential equipment failures.\tIn remote patient monitoring scenarios, patient health data is often collected by various devices and sent to a central system for analysis and monitoring.\tModern fleet management uses real-time vehicle data for route optimization, improved fuel efficiency, and predictive maintenance.\tEdge devices must process and analyze data in real-time to ensure business continuity (manufacturing, transportation), save lives (healthcare), or save money (retail). Rancher\tMulti-cluster management enables easy containerized app deployment and management across stores in various geo locations​​.\tManages edge deployments, providing a central point of control for all the clusters running on the factory floor.\tHelps manage the deployment of these applications across various devices and locations, ensuring uniformity and ease of management.\tManages edge deployments across various vehicles, providing a central point of control for all the clusters.\tCentralized management of distributed containerized apps on the edge. Linkerd\tGuarantees secure, reliable communication between store POS systems and cloud-based central inventory management systems. Provides real-time inventory updates and transaction processing. Seamlessly merges multiple clusters into a single secure mesh.\tGuarantees secure, reliable communication between sensors and the applications processing the data. Seamlessly merges multiple clusters into a single secure mesh.\tGuarantees secure and reliable communication between patient devices and central health monitoring system. Seamlessly merges multiple clusters into a single secure mesh.\tGuarantees secure and reliable communication between the onboard devices and the central fleet management system. Seamlessly merges multiple clusters into a single secure mesh.\tGuarantees secure and reliable communication from the edge to the central processing and analysis system. RKE2\tFor store backend systems, ensuring reliable and secure operation of the POS system.\tProvides secure, reliable Kubernetes runtime for the central systems processing and analyzing the sensor data.\tProvides secure, reliable Kubernetes runtime for central health monitoring systems, so patient data is processed accurately and securely.\tProvides secure, reliable Kubernetes runtime for central fleet management systems, ensuring real-time fleet data is processed accurately and securely.\tProvides secure, reliable Kubernetes runtime for the central system. K3s\tEfficiently deploy and manage containerized apps across multiple stores.\tRun data processing apps at the edge, close to data source, reducing latency and network load​​.\tEfficiently processes data at the edge, reducing latency and ensuring timely alerts in case of any health anomalies​.\tProcesses data at the edge, providing real-time insights and reducing network load​​.\tEfficiently processes data at the edge.  Edge Computing stack:    ","version":null,"tagName":"h2"},{"title":"Accelerate time-to-value with the Rancher and Buoyant teams​","type":1,"pageTitle":"Edge Computing Reference Architecture with Rancher and Linkerd","url":"/blog/Linkerd#accelerate-time-to-value-with-the-rancher-and-buoyant-teams","content":" As Edge computing use cases rapidly expand, the Rancher Prime, Linkerd, RKE2, and K3s toolkit offers a state-of-the-art, highly secure, and highly performant to these unique challenges. It provides organizations and developers with the tools and strategies they need to deliver fast, reliable performance and robust, secure communication between microservices and applications, all while efficiently managing and orchestrating Kubernetes clusters.  Practical use cases showcase how these open source tools synergize to create robust, efficient, and flexible Edge computing solutions. From Retail industry POS systems to remote patient healthcare monitoring, this stack has clear advantages. An easy integration streamlines your Edge computing implementation, and enables you to process data at the Edge, while ensuring reliable and secure data transfer, reducing latency, and providing scalability and flexibility.  As with any implementation, there are some challenges, however. The initial setup and configuration on the edge can be complex. A deep understanding of these tools and Kubernetes is required. If you need help and want to accelerate your time-to-value, the Buoyant and SUSE teams can help. Reach out, and let's chat!  Contact the Buoyant team at: https://buoyant.io/contact  Contact the SUSE team at: https://www.suse.com/contact/  To sum it up, combining Rancher Prime, Linkerd, RKE2, and K3s delivers a robust, observable, and easy-to-manage Edge computing solution. Organizations gain a powerful set of capabilities to improve edge computing performance and tackle the complexities and challenges of managing edge environments. As edge computing applications across industries continue to proliferate, these tools will play an increasingly critical role in shaping the future of how we process, manage, and utilize data in an increasingly decentralized world. ","version":null,"tagName":"h2"},{"title":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","type":0,"sectionRef":"#","url":"/blog/Flexran","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#introduction","content":" Intel® FlexRAN is a reference implementation for a virtualized 4G and 5G RAN stack. It's main purpose is to illustrate how to achieve real-time performance of a virtualized 4G or 5G PHY and MAC layer on an Intel-based hardware architecture, using Intel® Xeon® Scalable processors, the Intel® Advanced Vector Extensions 512 Intel® AVX 512 instruction set and a number of other hardware acceleration features and technologies found in modern Intel Xeon family processors. While some vendors of CloudRAN and in particular O-RAN software merely use it as inspiration, some vendors adopt Intel FlexRAN directly as part of their product. This article describes how to deploy FlexRAN on the SUSE Adaptive Telco Infrastructure Platform (ATIP) running on bare-metal infrastructure as a Containerized Network Function (CNF). In the ATIP architecture, the runtime stack consists of SUSE Linux Enterprise Micro and Rancher Kubernetes Engine v2 (RKE2) and we are using Rancher to perform cluster and application provisioning, life cycle management, application resource management and collection of telemetry data with role-based access control. The example RAN deployment option considered here is the ORAN Alliance 7-2x lower-level split, which in a nutshell means that the lower layer PHY processing for uplink and downlink (such as Cyclic Prefix addition/removal, iFFT/FFT, calculation of beamforming coefficients and PRACH pre-filtering) is performed in the Radio Unit (RU), and upper layer PHY processing as well as all MAC layer processing is performed in the Distributed Unit (DU). When DU and RU are deployed in different Kubernetes pods, this constitutes an excellent test for a telco cloud platform, as it requires near-realtime processing and transmission of fronthaul traffic over physical network interfaces, which is only possible with careful orchestration of a number of telco-grade platform features, as we will detail further below.  With this cloud-native setup, we can identify the appropriate configuration of the SUSE Edge ATIP (Adaptive Telco Infrastructure Platform) stack. Also, it demostrates that telecom containerised applications like DU/CU can be run on RKE2 cluster on top of the SLE Micro RT OS taking advantage of the accelerator cards. In addition, we show that we are applying the full power and potential of the accelerator card and CPU.  For more information about Intel FlexRan, see the official Intel FlexRan website For more information about SUSE ATIP, see the official SUSE Edge ATIP documentation  ","version":null,"tagName":"h2"},{"title":"Architecture​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#architecture","content":" The architecture is based on the following components:  Rancher Management Cluster: This component will be used to manage the lifecycle of the RKE2 clusters hosting the FlexRAN solution, as well as the monitoring platform based on the Rancher monitoring stack (Grafana and Prometheus installed on the downstream cluster). This cluster is based on a single-node RKE2 cluster on top of SLE Micro.Edge Cluster 1: This cluster will be used to deploy the FlexRan Timer Mode tests. This cluster is based on a single-node RKE2 cluster on top of SLE Micro RT Operating System.Edge Cluster 2: This cluster will be used to deploy the FlexRan Xran Mode tests. This cluster is based on a single-node RKE2 cluster on top of SLE Micro RT Operating System.  Note: The FlexRan tests could be deployed on the same edge cluster, but just for clarity and simplicity in the article, we will deploy them on different clusters.    ","version":null,"tagName":"h2"},{"title":"Hardware and Software components​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#hardware-and-software-components","content":" ","version":null,"tagName":"h2"},{"title":"Hardware​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#hardware","content":" The hardware used for this article is based on the following components:  Dell PowerEdge XR11 serversIntel XEON Gold 6338N 2.2G8 x 32GB RDIMM 3200MT/S2 x 480GB SSD SATA DiskIntel E810-CQDA2 Dual Port 100GbE QSFP28ACC100 FEC Accelerator card  ","version":null,"tagName":"h3"},{"title":"Software​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#software","content":" The software used for this article is based on the following components:  Operating system: SLE Micro 5.4Kernel Real Time: 5.14.21-150400.15.11-rtRKE2 version: v1.25.9+rke2r1CNI plugins: Multus 0.3.1 and Calico v3.25.0Rancher release: rancher-stable v2.7.5Dpdk version: 22.11SRIOV upstream v3.5.1  ","version":null,"tagName":"h3"},{"title":"Flexran​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#flexran","content":" The FlexRan deployment used for this article is based on the following components:  FlexRAN version: 22.07 using pre-defined containers from Intel.Intel OneAPI Base Toolkit: 2022.1.2.146  ","version":null,"tagName":"h3"},{"title":"Rancher Management Cluster​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#rancher-management-cluster","content":" In our case, the Rancher management cluster will be used to manage the lifecycle of the RKE2 edge clusters deployed for the FlexRAN solution, as well as the monitoring platform based on Grafana and Prometheus.    The operating system installation steps have been omitted for the sake of brevity. There are no special performance-related configuration settings required for the OS on the management cluster. For more information about how to install the Operating System, see the next link  ","version":null,"tagName":"h2"},{"title":"RKE2 Cluster Installation on the Management Server​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#rke2-cluster-installation-on-the-management-server","content":" Once you have the Operating System installed, you can proceed with the Rancher installation. First, we will install a RKE2 cluster and then, the Rancher Helm chart to install the Rancher management cluster.  Run the RKE2 installer:  curl -sfL https://get.rke2.io | sh -  if you want to install a particular version, the INSTALL_RKE2_VERSION variable can be used as:  curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=&quot;v1.25.9+rke2r1&quot; sh -  For more information about the installation, please refer to the documentation: https://docs.rke2.io/install/install_options/  Enable and start the rke2-server service:  systemctl enable --now rke2-server.service  In case you want to run the RKE2 agent (in case you need to add more nodes designated to run your apps and services), you can follow the next steps:  Run the RKE2 installer:  curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=&quot;agent&quot; sh -  Configure the config.yaml file located in /etc/rancher/rke2/ with the following content:  server: https://&lt;server&gt;:9345 token: &lt;token from server node&gt;   Enable and start the service:  systemctl enable --now rke2-agent.service  ","version":null,"tagName":"h3"},{"title":"Rancher Manager Install​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#rancher-manager-install","content":" Rancher is installed using the Helm package manager for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm, we can create configurable deployments instead of just using static files.  This section covers the deployment of Rancher on the management cluster. For more information about the Rancher manager installation, please refer to the documentation: https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster  Add the Helm repository  There are three Rancher manager releases available to be added as a Helm repository for Rancher. In our case, we will use the rancher-stable because it's the release recommended for production environments, but you could use rancher-latest or rancher-alpha if you want. Also, there is a rancher primer release that is the enterprise version of Rancher.  helm repo add rancher-stable https://releases.rancher.com/server-charts/stable  If you don't have helm installed previously, you could install it using the following command: curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 |bash  Choose your SSL Configuration  The Rancher management server is designed to be secure by default and requires SSL/TLS configuration.  There are three recommended options for SSL/TLS configuration:  Rancher-generated TLS certificateLet's EncryptBring your own certificate  For more information about the SSL/TLS configuration, please refer to the documentation: https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster/#3-choose-your-ssl-configuration  In our case we will use the Rancher-generated TLS certificate. This requires cert-manager to be deployed in the cluster as::  helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true \\ --version v1.11.1   Once you've installed cert-manager, you can verify the pods are running:kubectl get pods --namespace cert-manager  Install Rancher with Helm using the following command modifying the &lt;hostname&gt; and &lt;bootstrapPassword&gt; values to fit your environment:  helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --create-namespace \\ --set hostname=&lt;hostname&gt; \\ --set bootstrapPassword=&lt;bootstrapPassword&gt; \\ --set replicas=1 \\ --set global.cattle.psp.enabled=false   Verify the Rancher installation  You should wait a few minutes for Rancher to be rolled out:  kubectl -n cattle-system rollout status deploy/rancher  ","version":null,"tagName":"h3"},{"title":"Edge Cluster deployment​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#edge-cluster-deployment","content":" This section covers the bare-metal provisioning and installation of the edge cluster nodes in order to deploy the FlexRan tests. In our case, we will deploy two edge clusters, one for the FlexRan Timer Mode tests and another one for the FlexRan Xran Mode tests. The configuration of the edge cluster nodes is based on the following components, and it will be the same for the two edge clusters:    ","version":null,"tagName":"h2"},{"title":"BIOS Configuration for Dell PowerEdge XR11​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#bios-configuration-for-dell-poweredge-xr11","content":" The BIOS and NIC configuration used for this article based on the hardware shown above, could be imported directly using the Dell PowerEdge XR11 BIOS Exported Configuration File. In order to import the configuration file, you need to do it using the idrac web interface:    Note: This configuration depends on the hardware vendor, so please, check with your hardware vendor the best configuration to be used.  Pay special attention to the parameters suggested here as some of them could dramatically increase or decrease the performance of the tests being executed. The next table shows the recommended configuration as a reference for the most common hardware vendors:  Option\tValue\tDescriptionWorkload Profile\tTelco Optimized\tTelco profile to optimize the performance in the hardware. Boot Performance Mode\tMax. Performance\tMaximize the performance in the boot process. Hyper- Threading (Logical Proccesor)\tEnable\tThis option enables Intel® Hyper-Threading Technology for logical processor enabling and converting processor cores (pCores) to logical cores (lCores). Virtualization Technology (XAPIC)\tEnable\tThis option is for Extended Advanced Programmable Interrupt Controller (xAPIC) support for the Intel® Virtualization Technology for Directed I/O (Intel® VT-d) feature. uncore frequency scaling\tDisable\tIf enabled, Uncore Frequency Scaling (UFS) allows the uncore to operate at a lower frequency when the Power Control Unit (PCU) has detected low utilization. Conversely, UFS allows the uncore to operate at a higher frequency when the PCU has detected high utilization. CPU P-State Control (EIST PSD Function\tHW_ALL\toptimization of the voltage and CPU fequency during operation CPU C-State Control\tDisable\tThis option is for the CPU C-State Control feature, which provides power savings by placing the processor into lower power states when the processor is idle. CPU C1E Support\tDisable\tThis option is for the CPU Enhanced Halt (C1E) feature, which provides power savings by placing the processor into a low power state when the processor is idle. AVX License Pre-Grant\tEnable\tIf enabled, this option enables the pre-grant license level selection based on workload with the AVX ICCP Pre-Grant Level option. AVX ICCP Pre- Grant Level\tLevel 5\tThis option selects a workload level for the Intel® Advanced Vector Extensions (Intel® AVX): Intel® AVX-512 Heavy AVX P1\tLevel 2\tThis option serves a dual purpose: 1 -Specifies the base P1 ratio for Intel® Streaming SIMD Extensions (Intel® SSE) or Intel® AVX workloads. 2- Pre-grants a license level based on the workload level. Energy Efficient Turbo\tDisable\tThis option allows entry into the Intel® Turbo Boost Technology frequency when the Power Control Unit (PCU) has detected high utilization. Turbo Mode\tEnable\tEnabling this Intel® Turbo Boost Technology mode setting allows the CPU cores to operate at higher than the rated frequency. GPSS timer\t0us\tThis option allows the reduction of the Global P-State Selection (GPSS) timer to be set from: 0 μs to 500 μs LLC prefetch\tEnable\tThis option enables Last Level Cache (LLC) hardware prefetch logic. Frequency Prioritization (RAPL Prioritization)\tDisable\tThis setting controls whether the Running Average Power Limit (RAPL) balancer is enabled. If enabled, it activates per core power budgeting. Hardware P-States\tNative with no Legacy Support\tWhen enabled, this option allows the hardware to choose a Performance State (P-State) based on an OS request (that is, a legacy P-State). EPP enable3\tDisable\tWhen this option is enabled, the system uses the energy performance bias register for the Energy Performance Preference (EPP) input to make decision on Performance State (P-State) or Processor Core Idle State (C-State) transitions. APS Rocketing\tDisable\tRocketing mechanism in the HWP p-state selection for pcode algorithm. Rocketing enables the core ratio to jump to max turbo instantaneously as opposed to a smooth ramp Scalability\tDisable\tCore Performance to frequency scalability based on optimizations in the CPU. Native ASPM\tDisable\tASPM off not controlled by BIOS or OS. Power Performance Tuning\tOS Controls EPB\tThis option selects the BIOS or OS that controls the Energy Performance Bias (EPB) functionality. Workload Configuration\tI/O sensitive\tThis option allows the system power and performance profile to be set to favor compute intensive workload or I/O sensitive workload. Dynamic L1\tDisable\tThis option applies only to the package-level setting to allow dynamically entering the lower power link state L1. Set Fan Profile\tPerformance\tThis option allows the fan profile to be set to Performance, Balanced, or Quiet. Cooling Configuration - Fan Speed Offset\tMedium\tThis option allows the fan speed offset to be set to Low, Medium, or High.  ","version":null,"tagName":"h3"},{"title":"OS Installation​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#os-installation","content":" For this example, we will assume the SLE Micro RT 5.4 operating system has been already installed on the baremetal host used to deploy the edge clusters. To verify the Real Time Kernel version used, you can use the following command (pay attention to the -rt suffix):  uname -r 5.14.21-150400.15.11-rt   If you want to download the SLE Micro RT 5.4 ISO, you can use the following link paying special attention to the -RT in the image name, once you've logged in with your SUSE credentials: Download SLE Micro RT from SUSE Customer Center  For more information about how to install the operating system SLE Micro RT, see the next link  ","version":null,"tagName":"h3"},{"title":"OS Configuration and Tuning​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#os-configuration-and-tuning","content":" Once you have the Operating System installed, you can proceed with the Operating System configuration. For this article, we will configure the Operating System using the next steps:  CPU Tuned Configuration  The first thing is to create a profile for the CPU cores we want to isolate. In this case, we will isolate the cores 1-30 and 33-62 to be used by FlexRan, keeping also some cores available for the system.  echo &quot;export tuned_params&quot; &gt;&gt; /etc/grub.d/00_tuned echo &quot;isolated_cores=1-30,33-62&quot; &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf systemctl enable tuned; systemctl start tuned tuned-adm profile cpu-partitioning   Grub Configuration  We need to modify some grub options to be able to do the CPU isolation as well as another important parameters for the CPU performance on this scenario.  The following options have to be customized:  parameter\tvalue\tdescriptionisolcpu\t1-30,33-62\tIsolate the cores 1-30 and 33-62 skew_tick\t1\tAllows the kernel to skew the timer interrupts across the isolated CPUs. nohz\ton\tAllows the kernel to run the timer tick on a single CPU when the system is idle. nohz_full\t1-30,33-62\tkernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation. rcu_nocbs\t1-30,33-62\tAllows the kernel to run the RCU callbacks on a single CPU when the system is idle. kthread_cpus\t0,31,32,63\tAllows the kernel to run the kthreads on a single CPU when the system is idle. irqaffinity\t0,31,32,63\tAllows the kernel to run the interrupts on a single CPU when the system is idle. processor.max_cstate\t1\tPrevents the CPU from dropping into a sleep state when idle intel_idle.max_cstate\t0\tDisables the intel_idle driver and allows acpi_idle to be used iommu\tpt\tAllows to use vfio for the dpdk interfaces intel_iommu\ton\tEnables to use vfio for VFs. hugepagesz\t1G\tAllows to set the size of huge pages to 1G hugepages\t40\tNumber of hugepages defined before default_hugepagesz\t1G\tDefault value to enable huge pages  For mor information about theses parameters, please refer to the next link  With the values showed above, we are isolating 60 cores, and we are using 4 cores for the OS.  We can modify the grub config as follows:  vi /etc/default/grub GRUB_CMDLINE_LINUX=&quot;intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpu=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll&quot; transactional-update grub.cfg   To validate that the parameters are applied after reboot, you could check:  cat /proc/cmdline   Compile the DPDK drivers  In order to use the igb_uio driver, which is necessary for the ACC100 acceleration card, we need to compile the DPDK drivers for the SLE Micro RT kernel. The process to build the igb_uio driver will be explained in this section, but if you don't want to deal with the compilation, you can also download the compiled driver for SLE Micro RT 5.4 rpm files from here.  To do that, we will use an auxiliary virtual machine (only to compile and generate the rpm files) with SLE-RT 15 SP4 kernel that you can download from here.  Once you have available your virtual machine with the SLE 15 SP4 installed you need to verify -rt in the kernel to ensure we will compile for a real time kernel.  uname -r 5.14.21-150400.15.53-rt   The first thing you have to do to compile the driver is to download the dpdk source code from the DPDK repository.  git clone http://dpdk.org/git/dpdk-kmods   Once you have the source code, you can compile the driver using the following commands:  cd dpdk-kmods/igb_uio make -C /lib/modules/`uname -r`/build M=`pwd` modules   With this command you will get the igb_uio.ko driver compiled. But in our case, we will create a rpm package to be able to install it in the SLE Micro RT kernel.  First create an x509.genkey setup to define the type of key we need:  echo -e &quot;[ req ] \\n\\ default_bits = 4096 \\n\\ distinguished_name = req_distinguished_name \\n\\ prompt = no \\n\\ x509_extensions = myexts \\n\\ [ req_distinguished_name ] \\n\\ CN = Modules \\n\\ \\n\\ [ myexts ] \\n\\ basicConstraints=critical,CA:FALSE \\n\\ keyUsage=digitalSignature \\n\\ subjectKeyIdentifier=hash \\n\\ authorityKeyIdentifier=keyid&quot; &gt; x509.genkey   Then create the certificates based on these:  openssl req -new -nodes -utf8 -sha512 -days 36500 -batch -x509 -config x509.genkey -outform DER -out signing_key.x509 -keyout signing_key.priv   Note: The signing key instruction are only used for testing, please create &quot;proper&quot; certificates for production usage.  Install build, kernel-rt and kernel-devel-rt to create the rpm package:  zypper in build kernel-rt kernel-devel-rt   Modify the config file to adapt it for the RT kernel:  cp /usr/lib/build/configs/sle15.4.conf /usr/lib/build/configs/sle15.4-rt.conf sed -e 's/kernel-default/kernel-rt/g' -i /usr/lib/build/configs/sle15.4-rt.conf echo &quot;Prefer: wicked&quot; &gt;&gt; /usr/lib/build/configs/sle15.4-rt.conf   Prepare the spec file:  cat &lt;&lt; EOF &gt;&gt; igb_uio.spec # # spec file for package igb_uio kmp # # Copyright (c) 2023 SUSE LINUX GmbH, Nuernberg, Germany. # # All modifications and additions to the file contributed by third parties # remain the property of their copyright owners, unless otherwise agreed # upon. The license for this file, and modifications and additions to the # file, is the same license as for the pristine package itself (unless the # license for the pristine package is not an Open Source License, in which # case the license is the MIT License). An &quot;Open Source License&quot; is a # license that conforms to the Open Source Definition (Version 1.9) # published by the Open Source Initiative. # Please submit bugfixes or comments via http://bugs.opensuse.org/ # # norootforbuild Name: igb_uio Version: 1.0 Release: 0 Summary: Kernel Module Package for igb_uio module License: GPL-2.0 Group: System/Kernel URL: https://www.suse.com #Git-Clone: http://dpdk.org/git/dpdk-kmods Source0: %{name}-%{version}.tar.gz # Required to sign modules: Include certificate named “signing_key.x509” # Build structure should also include a private key named “signing_key.priv” # Private key should not be listed as a source file Source1: signing_key.x509 BuildRequires: %kernel_module_package_buildreqs BuildRequires:\tkernel-rt BuildRequires: kernel-rt-devel #BuildRequires: bash-sh #BuildRequires: libelf-devel #BuildRequires: systemd #BuildRequires: pam-config #BuildRequires: libffi7 #BuildRequires: ghc-bootstrap BuildRoot: %{_tmppath}/%{name}-%{version}-build # Required to sign modules: The -c option tells the macro to generate a # suse-hello-ueficert subpackage that enrolls the certificate %suse_kernel_module_package -c %_sourcedir/signing_key.x509 %description This package contains the igb_uio.ko module. %prep %setup # Required to sign modules: Copy the signing key to the build area cp %_sourcedir/signing_key.* . set -- * mkdir source mv &quot;$@&quot; source/ mkdir obj %build for flavor in %flavors_to_build; do rm -rf obj/$flavor cp -r source obj/$flavor make -C %{kernel_source $flavor} modules M=$PWD/obj/$flavor done %install export INSTALL_MOD_PATH=$RPM_BUILD_ROOT export INSTALL_MOD_DIR=updates for flavor in %flavors_to_build; do make -C %{kernel_source $flavor} modules_install M=$PWD/obj/$flavor # Required to sign modules: Invoke kernel-sign-file to sign each module for x in $(find $INSTALL_MOD_PATH/lib/modules/*-$flavor/ -name '*.ko'); do /usr/lib/rpm/pesign/kernel-sign-file -i pkcs7 sha256 $PWD/obj/$flavor/signing_key.priv $PWD/obj/$flavor/signing_key.x509 $x done done %changelog * Fri Jun 9 2023 Rhys Oxenham &lt;rhys.oxenham@suse.com&gt; - 1.0 - Initial spec file as base EOF cat &lt;&lt; EOF &gt;&gt; Kbuild ccflags-y := $(MODULE_CFLAGS) obj-m := igb_uio.o EOF   Create the rpm packages:  build --dist /usr/lib/build/configs/sle15.4-rt.conf   After creating the rpm files, the packages will be located here: /var/tmp/build-root/home/abuild/rpmbuild/RPMS/x86_64  Install the dependencies.  transactional-update shell cat &gt; /etc/zypp/repos.d/flexran-dependencies.repo &lt;&lt; EOF [home_amorgante_branches_home_dpitchumani] name=Branch project for package DPDK-22.11 (15.4) type=rpm-md baseurl=https://download.opensuse.org/repositories/home:/amorgante:/branches:/home:/dpitchumani/15.4/ gpgcheck=1 gpgkey=https://download.opensuse.org/repositories/home:/amorgante:/branches:/home:/dpitchumani/15.4/repodata/repomd.xml.key enabled=1 [home_amorgante] name=home:amorgante (15.4) type=rpm-md baseurl=https://download.opensuse.org/repositories/home:/amorgante/15.4/ gpgcheck=1 gpgkey=https://download.opensuse.org/repositories/home:/amorgante/15.4/repodata/repomd.xml.key enabled=1 EOF   Now, we can install the dependencies (*.rpm files) with the igb_uio driver compiled previously (or downloaded from here):   ```shell suseconnect -p PackageHub/15.4/x86_64 zypper in *.rpm zypper in dpdk dpdk-tools pf-bb-config pciutils exit   CPU Performance  Further improve the deterministic and power efficiency:  cpupower frequency-set -g performance  Set cpu core frequency to 2.6Ghz which is the maximum allowed in our case (based on the hardware):  cpupower frequency-set -u 2500000  cpupower frequency-set -d 2500000  Set cpu uncore to fixed – maximum allowed. Disable c6 and c1e in order to disable the powersaving features in your system (only if enabled):  cpupower idle-set -d 3  cpupower idle-set -d 2  In case you've got the following message Idlestate 3 not available on CPU x you can ignore it, because that's means that the idle state is already disabled.  Check the CPU performance  You should see the driver intel_cpufreq and the governor performance with a frequency range between 2.5 and 2.6Ghz:  cpupower frequency-info ... analyzing CPU 0: driver: intel_cpufreq CPUs which run at the same hardware frequency: 0 CPUs which need to have their frequency coordinated by software: 0 maximum transition latency: 20.0 us hardware limits: 800 MHz - 3.50 GHz available cpufreq governors: ondemand performance schedutil current policy: frequency should be within 800 MHz and 3.50 GHz. The governor &quot;performance&quot; may decide which speed to use within this range. current CPU frequency: Unable to call hardware current CPU frequency: 2.60 GHz (asserted by call to kernel) boost state support: Supported: yes Active: yes ...   ","version":null,"tagName":"h3"},{"title":"RKE2 Cluster Installation​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#rke2-cluster-installation","content":" The RKE2 installation could be done creating a new cluster from the Rancher UI or importing an existing RKE2 cluster to Rancher. In our case, for brevity we will install a new RKE2 Cluster from scratch importing it after that directly into Rancher.  If you want to install the RKE2 cluster from the Rancher UI you can follow this document  Run the RKE2 installer:  In this scenario we use the v1.25.9+rke2r1 version as:  curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=&quot;v1.25.9+rke2r1&quot; sh -   Create the /etc/rancher/rke2/config.yaml file (and the rancher/rke2 directory) with the following content to enable Multus + Calico CNI plugins:  cni: - multus - calico   Start rke2-server service:  systemctl daemon-reload &amp;&amp; systemctl enable --now rke2-server   Check the installation  Make sure the calico and multus pods are running:  $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE calico-system calico-kube-controllers-687bc88ddf-6dp4r 1/1 Running 0 3m13s calico-system calico-node-jkhx9 1/1 Running 0 3m13s calico-system calico-typha-869bd9756d-ft4bs 1/1 Running 0 3m13s kube-system cloud-controller-manager-xr11-2 1/1 Running 0 3m49s kube-system etcd-xr11-2 1/1 Running 0 3m47s kube-system helm-install-rke2-calico-crd-q2cp2 0/1 Completed 0 3m36s kube-system helm-install-rke2-calico-nv4rn 0/1 Completed 1 3m36s kube-system helm-install-rke2-coredns-55k9x 0/1 Completed 0 3m36s kube-system helm-install-rke2-ingress-nginx-fvmp4 0/1 Completed 0 3m36s kube-system helm-install-rke2-metrics-server-d2dhz 0/1 Completed 0 3m36s kube-system helm-install-rke2-multus-mm59z 0/1 Completed 0 3m36s kube-system helm-install-rke2-snapshot-controller-crd-vbcjb 0/1 Completed 0 3m36s kube-system helm-install-rke2-snapshot-controller-jw6pk 0/1 Completed 0 3m36s kube-system helm-install-rke2-snapshot-validation-webhook-w5sj2 0/1 Completed 0 3m36s kube-system kube-apiserver-xr11-2 1/1 Running 0 3m53s kube-system kube-controller-manager-xr11-2 1/1 Running 0 3m51s kube-system kube-proxy-xr11-2 1/1 Running 0 3m48s kube-system kube-scheduler-xr11-2 1/1 Running 0 3m51s kube-system rke2-coredns-rke2-coredns-6b9548f79f-bc54n 1/1 Running 0 3m26s kube-system rke2-coredns-rke2-coredns-autoscaler-57647bc7cf-bfggl 1/1 Running 0 3m26s kube-system rke2-ingress-nginx-controller-6vsgf 1/1 Running 0 2m18s kube-system rke2-metrics-server-7d58bbc9c6-qjkvr 1/1 Running 0 2m33s kube-system rke2-multus-ds-4zsqr 1/1 Running 0 3m26s kube-system rke2-snapshot-controller-7b5b4f946c-rhtxn 1/1 Running 0 2m32s kube-system rke2-snapshot-validation-webhook-7748dbf6ff-cfmpm 1/1 Running 0 2m1s tigera-operator tigera-operator-7bd6b54cb8-2jm92 1/1 Running 0 3m23s   ","version":null,"tagName":"h3"},{"title":"ACC100 Configuration​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#acc100-configuration","content":" The ACC100 accelerator card is a PCIe card that provides hardware acceleration for the Forward Error Correction (FEC) algorithm. This card is used by the FlexRan library to improve the performance of the DU/CU components.  Load the igb_uio kernel module  During the OS configuration section we have installed the igb driver, dpdk as well as the pf-bb-config tool requirements  We will start configuring the Accelerator Card by loading the ibg_uio and the vfio-pci module.  modprobe igb_uio modprobe vfio-pci   Get the interface ACC100 PCI address:  /sbin/lspci | grep -i acc 8a:00.0 Processing accelerators: Intel Corporation Device 0d5c   Bind the Physical Function (PF) with the igb_uio driver:  dpdk-devbind.py -b igb_uio 0000:8a:00.0   Create 2 Virtual Functions (vfs) from the PF and bind them with vfio-pci driver:  echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/max_vfs dpdk-devbind.py -b vfio-pci 0000:8b:00.0   Configure acc100 using the pf-bb-config tool: The Physical Function (PF) Baseband Device (BBDEV) Configuration Application (pf_bb_config) provides a means to configure a baseband device at the host level. The program accesses the configuration space and sets various parameters through memory-mapped I/O (MMIO) reads and writes. The parameters are parsed from a given configuration file (with .cfg extensions) that is specific to a particular baseband device, although they follow same format.  For more information about the pf-bb-config tool, please refer to the documentation  pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg Tue Jun 6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL Tue Jun 6 10:49:20 2023:INFO:Configuration in VF mode Tue Jun 6 10:49:21 2023:INFO: ROM version MM 99AD92 Tue Jun 6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version 1302020 != 10092020 Tue Jun 6 10:49:21 2023:INFO:PF ACC100 configuration complete Tue Jun 6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!   Check the new VFs created are available and ready to be used by the FlexRan library:  dpdk-devbind.py -s ... Baseband devices using DPDK-compatible driver ============================================= 0000:8a:00.0 'Device 0d5c' drv=igb_uio unused=vfio-pci 0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=igb_uio Other Baseband devices ====================== 0000:8b:00.1 'Device 0d5d' unused=igb_uio,vfio-pci ...   ","version":null,"tagName":"h3"},{"title":"DPDK Configuration​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#dpdk-configuration","content":" The Data Plane Development Kit (DPDK) is a set of data plane libraries and network interface controller drivers for fast packet processing. It is designed to run on any processors. In our case, we will use the DPDK libraries to accelerate the performance of the FlexRan library. Let's start to create some VFs to be available for the FlexRan workloads:  Create the VF PCI addresses in the node:  In this section we will create 4 VFs for each PF (2 PFs in total for the dual port E810 100G interface) binding to the vfio driver, and then, we will assign a MAC address to each VF. The MAC address is used by the FlexRan library to identify the VFs. This is not a mandatory step, but then, you will need to modify the FlexRan Docker entrypoint script in order to adapt the MAC addresses to the VFs created. We will talk more about the docker entrypoint script in the next section.  echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs ip link set p2p1 vf 0 mac 00:11:22:33:00:00 ip link set p2p1 vf 1 mac 00:11:22:33:00:10 ip link set p2p1 vf 2 mac 00:11:22:33:00:20 ip link set p2p1 vf 3 mac 00:11:22:33:00:30 echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs ip link set p2p2 vf 0 mac 00:11:22:33:00:01 ip link set p2p2 vf 1 mac 00:11:22:33:00:11 ip link set p2p2 vf 2 mac 00:11:22:33:00:21 ip link set p2p2 vf 3 mac 00:11:22:33:00:31 dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3   Review the configuration:  dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio Network devices using kernel driver =================================== 0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active* 0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci 0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci 0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci 0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci 0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci Baseband devices using DPDK-compatible driver ============================================= 0000:8a:00.0 'Device 0d5c' drv=igb_uio unused=vfio-pci 0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=igb_uio Other Baseband devices ====================== 0000:8b:00.1 'Device 0d5d' unused=igb_uio,vfio-pci   ","version":null,"tagName":"h3"},{"title":"SRIOV Configuration​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#sriov-configuration","content":" The Single Root I/O Virtualization SR-IOV is a specification that allows a PCIe device to appear to be multiple separate physical PCIe devices. The SR-IOV network device plugin is Kubernetes device plugin for discovering and advertising networking resources such as:  SR-IOV virtual functions VFsPCI physical functions PFsAuxiliary network devices, in particular Subfunctions SFs  To deploy workloads with SR-IOV VF, Auxiliary network devices or PCI PF, this plugin needs to work together with the following two CNI components:  Any CNI meta plugin supporting Device Plugin based network provisioning. In our case will be MultusA CNI capable of consuming the network device allocated to the Pod  For more information about the SR-IOV CNI, please refer to the documentation  Prepare the config map for the device plugin:  You could get the information to fill the config map from the lspci command. In our case we will have the next 3 types of devices:  FEC acceleartor card VF: 0d5d. This is the first VF created on the ACC100 card and should match with the first VF created on the previous section. ODU interface: 1889, 8086 and p2p1 as a filter. This is the first port of the E810 interface and should match with the 4 first VFs created on the previous section. ORU interface: 1889, 8086 and p2p2 as a filter. This is the second port of the E810 interface and should match with the 4 last VFs created on the previous section.  cat &lt;&lt;EOF | k apply -f - apiVersion: v1 kind: ConfigMap metadata: name: sriovdp-config namespace: kube-system data: config.json: | { &quot;resourceList&quot;: [ { &quot;resourceName&quot;: &quot;intel_fec_5g&quot;, &quot;devicetype&quot;: &quot;accelerator&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;0d5d&quot;] } }, { &quot;resourceName&quot;: &quot;intel_sriov_odu&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;1889&quot;], &quot;drivers&quot;: [&quot;vfio-pci&quot;], &quot;pfNames&quot;: [&quot;p2p1&quot;] } }, { &quot;resourceName&quot;: &quot;intel_sriov_oru&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;1889&quot;], &quot;drivers&quot;: [&quot;vfio-pci&quot;], &quot;pfNames&quot;: [&quot;p2p2&quot;] } } ] } EOF   Prepare the daemonset for the device plugin  No changes are needed in the daemonset, so you can use the same upstream daemonset as is for the FlexRAN deployment.  For more information about the daemonset, please refer to the documentation  cat &lt;&lt;EOF | k apply -f - --- apiVersion: v1 kind: ServiceAccount metadata: name: sriov-device-plugin namespace: kube-system --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-sriov-device-plugin-amd64 namespace: kube-system labels: tier: node app: sriovdp spec: selector: matchLabels: name: sriov-device-plugin template: metadata: labels: name: sriov-device-plugin tier: node app: sriovdp spec: hostNetwork: true nodeSelector: kubernetes.io/arch: amd64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule serviceAccountName: sriov-device-plugin containers: - name: kube-sriovdp image: ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin:latest-amd64 imagePullPolicy: IfNotPresent args: - --log-dir=sriovdp - --log-level=10 securityContext: privileged: true resources: requests: cpu: &quot;250m&quot; memory: &quot;40Mi&quot; limits: cpu: 1 memory: &quot;200Mi&quot; volumeMounts: - name: devicesock mountPath: /var/lib/kubelet/ readOnly: false - name: log mountPath: /var/log - name: config-volume mountPath: /etc/pcidp - name: device-info mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp volumes: - name: devicesock hostPath: path: /var/lib/kubelet/ - name: log hostPath: path: /var/log - name: device-info hostPath: path: /var/run/k8s.cni.cncf.io/devinfo/dp type: DirectoryOrCreate - name: config-volume configMap: name: sriovdp-config items: - key: config.json path: config.json EOF   After deploying the daemonset on the RKE2 edge cluster, you should see the pods running:  $ kubectl get pods -n kube-system | grep sriov kube-system kube-sriov-device-plugin-amd64-twjfl 1/1 Running 0 2m   Check the interfaces discovered and available in the node for the FlexRan workload:  $ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq { &quot;cpu&quot;: &quot;64&quot;, &quot;ephemeral-storage&quot;: &quot;256196109726&quot;, &quot;hugepages-1Gi&quot;: &quot;40Gi&quot;, &quot;hugepages-2Mi&quot;: &quot;0&quot;, &quot;intel.com/intel_fec_5g&quot;: &quot;1&quot;, &quot;intel.com/intel_sriov_odu&quot;: &quot;4&quot;, &quot;intel.com/intel_sriov_oru&quot;: &quot;4&quot;, &quot;memory&quot;: &quot;221396384Ki&quot;, &quot;pods&quot;: &quot;110&quot; }   As you can see in the output above, we have 2 types of resources available for the FlexRan workload:  The FEC will be intel.com/intel_fec_5g and the value will be 1 because we bind just only 1 of 2. The VFs will be intel.com/intel_sriov_odu or intel.com/intel_sriov_oru and the value will be 4 because we bind 4 VFs for each PF.  Basically, FlexRan will request some resources available in the host to be used as a VF for the tests we're going to run.  Important Note: If you don't get the resources available here, does not make sense continue with the flexran demo tests. Please, review the previous steps to ensure you have the VFs created and the SRIOV CNI plugin working properly.  ","version":null,"tagName":"h3"},{"title":"FlexRan tests​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#flexran-tests","content":" ","version":null,"tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#references","content":" For this article, we will use the next references to deploy the Intel FlexRan reference implementation on top of the ATIP edge cluster:  We will use the pre-defined containers from Intel: FlexRan pre-defined containersYou will also need to download the FlexRan-22.07 tarball from Intel to run the tests and mount the downloaded tests folder into the pre-defined containers because it's not included into the pre-defined containers.  In order to download those files, it is required to have access to the Intel website (you can ask your Intel representative).  FlexRAN-22.07-L1.tar.gz_part00FlexRAN-22.07-L1.tar.gz_part01Container Entrypoint script (Just in case you need to change anything else like the MAC addresses for the VFs)  ","version":null,"tagName":"h3"},{"title":"Prepare the files downloaded from Intel​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#prepare-the-files-downloaded-from-intel","content":" Once you have the tarball files downloaded from Intel, you can join the files and extract the content:  mkdir flexran; cp FlexRAN-22.07-L1.tar.gz_part* flexran; cd flexran cat FlexRAN-22.07-L1.tar.gz_part* | tar -xzvf -   Now, we need to execute the extract.sh script to get the tests folder available to be mounted into the containers. Also, we will copy the docker entrypoint script to be modified if needed.  During this process manual intervention is required to accept the license agreement  ./extract.sh mkdir /home/tmp_flexran cp -R tests/ /home/tmp_flexran/ cp build/docker/docker_entry.sh /home/tmp_flexran/   The `/home/tmp_flexran will be the folder to be mounted into the containers on the next section.  Before deploying the FlexRan containers, let's review the next steps:  Interfaces to be used in the flexran pods yaml files, should be referenced by their resource names. You could get those as: kubectl get nodes -o json | jq '.items[].status.allocatable' as explained in the previous sectionThe container entrypoint script contains the tests files customization for this specific environment. It will modify some parameters such as MAC addresses, VF information and the dpdk info into the tests. It will be explained in the next section.Tests should be mounted in /home/tmp_flexran/tests and exposed in /home/flexran/tests  ","version":null,"tagName":"h3"},{"title":"Container entrypoint​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#container-entrypoint","content":" The FlexRan containers run a script when the container is started. The script will modify the configuration files of the FlexRan tests applications to adapt the tests to our environment.  The modifications done in this file will be:  PCIDEVICE_INTEL_COM_INTEL_FEC_5G=$(env|grep PCIDEVICE_INTEL_COM_INTEL_FEC_5G= |awk -F '=' '{print $2}') in order to select the right PCI device for the FEC ACC100 card used for the 5G acceleration. export INTEL_COM_INTEL_CPULIST=$(cat /sys/fs/cgroup/cpuset/cpuset.cpus) to get the CPU list of the host machine.  Also, we need to change the CPU cores as well as the MAC addresses into the RU section because there isn't any substitution in the entrypoint script. We need to change the next lines:   sed -i &quot;s/ioCore=2/ioCore=62/g&quot; config_file_o_ru.dat sed -i &quot;s/duMac0=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac0=00:11:22:33:00:00/g&quot; config_file_o_ru.dat sed -i &quot;s/duMac1=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac1=00:11:22:33:00:10/g&quot; config_file_o_ru.dat sed -i &quot;s/ruMac0=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac0=00:11:22:33:00:01/g&quot; config_file_o_ru.dat sed -i &quot;s/ruMac1=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac1=00:11:22:33:00:11/g&quot; config_file_o_ru.dat sed -i &quot;s/duMac2=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac2=00:11:22:33:00:20/g&quot; config_file_o_ru.dat sed -i &quot;s/duMac3=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/duMac3=00:11:22:33:00:30/g&quot; config_file_o_ru.dat sed -i &quot;s/ruMac2=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac2=00:11:22:33:00:21/g&quot; config_file_o_ru.dat sed -i &quot;s/ruMac3=[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]:[0-9,a-z][0-9,a-z]/ruMac3=00:11:22:33:00:31/g&quot; config_file_o_ru.dat   Then, we need to remove (or comment with the hashtag # sign) the tests not covered by the flexran container version we are using (the test files are not present in the FlexRan tests suite used so the script will fail):  flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub3_mu1_20mhz_4x4/gnb/flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub6_mu1_100mhz_4x4/gnb/flexran/bin/nr5g/gnb/l1/orancfg/mmwave_mu3_100mhz_2x2/gnb/flexran/bin/nr5g/gnb/l1/orancfg/mmwave_mu3_100mhz_2x2/gnb/flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub3_mu1_20mhz_4x4/oru/flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub6_mu1_100mhz_4x4/oru/flexran/bin/nr5g/gnb/l1/orancfg/mmwave_mu3_100mhz_2x2/oru/  For instance, you will have the next block commented (or removed):  #cd /home/flexran/bin/nr5g/gnb/l1/orancfg/sub3_mu0_20mhz_sub3_mu1_20mhz_4x4/gnb/ #sed -i &quot;s/&lt;systemThread&gt;2, 0, 0&lt;\\/systemThread&gt;/&lt;systemThread&gt;$systemthread, 0, 0&lt;\\/systemThread&gt;/g&quot; phycfg_xran.xml #sed -i &quot;s/&lt;timerThread&gt;0, 96, 0&lt;\\/timerThread&gt;/&lt;timerThread&gt;$timerThread, 96, 0&lt;\\/timerThread&gt;/g&quot; phycfg_xran.xml #sed -i &quot;s/&lt;FpgaDriverCpuInfo&gt;3, 96, 0&lt;\\/FpgaDriverCpuInfo&gt;/&lt;FpgaDriverCpuInfo&gt;$FpgaDriverCpuInfo, 96, 0&lt;\\/FpgaDriverCpuInfo&gt;/g&quot; phycfg_xran.xml #sed -i &quot;s/&lt;FrontHaulCpuInfo&gt;3, 96, 0&lt;\\/FrontHaulCpuInfo&gt;/&lt;FrontHaulCpuInfo&gt;$FrontHaulCpuInfo, 96, 0&lt;\\/FrontHaulCpuInfo&gt;/g&quot; phycfg_xran.xml #sed -i &quot;s/&lt;radioDpdkMaster&gt;2, 99, 0&lt;\\/radioDpdkMaster&gt;/&lt;radioDpdkMaster&gt;$radioDpdkMaster, 99, 0&lt;\\/radioDpdkMaster&gt;/g&quot; phycfg_xran.xml   An example of docker entrypoint script is available here  ","version":null,"tagName":"h3"},{"title":"FlexRan Timer Mode​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#flexran-timer-mode","content":" FlexRAN Timer Mode use case does not use fronthaul. RF IQ samples are read from files and write to files. The test pod is configured with two containers. One container is for L1APP, and another one is for TestMAC. Smaller storage is built in pod as Kubernetes resources, but the storage is not enough for the test case. The test case requires 20GB storage for tests files, so it will be mounted from the host using the tests volume mount path. The test case is a setup for one peak cell and two average cells. 8 cores with HT enabled are pinned for L1APP, the average core utilization is between 45-65%. The below figure shows the threading model for this test case.    To deploy the Test Timer Mode, we will use the next yaml file:  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: labels: app: flexran-dockerimage_release name: flexran-dockerimage-release spec: containers: - securityContext: privileged: false capabilities: add: - IPC_LOCK - SYS_NICE command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ] args: [&quot; cp flexran/build/docker/docker_entry.sh .; sh docker_entry.sh -m timer ; cd /home/flexran/bin/nr5g/gnb/l1/; ./l1.sh -e ; top&quot;] tty: true stdin: true env: - name: LD_LIBRARY_PATH value: /opt/oneapi/lib/intel64 image: intel/flexran_vdu:v22.07 name: flexran-l1app resources: requests: memory: &quot;32Gi&quot; intel.com/intel_fec_5g: '1' hugepages-1Gi: 16Gi limits: memory: &quot;32Gi&quot; intel.com/intel_fec_5g: '1' hugepages-1Gi: 16Gi volumeMounts: - name: hugepage mountPath: /hugepages - name: varrun mountPath: /var/run/dpdk readOnly: false - name: tests mountPath: /home/flexran/tests readOnly: false - name: proc mountPath: /proc readOnly: false - securityContext: privileged: false capabilities: add: - IPC_LOCK - SYS_NICE command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ] args: [&quot;sleep 10; sh docker_entry.sh -m timer ; cd /home/flexran/bin/nr5g/gnb/testmac/; ./l2.sh --testfile=icelake-sp/icxsp.cfg; top&quot;] tty: true stdin: true env: - name: LD_LIBRARY_PATH value: /opt/oneapi/lib/intel64 image: intel/flexran_vdu:v22.07 name: flexran-testmac resources: requests: memory: &quot;12Gi&quot; hugepages-1Gi: 8Gi limits: memory: &quot;12Gi&quot; hugepages-1Gi: 8Gi volumeMounts: - name: hugepage mountPath: /hugepages - name: varrun mountPath: /var/run/dpdk readOnly: false - name: tests mountPath: /home/flexran/tests readOnly: false - name: proc mountPath: /proc readOnly: false volumes: - name: hugepage emptyDir: medium: HugePages - name: varrun emptyDir: {} - name: tests hostPath: path: &quot;/home/tmp_flexran/tests&quot; - name: proc hostPath: path: /proc EOF   The next resources are required:  intel.com/intel_fec_5g: '1' to request the FEC ACC100 resource.hugepages-1Gi: 16Gi to request the hugepages resource for the first container.hugepages-1Gi: 8Gi to request the hugepages resource for the second container./home/tmp_flexran/tests folder mounted to /home/flexran/tests in order to have the tests available for the containers.image: intel/flexran_vdu:v22.07 to use the pre-defined container from Intel pre-defined containers.  Once the pod is deployed, you can check the status of the pod:  kubectl get pods NAME READY STATUS RESTARTS AGE flexran-dockerimage-release 2/2 Running 0 2m   This will launch automatically the Timer Mode Tests which contains 83 tests for the FlexRan library. It could take up to 3 hours to complete the tests. The status of the tests can be found in the container logs:  kubectl logs flexran-dockerimage-release flexran-l1app   After a successful tests execution the following output should be shown:  All Tests Completed, Total run 83 Tests, PASS 83 Tests, and FAIL 0 Tests ---------------------------------------------------------------------------- mem_mgr_display_size: Num Memory Alloc: 11 Total Memory Size: 264,018 ----------------------------------------------------------------------------   Using htop we could see the CPU usage of the containers as well as the pinned cores for this workload:    Using the Rancher UI, you can check the status of the pods and the logs of the containers. Also, you could get the next metrics using prometheus and grafana to check the CPU usage , CPU cores isolated used, and the Memory usage of the containers:    ","version":null,"tagName":"h3"},{"title":"FlexRan Xran Mode​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#flexran-xran-mode","content":" FlexRAN XRAN Mode uses two Intel® Ethernet 100G 2P E810 Adapter on the two PCIe x16 slots. The two NIC cards are connected by fiber directly for fronthaul eCPRI C-Plane and U-Plane traffic. SR-IOV is enabled on the two E810 NIC cards. ACC100 accelerator cards and RF IQ samples over Ethernet VFs use SR-IOV resources. The O-DU test pod is configured with two containers. One container is for L1APP, and another one is for TestMAC. The O-RU test pod is configured with one container. Smaller storage is built in pod as Kubernetes resources, but the larger storage is mapped from the host. The test case is a setup for six average cells with O-RAN lower layer split option 7-2x. 2 cores with HT enabled are pinned for L1APP, the average core utilization is 65%. 2 cores are pinned for O-RU fronthaul, 2 cores are pinned for L1 fronthaul. The below figure shows the threading model for this test case.    To deploy the XRAN Mode, we will use the next yaml file:  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: flexran-vdu name: flexran-vdu spec: replicas: 1 selector: matchLabels: app: flexran-vdu template: metadata: labels: app: flexran-vdu spec: containers: - securityContext: privileged: true capabilities: add: - IPC_LOCK - SYS_NICE command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ] args: [ &quot;cp flexran/build/docker/docker_entry.sh . ; sh docker_entry.sh -m xran ; top&quot;] tty: true stdin: true env: - name: LD_LIBRARY_PATH value: /opt/oneapi/lib/intel64 image: intel/flexran_vdu:v22.07 name: flexran-vdu resources: requests: memory: &quot;24Gi&quot; hugepages-1Gi: 20Gi intel.com/intel_fec_5g: '1' intel.com/intel_sriov_odu: '4' limits: memory: &quot;24Gi&quot; hugepages-1Gi: 20Gi intel.com/intel_fec_5g: '1' intel.com/intel_sriov_odu: '4' volumeMounts: - name: hugepage mountPath: /hugepages - name: varrun mountPath: /var/run/dpdk readOnly: false - name: tests mountPath: /home/flexran/tests readOnly: false - name: proc mountPath: /proc readOnly: false volumes: - name: hugepage emptyDir: medium: HugePages - name: varrun emptyDir: {} - name: tests hostPath: path: &quot;/home/tmp_flexran/tests&quot; - name: proc hostPath: path: /proc --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: flexran-vru name: flexran-vru spec: replicas: 1 selector: matchLabels: app: flexran-vru template: metadata: labels: app: flexran-vru spec: containers: - securityContext: privileged: true capabilities: add: - IPC_LOCK - SYS_NICE command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ] args: [&quot;cp flexran/build/docker/docker_entry.sh . ; sh docker_entry.sh -m xran ; top&quot;] tty: true stdin: true env: - name: LD_LIBRARY_PATH value: /opt/oneapi/lib/intel64 image: intel/flexran_vdu:v22.07 name: flexran-oru resources: requests: memory: &quot;24Gi&quot; hugepages-1Gi: 16Gi intel.com/intel_sriov_oru: '4' limits: memory: &quot;24Gi&quot; hugepages-1Gi: 16Gi intel.com/intel_sriov_oru: '4' volumeMounts: - name: hugepage mountPath: /hugepages - name: varrun mountPath: /var/run/dpdk readOnly: false - name: tests mountPath: /home/flexran/tests readOnly: false - name: proc mountPath: /proc readOnly: false volumes: - name: hugepage emptyDir: medium: HugePages - name: varrun emptyDir: {} - name: tests hostPath: path: &quot;/home/tmp_flexran/tests&quot; - name: proc hostPath: path: /proc EOF   The next resources are required:  intel.com/intel_fec_5g: '1' to request the FEC ACC100 resource.intel.com/intel_sriov_odu: '4' to request the 4 VFs for the ODU interface.intel.com/intel_sriov_oru: '4' to request the 4 VFs for the ORU interface.hugepages-1Gi: 20Gi to request the hugepages resource for the first container.hugepages-1Gi: 16Gi to request the hugepages resource for the second container./home/tmp_flexran/tests folder mounted to /home/flexran/tests in order to have the tests available for the containers.  Once the pods are deployed, you need to launch the tests manually. You can do that using the next commands:  Open a terminal in the flexran-vdu pod and run the following commands:  kubectl exec -ti flexran-vdu-66c9bf5765-ftptg /bin/bash cd flexran/bin/nr5g/gnb/l1/orancfg/sub6_mu1_100mhz_4x4/gnb/ ./l1.sh -oru   Open a new terminal in the flexran-vdu pod and run the following commands:  kubectl exec -ti flexran-vdu-66c9bf5765-ftptg /bin/bash cd flexran/bin/nr5g/gnb/testmac ./l2.sh --testfile=../l1/orancfg/sub6_mu1_100mhz_4x4/gnb/testmac_clxsp_mu1_100mhz_hton_oru.cfg   Open a third terminal to run the flexran-vru pod and run the following commands:  kubectl exec -ti flexran-vru-66casd2e5765-23resd /bin/bash cd flexran/bin/nr5g/gnb/l1/orancfg/sub6_mu1_100mhz_4x4/oru/ chmod +x run_o_ru.sh; taskset -c 20 ./run_o_ru.sh   Using htop we could see the CPU usage of the containers as well as the pinned cores for this workload:    Using the Rancher UI, you can check the status of the pods and the logs of the containers. Also, you could get the next metrics using prometheus and grafana to check the CPU usage , CPU cores isolated used, and the Memory usage of the containers:    ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Deploying Intel FlexRan on the SUSE Adaptive Telco Infrastructure Platform","url":"/blog/Flexran#conclusion","content":" Building, testing, and deploying a properly configured Intel® FlexRAN implementation can show the benefits of VNFs and vRAN with Intel® Xeon® Scalable Processors and Intel® Advanced Vector Extensions. SUSE provides all the components for an open-source, enterprise-grade, software-defined stack for cloud-native orchestration and management. SUSE Linux Enterprise Micro Real Time, Rancher Kubernetes Engine v2 (RKE2) and Rancher Management were used and illustrated as key ingredients to simplify the deployment of Intel® FlexRAN. ","version":null,"tagName":"h2"},{"title":"Welcome to SUSE Edge","type":0,"sectionRef":"#","url":"/blog/welcome","content":"Thank you for visiting our SUSE Engineering Page. There are many things we aim to accomplish with this website.","keywords":"","version":null},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs","content":"Overview Welcome to the SUSE Edge Engineering docs. warning We suggest you to please put on your hard hat, as we are rapidly building out our documentation. ATIP documentation can be found here","keywords":"","version":"Next"},{"title":"Elemental","type":0,"sectionRef":"#","url":"/docs/components/elemental","content":"Elemental https://elemental.docs.rancher.com","keywords":"","version":"Next"},{"title":"Metal3","type":0,"sectionRef":"#","url":"/docs/components/metal3","content":"Metal3","keywords":"","version":"Next"},{"title":"Fleet","type":0,"sectionRef":"#","url":"/docs/components/fleet","content":"Fleet https://fleet.rancher.io","keywords":"","version":"Next"},{"title":"Longhorn","type":0,"sectionRef":"#","url":"/docs/components/longhorn","content":"Longhorn https://longhorn.io/docs/","keywords":"","version":"Next"},{"title":"Rancher","type":0,"sectionRef":"#","url":"/docs/components/rancher","content":"Rancher https://ranchermanager.docs.rancher.com","keywords":"","version":"Next"},{"title":"Neuvector","type":0,"sectionRef":"#","url":"/docs/components/neuvector","content":"Neuvector https://open-docs.neuvector.com","keywords":"","version":"Next"},{"title":"Observability","type":0,"sectionRef":"#","url":"/docs/concepts/observability","content":"Observability","keywords":"","version":"Next"},{"title":"SLE Micro","type":0,"sectionRef":"#","url":"/docs/components/sle-micro","content":"SLE Micro","keywords":"","version":"Next"},{"title":"Terminology","type":0,"sectionRef":"#","url":"/docs/concepts/terms","content":"Terminology There is a lot of terminology used in this space. Let's keep track of what means what. KubernetesContainerCluster API","keywords":"","version":"Next"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/components/metallb","content":"","keywords":"","version":"Next"},{"title":"MetalLB​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#metallb","content":" Via the official docs:  MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.  Network load balancers in bare-metal environments are much more difficult than in cloud environments. Instead of performing an API call, it involves having either network appliances or a combination of a load balancer + VIP to handle HA (or a single node load balancer SPOF). Those are not easily automated so having a K8s deployment where things go up and down all the time is challenging.  MetalLB tries to fix this by leveraging the K8s model to create LoadBalancer type of services like if they were in the cloud... but on bare-metal.  There are two different approaches, via L2 mode (using ARP tricks) or via BGP. Mainly L2 doesn't need any special network gear but BGP is in general better. It depends on the use cases.  ","version":"Next","tagName":"h2"},{"title":"MetalLB on K3s (using L2)​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#metallb-on-k3s-using-l2","content":" In this quickstart, L2 mode will be used so it means we don't need any special network gear but just a couple of free IPs in our network range, ideally outside of the DHCP pool so they are not assigned.  In this example, our DHCP pool is 192.168.122.100-192.168.122.200 (yes, 3 IPs, see Traefik and MetalLB for the reason of the extra IP) for a 192.168.122.0/24 network so anything outside this range is ok (besides the gateway and other hosts that can be already running!)  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#prerequisites","content":" A K3s cluster where MetalLB is going to be deployed. Hint, you can use the K3s on SLE Micro guide.  ⚠️ K3S comes with its own service load balancer named Klipper. You need to disable it in order to run MetalLB. To disable Klipper, K3s needs to be installed using the --disable=servicelb flag.  HelmA couple of free IPs in our network range. In this case 192.168.122.10-192.168.122.12  ","version":"Next","tagName":"h3"},{"title":"Deployment​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#deployment","content":" MetalLB leverages Helm (and other methods as well), so:  helm repo add metallb https://metallb.github.io/metallb helm install --create-namespace -n metallb-system metallb metallb/metallb while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get pods -n metallb-system -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done   ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#configuration","content":" At this point, the installation is completed. Now it is time to configure using our example values:  cat &lt;&lt;-EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: ip-pool namespace: metallb-system spec: addresses: - 192.168.122.10/32 - 192.168.122.11/32 - 192.168.122.12/32 EOF cat &lt;&lt;-EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: ip-pool-l2-adv namespace: metallb-system spec: ipAddressPools: - ip-pool EOF   At this point, it is ready to be used. There are a lot of things you can customize for L2 mode such as:  IPv6 And Dual Stack ServicesControl automatic address allocationReduce the scope of address allocation to specific Namespaces and servicesLimiting the set of nodes where the service can be announced fromSpecify network interfaces that LB IP can be announce from  And a lot more for BGP  ","version":"Next","tagName":"h3"},{"title":"Traefik and MetalLB​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#traefik-and-metallb","content":" Traefik is deployed by default with K3s (it can be disabled with --disable=traefik) and it is by default exposed as LoadBalancer (to be used with Klipper). However, as Klipper needs to be disabled, Traefik service for ingress is still a LoadBalancer type... so at the moment of deploying MetalLB the first IP will be assigned automatically to Traefik Ingress.  # Before deploying MetalLB kubectl get svc -n kube-system traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.44.113 &lt;pending&gt; 80:31093/TCP,443:32095/TCP 28s # After deploying MetalLB kubectl get svc -n kube-system traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik LoadBalancer 10.43.44.113 192.168.122.10 80:31093/TCP,443:32095/TCP 3m10s   We will leverage this later.  ","version":"Next","tagName":"h3"},{"title":"Usage​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#usage","content":" Let's create an example deployment:  cat &lt;&lt;- EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: hello-kubernetes --- apiVersion: v1 kind: ServiceAccount metadata: name: hello-kubernetes namespace: hello-kubernetes labels: app.kubernetes.io/name: hello-kubernetes --- apiVersion: apps/v1 kind: Deployment metadata: name: hello-kubernetes namespace: hello-kubernetes labels: app.kubernetes.io/name: hello-kubernetes spec: replicas: 2 selector: matchLabels: app.kubernetes.io/name: hello-kubernetes template: metadata: labels: app.kubernetes.io/name: hello-kubernetes spec: serviceAccountName: hello-kubernetes containers: - name: hello-kubernetes image: &quot;paulbouwer/hello-kubernetes:1.10&quot; imagePullPolicy: IfNotPresent ports: - name: http containerPort: 8080 protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http env: - name: HANDLER_PATH_PREFIX value: &quot;&quot; - name: RENDER_PATH_PREFIX value: &quot;&quot; - name: KUBERNETES_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: KUBERNETES_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: KUBERNETES_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: CONTAINER_IMAGE value: &quot;paulbouwer/hello-kubernetes:1.10&quot; EOF   And finally, the service:  cat &lt;&lt;- EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: hello-kubernetes namespace: hello-kubernetes labels: app.kubernetes.io/name: hello-kubernetes spec: type: LoadBalancer ports: - port: 80 targetPort: http protocol: TCP name: http selector: app.kubernetes.io/name: hello-kubernetes EOF   Let's see it in action:  kubectl get svc -n hello-kubernetes NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-kubernetes LoadBalancer 10.43.127.75 192.168.122.11 80:31461/TCP 8s curl http://192.168.122.11 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello Kubernetes!&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/css/main.css&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://fonts.googleapis.com/css?family=Ubuntu:300&quot; &gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;main&quot;&gt; &lt;img src=&quot;/images/kubernetes.png&quot;/&gt; &lt;div class=&quot;content&quot;&gt; &lt;div id=&quot;message&quot;&gt; Hello world! &lt;/div&gt; &lt;div id=&quot;info&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;namespace:&lt;/th&gt; &lt;td&gt;hello-kubernetes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;pod:&lt;/th&gt; &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;node:&lt;/th&gt; &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div id=&quot;footer&quot;&gt; paulbouwer/hello-kubernetes:1.10 (linux/amd64) &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;   ","version":"Next","tagName":"h3"},{"title":"Ingress with MetalLB​","type":1,"pageTitle":"Intro","url":"/docs/components/metallb#ingress-with-metallb","content":" As Traefik is already serving as an ingress controller, we can expose any http/https traffic via an Ingress object such as:  IP=$(kubectl get svc -n kube-system traefik -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;) cat &lt;&lt;- EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: hello-kubernetes-ingress namespace: hello-kubernetes spec: rules: - host: hellok3s.${IP}.sslip.io http: paths: - path: &quot;/&quot; pathType: Prefix backend: service: name: hello-kubernetes port: name: http EOF   And then:  curl http://hellok3s.${IP}.sslip.io &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello Kubernetes!&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/css/main.css&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://fonts.googleapis.com/css?family=Ubuntu:300&quot; &gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;main&quot;&gt; &lt;img src=&quot;/images/kubernetes.png&quot;/&gt; &lt;div class=&quot;content&quot;&gt; &lt;div id=&quot;message&quot;&gt; Hello world! &lt;/div&gt; &lt;div id=&quot;info&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;namespace:&lt;/th&gt; &lt;td&gt;hello-kubernetes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;pod:&lt;/th&gt; &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;node:&lt;/th&gt; &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div id=&quot;footer&quot;&gt; paulbouwer/hello-kubernetes:1.10 (linux/amd64) &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;   Also to verify that MetalLB is working correctly arping can be used as:  arping hellok3s.${IP}.sslip.io  Expected result:  ARPING 192.168.64.210 60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec 60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec 60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec   In the example above, the traffic flows as follows:  hellok3s.${IP}.sslip.io is resolved to the actual IP.Then the traffic is handled by the metallb-speaker pod.metallb-speaker redirects the traffic to the traefik controller.Finally Traefik forwards the request to the hello-kubernetes Service. ","version":"Next","tagName":"h2"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/demo_setup/elemental-utm-aarch64","content":"","keywords":"","version":"Next"},{"title":"Elemental​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#elemental","content":" Via the official docs:  Elemental is a software stack enabling a centralized, full cloud-native OS management with Kubernetes. The Elemental Stack consists of some packages on top of SLE Micro for Rancher: elemental-toolkit - includes a set of OS utilities to enable OS management via containers. Includes dracut modules, bootloader configuration, cloud-init style &gt; configuration services, etc.elemental-operator - this connects to Rancher Manager and handles MachineRegistration and MachineInventory CRDselemental-register - this registers machines via machineRegistrations and installs them via elemental-clielemental-cli - this installs any elemental-toolkit based derivative. Basically an installer based on our A/B install and upgrade systemrancher-system-agent - runs on the installed system and gets instructions (&quot;Plans&quot;) from Rancher Manager what to install and run on the system Cluster Node OSes are built and maintained via container images through the Elemental CLI and they can be installed on new hosts using the Elemental UI plugin for &gt; Rancher Manager or the Elemental CLI. The Elemental Operator and the Rancher System Agent enable Rancher Manager to fully control Elemental clusters, from the installation and management of the OS on the &gt; Nodes to the provisioning of new K3s or RKE2 clusters in a centralized way. What is Elemental Teal ?​ Elemental Teal is the combination of &quot;SLE Micro for Rancher&quot; with the Rancher Elemental stack. SLE Micro for Rancher is a containerized and &quot;stripped to the bones&quot; operating system layer. At its core, it only requires grub2, dracut, a kernel, and systemd. Its sole purpose is to run Kubernetes (k3s or RKE2), with everything controlled through Rancher Manager. Elemental Teal is built in the openSUSE Build Service and available through the openSUSE Registry.  ","version":"Next","tagName":"h2"},{"title":"Elemental on OSX on Apple Silicon​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#elemental-on-osx-on-apple-silicon","content":" Elemental is a Kubernetes thing, so it only requires a proper cluster up &amp; running. However, in order to provision real clusters and hosts, it requires to perform some steps such as downloading and customizing an ISO (or an image file) and boot the ISO. This quickstart uses UTM to create a VM and a few steps to create a proper image to boot from.  The trick here is there is no ARM64 image yet, but just a Raspberry Pi one... so that's the one we will use. It is not generic, but it works.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#prerequisites","content":" A Kubernetes cluster where Elemental is deployed. Hint, you can use the K3s on SLE Micro guide.Rancher server configured (server-url set). Hint: you can use the official Rancher docs or the create_vm.sh script for inspiration.Helmjq  ","version":"Next","tagName":"h3"},{"title":"Elemental UI Rancher extension​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#elemental-ui-rancher-extension","content":" This is an optional step to enable the Elemental UI extension in Rancher (see more about Rancher extensions):  helm repo add rancher-charts https://charts.rancher.io/ helm upgrade --create-namespace -n cattle-ui-plugin-system --install ui-plugin-operator rancher-charts/ui-plugin-operator helm upgrade --create-namespace -n cattle-ui-plugin-system --install ui-plugin-operator-crd rancher-charts/ui-plugin-operator-crd # Wait for the operator to be up while ! kubectl wait --for condition=ready -n cattle-ui-plugin-system $(kubectl get pods -n cattle-ui-plugin-system -l app.kubernetes.io/instance=ui-plugin-operator -o name) --timeout=10s; do sleep 2 ; done # Deploy the elemental UI plugin # NOTE: TABs and then spaces... cat &lt;&lt;- FOO | kubectl apply -f - apiVersion: catalog.cattle.io/v1 kind: UIPlugin metadata: name: elemental namespace: cattle-ui-plugin-system spec: plugin: endpoint: https://raw.githubusercontent.com/rancher/ui-plugin-charts/main/extensions/elemental/1.1.0 name: elemental noCache: false version: 1.1.0 FOO # Or # helm repo add rancher-ui-plugins https://raw.githubusercontent.com/rancher/ui-plugin-charts/main # helm upgrade --install elemental rancher-ui-plugins/elemental --namespace cattle-ui-plugin-system --create-namespace   After a while, the plugin will be shown in the UI as:    ","version":"Next","tagName":"h3"},{"title":"Elemental Operator​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#elemental-operator","content":" Elemental is managed by an operator deployed via Helm as:  helm upgrade --create-namespace -n cattle-elemental-system --install --set image.imagePullPolicy=Always elemental-operator oci://registry.opensuse.org/isv/rancher/elemental/dev/charts/rancher/elemental-operator-chart   The values.yaml file have some variables interesting to see  After a few seconds you should see the operator pod appear on the cattle-elemental-system namespace:  kubectl get pods -n cattle-elemental-system NAME READY STATUS RESTARTS AGE elemental-operator-64f88fc695-b8qhn 1/1 Running 0 16s   ","version":"Next","tagName":"h3"},{"title":"Kubernetes resources​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#kubernetes-resources","content":" Based on the Elemental quickstart guide, a few Kubernetes resources need to be created.  NOTE: It is out of the scope of this document to provide an explanation about the resources managed by Elemental, however the official documentation explains all those in good detail.  NOTE: In order to deploy more than one elemental machine, be sure that spec.config.elemental.registration.emulated-tpm-seed=-1 is set in your MachineRegistration so the seed used for the TPM emulation is randomized per machine. Otherwise, you will get the same TPM Hash for all deployed machines and only the last one to be registered will be valid. See the official docs for tpm and machineregistration for more information.  cat &lt;&lt;- EOF | kubectl apply -f - apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate metadata: name: my-machine-selector namespace: fleet-default spec: template: spec: selector: matchExpressions: - key: location operator: In values: [ 'europe' ] EOF cat &lt;&lt;- EOF | kubectl apply -f - kind: Cluster apiVersion: provisioning.cattle.io/v1 metadata: name: my-cluster namespace: fleet-default spec: rkeConfig: machineGlobalConfig: etcd-expose-metrics: false profile: null machinePools: - controlPlaneRole: true etcdRole: true machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: my-machine-selector name: pool1 quantity: 1 unhealthyNodeTimeout: 0s workerRole: true machineSelectorConfig: - config: protect-kernel-defaults: false registries: {} kubernetesVersion: v1.24.8+k3s1 EOF cat &lt;&lt;- 'EOF' | kubectl apply -f - apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: root passwd: root elemental: install: reboot: true device: /dev/vda debug: true disable-boot-entry: true registration: emulate-tpm: true emulated-tpm-seed: -1 machineInventoryLabels: manufacturer: &quot;${System Information/Manufacturer}&quot; productName: &quot;${System Information/Product Name}&quot; serialNumber: &quot;${System Information/Serial Number}&quot; machineUUID: &quot;${System Information/UUID}&quot; EOF   This creates a MachineRegistration object which will provide a unique URL which will be used with elemental-register to register the node during installation, so the operator can create a MachineInventory which will be using to bootstrap the node. See that the label has been see to match the selector here already, although it can always be added later to the MachineInventory.    NOTE: At this point the x86_64 and ARM64 quickstart differs because for x86_64 there is a SeedImage object that needs to be created and that doesn't exist for ARM64 (yet).  ","version":"Next","tagName":"h3"},{"title":"Preparing the installation image​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#preparing-the-installation-image","content":" Elemental's support for Raspberry Pi is primarily for demonstration purposes at this point. Therefore the installation process is modelled similar to x86-64. You boot from a seed image (USB stick in this case) and install to a storage medium (SD-card for Raspberry Pi).  NOTE: The steps below should to be ran in a linux machine (SLE Micro for example).  First step is to download the machineregistration object that will instruct where to get the config for the node to be installed:  curl -k $(kubectl get machineregistration -n fleet-default my-nodes -o jsonpath=&quot;{.status.registrationURL}&quot;) -o livecd-cloud-config.yaml     Then, the rpi.raw image is downloaded and checked the integrity just to be safe:  curl -Lk https://download.opensuse.org/repositories/isv:/Rancher:/Elemental:/Stable:/Teal53/images/rpi.raw -o rpi.raw curl -Lk https://download.opensuse.org/repositories/isv:/Rancher:/Elemental:/Stable:/Teal53/images/rpi.raw.sha256 -o rpi.raw.sha256 sha256sum -c rpi.raw.sha256   Finally, the livecd-cloud-config.yaml file is injected in the vanilla rpi.raw image:  IMAGE=rpi.raw DEST=$(mktemp -d) SECTORSIZE=$(sfdisk -J ${IMAGE} | jq '.partitiontable.sectorsize') DATAPARTITIONSTART=$(sfdisk -J ${IMAGE} | jq '.partitiontable.partitions[1].start') mount -o rw,loop,offset=$((${SECTORSIZE}*${DATAPARTITIONSTART})) ${IMAGE} ${DEST} mv livecd-cloud-config.yaml ${DEST}/livecd-cloud-config.yaml umount ${DEST}   NOTE: The rpi.raw image has two partitions. RPI_BOOT contains the boot loader files and COS_LIVE the Elemental files, where the livecd-cloud-config.yaml file needs to be copied.  ","version":"Next","tagName":"h3"},{"title":"UTM VM​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/elemental-utm-aarch64#utm-vm","content":" Then, a new UTM VM needs to be created and the rpi.raw file configured as USB.        Map the raw file as an ISO and configure the hardware as you please:    Set a proper name:    Finally, it is needed to configure the raw disk as USB:      NOTE: The operating system disk device should be the first one, then the USB, so the USB will boot just once as a fallback.  After a while, a new machineinventory host will be present:  kubectl get machineinventory -n fleet-default m-ed0a3f46-d6f8-4737-9884-e3a898094994 -o yaml apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventory metadata: annotations: elemental.cattle.io/registration-ip: 192.168.205.106 creationTimestamp: &quot;2023-05-03T14:04:56Z&quot; generation: 1 labels: machineUUID: ec49ff2a-e14f-42bf-8098-4162f14ee1f9 manufacturer: QEMU productName: QEMU-Virtual-Machine serialNumber: Not-Specified name: m-ed0a3f46-d6f8-4737-9884-e3a898094994 namespace: fleet-default resourceVersion: &quot;15848&quot; uid: 79608121-034d-4d64-8b48-6624607bbadd spec: tpmHash: a2e5b231dac4e90151454e2ebc76a6b118f7d1b826b810d22868b2d09b38b7f7 status: conditions: - lastTransitionTime: &quot;2023-05-03T14:07:45Z&quot; message: plan successfully applied reason: PlanSuccessfullyApplied status: &quot;True&quot; type: Ready - lastTransitionTime: &quot;2023-05-03T14:04:56Z&quot; message: Waiting to be adopted reason: WaitingToBeAdopted status: &quot;False&quot; type: AdoptionReady plan: checksum: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a secretRef: name: m-ed0a3f46-d6f8-4737-9884-e3a898094994 namespace: fleet-default state: Applied   Finally, labeling the machineinventory of the discovered new host will trigger the installation:  kubectl -n fleet-default label machineinventory $(kubectl get machineinventory -n fleet-default --no-headers -o custom-columns=&quot;:metadata.name&quot;) location=europe     kubectl get cluster -n fleet-default NAME READY KUBECONFIG my-cluster true my-cluster-kubeconfig   Profit!  kubectl get secret -n fleet-default my-cluster-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt;&gt; ~/my-cluster-kubeconfig KUBECONFIG=~/my-cluster-kubeconfig kubectl get nodes NAME STATUS ROLES AGE VERSION m-ed0a3f46-d6f8-4737-9884-e3a898094994 Ready control-plane,etcd,master,worker 6m25s v1.24.8+k3s1 KUBECONFIG=~/my-cluster-kubeconfig kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE cattle-fleet-system fleet-agent-7ffcdff7c5-2rvvl 1/1 Running 0 2m47s cattle-system apply-system-agent-upgrader-on-m-ed0a3f46-d6f8-4737-9884-1jhpkx 0/1 Completed 0 2m1s cattle-system cattle-cluster-agent-684c4687c8-scgvb 1/1 Running 0 61s cattle-system helm-operation-hjkcr 0/2 Completed 0 5m35s cattle-system rancher-webhook-85bb446df8-r8g6r 1/1 Running 0 5m22s cattle-system system-upgrade-controller-65bcf49944-rp2gr 1/1 Running 0 2m47s kube-system coredns-7b5bbc6644-2zdlk 1/1 Running 0 6m20s kube-system helm-install-traefik-crd-ksm4q 0/1 Completed 0 61s kube-system helm-install-traefik-kg4qv 0/1 Completed 0 61s kube-system local-path-provisioner-687d6d7765-j54vp 1/1 Running 0 6m20s kube-system metrics-server-84f8d4c4fc-6t6kc 1/1 Running 0 6m20s kube-system svclb-traefik-7ca8393f-gvdcc 2/2 Running 0 5m58s kube-system traefik-6b8f69d897-bwtgq 1/1 Running 0 5m58s    ","version":"Next","tagName":"h3"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/demo_setup/k3s-on-slemicro","content":"","keywords":"","version":"Next"},{"title":"K3s​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s","content":" K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances.  It is packaged as a single and small binary so installations and updates are fast and easy.  The installation procedure can be as simple as downloading the k3s binary and run it. However, the preferred way is to use the install script as it creates and configures a service.  The script supports different installation parameters to customize K3s, including HA support, install control-plane nodes, dedicated etcd nodes, agents, etc.  Once installed, the parameters and flags can be modified, added or removed just by changing the systemd unit file or the config file and restarting the service. Neat!  ","version":"Next","tagName":"h2"},{"title":"K3s on SLE Micro​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-on-sle-micro","content":" The installation scripts supports SLE Micro, it recognizes the underlying operating system, installs the k3s-selinux package using transactional-updates and creates the k3s or k3s-agent services.  NOTE: On SLE Micro, the install script doesn't start the k3s or k3s-agent service (ideally you should reboot the host once you run a transactional-update), but this can be override by using the INSTALL_K3S_SKIP_START=false environment variable.  ","version":"Next","tagName":"h2"},{"title":"K3s all-in-one​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-all-in-one","content":" The simplest way to run K3s is an all-in-one server (not suited for production environments) is by running:  curl -sfL https://get.k3s.io | sh -   A few environment variables to tweak our installation can be used as well as:  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -   The settings can be specified either as environment variables, command line flags, a configuration file, or both, it is just a personal choice:  curl -sfL https://get.k3s.io | sh -s - server --token foobar --cluster-init --write-kubeconfig-mode=644   write-kubeconfig-mode: &quot;0644&quot; cluster-init: true token: &quot;foobar&quot;   In this example:  write-kubeconfig-mode is self explanatory (the default is 0600)cluster-init enables clustering by deploying an embedded etcd databasetoken a random token is generated to be able to add nodes to the cluster, specifying it at installation time makes things easier as it is known upfront  The official documentation explains all the flags in detail.  ","version":"Next","tagName":"h2"},{"title":"Adding agents​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#adding-agents","content":" Adding an agent is as simple as running the install script with a few parameters, including the URL of the cluster as:  curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=foobar sh -   ","version":"Next","tagName":"h2"},{"title":"K3s HA​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-ha","content":" The easiest way to run a K3s HA cluster is by installing a first node using the --cluster-init flag and then, start adding nodes.  # First node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -   # Rest of the nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --server https://myserver:6443 --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -   # Agent nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;agent --server https://myserver:6443&quot; K3S_TOKEN=foobar sh -   This is what a cluster with 3 control-plane nodes and 2 agents looks like:  NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cp01 Ready control-plane,etcd,master 2m26s v1.26.4+k3s1 192.168.205.99 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 cp02 Ready control-plane,etcd,master 98s v1.26.4+k3s1 192.168.205.100 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 cp03 Ready control-plane,etcd,master 71s v1.26.4+k3s1 192.168.205.101 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 w01 Ready &lt;none&gt; 63s v1.26.4+k3s1 192.168.205.102 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 w02 Ready &lt;none&gt; 39s v1.26.4+k3s1 192.168.205.103 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1   ","version":"Next","tagName":"h2"},{"title":"K3s API HA​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-api-ha","content":" The previous section lacks an important detail, the Kubernetes API is served by the 3 control-plane nodes, but the API certificate is generated just for the first node. If the first node is down, the clients needs their API endpoint to be tweaked to point to another node (i.e.- for kubectl, using the -s flag or modifying the kubeconfig file) and the certificate won't be accepted as it doesn't contain the IP/hostname of that other node (it can be forced to be ignored using --insecure-skip-tls-verify=true for kubectl but that's not a good practice).  Ideally a mechanism to expose the K3s API in a high availability scenario is required. This usually means running a load balancer outside of the K3s cluster to serve and redirect the requests to the K3s API endpoints, so if one of the servers fail, the load balancer will re-route the requests to the other ones. This solves the HA problem but it adds complexity as it requires an external service, which sometimes is not available (typically in non-cloud environments such as baremetal deployments).  One approach can be to run a self-contained solution involving kube-vip to expose the K3s API over a virtual IP (optionally including a load balancer as well). This solves the HA problem but the certificate can still be a problem... but K3s got you covered. By using the --tls-san flag at K3s installation time, a list of IPs and/or hostnames can be provided for the certificate to be included as Subject Alternative Names, meaning the K3s API will be happily served from those IPs/hostnames, and if those are the ones being served by the VIP, the solution is now HA and certificate-proof! Let's see it in more detail in the next section.  NOTE: kube-vip can be used also to expose Kubernetes services, but this is out of scope of this document.  ","version":"Next","tagName":"h2"},{"title":"VIP reservation​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#vip-reservation","content":" The VIP needs to be an IP available in the same subnet than the one where the control plane hosts are running (this is technically not true for the VIP itself but for load-balancing).  NOTE: If you are using OSX to virtualize the SLE Micro OS where K3s is going to be installed, you can see the dhcp leases in the /var/db/dhcpd_leases file and the subnet range in the /Library/Preferences/SystemConfiguration/com.apple.vmnet.plist one. You can use a free IP in that range, but if you find a way to reserve an IP in that range, please open a GitHub issue or a pull request with instructions to do it!.  ","version":"Next","tagName":"h3"},{"title":"K3s installation - First node​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-installation---first-node","content":" The first step is to install K3s in HA and using the --tls-san flag as well. This flag can be repeated many times, so in this example will be used to add both the IP (192.168.205.10 in this example) and the DNS name of the VIP (using sslip.io as a poor's man DNS):  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --write-kubeconfig-mode=644 --tls-san=192.168.205.10 --tls-san=https://192.168.205.10.sslip.io&quot; K3S_TOKEN=foobar sh -   The rest of the nodes will be installed after kube-vip as the server URL for them to join the cluster will be the VIP.  ","version":"Next","tagName":"h3"},{"title":"Kube-vip installation​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#kube-vip-installation","content":" The official kube-vip documentation explains the steps in more detail, but essentially it means creating the required resource files for kube-vip to run (RBAC and a DaemonSet).  IMPORTANT: IPVS modules must be loaded in order for the load balancer feature to work. This is achieved by creating the following file:  cat &lt;&lt;- EOF &gt; /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF   Configurations stored under /etc/modules-load.d will be automatically picked up and loaded on boot. Loading them for the first time, however, can be achieved without rebooting by executing:  for i in $(cat /etc/modules-load.d/ipvs.conf); do modprobe ${i}; done   The Kubernetes resources can be created by leveraging K3s auto-deploy feature (aka. any manifest stored in a particular folder of the host /var/lib/rancher/k3s/server/manifests will be automatically deployed at the K3s service startup or when the file changes via something similar to kubectl apply -f).  NOTE: In this case, the --services flag for kube-vip won't be used.  export VIP=192.168.205.10 cat &lt;&lt;- EOF &gt; /var/lib/rancher/k3s/server/manifests/kube-vip.yaml apiVersion: v1 kind: ServiceAccount metadata: name: kube-vip namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; name: system:kube-vip-role rules: - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;, &quot;services/status&quot;, &quot;nodes&quot;, &quot;endpoints&quot;] verbs: [&quot;list&quot;,&quot;get&quot;,&quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;coordination.k8s.io&quot;] resources: [&quot;leases&quot;] verbs: [&quot;list&quot;, &quot;get&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-vip-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-vip-role subjects: - kind: ServiceAccount name: kube-vip namespace: kube-system --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app.kubernetes.io/name: kube-vip-ds app.kubernetes.io/version: v0.5.12 name: kube-vip-ds namespace: kube-system spec: selector: matchLabels: app.kubernetes.io/name: kube-vip-ds template: metadata: labels: app.kubernetes.io/name: kube-vip-ds app.kubernetes.io/version: v0.5.12 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/master operator: Exists - matchExpressions: - key: node-role.kubernetes.io/control-plane operator: Exists containers: - args: - manager env: - name: vip_arp value: &quot;true&quot; - name: port value: &quot;6443&quot; - name: vip_interface value: eth0 - name: vip_cidr value: &quot;32&quot; - name: cp_enable value: &quot;true&quot; - name: cp_namespace value: kube-system - name: vip_ddns value: &quot;false&quot; - name: vip_leaderelection value: &quot;true&quot; - name: vip_leaseduration value: &quot;5&quot; - name: vip_renewdeadline value: &quot;3&quot; - name: vip_retryperiod value: &quot;1&quot; - name: address value: ${VIP} - name: prometheus_server value: :2112 - name: lb_enable value: &quot;true&quot; image: ghcr.io/kube-vip/kube-vip:v0.5.12 imagePullPolicy: Always name: kube-vip securityContext: capabilities: add: - NET_ADMIN - NET_RAW hostNetwork: true serviceAccountName: kube-vip tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists EOF   ","version":"Next","tagName":"h3"},{"title":"K3s installation - Control-plane nodes​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-installation---control-plane-nodes","content":" Once kube-vip is in place, the rest of the control-plane nodes can be added to the cluster by pointing them to the VIP as:  export VIP=192.168.205.10 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --server https://${VIP}:6443 --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -   NOTE: For a real HA scenario, it is required for etcd to have an odd number of nodes, so it would be required to add two more control plane nodes.  After a while, the nodes will join the cluster successfully and an HA cluster will be ready.  ","version":"Next","tagName":"h3"},{"title":"Kubeconfig tweaks​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#kubeconfig-tweaks","content":" The kubeconfig file that is generated as part of the installation has localhost as the Kubernetes API endpoint, so in order to use it from outside, it needs to be changed to the VIP as:  MacOSSUSE scp 192.168.205.10:/etc/rancher/k3s/k3s.yaml ~/.kube/config &amp;&amp; sed -i '' 's/127.0.0.1/192.168.205.10/g' ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config   ","version":"Next","tagName":"h3"},{"title":"K3s installation - adding agents​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-installation---adding-agents","content":" Agents can be added as usual, pointing to the VIP address as:  export VIP=192.168.205.10 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;agent --server https://${VIP}:6443&quot; K3S_TOKEN=foobar sh -   ","version":"Next","tagName":"h3"},{"title":"Final picture​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#final-picture","content":" kubectl get nodes -o jsonpath=&quot;{.items[*].status.addresses[?(@.type=='InternalIP')].address}&quot; 192.168.205.69 192.168.205.70 192.168.205.71 192.168.205.72 192.168.205.73% kubectl cluster-info Kubernetes control plane is running at https://192.168.205.10:6443   As you can see, the control plane IP is the VIP and the nodes have their own IP. Sweet!  ","version":"Next","tagName":"h3"},{"title":"K3s tips​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#k3s-tips","content":" ","version":"Next","tagName":"h2"},{"title":"Access Traefik dashboard​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/k3s-on-slemicro#access-traefik-dashboard","content":" kubectl port-forward $(kubectl get pods --selector &quot;app.kubernetes.io/name=traefik&quot; -o=name -n kube-system) -n kube-system 9000:9000   Then, browse http://localhost:9000/dashboard to observe the Traefik dashboard:   ","version":"Next","tagName":"h3"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/demo_setup/slemicro-utm-aarch64","content":"","keywords":"","version":"Next"},{"title":"OSX Virtualization​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#osx-virtualization","content":" Virtualization of Linux hosts on OSX can be achieved with various tools. There are commercial products such as VMWare Fusion or Parallels Desktop as well as open-source projects such as VirtualBox, UTM or Lima.  UTM is an OSX application that uses QEMU under the hood and offers a GUI to manage the VM lifecycle. It supports Apple silicon CPUs, and it can use native OSX virtualization (Virtualization.framework) as well. It also has a scripting interface via Apple Script to automate some processes and a proper CLI (utmctl) is on the works.  Lima is based on QEMU (experimental support for Virtualization.framework) as well and it launches Linux virtual machines with automatic file sharing and port forwarding (like WSL2), and containerd. Lima is expected to be used on macOS hosts, but can be used on Linux hosts as well. Lima has a proper CLI tool (limactl) and the best part is VMs can be defined in yaml files, so you can even deploy K8s clusters with just a single command (see https://github.com/lima-vm/lima/blob/master/examples/k8s.yaml)  NOTE: Rancher desktop is based on Lima  However, Lima doesn't support SLE Micro (yet) as Lima customizes the VM at boot to install some packages and services and SLE Micro uses a different approach to those things (for example as it is immutable, it requires installing packages using ignition/combustion)  ","version":"Next","tagName":"h2"},{"title":"SLE Micro installation automation: ISO vs Image​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#sle-micro-installation-automation-iso-vs-image","content":" SLE Micro can be installed traditionally using an ISO file that boots once and using click-ops you can customize it as you wish (see https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#cha-install) but that won't be useful.  ISO installation can be customized using boot parameters (see https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#sec-boot-parameters-list) but those don't cover all the options. However, ISO based installation supports using AutoYaST (see https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-autoyast/) to automate the installation process.  SLE Micro can be also deployed using pre-built images. Currently, there are two types of images available: raw disk images and selfinstall ISOs.  SLE Micro raw images are delivered for the AMD64/Intel 64 architecture, IBM Z ZSeries and also AArch64, however the selfinstall images are currently delivered only for the AMD64/Intel 64 architecture. The pre-built images (both selfinstall ISOs and raw disk images) are intended to be configured on the first boot by using either Ignition or Combustion.  To summarize, the two ways as of today to deploy SLE Micro on Aarch64 on an automated fashion would be using the ISO + AutoYaST or raw images + Ignition/Combustion.  ","version":"Next","tagName":"h2"},{"title":"Ignition vs Butane vs Combustion​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#ignition-vs-butane-vs-combustion","content":" Ignition is a provisioning tool that enables you to configure a system according to your specification on the first boot. When the system is booted for the first time, Ignition is loaded as part of an initramfs and searches for a configuration file within a specific directory (on a USB flash disk, or you can provide a URL). All changes are performed before the kernel switches from the temporal file system to the real root file system (before the switch_root command is issued). Ignition uses a configuration file in the JSON format. The file is called config.ign. SLE Micro supportsIgnition config spec 3.3.0(seehttps://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#sec-ignition-configurationfor more information).  Ignition files can be complex to generate manually (specially for the file permissions syntax in hex or multiline things) so you can useopensuse.github.io/fuel-ignitionto help you generate a basic one.  Butane is a more human readable (and writable) configuration syntax based on yaml that can be translated to Ignition easily with the butane CLI as Butane is not consumable by Ignition.  Combustion is a dracut module that enables you to configure your system on its first boot. Combustion reads a provided file called scriptand executes commands in it and thus performs changes to the file system. You can use Combustion to change the default partitions, set users' passwords, create files, install packages, etc.  The Combustion dracut module is invoked after the ignition.firstbootargument is passed to the kernel command line. Combustion then reads the configuration from script. Combustion tries to configure the network, if the network flag has been found in script. After /sysroot is mounted, Combustion tries to activate all mount points in /etc/fstab and then call transactional-update to apply other changes (like setting root password or installing packages). See https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#sec-combustion-scriptfor more information.  ","version":"Next","tagName":"h2"},{"title":"Ignition/Combustion and UTM​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#ignitioncombustion-and-utm","content":" Ignition and Combustion are intended to automate the deployment of SLE Micro systems. To use them with UTM there are a couple of alternatives:  Use the QEMU fw_cfg flag as -fw_cfg name=opt/org.opensuse.combustion/script,file=/var/combustion-scriptfor combustion or -fw_cfg name=opt/com.coreos/config,file=PATH_TO_config.ign for ignition Create a raw disk or ISO file to host the Ignition or Combustion (or both) files.  For Ignition, the configuration file config.ign must reside in theignition subdirectory on the configuration media labeled ignition. The directory structure must look as follows:  &lt;root directory&gt; └── ignition └── config.ign   For Combustion, the configuration device needs to be named combustion, a specific directory structure in that configuration medium needs to be created and include a configuration file named script. In the root directory of the configuration medium, create a directory calledcombustion and place the script into this directory along with other files---SSH key, configuration files, etc. The directory structure then should look as follows:  &lt;root directory&gt; └── combustion └── script └── other files   Combustion can be used along with Ignition. If you intend to do so, label your configuration medium ignition and include the ignitiondirectory with the config.ign to your directory structure as shown below:  &lt;root directory&gt; └── combustion └── script └── other files └── ignition └── config.ign   In this scenario, Ignition runs before Combustion.  Image-based process step by step  NOTE: There is a helper script that automates all the steps included here.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#prerequisites","content":" SLE Micro raw image Download the raw image file from the SUSE website at https://www.suse.com/download/sle-micro/ Select ARM architectureLook for the raw file (I.e.- SLE-Micro.aarch64-5.3.0-Default-GM.raw.xz) NOTE: You need to have a valid user on the SUSE site to be able to download the file. Access to SCC.suse.com to generate a registration code Search for SUSE Linux Enterprise Micro via the Products menu, select the arch/version then copy and manually activate the registration code Butane, qemu and cdrtools installed (using brew for example) brew install butane cdrtools qemu UTM installed (using brew for example) brew install --cask utm   Note: If using the previous script, it is required to install UTM 4.2.2 at least as it includes the proper support for the automation.  ","version":"Next","tagName":"h2"},{"title":"Image preparation​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#image-preparation","content":" Uncompress the SLE Micro image xz -d ~/Downloads/SLE-Micro.*-Default-GM.raw.xz Move the file to a proper location and rename it to fit the VM hostname cp ~/Downloads/SLE-Micro.*-Default-GM.raw ~/VMs/slemicro.raw Resize the image file. In this example, to 30G qemu-img resize -f raw ~/VMs/slemicro.raw 30G &gt; /dev/null   ","version":"Next","tagName":"h2"},{"title":"Ignition & Combustion files​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#ignition--combustion-files","content":" To automate the installation, we will leverage Butane, Ignition and Combustion as explained before:  Create a temporary folder to store the assets TMPDIR=$(mktemp -d) Create the required folders for ignition and combustion mkdir -p ${TMPDIR}/{combustion,ignition} Create a config.fcc butane config file as required. See the following example to set a root password for the root user, and to configure the hostname to be &quot;slemicro&quot;' cat &lt;&lt; 'EOF' &gt; ${TMPDIR}/config.fcc variant: fcos version: 1.4.0 storage: files: - path: /etc/hostname mode: 0644 overwrite: true contents: inline: &quot;slemicro&quot; passwd: users: - name: root password_hash: &quot;$y$j9T$/t4THH10B7esLiIVBROsE.$G1lyxfy/MoFVOrfXSnWAUq70Tf3mjfZBIe18koGOuXB&quot; EOF Create a script combustion file as required. See the following example to register the SLE Micro instance to SUSE's SCC (use your own email/regcode) and to create a /etc/issue.d/combustion file cat &lt;&lt; EOF &gt; ${TMPDIR}/combustion/script #!/bin/bash # combustion: network # Redirect output to the console exec &gt; &gt;(exec tee -a /dev/tty0) 2&gt;&amp;1 # Set hostname at combustion phase for SUSEConnect hostname slemicro # Registration if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then zypper --non-interactive install suseconnect-ng fi SUSEConnect --email foobar@suse.com --url https://scc.suse.com --regcode YOURCODE # Leave a marker echo &quot;Configured with combustion&quot; &gt; /etc/issue.d/combustion EOF Convert the butane config to ignition butane -p -o ${TMPDIR}/ignition/config.ign ${TMPDIR}/config.fcc Create an ISO file. It is requried for both ignition and combustion to work that the ISO is labeled as ignition (hence the -V parameter) mkisofs -full-iso9660-filenames -o ignition-and-combustion.iso -V ignition ${TMPDIR} Optional: Remove the temporary folder rm -Rf ${TMPDIR}   ","version":"Next","tagName":"h2"},{"title":"VM Creation​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-utm-aarch64#vm-creation","content":" Now it is time to finally use UTM to boot the VM    Create a New Virtual Machine using Virtualization    Select &quot;Other&quot;    Enable the &quot;Skip ISO boot&quot; option as we will use the raw disk directly    Select the required CPU/RAM:    Accept the storage size as it is, it will be deleted before booting it    Skip the Shared Directory    Edit the VM name and enable the &quot;Open VM Settings&quot; toggle to customize it further:    Delete the VirtIO Drive    Add a new Drive and select &quot;Import&quot;    Select the raw image file (~/VMs/slemicro.raw in this case)    Repeat the last two steps to add the ignition-and-combustion.iso file    Configure the ISO as Read Only and &quot;CD/DVD (ISO) Image&quot;    Finally, boot the VM.  After a couple of seconds, the VM will boot up and will configure itself using the ignition and combustion scripts, including registering itself to SCC      NOTE: In case the VM doesn't get network connectivity, tryhttps://github.com/utmapp/UTM/discussions/3530#discussioncomment-5072113  NOTE: Once the VM is running, you can access via SSH via its IP as ssh root@&lt;ip&gt;  ISO Process (TBD)  Download the ISO fileCreate a new VM on UTM using the ISO fileCreate the autoyast answer fileUse the AutoYaST boot parameter to map to the answer fileBoot the VMProfit! ","version":"Next","tagName":"h2"},{"title":"Using the Linkerd Service Mesh","type":0,"sectionRef":"#","url":"/docs/integrations/linkerd","content":"Using the Linkerd Service Mesh","keywords":"","version":"Next"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/demo_setup/slemicro-virt-install-x86_64","content":"","keywords":"","version":"Next"},{"title":"Libvirtd​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#libvirtd","content":" The libvirtd program is the server side daemon component of the libvirt virtualization management system. This daemon runs on host servers and performs required management tasks for virtualized guests. This includes activities such as starting, stopping and migrating guests between host servers, configuring and manipulating networking, and managing storage for use by guests. The libvirt client libraries and utilities connect to this daemon to issue tasks and collect information about the configuration and resources of the host system and guests. (see https://libvirt.org/manpages/libvirtd.html)  ","version":"Next","tagName":"h2"},{"title":"Virt-install​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#virt-install","content":" virt-install is a command line tool for creating new KVM , Xen, or Linux container guests using the &quot;libvirt&quot; hypervisor management library. See the EXAMPLES section at the end of this document to quickly get started.virt-install tool supports both text based &amp; graphical installations, using VNC or SDL graphics, or a text serial console. The guest can be configured to use one or more virtual disks, network interfaces, audio devices, physical USB or PCI devices, among others. The installation media can be held locally or remotely on NFS , HTTP , FTP servers. In the latter case virt-install will fetch the minimal files necessary to kick off the installation process, allowing the guest to fetch the rest of the OS distribution as needed. PXE booting, and importing an existing disk image (thus skipping the install phase) are also supported.  To see more details about virt-install options, please visit https://linux.die.net/man/1/virt-installTo see more details about virt-manager and the graphical interface, please visit https://virt-manager.org/  Image-based process step by step  We have to create the image based and prepare the image with ignition and combustion files. Basically we will use the following documents as reference to create the image changing the base SLEMicro image to be downloaded (in this case will be SLE Micro x86_64):  Download the raw image file from the SUSE website at https://www.suse.com/download/sle-micro/ Select AMD64/Intel 64 architectureLook for the raw file (I.e.- SLE-Micro.x86_64-5.4.0-Default-GM.raw.xz)  NOTE: You need to have a valid user on the SUSE site to be able to download the file.  If you are trying to download to a remote server, you can use scp to copy that file to the server.  Access to SCC.suse.com to generate a registration code Search for SUSE Linux Enterprise Micro via the Products menu, select the arch/version then copy and manually activate the registration code Butane, qemu-img and cdrtools installed (using zypper for example)   sudo zypper install butane qemu-tools xz mkisofs   Unzip the file   xz -d SLE-Micro.x86_64-5.4.0-Default-GM.raw.xz   Resize the image file. In this example, to 30G   qemu-img resize -f raw ~/PATH-TO-FILE/SLE-Micro.x86_64-5.4.0-Default-GM.raw 30G &gt; /dev/null   ","version":"Next","tagName":"h2"},{"title":"Convert the raw image to qcow2​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#convert-the-raw-image-to-qcow2","content":" qemu-img convert -O qcow2 SLE-Micro.x86_64-5.4.0-Default-GM.raw slemicro   ","version":"Next","tagName":"h2"},{"title":"Ignition & Combustion files​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#ignition--combustion-files","content":" To automate the installation, we will leverage Butane, Ignition and Combustion as explained before:  Create a temporary folder to store the assets TMPDIR=$(mktemp -d) Create the required folders for ignition and combustion mkdir -p ${TMPDIR}/{combustion,ignition} Create a config.fcc butane config file as required. See the following example to set a root password for the root user, and to configure the hostname to be &quot;slemicro&quot;' cat &lt;&lt; 'EOF' &gt; ${TMPDIR}/config.fcc variant: fcos version: 1.4.0 storage: files: - path: /etc/hostname mode: 0644 overwrite: true contents: inline: &quot;slemicro&quot; passwd: users: - name: root password_hash: &quot;$y$j9T$/t4THH10B7esLiIVBROsE.$G1lyxfy/MoFVOrfXSnWAUq70Tf3mjfZBIe18koGOuXB&quot; EOF Create a script combustion file as required. See the following example to register the SLE Micro instance to SUSE's SCC (use your own email/regcode) and to create a /etc/issue.d/combustion file cat &lt;&lt; EOF &gt; ${TMPDIR}/combustion/script #!/bin/bash # combustion: network # Redirect output to the console exec &gt; &gt;(exec tee -a /dev/tty0) 2&gt;&amp;1 # Set hostname at combustion phase for SUSEConnect hostname slemicro # Registration if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then zypper --non-interactive install suseconnect-ng fi SUSEConnect --email foobar@suse.com --url https://scc.suse.com --regcode YOURCODE # Leave a marker echo &quot;Configured with combustion&quot; &gt; /etc/issue.d/combustion EOF Convert the butane config to ignition butane -p -o ${TMPDIR}/ignition/config.ign ${TMPDIR}/config.fcc Create an ISO file. It is requried for both ignition and combustion to work that the ISO is labeled as ignition (hence the -V parameter) mkisofs -full-iso9660-filenames -o ignition-and-combustion.iso -V ignition ${TMPDIR} Optional: Remove the temporary folder rm -Rf ${TMPDIR}   ","version":"Next","tagName":"h2"},{"title":"Create the VM​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#create-the-vm","content":" virt-install --name MyVM --memory 4096 --vcpus 4 --disk ./slemicro --import --cdrom ./ignition-and-combustion.iso --network default --osinfo detect=on,name=sle-unknown   NOTES:  Pass the --noautoconsole flag in case your console hangs on the installation, this will allow you to run other commands without CTRL-C interruptPass the --debug flag if you run into issuesIf you run into an issue and you need to restart, or if you get an error saying that MyVM is already running, run this command:   virsh destroy MyVM ; virsh undefine MyVM   After a couple of seconds, the VM will boot up and will configure itself using the ignition and combustion scripts, including registering itself to SCC  virsh list Id Nombre State ---------------------------------- 14 MyVM running   ","version":"Next","tagName":"h2"},{"title":"Access to the vm​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#access-to-the-vm","content":" You can access to the VM using virsh console:  virsh console MyVM Connected to domain MyVM   or using ssh directly and the user set in the ignition file (in this case root)  virsh domifaddr MyVM Nombre MAC address Protocol Address ------------------------------------------------------------------------------- vnet14 52:54:00:f0:be:e5 ipv4 192.168.122.221/24 ssh root@192.168.122.221   ","version":"Next","tagName":"h2"},{"title":"Delete the VM​","type":1,"pageTitle":"Intro","url":"/docs/demo_setup/slemicro-virt-install-x86_64#delete-the-vm","content":" virsh destroy MyVM ; virsh undefine MyVM  ","version":"Next","tagName":"h2"},{"title":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","type":0,"sectionRef":"#","url":"/docs/integrations/create-package-obs","content":"","keywords":"","version":"Next"},{"title":"openSUSE Build Service​","type":1,"pageTitle":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","url":"/docs/integrations/create-package-obs#opensuse-build-service","content":" The openSUSE Build Service is the public instance of the Open Build Service used for development of the openSUSE distribution and to offer packages from same source for Fedora, Debian, Ubuntu, SUSE Linux Enterprise and other distributions.  This service is also able to build container images, using either a Dockerfile or a KIWI configuration.  Everyone can create a SUSE IdP account to be able to use this service.  A published container image will be available on registry.opensuse.org and a published package would be available at download.opensuse.org  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","url":"/docs/integrations/create-package-obs#prerequisites","content":" In order to use OBS you first need an IdP account (sign up here) and you need to log into build.opensuse.org  You will also need the osc (for openSUSE Commander) command as this quickstart will do it the CLI way, but most things can be done in the WebUI if you prefer.  To install osc:  SUSEMacOSPIP zypper install osc   ","version":"Next","tagName":"h2"},{"title":"Create and configure a project​","type":1,"pageTitle":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","url":"/docs/integrations/create-package-obs#create-and-configure-a-project","content":" We are going to create a project under your home namespace, this will bring up your editor to configure it right away.  osc meta prj -e &quot;home:$USERNAME:$PROJECTNAME&quot;   note If you want to use your home project root just specify home:$USERNAME here and in following steps.  In the editor you can now fill the metadata to look similar to this:  &lt;project name=&quot;home:$USERNAME&quot;&gt; &lt;title/&gt; &lt;description/&gt; &lt;person userid=&quot;$USERNAME&quot; role=&quot;maintainer&quot;/&gt; &lt;!-- If you want to build RPM packages you need a block like this one, here for SLE-15 SP5 replace accordingly to the distribution you want to target --&gt; &lt;repository name=&quot;sp5&quot;&gt; &lt;path project=&quot;SUSE:SLE-15-SP5:Update&quot; repository=&quot;standard&quot;/&gt; &lt;path project=&quot;SUSE:SLE-15-SP5:GA&quot; repository=&quot;standard&quot;/&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;arch&gt;aarch64&lt;/arch&gt; &lt;/repository&gt; &lt;!-- If you want to build container images you need a block akin to this one --&gt; &lt;repository name=&quot;containers&quot;&gt; &lt;!-- This defines the available source images for the build (here any from registry.suse.com) --&gt; &lt;path project=&quot;SUSE:Registry&quot; repository=&quot;standard&quot;/&gt; &lt;!-- This defines package repositories available during build, I am refering to the one above here so I can use the RPM packages published in this project for the container images of the project --&gt; &lt;path project=&quot;home:$USERNAME:$PROJECTNAME&quot; repository=&quot;sp4&quot;/&gt; &lt;!-- This is the list of architecture you want to build for --&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;arch&gt;aarch64&lt;/arch&gt; &lt;/repository&gt; &lt;/project&gt;   If you want to build containers you need to tweak the configuration of the project as well:  osc meta prjconf -e &quot;home:$USERNAME:$PROJECTNAME&quot;   The configuration is different whether you want to use KIWI or Dockerfile build system:  DockerfileKiwi %if &quot;%_repository&quot; == &quot;containers&quot; Type: docker Repotype: none Patterntype: none BuildEngine: podman %endif   note If you want to build containers using both KIWI and Dockerfiles in the same project, you need two repositories in your project's metadata (with different names) and both snippets in project's configuration (one for each repository).  ","version":"Next","tagName":"h2"},{"title":"Create a package​","type":1,"pageTitle":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","url":"/docs/integrations/create-package-obs#create-a-package","content":" To create a package in your project use the following command:  osc meta pkg -e home:$USERNAME:$PROJECTNAME $PACKAGENAME   There you'll get another XML file to edit, you only have to set a title and description here.  Now you can checkout the directory to start adding your files:  osc co home:$USERNAME:$PROJECTNAME/$PACKAGENAME   Now go into the directory and when all is ready you can add your files and commit using:  osc add &lt;files&gt;... osc ci   Now let's see the specificities of RPM and Container packages  ","version":"Next","tagName":"h2"},{"title":"RPM package​","type":1,"pageTitle":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","url":"/docs/integrations/create-package-obs#rpm-package","content":" An RPM package is defined by the presence of a spec file, I will not go into the details of that file as this is way beyond the scope of that quickstart, please refer to https://en.opensuse.org/Portal:Packaging for more details about packaging.  I will however get into more details about the _service and _constraints special files that may change the behavior of OBS.  The _service file allows one to define automation to happen on said time, for RPM packages these are usually manually triggered. It is then possible to automate fetching a git repository into a tarball, updating the specfile version from git info, vendoring go or rust dependencies, etc...You can get more insight into what is possible herehttps://en.opensuse.org/openSUSE:Build_Service_Concept_SourceService .  Here is one for a rust project for example:  _services &lt;services&gt; &lt;service name=&quot;tar_scm&quot; mode=&quot;manual&quot;&gt; &lt;param name=&quot;scm&quot;&gt;git&lt;/param&gt; &lt;param name=&quot;url&quot;&gt;https://github.com/project-akri/akri&lt;/param&gt; &lt;param name=&quot;filename&quot;&gt;akri&lt;/param&gt; &lt;param name=&quot;versionformat&quot;&gt;@PARENT_TAG@&lt;/param&gt; &lt;param name=&quot;versionrewrite-pattern&quot;&gt;v(.*)&lt;/param&gt; &lt;param name=&quot;revision&quot;&gt;v0.10.4&lt;/param&gt; &lt;/service&gt; &lt;service name=&quot;recompress&quot; mode=&quot;manual&quot;&gt; &lt;param name=&quot;file&quot;&gt;*.tar&lt;/param&gt; &lt;param name=&quot;compression&quot;&gt;xz&lt;/param&gt; &lt;/service&gt; &lt;service name=&quot;set_version&quot; mode=&quot;manual&quot; /&gt; &lt;service name=&quot;cargo_vendor&quot; mode=&quot;manual&quot;&gt; &lt;param name=&quot;srcdir&quot;&gt;akri&lt;/param&gt; &lt;param name=&quot;compression&quot;&gt;xz&lt;/param&gt; &lt;/service&gt; &lt;/services&gt;   The _constraints file allow to define &quot;restrictions&quot; about the builder selected by OBS, like for example the disk size, if your build complains about having not enough space, this is the file you should edit/create. See here for the complete guide: https://openbuildservice.org/help/manuals/obs-user-guide/cha.obs.build_job_constraints.html  ","version":"Next","tagName":"h3"},{"title":"Container image​","type":1,"pageTitle":"Create a package (RPM or Container image) using OBS (openSUSE Build Service)","url":"/docs/integrations/create-package-obs#container-image","content":" You can build a container image in two different ways, you can either use a Dockerfile or a KIWI configuration.  Each method has their own benefits and drawbacks. Kiwi supports using the package manager from the host/build system, so it can build base images and derive images which don't contain a package manager, like opensuse/busybox. With Dockerfile, it's practically required to use a full base image like opensuse/tumbleweed.  I won't go into details how a Dockerfile or a kiwi build works, I'll just tell about the interaction with OBS.  First the kiwi_metainfo_helper service that you can add as a buildtime source service allows to substitute buildtime placeholders to use in you Dockerfile or kiwi configuration. You can find a list of the placeholders here: https://build.opensuse.org/package/view_file/openSUSE:Factory/obs-service-kiwi_metainfo_helper/README  Another useful service is the replace_using_package_version service, that allows to replace a placeholder with the version of a RPM package. For example if I have foobar package in version 1.2.3 in an available RPM repository, I can use this service to automatically tag an image that has this package installed. Here it would replace %PKG_VERSION% to 1.2.  _services &lt;services&gt; &lt;service mode=&quot;buildtime&quot; name=&quot;kiwi_metainfo_helper&quot;/&gt; &lt;service mode=&quot;buildtime&quot; name=&quot;replace_using_package_version&quot;&gt; &lt;param name=&quot;file&quot;&gt;Dockerfile&lt;/param&gt; &lt;param name=&quot;regex&quot;&gt;%PKG_VERSION%&lt;/param&gt; &lt;param name=&quot;parse-version&quot;&gt;minor&lt;/param&gt; &lt;param name=&quot;package&quot;&gt;foobar&lt;/param&gt; &lt;/service&gt; &lt;/services&gt;   You now have to tell OBS about the name and tag of your image:  DockerfileKiwi You can use one or multiple BuildTag as comments in your Dockerfilelike this: #!BuildTag: foo/bar:latest foo/bar:%PKG_VERSION%.%RELEASE% #!BuildTag: foo/bar:tag foo/bar:anothertag  ","version":"Next","tagName":"h3"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/integrations/nats","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#architecture","content":" NATS is an infrastructure that allows data exchange between applications in the form of messages.  ","version":"Next","tagName":"h2"},{"title":"NATS Client Applications​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#nats-client-applications","content":" NATS client libraries can be used to allow the applications to publish, subscribe, request, and reply between different instances. These applications are generally referred to as client applications.  ","version":"Next","tagName":"h3"},{"title":"NATS Service Infrastructure​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#nats-service-infrastructure","content":" The NATS services are provided by one or more NATS server processes that are configured to interconnect with each other and provide a NATS service infrastructure. The NATS service infrastructure can scale from a single NATS server process running on an end device to a public global super-cluster of many clusters spanning all major cloud providers and all regions of the world.  ","version":"Next","tagName":"h3"},{"title":"Simple messaging design​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#simple-messaging-design","content":" NATS makes it easy for applications to communicate by sending and receiving messages. These messages are addressed and identified by subject strings and do not depend on network location. Data is encoded and framed as a message and sent by a publisher. The message is received, decoded, and processed by one or more subscribers.  ","version":"Next","tagName":"h3"},{"title":"NATS JetStream​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#nats-jetstream","content":" NATS has a built-in distributed persistence system called JetStream. JetStream was created to solve the problems identified with streaming in technology today - complexity, fragility, and a lack of scalability. JetStream also solves the problem with the coupling of the publisher and the subscriber (the subscribers need to be up and running to receive the message when it is published). More information about NATS JetStream can be found here.  ","version":"Next","tagName":"h3"},{"title":"Installation​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#installation","content":" ","version":"Next","tagName":"h2"},{"title":"Install NATS on top of K3s​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#install-nats-on-top-of-k3s","content":" NATS is built for multiple architectures, so it can easily be installed on the the K3s on SLE Micro Setup.  Let's create a values file that will be used for overwriting the default values of NATS.  cat &gt; values.yaml &lt;&lt;EOF cluster: # Enable the HA setup of the NATS enabled: true replicas: 3 nats: jetstream: # Enable JetStream enabled: true memStorage: enabled: true size: 2Gi fileStorage: enabled: true size: 1Gi storageDirectory: /data/ EOF   Now let's install NATS via helm:  helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm install nats nats/nats --namespace nats --values values.yaml --create-namespace   With the values.yaml file above, the following components will be in the nats namespace:  HA version of NATS Statefulset containing 3 containers: NATS server + Config reloader and Metrics sidecars.NATS box container which comes with a set of NATS utilities that can be used to verify the setup.JetStream Key-Value backend also will be enabled which comes with PVCs bounded to the pods.  Test the setup​  kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l  # Create a subscription for the test subject nats sub test &amp; # Send a message to the test subject nats pub test hi   Clean up​  helm -n nats uninstall nats rm values.yaml   ","version":"Next","tagName":"h3"},{"title":"NATS as a backend for K3s​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nats#nats-as-a-backend-for-k3s","content":" One component K3s leverages is KINE, which is a shim enabling the replacement of etcd with alternate storage backends originally targeting relational databases. As JetStream provides a Key Value API, this makes it possible to have NATS as a backend for the K3s cluster.  There is already merged PR which makes the built-in NATS in K3s straight-forward but the change is still not included in the K3s releases.  For this reason, the K3s binary should be built manually.  In this tutorial, SLE Micro on OSX on Apple Silicon (UTM) VM will be used.  NOTE: Run the commands bellow on the OSX PC.  Build K3s​  git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s # The following command will add `nats` in the build tags which will enable the NATS built-in feature in K3s sed -i '' 's/TAGS=&quot;ctrd/TAGS=&quot;nats ctrd/g' scripts/build make local # Replace &lt;node-ip&gt; with the actual IP of the node where the K3s will be started export NODE_IP=&lt;node-ip&gt; sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s   NOTE: Locally building K3s requires the buildx Docker CLI plugin. It can be manually installed if $ make local fails.  Install NATS CLI​  TMPDIR=$(mktemp -d) nats_version=&quot;nats-0.0.35-linux-arm64&quot; curl -o &quot;${TMPDIR}/nats.zip&quot; -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip unzip &quot;${TMPDIR}/nats.zip&quot; -d &quot;${TMPDIR}&quot; sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats rm -rf ${TMPDIR}   Run NATS as K3s backend​  Let's ssh on the node and run the K3s with the --datastore-endpoint flag pointing to nats.  NOTE: The command below will start K3s as a foreground process, so the logs can be easily followed to see if there are any issues. If you want to not block the current terminal a &amp; flag could be added before the command to start it as a background process.  k3s server --datastore-endpoint=nats://  NOTE: For making the K3s server with the NATS backend permanent on your slemicro VM, the script below can be run, which will create a systemd service with the needed configurations.  export INSTALL_K3S_SKIP_START=false export INSTALL_K3S_SKIP_DOWNLOAD=true curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --datastore-endpoint=nats://&quot; sh -   Troubleshooting​  The following commands can be run on the node to verify that everything with the stream is working properly:  nats str report -a nats str view -a  ","version":"Next","tagName":"h3"},{"title":"Cluster API core concepts","type":0,"sectionRef":"#","url":"/docs/misc/cluster-api-concepts","content":"","keywords":"","version":"Next"},{"title":"Intro​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#intro","content":" Via the official docs:  Cluster API is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify provisioning, upgrading, and operating multiple Kubernetes clusters. Started by the Kubernetes Special Interest Group (SIG) Cluster Lifecycle, the Cluster API project uses Kubernetes-style APIs and patterns to automate cluster lifecycle management for platform operators. The supporting infrastructure, like virtual machines, networks, load balancers, and VPCs, as well as the Kubernetes cluster configuration are all defined in the same way that application developers operate deploying and managing their workloads. This enables consistent and repeatable cluster deployments across a wide variety of infrastructure environments.  ","version":"Next","tagName":"h2"},{"title":"Cluster types​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#cluster-types","content":" ","version":"Next","tagName":"h2"},{"title":"Management Cluster​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#management-cluster","content":" A Management cluster manages the state and lifecycle of Workload clusters using components called providers. Each Management cluster stores and reconciles the Cluster API resources (e.g. Machine, MachineDeployment, etc.) of Workload clusters by running one or more providers.  ","version":"Next","tagName":"h3"},{"title":"Workload Cluster​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#workload-cluster","content":" Workload clusters, as the name suggests, are used to run and orchestrate the application workloads of the user. Workload clusters, in the context of Cluster API, are always managed by a Management cluster.  ","version":"Next","tagName":"h3"},{"title":"Providers​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#providers","content":" ","version":"Next","tagName":"h2"},{"title":"Infrastructure Provider​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#infrastructure-provider","content":" Infrastructure providers are responsible for provisioning the necessary infrastructure and compute resources. Each node, regardless of its type (e.g. a VM or baremetal), requires specific configuration options which these providers use during the provisioning process e.g. OS image and checksum, network settings, etc.  A popular and widely adopted baremetal infrastructure provider is theCAPM3 project (Cluster API Provider Metal³). It enables users to deploy a Cluster API based cluster using Metal3.  ","version":"Next","tagName":"h3"},{"title":"Bootstrap Provider​","type":1,"pageTitle":"Cluster API core concepts","url":"/docs/misc/cluster-api-concepts#bootstrap-provider","content":" Bootstrap providers are responsible for turning a fully provisioned server into a Kubernetes node. This includes, but is not limited to, configuring, initializing and joining control plane and worker nodes, generating kubeconfig and cluster certificates, etc.  The CAPRKE2 project (Cluster API Provider RKE2) aims to provide both Control Plane and Bootstrap providers for RKE2 based clusters. It is currently in early development by the Rancher team. ","version":"Next","tagName":"h3"},{"title":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","type":0,"sectionRef":"#","url":"/docs/misc/create-a-simple-container-image-obs","content":"","keywords":"","version":"Next"},{"title":"Create the project to host the assets​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#create-the-project-to-host-the-assets","content":" In this case it will be a subproject of the &quot;home:foobar&quot; project  Go to https://build.opensuse.org/Log inSelect &quot;Your Home Project&quot; (Left Nav menu)Select the &quot;Subprojects&quot; tabPress &quot;Create Subproject&quot; (In Left Nav menu);Fill in the name (e.g. containers).  ","version":"Next","tagName":"h2"},{"title":"Enable container builds in the project config​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#enable-container-builds-in-the-project-config","content":" Select &quot;Your Home Project&quot; (Left Nav menu)Select the &quot;Subprojects&quot; tabSelect the subproject you have created (e.g. containers)Select the &quot;Project Config&quot; tabPaste the following code:  %if &quot;%_repository&quot; == &quot;images&quot; Type: docker Repotype: none Patterntype: none BuildEngine: podman %endif   ","version":"Next","tagName":"h2"},{"title":"Add the Tumbleweed images repository​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#add-the-tumbleweed-images-repository","content":" Go to the subproject home page (e.g. https://build.opensuse.org/repositories/home:foobar:containers)Select the &quot;Repositories&quot; tabPress the &quot;Add from a Project&quot; buttonFill in &quot;Project&quot; field with &quot;openSUSE:Templates:Images:Tumbleweed&quot;Choose &quot;images&quot; in Repositories dropdownRename it as &quot;images&quot; (this is important as it will be later on used in the registry path)Unselect all the architectures you don't need  ","version":"Next","tagName":"h2"},{"title":"Create a package for the subproject​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#create-a-package-for-the-subproject","content":" Go to https://build.opensuse.org/project/show/home:foobar:containersPress &quot;Create Package&quot; buttonFill in the name (e.g. mytoolbox).  ","version":"Next","tagName":"h2"},{"title":"Create the Dockerfile​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#create-the-dockerfile","content":" Create a simple Dockerfile locally, something like:  # The container image tag needs to be specified as follows: #!BuildTag: mytoolbox:latest FROM opensuse/tumbleweed RUN zypper -n in traceroute iputils netcat-openbsd curl &amp;&amp; \\ zypper clean -a   ","version":"Next","tagName":"h2"},{"title":"Upload the Dockerfile​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#upload-the-dockerfile","content":" Go to https://build.opensuse.org/package/show/home:foobar:containers/mytoolboxPress the &quot;Add File&quot; buttonChoose the file and upload it  ","version":"Next","tagName":"h2"},{"title":"Build results​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#build-results","content":" Go to https://build.opensuse.org/package/show/home:foobar:containers/mytoolboxImages will appear in Build Results sectionPress the &quot;Refresh&quot; button in Build Results sectionWait for build results.  ","version":"Next","tagName":"h2"},{"title":"Resulting images​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#resulting-images","content":" If everything went as it should, the container image will be hosted at the openSUSE registry  Go to https://registry.opensuse.org/On the search bar, type &quot;project=^home:foobar:&quot; and press enterClick on the package icon or name (home/foobar/containers/images/mytoolbox)Expand the tag (latest) to see the Image IDs, arch, build time, etc as well as the podman pull command.  ","version":"Next","tagName":"h2"},{"title":"Modify the Dockerfile file via CLI​","type":1,"pageTitle":"Create a simple container image based on Tumbleweed using OBS (openSUSE Build Service)","url":"/docs/misc/create-a-simple-container-image-obs#modify-the-dockerfile-file-via-cli","content":" Install osc via your favourite package manager (see https://en.opensuse.org/openSUSE:OSC)Run osc checkout home:foobar:containers. It will ask your username/password and a method to store the password safely.Navigate to the downloaded project cd home\\:foobar\\:containers/mytoolbox/Edit the Dockerfile as you pleaseRun osc commit and put a proper commit messageThe build will be automatically triggered ","version":"Next","tagName":"h2"},{"title":"Requisites","type":0,"sectionRef":"#","url":"/docs/misc/modify-sle-micro-iso","content":"⚠️ This is totally unsupported. Via elemental-iso-add-registration. Requisites SLE Micro ISO (or any SLE ISO)xorriso. It can be installed with zypper or via the registry.opensuse.org/isv/rancher/elemental/stable/teal53/15.4/rancher/elemental-builder-image/5.3:latest container image. Usage Imagine you want to modify the /boot/grub2/grub.cfg file. You just need to: mount the ISO somewhere ISO=${${HOME}/SLE-Micro.x86_64-5.4.0-Default-SelfInstall-GM.install.iso} DIR=$(mktemp -d) sudo mount ${ISO} ${DIR} extract the file cp ${DIR}/boot/grub2/grub.cfg /tmp/mygrub.cfg perform the modifications as neededUmount the ISO (not really needed) sudo umount ${DIR} rmdir ${DIR} rebuild the ISO as xorriso -indev ${ISO} -outdev SLE-Micro-tweaked.iso -map /tmp/mygrub.cfg /boot/grub2/grub.cfg -boot_image any replay xorriso 1.4.6 : RockRidge filesystem manipulator, libburnia project. xorriso : NOTE : ISO image bears MBR with -boot_image any partition_offset=16 xorriso : NOTE : Loading ISO image tree from LBA 0 xorriso : UPDATE : 371 nodes read in 1 seconds libisofs: WARNING : Found hidden El-Torito image. Its size could not be figured out, so image modify or boot image patching may lead to bad results. xorriso : NOTE : Detected El-Torito boot information which currently is set to be discarded Drive current: -indev './SLE-Micro.x86_64-5.4.0-Default-RT-SelfInstall-GM.install.iso' Media current: stdio file, overwriteable Media status : is written , is appendable Boot record : El Torito , MBR grub2-mbr cyl-align-off Media summary: 1 session, 494584 data blocks, 966m data, 114g free Volume id : 'INSTALL' Drive current: -outdev 'SLE-Micro-tweaked.iso' Media current: stdio file, overwriteable Media status : is blank Media summary: 0 sessions, 0 data blocks, 0 data, 114g free xorriso : UPDATE : 1 files added in 1 seconds Added to ISO image: file '/boot/grub2/grub.cfg'='/tmp/mygrub.cfg' xorriso : NOTE : Replayed 21 boot related commands xorriso : NOTE : Copying to System Area: 32768 bytes from file '--interval:imported_iso:0s-15s:zero_mbrpt:./SLE-Micro.x86_64-5.4.0-Default-RT-SelfInstall-GM.install.iso' xorriso : NOTE : Preserving in ISO image: -boot_image any partition_offset=16 xorriso : UPDATE : Writing: 32768s 6.5% fifo 100% buf 50% xorriso : UPDATE : Writing: 67205s 13.3% fifo 96% buf 50% xorriso : UPDATE : Writing: 442368s 87.6% fifo 100% buf 50% 553.8xD ISO image produced: 504777 sectors Written to medium : 504784 sectors at LBA 48 Writing to 'SLE-Micro-tweaked.iso' completed successfully. ","keywords":"","version":"Next"},{"title":"Create a custom single-iso image (using SLE Micro installer and combustion image) to use it on Virtual CD-ROM","type":0,"sectionRef":"#","url":"/docs/misc/create-a-single-iso-image-customized","content":"","keywords":"","version":"Next"},{"title":"Clone the SLE Micro installer repository from OBS​","type":1,"pageTitle":"Create a custom single-iso image (using SLE Micro installer and combustion image) to use it on Virtual CD-ROM","url":"/docs/misc/create-a-single-iso-image-customized#clone-the-sle-micro-installer-repository-from-obs","content":" Log in to OBSGo to the SLE Micro installer repositoryCreate a branch from this project to link 2 packages (combustion and SLE-Micro) to modify the combustion package to add some extra code. Then we need to link the SLE Micro image to be able to build a new image with the combustion package modified.  To create the link between the 2 packages, go to the Meta tab and then add the next lines:  &lt;repository name=&quot;standard&quot;&gt; &lt;path project=&quot;SUSE:SLE-15-SP4:Update:Products:Micro54&quot; repository=&quot;standard&quot;/&gt; &lt;arch&gt;aarch64&lt;/arch&gt; &lt;arch&gt;ppc64le&lt;/arch&gt; &lt;arch&gt;s390x&lt;/arch&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;/repository&gt; &lt;repository name=&quot;images&quot; rebuild=&quot;local&quot;&gt; &lt;path project=&quot;home:&lt;user&gt;:branches:SUSE:SLE-15-SP4:Update:Products:Micro54&quot; repository=&quot;standard&quot;/&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;/repository&gt;   After that click on Save and then you should see something like:    Now, for any modification to the combustion package, the SLE Micro image will be automatically rebuilt with the new combustion package changes.  ","version":"Next","tagName":"h2"},{"title":"Modify the combustion package​","type":1,"pageTitle":"Create a custom single-iso image (using SLE Micro installer and combustion image) to use it on Virtual CD-ROM","url":"/docs/misc/create-a-single-iso-image-customized#modify-the-combustion-package","content":" To modify the combustion package, we need to go to the combustion package and then download the next file:  osc getbinaries home:&lt;user&gt;:branches:SUSE:SLE-15-SP4:Update:Products:Micro54 combustion standard x86_64 combustion-1.0+git2.obscpio   This file contains the combustion image that will be used by the SLE Micro installer to create the final image.  To extract the content of this file, we need to execute the next command:  cpio -idmv &lt; combustion-1.0+git2.obscpio   After that, we should see something like:  $ ls -l total 68 drwxr-xr-x 4096 sep 14 13:20 . drwx------. 4096 sep 14 13:20 .. -rw-r--r-- 6064 sep 12 16:09 combustion -rw-r--r-- 512 sep 12 16:07 combustion-1.0+git2.obscpio -rw-r--r-- 1032 sep 12 16:07 combustion-prepare.service -rw-r--r-- 1488 sep 12 16:07 combustion.rules -rw-r--r-- 1009 sep 12 16:07 combustion.service -rw-r--r-- 18092 sep 12 16:07 LICENSE -rw-r--r-- 408 sep 12 16:07 Makefile -rw-r--r-- 1240 sep 14 13:20 module-setup.sh -rw-r--r-- 4686 sep 12 16:07 README.md   Let's change the next things:  Timeout to wait for the config drive from 10 to 15 seconds  sed -i 's/devtimeout=10/devtimeout=15/g' module-setup.sh  Combustion labels to be able to mount the config drive adding the labels install and INSTALL  ... ... for label in combustion COMBUSTION ignition IGNITION install INSTALL; do ... ...   After changing the code, we need to create a new combustion-1.0+git2.obscpio file:  find combustion-1.0+git2 -type f -print | cpio -ocv &gt; combustion-1.0+git2.obscpio   And upload again to the combustion package OBS to build a new package with the modifications  osc add combustion-1.0+git2.obscpio osc commit -m &quot;Update combustion-1.0+git2.obscpio&quot;   After that you should see a new build is running:  osc results   ","version":"Next","tagName":"h2"},{"title":"Build a new SLE Micro OBS custom image with the new combustion package modified​","type":1,"pageTitle":"Create a custom single-iso image (using SLE Micro installer and combustion image) to use it on Virtual CD-ROM","url":"/docs/misc/create-a-single-iso-image-customized#build-a-new-sle-micro-obs-custom-image-with-the-new-combustion-package-modified","content":" After the combustion package is built, we need to rebuild a new SLE Micro image with the new combustion package.  To do that you can go to the SLE-Micro package and then click on Trigger Rebuild and then select the images repository and then click on Trigger Rebuild again. Another easier option to do that, is to modify the SLE-Micro.changes to add some information about the new combustion changes and then commit the changes and then the image will be automatically rebuilt.  ","version":"Next","tagName":"h2"},{"title":"Download the new iso image to prepare it with xorriso and adding combustion​","type":1,"pageTitle":"Create a custom single-iso image (using SLE Micro installer and combustion image) to use it on Virtual CD-ROM","url":"/docs/misc/create-a-single-iso-image-customized#download-the-new-iso-image-to-prepare-it-with-xorriso-and-adding-combustion","content":" After the image is built, we need to download the new iso image to prepare it with xorriso and adding combustion. To do that, we need to go to the images repository and then download the new iso image.  Now, we should have the next files to generate the final single-iso image:  Input: SLE-Micro.x86_64-5.4.0-Default-SelfInstall-Build15.1.install.iso (This is the new build image with the combustion package modified)combustion folder with the next structure: ./script (this is the combustion script with the tasks we want to execute during the installation in the combustion phase) Output: SLE-Micro-Selfinstall-with-mycombustion-single-iso.iso (This is the final single-iso image with the combustion script included and the installer iso image)  Using xorriso we will create the final single-iso:  xorriso -indev ./SLE-Micro.x86_64-5.4.0-Default-SelfInstall-Build15.1.install.iso \\ -outdev ./SLE-Micro-Selfinstall-with-mycombustion-single-iso.iso \\ -map ~/my-local-path/combustion /combustion \\ -boot_image any replay -changes_pending yes   After that, we should have the final iso image with the combustion script included SLE-Micro-Selfinstall-with-mycombustion-single-iso.iso ","version":"Next","tagName":"h2"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/misc/metallb-kube-api","content":"","keywords":"","version":"Next"},{"title":"MetalLB in front of the Kubernetes API server​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#metallb-in-front-of-the-kubernetes-api-server","content":" In this guide a MetalLB service will be used to expose the K3s API externally on an HA K3s cluster with 3 control-plane nodes. To achieve this, a Kubernetes Service of type LoadBalancer and Endpoints will be manually created. The Endpoints will keep the IPs of all control plane nodes available in the cluster. In order for the Endpoint to be continuously synchronized with the events occurring in the cluster (adding/removing a node or a node goes offline), the Endpoint Copier Operator will be deployed. The operator monitors the events happening in the default kubernetes Endpoint and updates the managed one automatically to keep them in sync. Since the managed Service will be of type LoadBalancer, MetalLB will assign it a static ExternalIP. This ExternalIP will be used in order to communicate with the API Server.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#prerequisites","content":" 3 hosts to deploy K3s on top. Hint SLE Micro on OSX on Apple Silicon (UTM) can be used. Ensure the hosts have different hostnames. At least 2 available IPs in the network (One for the Traefik and one for the managed service).Helm  ","version":"Next","tagName":"h3"},{"title":"Install K3s​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#install-k3s","content":" NOTE: In case you don't want a fresh cluster but want to use an already existing one, just skip this step and proceed to the next one.  First a free IP in the network must be reserved that will be used later for ExternalIP of the managed Service.  SSH to the first host and install K3s in cluster mode as:  # Export the free IP mentioned above export VIP_SERVICE_IP=&lt;ip&gt; export INSTALL_K3S_SKIP_START=false curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} --tls-san=https://${VIP_SERVICE_IP}.sslip.io&quot; K3S_TOKEN=foobar sh -   NOTE: Make sure that --disable=servicelb flag is provided in the k3s server command.  NOTE: From now on, the commands should be run on the local machine.  In order to access the API server from outside, the IP of the K3s VM will be used.  # Replace &lt;node-ip&gt; with the actual IP of the machine export NODE_IP=&lt;node-ip&gt; scp ${NODE_IP}:/etc/rancher/k3s/k3s.yaml ~/.kube/config &amp;&amp; sed -i '' &quot;s/127.0.0.1/${NODE_IP}/g&quot; ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config   ","version":"Next","tagName":"h3"},{"title":"Configure existing K3s cluster​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#configure-existing-k3s-cluster","content":" NOTE: This step is valid only if you want to use an existing K3s cluster.  To use an existing K3s cluster, the servicelb lb should be disabled and also tls-san flags modified.  In order to change the K3s flags, /etc/systemd/system/k3s.service should be modified on all of the VMs in the cluster.  The flags should be inserted in the ExecStart. For example:  # Replace the &lt;vip-service-ip&gt; with the actual ip ExecStart=/usr/local/bin/k3s \\ server \\ '--cluster-init' \\ '--write-kubeconfig-mode=644' \\ '--disable=servicelb' \\ '--tls-san=&lt;vip-service-ip&gt;' \\ '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \\   Then the following commands should be executed in order for K3S to load the new configurations:  systemctl daemon-reload systemctl restart k3s   ","version":"Next","tagName":"h3"},{"title":"Install MetalLB​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#install-metallb","content":" To deploy MetalLB, the MetalLB on K3s guide can be used.  NOTE: Ensure that the IP addresses of the ip-pool IPAddressPool do not overlap with the IP addresses previously selected for the LoadBalancer service.  Create a separate IpAddressPool that will be used only for the managed Service.  # Export the VIP_SERVICE_IP on the local machine # Replace with the actual IP export VIP_SERVICE_IP=&lt;ip&gt; cat &lt;&lt;-EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: kubernetes-vip-ip-pool namespace: metallb-system spec: addresses: - ${VIP_SERVICE_IP}/32 serviceAllocation: priority: 100 namespaces: - default EOF cat &lt;&lt;-EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: ip-pool-l2-adv namespace: metallb-system spec: ipAddressPools: - ip-pool - kubernetes-vip-ip-pool EOF   ","version":"Next","tagName":"h3"},{"title":"Install the Endpoint Copier Operator​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#install-the-endpoint-copier-operator","content":" helm repo add endpoint-copier-operator https://suse-edge.github.io/endpoint-copier-operator helm install --create-namespace -n endpoint-copier-operator endpoint-copier-operator endpoint-copier-operator/endpoint-copier-operator   The command above will deploy three different resources in the cluster:  The endpoint-copier-operator operator Deployment with 2 replicas. One will be the leader and the other will take over the leader role if needed.A Kubernetes service called kubernetes-vip in the default namespace that will be a copy of the kubernetes Service but from type LoadBalancer.An Endpoint resource called kubernetes-vip in the default namespace that will be a copy of the kubernetes Endpoint.  Verify that the kubernetes-vip Service has the correct IP address:  kubectl get service kubernetes-vip -n default -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'   Ensure that the kubernetes-vip and kubernetes Endpoints resources in the default namespace point to the same IPs.  kubectl get endpoints kubernetes kubernetes-vip   If everything is correct, the last thing left is to use the VIP_SERVICE_IP in our Kubeconfig.  sed -i '' &quot;s/${NODE_IP}/${VIP_SERVICE_IP}/g&quot; ~/.kube/config   From now on, all the kubectl will go through the kubernetes-vip service.  ","version":"Next","tagName":"h3"},{"title":"Add control-plane nodes​","type":1,"pageTitle":"Intro","url":"/docs/misc/metallb-kube-api#add-control-plane-nodes","content":" To monitor the entire process, two more terminal tabs can be opened.  First terminal:  watch kubectl get nodes   Second terminal:  watch kubectl get endpoints   Now execute the commands below on the second and third nodes.  # Export the VIP_SERVICE_IP in the VM # Replace with the actual IP export VIP_SERVICE_IP=&lt;ip&gt; export INSTALL_K3S_SKIP_START=false curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -  ","version":"Next","tagName":"h3"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/integrations/nvidia-slemicro","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nvidia-slemicro#prerequisites","content":" If you're following this guide, it's assumed that you've got the following already available:  At least one host with SLE Micro 5.3+ installed; this can be physical or virtual.Your host(s) is/are attached to a subscription as this will be required for package access - an evaluation is available here.A compatible NVIDIA GPU installed (or passed through to the virtual machine in which SLE Micro is running).Access to the root user - these instructions assume you're the root user, and not escalating your privileges via sudo.  NOTE: SUSE is in the process of ensuring that the NVIDIA drivers are part of the SLE Micro 5.3+ repositories. We can be sure that the drivers are in SLE Micro 5.5, but we may not have completed the tasks for 5.3 and 5.4 yet.  ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nvidia-slemicro#installation","content":" In this section you're going to install the NVIDIA drivers directly onto the SLE Micro operating system as the NVIDIA open-driver is now part of the core SLE Micro package repositories, which makes it incredibly easy to install. In the example below we're specifically pulling the &quot;G06&quot; generation of driver, which supports the latest GPU's (please see here for further information), so please ensure that you're selecting an appropriate GPU version.  In addition, the example below calls for 535.86.05 of the driver; please make sure that the driver version that you're selecting is compatible with your GPU, and in addition meets the CUDA requirements (if applicable) by checking here. It's also advisable to check the NVIDIA SLE15-SP4 repository to ensure that the driver version that you've chosen has an equivalent nvidia-compute-utils-G06 package with the same version string; this repository is regularly refreshed by NVIDIA, but the versions need to match; there's a possibility that we have a newer driver version in the SUSE repo than NVIDIA has in theirs (or vice versa), so it's important to match the versions here.  When you've confirmed the above, you're ready to install the packages on the host operating system, and for this we need to open up a transactional-update session, which creates a new read/write snapshot of the underlying operating system so we can make changes to the immutable platform (for further instructions on transactional-update see here):  transactional-update shell   When you're in your transactional-update shell, add the additional required package repositories from NVIDIA; this will allow us to pull in additional utilities, e.g. nvidia-smi, along with access to CUDA packages that you may want to utilise:  zypper ar https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ nvidia-sle15sp4-cuda zypper ar https://download.nvidia.com/suse/sle15sp4/ nvidia-sle15sp4-main   You can then install the driver and the nvidia-compute-utils for additional utilities:  zypper install -y nvidia-open-driver-G06-signed-kmp=535.86.05 kernel-firmware-nvidia-gspx-G06 nvidia-compute-utils-G06   NOTE: If this fails to install it's likely that there's a dependency mismatch between the selected driver version and what NVIDIA is shipping in their repositories - please revisit the section above to validate that your versions match. You may want to attempt to install a different driver version.  Next, if you're not using a supported GPU, remembering that the list can be found here, you can see if the driver will work by enabling support at the module level, but your mileage may vary -- skip this step if you're using a supported GPU:  sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf   Now that you've installed these packages, it's time to exit the transactional-update session:  exit   NOTE: Please make sure that you've exited the transactional-update session before proceeding!  Now that you've got your drivers installed, it's time to reboot, as SLE Micro is an immutable operating system it needs to reboot into the new snapshot that you created in a previous step; the drivers are only installed into this new snapshot, and hence it's not possible to load the drivers without rebooting into this new snapshot, which will happen automatically. Issue the reboot command when you're ready:  reboot   Once the system has rebooted successfully, log back in and try to use the nvidia-smi tool to verify that the driver is loaded successfully and that it's able to both access and enumerate your GPU(s):  nvidia-smi   The output of this command should show you something similar to the following output, noting that in the example below we have two GPU's:  Mon Sep 18 06:58:12 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.86.05 Driver Version: 535.86.05 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA A100-PCIE-40GB Off | 00000000:17:00.0 Off | 0 | | N/A 29C P0 35W / 250W | 4MiB / 40960MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA A100-PCIE-40GB Off | 00000000:CA:00.0 Off | 0 | | N/A 30C P0 33W / 250W | 4MiB / 40960MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+   ...and that's it! You've successfully installed and verified that the NVIDIA drivers are loaded into SLE Micro.  ","version":"Next","tagName":"h2"},{"title":"Further Validation​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nvidia-slemicro#further-validation","content":" At this stage, all we've been able to verify is that at the host level the NVIDIA device can be accessed and that the drivers are loading successfully. However, if we want to be sure that it's functioning, a simple test would be to try and validate that the GPU can take instruction from a user-space application, ideally via a container, and through the CUDA library, as that's typically what a real workload would utilise. For this, we can make a further modification to the host OS by installing the nvidia-container-toolkit. First, open up another transactional-update shell, noting that we could have done this in a single transaction in the previous step, but to many (e.g. customers wanting to use Kubernetes) this step won't be required:  transactional-update shell   Next, install the nvidia-container-toolkit package, which comes from one of the repo's that we configured in a previous step. Note that this command will initially appear to fail as it has a dependency on libseccomp, whereas this package is libseccomp2 in SLE Micro, so you can safely select the second option (&quot;break dependencies&quot;) here:  zypper in install nvidia-container-toolkit   Your output should look like the following:  Refreshing service 'SUSE_Linux_Enterprise_Micro_5.4_x86_64'. Refreshing service 'SUSE_Linux_Enterprise_Micro_x86_64'. Refreshing service 'SUSE_Package_Hub_15_SP4_x86_64'. Loading repository data... Reading installed packages... Resolving package dependencies... Problem: nothing provides 'libseccomp' needed by the to be installed nvidia-container-toolkit-1.14.1-1.x86_64 Solution 1: do not install nvidia-container-toolkit-1.14.1-1.x86_64 Solution 2: break nvidia-container-toolkit-1.14.1-1.x86_64 by ignoring some of its dependencies Choose from above solutions by number or cancel [1/2/c/d/?] (c): 2 (...)   NOTE: We're working on fixing this dependency issue, so this should be a lot cleaner in the coming weeks.  When you're ready, you can exit the transactional-update shell:  exit   ...and reboot the machine into the new snapshot:  reboot   NOTE: As before, you will need to ensure that you've exited the transactional-shell and rebooted the machine for your changes to be enacted.  Now that the machine has rebooted, you can validate that the system is able to successfully enumerate the devices via the NVIDIA container toolkit (the output should be verbose, and it should provide a number of INFO and WARN messages, but no ERROR messages):  nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml   When ready, you can then run a podman-based container (doing this via podman gives us a good way of validating access to the NVIDIA device from within a container, which should give confidence for doing the same with Kubernetes), giving it access to the labelled NVIDIA device(s) that were taken care of by the previous command, based on SLE BCI and simply running bash:  podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash   When we're in the temporary podman container we can install the required CUDA libraries, again checking the correct CUDA version for your driver here although the previous output of nvidia-smi should show the required CUDA version. In the example below we're installing CUDA 12.1 and we're pulling a large number of examples, demo's, and development kits so you can fully validate the GPU:  zypper ar http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-sle15-sp4 zypper in -y cuda-libraries-devel-12-1 cuda-minimal-build-12-1 cuda-demo-suite-12-1   Once this has been installed successfully, don't exit from the container, we'll run the deviceQuery CUDA example, which will comprehensively validate GPU access via CUDA, and from within the container itself:  /usr/local/cuda-12/extras/demo_suite/deviceQuery   If successful, you should see output that shows similar to the following, noting the Result = PASS message at the end of the command:  /usr/local/cuda-12/extras/demo_suite/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 2 CUDA Capable device(s) Device 0: &quot;NVIDIA A100-PCIE-40GB&quot; CUDA Driver Version / Runtime Version 12.2 / 12.1 CUDA Capability Major/Minor version number: 8.0 Total amount of global memory: 40339 MBytes (42298834944 bytes) (108) Multiprocessors, ( 64) CUDA Cores/MP: 6912 CUDA Cores GPU Max Clock rate: 1410 MHz (1.41 GHz) Memory Clock rate: 1215 Mhz Memory Bus Width: 5120-bit L2 Cache Size: 41943040 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 3 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 23 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; Device 1: &quot;NVIDIA A100-PCIE-40GB&quot; CUDA Driver Version / Runtime Version 12.2 / 12.1 CUDA Capability Major/Minor version number: 8.0 Total amount of global memory: 40339 MBytes (42298834944 bytes) (108) Multiprocessors, ( 64) CUDA Cores/MP: 6912 CUDA Cores GPU Max Clock rate: 1410 MHz (1.41 GHz) Memory Clock rate: 1215 Mhz Memory Bus Width: 5120-bit L2 Cache Size: 41943040 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 3 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 202 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; &gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes &gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.1, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB Result = PASS   From here, you can continue to run any other CUDA workload - you can utilise compilers, and any other aspect of the CUDA ecosystem to run some further tests. When you're done you can exit from the container, noting that whatever you've installed in there is ephemeral (so will be lost!), and hasn't impacted the underlying operating system:  exit   ","version":"Next","tagName":"h2"},{"title":"Implementation with Kubernetes​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nvidia-slemicro#implementation-with-kubernetes","content":" (Coming soon!)  ","version":"Next","tagName":"h2"},{"title":"Resolving issues​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nvidia-slemicro#resolving-issues","content":" ","version":"Next","tagName":"h2"},{"title":"nvidia-smi does not find the GPU​","type":1,"pageTitle":"Intro","url":"/docs/integrations/nvidia-slemicro#nvidia-smi-does-not-find-the-gpu","content":" Check the kernel messages using dmesg. In case this indicates that it fails to allocate NvKMSKapDevice, then apply the unsupported GPU workaround:  transactional-update run sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf   NOTE: You will need to reboot at this stage. ","version":"Next","tagName":"h3"},{"title":"Rancher portfolio disambiguation","type":0,"sectionRef":"#","url":"/docs/misc/rancher-disambiguation","content":"","keywords":"","version":"Next"},{"title":"TL;DR​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#tldr","content":" RKE1, RKE2 and K3s are flavours of Kubernetes, Rancher Manager can be used to manage and provision different deployments of Kubernetes itself with a primary focus on RKE1/RKE2, Fleet can watch Git Repositories, detect changes and tell Kubernetes what it needs to be running, Elemental considers a specific approach to provisioning Kubernetes in Edge scenarios where the provisioning can be preloaded at the OS level for Rancher Manager to control later  ","version":"Next","tagName":"h2"},{"title":"Rancher​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#rancher","content":" Rancher (or Rancher Manager) is a multi cluster management solution for provisioning, managing and accessing multiple downstream kubernetes clusters. To provision new clusters Rancher can interact with different infrastructure and virtualization tools (vSphere/AWS etc) as an api client, request VMs and networks and setup a kubernetes cluster inside of those, it also works with bare metal machines by generating an join command you an run each time.  ","version":"Next","tagName":"h3"},{"title":"Fleet​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#fleet","content":" Fleet is usually a component of Rancher (although it can be used independently) that allows you to use a GitOps workflow for multi-cluster (i.e it allows you to define your git repositories and the clusters they should apply to at the management cluster level).  ","version":"Next","tagName":"h3"},{"title":"Elemental​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#elemental","content":" Elemental is a way to automatically deploy/register new clusters and manage the OS of their node, you can define clusters and their nodes on the management cluster then generate an OS installer image, when booting your node from that image it will install the node, register it to the manager and configure it for its role in the local cluster. This is the SUSE/Rancher way of doing zero touch provisioning. Elemental takes a different view of cluster provisioning focused on Edge deployments, typically Rancher services datacentre deployments of Kubernetes with enterprise servers etc; in an Edge scenario e.g. factory or cruise ship theres no guarantee of access for Rancher to contact and provision a cluster directly (i.e. limited bandwidth, firewalls etc) - Elemental instead is used to preload an operating system with all the information needed to set the cluster up, you can install that into the servers that you want to cluster and then it will reach back to Rancher to be under management at that point  ","version":"Next","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#kubernetes","content":" Kubernetes as a standard and core technology is really a cross industry effort like Linux and has become core to DevOps as a cultural movement - as it enables defining and deploying your infrastructure as code and with lots of automation for extensive business continuity and high availability  Kubernetes is a receptacle though - it runs what you tell it to run, usually people use automation to tell it what to do and this requires some kind of application to detect application configuration and apply it to Kubernetes - usually this is fulfilled through developer pipelines (CI/CD) where things are deployed as they are developed  ","version":"Next","tagName":"h3"},{"title":"Kubernetes distributions​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#kubernetes-distributions","content":" Kubernetes Distributions, like Linux OSes, come in different flavours, RKE and RKE2 are two different flavours of Kubernetes in this manner; but like Ubuntu vs SUSE do for an OS they are ultimately just packaging an implementation of Kubernetes. Other examples include EKS,AKS and GKE which are flavours produced by AWS, Azure and GCP respectively. When we say a kubernetes cluster we mean a specific instance of a distribution installed on servers that are managed as a group (each server being a node in the cluster)  ","version":"Next","tagName":"h3"},{"title":"K3Ss​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#k3ss","content":" K3s is a fully compliant and lightweight Kubernetes distribution focused on Edge, IoT, ARM or just for situations where a PhD in K8s clusterology is infeasible  ","version":"Next","tagName":"h3"},{"title":"RKE (or RKE1)​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#rke-or-rke1","content":" Rancher Kubernetes Engine is a Kubernetes distribution that uses an older architecture and relies on Docker Engine to run containers  ","version":"Next","tagName":"h3"},{"title":"RKE2​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#rke2","content":" RKE2 also known as RKE Government, is Rancher's next-generation Kubernetes distribution that uses a newer architecture based on ContainerD. RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream.  ","version":"Next","tagName":"h3"},{"title":"RKE2 using Air-gap install​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#rke2-using-air-gap-install","content":" air-gap install is an RKE2 Installation where all package dependencies are installed using two different methods. Using the tarball release 'rke2-airgap-images' or by using a private registry and passing the parameter 'system-default-registry' during the installation to point directly to the private registry where images are located (as a mirror for docker.io)  ","version":"Next","tagName":"h3"},{"title":"Rancher vs K3s vs RKE​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/docs/misc/rancher-disambiguation#rancher-vs-k3s-vs-rke","content":" You don’t need Rancher to set up K3s or RKE1 or RKE2 on their own it just makes the whole process easier. Rancher runs as a Management Interface that can interact with running clusters and also provision new clusters - as well as manage authentication to the downstream clusters, and it can also do other things like interact with applications that kubernetes is orchestrating and provides monitoring tools ","version":"Next","tagName":"h3"},{"title":"Intro","type":0,"sectionRef":"#","url":"/docs/misc/rke2-selinux","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/docs/misc/rke2-selinux#prerequisites","content":" 1 VM. Hint SLE Micro on OSX on Apple Silicon (UTM) or SLE Micro on X86_64 on libvirt (virt-install) can be used as the base platform for validation here, but these instructions should work on any SLE Micro based system. The VM should meet the RKE2 requirements.  ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"Intro","url":"/docs/misc/rke2-selinux#installation","content":" ","version":"Next","tagName":"h2"},{"title":"Installation for x86-64 architecture​","type":1,"pageTitle":"Intro","url":"/docs/misc/rke2-selinux#installation-for-x86-64-architecture","content":" Once we've got the VM started and running, let's prepare the config to enable SELinux mode in the RKE2 configuration file:  mkdir -p /etc/rancher/rke2 &amp;&amp; echo &quot;selinux: true&quot; &gt;&gt; /etc/rancher/rke2/config.yaml   Install RKE2 cluster  curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL=stable INSTALL_RKE2_METHOD=rpm RKE2_SELINUX=true sh - # Enable and Start RKE2 systemctl enable rke2-server.service   Now, the VM should be rebooted for the transactional-update to finish properly:  reboot   ","version":"Next","tagName":"h3"},{"title":"Installation on arm64 architecture​","type":1,"pageTitle":"Intro","url":"/docs/misc/rke2-selinux#installation-on-arm64-architecture","content":" As there are no RPM builds for RKE2 on arm64 architecture, the tarball method will be used and the rke2-selinux policy will be installed manually.  Install rke2-selinux​  The first thing that will be installed is the rke2-selinux policy.  Let's connect to the VM and run:  cat &gt;&gt; install-selinux.sh &lt;&lt; 'END' #!/bin/bash # Install rpm-testing.rancher.io repository key and the rke2-selinux package mkdir -p /var/lib/rpm-state curl -o /root/public.key https://rpm-testing.rancher.io/public.key curl -L -o /root/rke2-selinux.rpm https://github.com/rancher/rke2-selinux/releases/download/v0.15.testing.1/rke2-selinux-0.15-1.slemicro.noarch.rpm rpmkeys --import /root/public.key # Install RKE2 with SELinux zypper install -y /root/rke2-selinux.rpm END chmod +x install-selinux.sh &amp;&amp; transactional-update run /root/install-selinux.sh   Now, the VM should be rebooted for the transactional-update to finish properly:  reboot   After restarting the VM, we can verify that the policy was successfully installed as follows:  rpm -qa | grep rke2   Install RKE2​  As a second step, an RKE2 cluster will be installed, which will use the policy installed in the previous section.  As the rke2-policy was installed manually on the VM, some of its paths may not be created correctly, so the following commands will ensure that all the paths are fine.  Let's connect to the VM and run:  mkdir -p /var/lib/cni mkdir -p /opt/cni mkdir -p /var/lib/kubelet/pods mkdir -p /var/lib/rancher/rke2/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots mkdir -p /var/lib/rancher/rke2/data mkdir -p /var/run/flannel mkdir -p /var/run/k3s restorecon -R -i /etc/systemd/system/rke2.service restorecon -R -i /usr/lib/systemd/system/rke2.service restorecon -R /var/lib/cni restorecon -R /opt/cni restorecon -R /var/lib/kubelet restorecon -R /var/lib/rancher restorecon -R /var/run/k3s restorecon -R /var/run/flannel   It's time for the RKE2 cluster to be installed but before that, RKE2 must be running Selinux mode:  mkdir -p /etc/rancher/rke2 &amp;&amp; echo &quot;selinux: true&quot; &gt;&gt; /etc/rancher/rke2/config.yaml   Install RKE2 Using Install Script  curl -sfL https://get.rke2.io | INSTALL_RKE2_EXEC=&quot;server&quot; RKE2_SELINUX=true INSTALL_RKE2_VERSION=v1.27.3+rke2r1 sh - # Enable and Start RKE2 systemctl enable --now rke2-server.service   NOTE: RKE2 version 1.27 is the first that supports arm64 architecture and it is still an experimental feature.  ","version":"Next","tagName":"h3"},{"title":"Get the Kubeconfig​","type":1,"pageTitle":"Intro","url":"/docs/misc/rke2-selinux#get-the-kubeconfig","content":" To use the Kubeconfig outside of the node, the following commands can be used:  # Replace &lt;node-ip&gt; with the actual ip export NODE_IP=&lt;node-ip&gt; sudo scp ${NODE_IP}:/etc/rancher/rke2/rke2.yaml ~/.kube/config &amp;&amp; sed -i '' &quot;s/127.0.0.1/${NODE_IP}/g&quot; ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config   ","version":"Next","tagName":"h3"},{"title":"Verify the setup​","type":1,"pageTitle":"Intro","url":"/docs/misc/rke2-selinux#verify-the-setup","content":" Check SELinux status:  sestatus   The output should be similar to this one:  SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: requested (insecure) Max kernel policy version: 33   Check that all pods are in Running state:  kubectl get pod -A   The output should be similar to this one:  NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cloud-controller-manager-slemicro 1/1 Running 0 (2m3s ago) 3d5h kube-system etcd-slemicro 1/1 Running 0 (2m9s ago) 3d5h kube-system kube-apiserver-slemicro 1/1 Running 0 (2m9s ago) 3d5h kube-system kube-controller-manager-slemicro 1/1 Running 0 (2m7s ago) 3d5h (2m9s ago) 3d5h ...  ","version":"Next","tagName":"h3"},{"title":"Requisites","type":0,"sectionRef":"#","url":"/docs/misc/sushy-emulator-sles-certificates-container","content":"","keywords":"","version":"Next"},{"title":"Testing​","type":1,"pageTitle":"Requisites","url":"/docs/misc/sushy-emulator-sles-certificates-container#testing","content":" curl https://localhost:8443/redfish/v1/Systems curl: (60) SSL certificate problem: self signed certificate More details here: https://curl.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above.   Ignoring the certificate:  curl https://localhost:8443/redfish/v1/Systems -k { &quot;@odata.type&quot;: &quot;#ComputerSystemCollection.ComputerSystemCollection&quot;, &quot;Name&quot;: &quot;Computer System Collection&quot;, &quot;Members@odata.count&quot;: 6, &quot;Members&quot;: [ { &quot;@odata.id&quot;: &quot;/redfish/v1/Systems/1a3cc749-dd03-4f2b-a588-981c8fbf2911&quot; }, { &quot;@odata.id&quot;: &quot;/redfish/v1/Systems/7201e8ab-28e3-4847-a68b-f008d8af21df&quot; }, { &quot;@odata.id&quot;: &quot;/redfish/v1/Systems/eaa7c6b1-5195-4677-8473-e96114c88d02&quot; }, { &quot;@odata.id&quot;: &quot;/redfish/v1/Systems/5940f178-aa88-4b12-8640-b3e87723d0dd&quot; }, { &quot;@odata.id&quot;: &quot;/redfish/v1/Systems/57e13c2f-aabb-488d-a895-db8150d0bf34&quot; }, { &quot;@odata.id&quot;: &quot;/redfish/v1/Systems/a2436047-7663-46d7-bd83-ea7b359cf237&quot; } ], &quot;@odata.context&quot;: &quot;/redfish/v1/$metadata#ComputerSystemCollection.ComputerSystemCollection&quot;, &quot;@odata.id&quot;: &quot;/redfish/v1/Systems&quot;, &quot;@Redfish.Copyright&quot;: &quot;Copyright 2014-2016 Distributed Management Task Force, Inc. (DMTF). For the full DMTF copyright policy, see http://www.dmtf.org/about/policies/copyright.&quot; }  ","version":"Next","tagName":"h2"},{"title":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/architecture","content":"","keywords":"","version":"Next"},{"title":"Components​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/architecture#components","content":" Components used in ATIP, versions, roles etc  ","version":"Next","tagName":"h2"},{"title":"ATIP Architecture​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/architecture#atip-architecture","content":" Architecture of ATIP, relationship between components etc  ","version":"Next","tagName":"h2"},{"title":"Example deployment flows​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/architecture#example-deployment-flows","content":" Example high level sequences for deployment and use of ATIP in common scenarios  Management on central site, edge sites with Multus + SRIOVManagement on Public cloud, edge sites with RT kerneletc ","version":"Next","tagName":"h2"},{"title":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/edge-site","content":"","keywords":"","version":"Next"},{"title":"Edge site definition​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/edge-site#edge-site-definition","content":" The new Edge definition will be steps required to create and configure properly a new edge cluster from the Rancher Management tool. We will also offer some extra options which are really common used in the Telco industry to customize the edge site creation in case you need some of them.  ","version":"Next","tagName":"h2"},{"title":"Create new edge site​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/edge-site#create-new-edge-site","content":" To create a new edge site, you need to go to the Clusters section and click on Create button:    Then, you could select the provider to be used to create the new edge site. In our case, we will use Custom provider:    Next step is to name the new edge site and select the cluster version to be used:    Now, it's time to customize our installation defining some of the most common Telco/Edge options. To do that we will use the next tabs on the left side:    ","version":"Next","tagName":"h3"},{"title":"Custom Registry​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/edge-site#custom-registry","content":" Rancher offers the possibility to use extra registries to be used by the edge site. This is really useful in case you need to use some extra images which are not available in the default registries, but you've got deployed another internal registry to be used by your edge site. In case you need to add some extra registries, you could do it from the Custom Registry tab:  Enable the extra registry adding the private registry information  To enable the private registries, you need to click on Enable cluster scoped container registry but take care don't change the value, in order to keep the default Rancher registry available for the system images. The idea is to keep the default registry for the system images using the rancher registry, and then, add a new registry (named mirror) to use a secondary private registry where your application images are stored:  To do that, you should enable the option Enable cluster scoped container registry keeping the container registry empty (to use the default for system images), and then, add a new private registry information:    You could also add some information regarding the authentication to be used by the private registry:    If you want to modify directly using a yaml file the private registry information, you could do it using the next yaml file (you could download it from the Edit as YAML button):    ","version":"Next","tagName":"h3"},{"title":"CPU Management Policy​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/edge-site#cpu-management-policy","content":" Rancher offers the possibility to use different CPU management policies on RKE2 to be used by the edge site. This is really useful if you need to tune the cpu cores management that you will offer to your workloads running on top of kubernetes.  For more information about the different CPU management policies available in Kubernetes, please check the official documentation  To modify the kubelet in order to allow some cpu management policies you could do it from the Advanced tab:    You could add some cpu management policies using the kubeReserved and systemReserved options:    Also, you could add some machine labels in order to match with the node labels that you will use in your workloads.  If you want to modify directly the information regarding the kubelet, you could do it using the next yaml file (you could download it from the Edit as YAML button):    ","version":"Next","tagName":"h3"},{"title":"Networking​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/edge-site#networking","content":" Rancher offers the possibility to use different networking options to be used by the edge site. Also you could install directly some CNI plugins to be used by the edge site like Calico, Cilium, Multus, and the combination with Multus  In our case, we will use the Multus + calico combination, but you could use whatever you want.  Configure the Container Network Interface (CNI) plugin to be used by the edge site:    Define some extra options using the Add-On Config tab (optional - if you need some specific options for the CNI plugins):  In case you need to modify some more specific options regarding the CNI plugins you could do it using the next yamls files which contains the configuration for the CNI plugins:    Also, you could define the general network for the cluster to be used by the edge site:    ","version":"Next","tagName":"h3"},{"title":"Installation process​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/edge-site#installation-process","content":" After the edge site definition, you could click on the Create button to start the installation process:    In our case, we've selected the node role etcd, control plane, and worker because we want to deploy a single node cluster, but you could select whatever you want depending on your use case.  The next step should be copy the registration command to be used by the new edge node, and then, execute it on the new edge cluster server to start the installation process.  Note: you could use the insecure check option in order to skip the TLS verification if your server has a self-signed certificate.  During the installation you could see the process using the Conditions and the Recent Events tabs:    After the installation process, you could see the new edge site created successfully on the Home page or in the Clusters section. ","version":"Next","tagName":"h2"},{"title":"*DRAFT - SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/introduction","content":"","keywords":"","version":"Next"},{"title":"TL;DR​","type":1,"pageTitle":"*DRAFT - SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/introduction#tldr","content":" ATIP Comprises multiple components including SLE Micro, RKE2, Rancher and others. This documentation will provide instructions on their installation, configuration and lifecycle management  ","version":"Next","tagName":"h2"},{"title":"Contents​","type":1,"pageTitle":"*DRAFT - SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/introduction#contents","content":" Architecture and Concepts  ATIP ArchitectureComponentsExample deployment flows  Pre-requisites  HardwareNetworkServices (DHCP, DNS, etc)  Management Cluster Installation  OS InstallRKE InstallRancher InstallInitial ConfigurationBare Metal Management Configuration  Edge Site Installation  Edge site definitionInstallation processCluster Commissioning  Feature Configuration  Real TimeMultusSRIOVDPDKHuge PagesCPU PinningNUMA Aware schedulingMetal LB (Beta)  Lifecycle Actions  Software lifecyclesManagement Cluster upgradesRancher UpgradesOperating system upgradesRKE2 Upgrades ","version":"Next","tagName":"h2"},{"title":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/lifecycle","content":"","keywords":"","version":"Next"},{"title":"Software lifecycles​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/lifecycle#software-lifecycles","content":" Roadmap, support lifecycle diagram etc. Support combiantions.  ","version":"Next","tagName":"h2"},{"title":"Management Cluster upgrades​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/lifecycle#management-cluster-upgrades","content":" Updating management cluster OS + RKE  ","version":"Next","tagName":"h2"},{"title":"Rancher Upgrades​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/lifecycle#rancher-upgrades","content":" Upgrading rancher versions  ","version":"Next","tagName":"h2"},{"title":"Operating system upgrades​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/lifecycle#operating-system-upgrades","content":" Upgrading OS on edge sites  ","version":"Next","tagName":"h2"},{"title":"RKE2 Upgrades​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/lifecycle#rke2-upgrades","content":" Upgrading RKE2 on edge sites ","version":"Next","tagName":"h2"},{"title":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/requirements","content":"","keywords":"","version":"Next"},{"title":"Hardware​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/requirements#hardware","content":" Minimum hardware specs, support, validated designs, stack validation etc  ","version":"Next","tagName":"h2"},{"title":"Network​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/requirements#network","content":" Expected network setup, example setups  ","version":"Next","tagName":"h2"},{"title":"Services (DHCP, DNS, etc)​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/requirements#services-dhcp-dns-etc","content":" External services required by ATIP, common options for DHCP, DNS, etc.  ","version":"Next","tagName":"h2"},{"title":"Disable rebootmgr​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/requirements#disable-rebootmgr","content":" rebootmgr is a service which allow to configure a strategic for reboot in case system have some updates available pending. For telco workloads is really important to disable or configure properly the rebootmgr service in order to avoid the reboot of the nodes in case of updates scheduled by the system.  For more information about rebootmgr, please check: https://github.com/SUSE/rebootmgr  You could verify the strategic being used as:  cat /etc/rebootmgr.conf [rebootmgr] window-start=03:30 window-duration=1h30m strategy=best-effort lock-group=default   and you could disable it as:  sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf   or using the rebootmgrctl command:  rebootmgrctl strategy off  ","version":"Next","tagName":"h2"},{"title":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/management-cluster","content":"","keywords":"","version":"Next"},{"title":"Requirements​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#requirements","content":" A minimum of 1GB of RAM per node available to be used by the Rancher management clusterAccurate time synchronization (ntp or chronyd) between all nodes in the cluster.A minimum of 2 vCPUs per node available to be used by the Rancher management clusterA minimum of 12GB of disk space (recomended 20GB of disk) per node available to be used by the Rancher management cluster  For more information about time synchronization, please check: https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-ntp.html  ","version":"Next","tagName":"h2"},{"title":"OS Install​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#os-install","content":" This section covers the installation of the OS on the management cluster nodes. The OS used for the management cluster installation is SLE Micro 5.4 that can be downloaded from here. Using SUSE Linux Enterprise Micro, you can build and scale differentiating edge systems across a wide range of industries including aerospace, telecom, automotive, defense, healthcare, hospitality, and manufacturing. SUSE Linux Enterprise Micro 5.4 is available on the AMD64 and Intel 64 (x86_64), Arm 64 and IBM z System or LinuxONE (s390x) hardware architectures.  SUSE Linux Enterprise Micro provides self-install ISO images that enable you to deploy SLE Micro to your machine easily (either a virtual machine or a bare metal) and configure the system on the first boot. For this installation we will use the self-install ISO: SLE-Micro.x86_64-5.4.0-Default-SelfInstall-GM.install.iso  For more information, please refer Release Notes: https://www.suse.com/releasenotes/x86_64/SLE-Micro/5.4/ For more information about the installation, please refer to the documentation: https://documentation.suse.com/en-us/sle-micro/5.4/html/SLE-Micro-all/  ","version":"Next","tagName":"h2"},{"title":"Preparing the configuration device (Optional)​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#preparing-the-configuration-device-optional","content":" The following procedure describes how to prepare the configuration device in case you want to configure the system using combustion and ignition files, instead of using the JEOS Firstboot wizard:  Format the disk to any file system supported by SLE Micro: Ext3, Ext4, etc.:  sudo mkfs.ext4 /dev/sdY  Set the device label to either ignition (when either Ignition or Combustion is used) or combustion (when only Combustion is used). For the Ext4 file system:  sudo e2label /dev/sdY ignition  You can use any type of configuration storage media that your virtualization system or your hardware supports: ISO image, a USB flash disk, etc.  Mount the device:  sudo mount /dev/sdY /mnt  Create the directory structure as mentioned in this link:  sudo mkdir -p /mnt/ignition/  or:  sudo mkdir -p /mnt/combustion/  Prior to booting for the first time, prepare all elements of the configuration that will be used by Ignition or Combustion. To log in to your system, you need to provide a password for root or set up passwordless authentication, otherwise the system will not be accessible after the first boot.  For more information about the configuration files using combustion and ignition, please refer to the documentation: https://documentation.suse.com/sle-micro/5.4/html/SLE-Micro-all/cha-images-ignition.html#  ","version":"Next","tagName":"h3"},{"title":"Installation​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#installation","content":" Boot the system from the ISO image and select Install SLE Micro    Select the Installation Disk and click OK    Installation will start copying the files to disk    Once the installation is finished, the system will reboot After reboot, the system will start the JEOS Firstboot wizard  If you want to configure the system using combustion and ignition files, instead of using the JEOS Firstboot wizard, please refer to the section Preparing the configuration device (Optional) above, and ensure your media is connected during the first boot.              During the deployment of the selfinstall ISO, the image of the system is just copied to the selected disk, therefore, an EFI boot entry is not created (like it normally would if the system is deployed using an installer). You might need to manually boot your system using the EFI shell by selecting the SLE Micro boot loader. After the first boot, you can use efibootmgr to create the boot entry. efibootmgr is available by default in the deployed image.  ","version":"Next","tagName":"h3"},{"title":"Post Installation​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#post-installation","content":" Registering the system is possible from the command line using the transactional-update register command. For information that goes beyond the scope of this section, refer to the inline documentation with `SUSEConnect --help  To register SUSE Linux Enterprise Micro with SUSE Customer Center, run transactional-update register as follows:  transactional-update register -r REGISTRATION_CODE -e EMAIL_ADDRESS  To register with a local registration server, additionally provide the URL to the server:  transactional-update register -r REGISTRATION_CODE -e EMAIL_ADDRESS \\ --url &quot;https://suse_register.example.com/&quot;   Replace REGISTRATION_CODE with the registration code you received with your copy of SUSE Linux Enterprise Micro. Replace EMAIL_ADDRESS with the e-mail address associated with the SUSE account you or your organization uses to manage subscriptions.  Reboot your system to switch to the latest snapshot. SUSE Linux Enterprise Micro is now registered.  ","version":"Next","tagName":"h3"},{"title":"RKE2 Cluster Install​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#rke2-cluster-install","content":" This section covers the installation of the RKE2 cluster on the management cluster nodes:  For more information, please refer to the documentation: https://docs.rke2.io/install/quickstart/  ","version":"Next","tagName":"h2"},{"title":"Server node installation command:​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#server-node-installation-command","content":" Run the installer:  curl -sfL https://get.rke2.io | sh -  if you want to install a especific version, you can use the following command (i.e. v1.25.9+rke2r1):  curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=&quot;v1.25.9+rke2r1&quot; sh -  For more information about the installation, please refer to the documentation: https://docs.rke2.io/install/install_options/  Enable the rke2-server service:  systemctl enable rke2-server.service  Start the service:  systemctl start rke2-server.service  ","version":"Next","tagName":"h3"},{"title":"Agent node installation command:​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#agent-node-installation-command","content":" Run the installer:  curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=&quot;agent&quot; sh -  Enable the rke2-agent service:  systemctl enable rke2-agent.service  Configure the config.yaml file located in /etc/rancher/rke2/ with the following content:  server: https://&lt;server&gt;:9345 token: &lt;token from server node&gt;   Start the service:  systemctl start rke2-agent.service  ","version":"Next","tagName":"h3"},{"title":"Rancher Manager Install​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#rancher-manager-install","content":" Rancher is installed using the Helm package manager for Kubernetes. Helm charts provide templating syntax for Kubernetes YAML manifest documents. With Helm, we can create configurable deployments instead of just using static files.  This section covers the installation of Rancher on the management cluster nodes. For more information about the installation, please refer to the documentation: https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster  ","version":"Next","tagName":"h2"},{"title":"1. Add the Helm repository​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#1-add-the-helm-repository","content":" There are three releases available to be added as a Helm repository for Rancher. In our case, we will use the rancher-stable because it's the release recommended for production environments, but you could use rancher-latest or rancher-alpha if you want. Also, there is a rancher primer release that is the enterprise version of Rancher.  helm repo add rancher-stable https://releases.rancher.com/server-charts/stable  If you don't have helm installed previously, you could install it using the following command: curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 |bash  ","version":"Next","tagName":"h3"},{"title":"2. Choose your SSL Configuration​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#2-choose-your-ssl-configuration","content":" The Rancher management server is designed to be secure by default and requires SSL/TLS configuration.  There are three recommended options for SSL/TLS configuration:  Rancher-generated TLS certificateLet's EncryptBring your own certificate  For more information about the SSL/TLS configuration, please refer to the documentation: https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster/#3-choose-your-ssl-configuration  In our case we will use the Rancher-generated TLS certificate:  helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true \\ --version v1.11.1   Once you've installed cert-manager, you can verify the pods are running:kubectl get pods --namespace cert-manager  ","version":"Next","tagName":"h3"},{"title":"3. Install Rancher with Helm and your chosen certificate option​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#3-install-rancher-with-helm-and-your-chosen-certificate-option","content":" You can install Rancher with helm using the following command modifying the &lt;hostname&gt; and &lt;bootstrapPassword&gt; values:  helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --create-namespace \\ --set hostname=&lt;hostname&gt; \\ --set bootstrapPassword=&lt;bootstrapPassword&gt; \\ --set replicas=1 \\ --set global.cattle.psp.enabled=false   ","version":"Next","tagName":"h3"},{"title":"4. Verify the Rancher installation​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#4-verify-the-rancher-installation","content":" You should wait a few minutes for Rancher to be rolled out:  kubectl -n cattle-system rollout status deploy/rancher  ","version":"Next","tagName":"h3"},{"title":"Initial Configuration​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#initial-configuration","content":" TBC - Initial configuration necessary to begin creating edge cluster  ","version":"Next","tagName":"h2"},{"title":"Bare Metal Management Configuration​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/management-cluster#bare-metal-management-configuration","content":" TBC - Integration and setup of Metal3/CAPI components ","version":"Next","tagName":"h2"},{"title":"Standalone Clusters with Edge Image Builder","type":0,"sectionRef":"#","url":"/docs/quickstart/eib","content":"","keywords":"","version":"Next"},{"title":"What is the Edge Image Builder?​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#what-is-the-edge-image-builder","content":" The Edge Image Builder is a tool that allows for generating a fully customized disk image for installing and bootstrapping nodes without the usual pain and suffering.  It supports a variety of configuration options including (but not limited to):  Users, passwords, and ssh-keysKernel command line argumentsSystemd-services to be enabled/disabled at boot-timeConfiguration of Network Devices (static IP, hostname, VLAN’s, bonding, etc.)Airgapped installation of host-level packages (including dependency resolution)Standard system configuration (e.g. proxies, NTP, custom SSL certificates, etc.)Registration to Rancher via Elemental APIRegistration to SUSE Manager for OS managementFallback for custom scripts that enables any manual tasksFully air-gapped deployments of Kubernetes (single &amp; multi-node deployments)Fully air-gapped workload management (enabling customer images &amp; manifests)Fully unattended node deployment  ","version":"Next","tagName":"h2"},{"title":"Why use this method​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#why-use-this-method","content":" While the Edge Image Builder tool is a part of the process from all three provisioning methods, it really shows it's value in scenarios where the clusters being deployed have limited networking or are fully air-gapped.  ","version":"Next","tagName":"h2"},{"title":"How to use the Edge Image Builder​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#how-to-use-the-edge-image-builder","content":" Edge Image Builder is typically run from inside a container so, if you don't already have a way to run containers, we need to start by installing a container runtime such as Podman or Rancher Desktop. For this guide, we will assume you already have a container runtime available.  note If you are running Rancher Desktop, you need to switch to the dockerd (moby) container runtime  ","version":"Next","tagName":"h2"},{"title":"Directory structure​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#directory-structure","content":" When running the tool, we will mount in a directory from the host.  This directory has the following structure (all subdirectories other than images are optional):  ./ ├── eib-config-iso.yaml ├── base-images/ │ └ SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso ├── network/ ├── rpms/ ├── elemental/ └── custom/   note For this quickstart, we will be ignoring the elemental subdirectory. If you are working with Elemental, please check out it's quickstart guide for more information!  ","version":"Next","tagName":"h3"},{"title":"Building the config file​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#building-the-config-file","content":" The config file is where we define what changes need to be included in your environment.  The base contents of the file are:  apiVersion: 1.0 image: imageType: iso arch: x86_64 baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso outputImageName: eib-image.iso   Let's save this file as iso-config.yaml  Users​  The first thing we can add are users of the system. Let's say we want to include a password for the root user (which is not typically a good idea for production but it makes a good demo).  First, we need to encrypt the password using:  openssl passwd -6 MyPassword!123   This will output something similar to:  $6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31   We can then add a section in the config file called operatingSystem with a users array inside it. The resulting file should look like:  apiVersion: 1.0 image: imageType: iso baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso outputImageName: eib-image.iso operatingSystem: users: - username: root encryptedPassword: $6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31   Packages​  Let's extend our image a bit with some additional packages.  For an example in this quickstart, let's say you were still testing stuff locally and wanted to install the yq command to make working with yaml easier. This package can be found in the PackageHub repo.  To have this package installed, we can add a section to the  apiVersion: 1.0 image: imageType: iso baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso outputImageName: eib-image.iso operatingSystem: users: - username: root encryptedPassword: $6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31 packages: packageList: - yq additionalRepos: - https://updates.suse.com/SUSE/Backports/SLE-15-SP5_x86_64/standard #registrationCode: INTERNAL-USE-ONLY-foo-bar   TODO: this isn't working  This will look through the requested packages, collect all the dependencies, pull the required rpms, and include them on the image to be installed without needing to connect to the network.  note More Operating System settings can be found here in the Edge Image Builder documentation  Network Configuration​  Lastly, for this quickstart example, let's setup a network device!  This is done by adding a file in the network subdirectory called host1.local.yaml with the contents:  //TODO: what's the minimum needed, and do we really need to specify the MAC address?  dns-resolver: {} routes: running: - destination: 0.0.0.0/0 next-hop-interface: eth0 next-hop-address: 192.168.1.1 table-id: 254 config: [] interfaces: - name: eth0 type: ethernet state: up mac-address: 0E:4D:C6:B8:C4:72 ipv4: enabled: true address: - ip: 192.168.1.99 prefix-length: 24 ethernet: auto-negotiation: false - name: lo type: loopback state: up mac-address: 00:00:00:00:00:00 mtu: 65536 ipv4: enabled: true address: - ip: 127.0.0.1 prefix-length: 8   This will be turned into the needed nmstate configuration files when the image is built.  ","version":"Next","tagName":"h3"},{"title":"Running the image build​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#running-the-image-build","content":" To build the image, we can run:  docker run --rm -it -v $PWD/eib/:/eib registry.opensuse.org/home/atgracey/eib/container/containerfile/eib:latest /usr/bin/eib -config-file eib-config.yaml -config-dir /eib -build-dir /eib/_build   TODO: Change the image to the released version when available  This will create a timestamped folder in $PWD/eib/_build/ that includes the produced iso image (called eib-image.iso)  ","version":"Next","tagName":"h3"},{"title":"Using your newly built image​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#using-your-newly-built-image","content":" We can use this new image by burning it to a USB drive.  // TODO: add tabs for each OS  ","version":"Next","tagName":"h3"},{"title":"Updating your system while airgapped​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#updating-your-system-while-airgapped","content":" // TODO: ???  ","version":"Next","tagName":"h3"},{"title":"Next steps​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#next-steps","content":" Due to how many configuration options the Edge Image Builder offers, a quickstart that goes through all of them would not be easily readable. Here are some links to commonly needed configurations options:  // TODO: add links  Systemd-servicesRegistration with SUSE ManagerAirgapped container side-loadingKubernetes deployment???  ","version":"Next","tagName":"h2"},{"title":"Planned changes​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#planned-changes","content":" ","version":"Next","tagName":"h2"},{"title":"Additional Resources​","type":1,"pageTitle":"Standalone Clusters with Edge Image Builder","url":"/docs/quickstart/eib#additional-resources","content":" //TODO: what should be here ","version":"Next","tagName":"h2"},{"title":"BMC automated deployments with Metal3","type":0,"sectionRef":"#","url":"/docs/quickstart/metal3","content":"","keywords":"","version":"Next"},{"title":"What is Metal3?​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#what-is-metal3","content":" ","version":"Next","tagName":"h2"},{"title":"Why use this method​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#why-use-this-method","content":" ","version":"Next","tagName":"h2"},{"title":"High level architecture​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#high-level-architecture","content":" ","version":"Next","tagName":"h2"},{"title":"How to use Metal3​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#how-to-use-metal3","content":" ","version":"Next","tagName":"h2"},{"title":"Setup bootstrap cluster​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#setup-bootstrap-cluster","content":" ","version":"Next","tagName":"h3"},{"title":"Add systems​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#add-systems","content":" ","version":"Next","tagName":"h3"},{"title":"Create downstream clusters​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#create-downstream-clusters","content":" ","version":"Next","tagName":"h3"},{"title":"Next steps​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#next-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Planned changes​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#planned-changes","content":" ","version":"Next","tagName":"h2"},{"title":"Additional Resources​","type":1,"pageTitle":"BMC automated deployments with Metal3","url":"/docs/quickstart/metal3#additional-resources","content":"","version":"Next","tagName":"h2"},{"title":"Remote host onboarding with Elemental","type":0,"sectionRef":"#","url":"/docs/quickstart/elemental","content":"","keywords":"","version":"Next"},{"title":"What is Elemental?​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#what-is-elemental","content":" Elemental is a software stack that provides for remote control over the lifecycles of the Operating System, Kubernetes, and workload in a secure and declarative way.  ","version":"Next","tagName":"h2"},{"title":"Why use this method​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#why-use-this-method","content":" Elemental can be very useful for scenarios where the devices that you want to control are not on the same network as the upstream cluster or don't have a BMC onboard to allow more direct control.  TODO: expand  ","version":"Next","tagName":"h2"},{"title":"High Level Architecture​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#high-level-architecture","content":" TODO: Draw image  ","version":"Next","tagName":"h2"},{"title":"Resources Needed​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#resources-needed","content":" At the very least, you need:  A host for the &quot;upstream&quot; cluster (the one hosting Rancher and Elemental) with: Minimum 8GB RAM and 20GB disk space for development or testingFor production use: See https://ranchermanager.docs.rancher.com/pages-for-subheaders/installation-requirements#hardware-requirements A host to be controlled Could be physical device or Virtual MachineMinimum 25GB disk A resolvable hostname Or static ip address to use with a service like sslip.io A host to build the installation media with This would likely be your local computer for most peopleKubectl, Podman, and Helm need to be installedTODO: edit this after writing the EIB doc (If using physical hardware) A USB flash drive to boot from Existing will be overwritten as part of the process  In the writing of this guide, I will be using a Digital Ocean droplet to host the upstream cluster and an Intel NUC as the downstream device. For building the installation media, I will be using my desktop running openSUSE Tumbleweed.  ","version":"Next","tagName":"h2"},{"title":"How to Use Elemental​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#how-to-use-elemental","content":" The basic steps to install and use Elemental are:  Build bootstrap clusterInstall RancherInstall Elemental Operator and Rancher UI ExtensionBuild installer diskBoot device with installerCreate downstream cluster  TODO: Record video and embed  ","version":"Next","tagName":"h2"},{"title":"Build Bootstrap Cluster​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#build-bootstrap-cluster","content":" The first thing we need to do is create a cluster that can host Rancher and Elemental. This cluster needs to be routable from the network that the downstream nodes are connected to.  TODO: make prose  Create Kubernetes ClusterInstall RancherInstall Elemental(Optionally) Install Elemental UI Extension  Create Kubernetes Cluster​  If you are using a hyperscaler (such as Azure, AWS, or Google Cloud), it's likely easiest to set up a cluster using their built in tooling. For the sake of conciseness in this guide, we will not be detailing the process of each of these options.  If you are installing onto bare metal or another hosting service where you need to also provide the Kubernetes distribution itself, we recommend using RKE2.  TODO: Should I add more detail?  Setup DNS​  Before continuing, you will need to setup access to your cluster. As with the setup of the cluster itself, how you configure DNS will be different depending on where it's being hosted.  tip If you don't want to deal with setting up DNS records (for example this is just an ephemeral test server), you can use a service like sslip.io instead. With this service, you can resolve any ip address with &lt;address&gt;.sslip.io.  Install Rancher​  To install Rancher, you need to get access to the kubeapi of the cluster you just created. This looks differently depending on what distribution of Kubernetes is being used.  For RKE2,the kubeconfig file will have been written to /etc/rancher/rke2/rke2.yaml. Save this file to ~/.kube/config on your local system. You may need to edit the file to include the correct externally routable IP address or hostname.  The simplest way to get Rancher installed is with these commands that can be found in the Rancher Documentation:  TODO: simplify these a bit TODO: Do I need to explicitly allow for &quot;local&quot; management in the helm install?  LinuxMac OSWindows Install cert-manager: helm repo add rancher-latest https://releases.rancher.com/server-charts/latest kubectl create namespace cattle-system kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace Then install Rancher itself: helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set hostname=&lt;DNS or sslip from above&gt; \\ --set replicas=1 \\ --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt;   note If this is intended to be a production system, please use cert-manager to configure a real certificate (such as one from Let's Encrypt).  Browse to the hostname you set up and log in to Rancher with the bootstrapPassword you used. There is a short setup process that you will be guided through.  Install Elemental​  With Rancher installed, you can now install Elemental itself!  The helm chart for Elemental is published as an OCI artifact so the installation is a little simpler than other charts. It can be installed from either the same shell you used to install Rancher or in the browser from within Rancher's shell.  helm install --create-namespace -n cattle-elemental-system elemental-operator-crds oci://registry.suse.com/rancher/elemental-operator-crds-chart helm install --create-namespace -n cattle-elemental-system elemental-operator oci://registry.suse.com/rancher/elemental-operator-chart   (Optionally) Install the Elemental UI Extension​  If you want to use the Elemental UI, you can go log in to your Rancher instance, click on the &quot;hamburger menu&quot; in the top left, then    From the &quot;Available&quot; tab on this page, you can then click &quot;Install&quot; on the Elemental card:    Confirm that you want to install the extension:    After this installs, you will be prompted to reload the page.    Once you reload, you can access the Elemental Extension through the &quot;OS Management&quot; global app.    TODO: annotate screenshots  TODO: retake screenshots with Rancher 2.8.2 and Elemental 1.4 once released  Configure Elemental​  To allow machines to register to Elemental, we need to create a MachineRegistration object in the fleet-default namespace.  A fairly basic version of this object is:  apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: ele-quickstart-nodes namespace: fleet-default spec: machineName: &quot;${System Information/Manufacturer}-${System Information/Serial Number}&quot; machineInventoryLabels: manufacturer: &quot;${System Information/Manufacturer}&quot; productName: &quot;${System Information/Product Name}&quot;   We can create this object in a few ways:  YAMLUI Extension Create a file with the yaml from above called registration.yaml the apply it with: kubectl apply -f registration.yaml   Afterwards, find and note the endpoint that gets assigned:  YAMLUI Extension kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath '{.status.registrationURL}'   This URL will be used in the next step.  ","version":"Next","tagName":"h3"},{"title":"Build installation media​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#build-installation-media","content":" While the current version of Elemental (at time of writing) does have a way to build it's own installation media, we need to do this with the Edge Image Builder instead so the resulting system is built with SLE Micro as the base Operating System.  tip For more details on the Edge Image Builder, you can check out the Getting Started Guide for it. Or, for a more full discussion on it's capabilities there is a [full page documenting it].  TODO: write the simplest EIB flow possible here  From a system with Podman, Docker, or Rancher desktop installed, run  LinuxMac OSWindows TODO: Add elemental rpms mkdir -p eib_quickstart/base-images mkdir -p eib_quickstart/elemental mkdir -p eib_quickstart/rpms curl &lt;REGISTRATION URL FROM ABOVE&gt; -o eib_quickstart/elemental/elemental.yaml echo &gt; /eib_quickstart/eib-config.yaml &lt;&lt;&lt; HEREDOC apiVersion: 1.0 image: imageType: iso baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso outputImageName: elemental-image.iso HEREDOC docker run --rm -it -v $PWD/eib_quickstart/:/eib registry.opensuse.org/home/atgracey/eib/container/containerfile/eib:latest /usr/bin/eib -config-file eib-config.yaml -config-dir /eib -build-dir /eib/_build TODO: update image to released image on release note Podman can be used in place of docker  If you are booting a physical device, we need to burn the image to a USB flash drive. This can be done with:  LinuxMac OSWindows TODO: how to find sdX sudo dd if=/path/to/output.iso of=/dev/sdX   ","version":"Next","tagName":"h3"},{"title":"Boot downstream nodes​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#boot-downstream-nodes","content":" Now that we have the installation media created, we can boot our downstream nodes with it.  For each of the systems that you want to control with Elemental, add the installation media and boot the device. After installation, it will reboot and register itself.  If you are using the UI Extension, you should see your node appear in the &quot;Inventory of Machines&quot;.  ","version":"Next","tagName":"h3"},{"title":"Create downstream clusters​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#create-downstream-clusters","content":" Now that Elemental can see and control our downstream machines, we can create a cluster from those machine.  YAMLUI Extension There are two objects that we need to create when provisioning a new cluster using Elemental. The first is the MachineInventorySelectorTemplate. This object allows us to specify a mapping between Clusters and the machines in inventory. Create a file called selector.yaml with the contents: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate metadata: name: location-123-selector namespace: fleet-default spec: template: spec: selector: matchLabel: locationID: 123 The we can create it with kubectl create -f selector.yaml This will match any machine in the inventory with the label locationID: 123 so we need to add this label to the machines that should be matched. We can do this with: kubectl label MachineInventory -n fleet-default &lt;name of MachineInventory object&gt; locationID=123 Next, we create the cluster itself. There are a lot of options here but a simple single-node K3s Cluster looks like: apiVersion: provisioning.cattle.io/v1 kind: Cluster metadata: name: location-123 namespace: fleet-default spec: kubernetesVersion: v1.26.11+k3s2 rkeConfig: machinePools: - name: pool1 etcdRole: true controlPlaneRole: true workerRole: true machineConfigRef: kind: MachineInventorySelectorTemplate name: location-123-selector apiVersion: elemental.cattle.io/v1beta1 quantity: 1 To provision this cluster, we can create a file with these contents called cluster.yaml and run kubectl create -f cluster.yaml   After creating these objects, you should see a new kubernetes cluster spin up using the new node you just installed with.  tip To allow for easier grouping of systems, you could add a startup script that finds something in the environment that is known to be unique to that location. For example, if you know that each location will have a unique subnet, you would write a script that finds the network prefix and adds a label to the corresponding MachineInventory. This would typically be custom to your system's design but could look like: INET=`ip addr show dev eth0 | grep &quot;inet\\ &quot;` elemental-register --label &quot;network=$INET&quot; --label &quot;network=$INET&quot; /oem/registration   ","version":"Next","tagName":"h3"},{"title":"Next steps​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#next-steps","content":" TODO: prose  Some recommended next steps after using this guide are:  End to end automation with Fleet (TODO: Create page and link)Additional network configuration options (TODO: Link to right section in EIB)TODO: ???  ","version":"Next","tagName":"h2"},{"title":"Planned changes​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#planned-changes","content":" There are a few changes to this that are currently planned (at time of writing):  Improved support for and integration with SLE Micro 6.x​  TODO: write more about this  Image building done in the Elemental OperatorBoth A/B and RPM based transactional updatesUI improvements for non-teal images  These improvements are expected to be included in the SUSE Edge 3.1  ","version":"Next","tagName":"h2"},{"title":"Additional Resources​","type":1,"pageTitle":"Remote host onboarding with Elemental","url":"/docs/quickstart/elemental#additional-resources","content":" TODO: What additional resources should we link? ","version":"Next","tagName":"h2"},{"title":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","type":0,"sectionRef":"#","url":"/docs/product/atip/features","content":"","keywords":"","version":"Next"},{"title":"Bios configuration​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#bios-configuration","content":" Note: This configuration depends on the hardware vendor, so please, check with your hardware vendor the best configuration to be used.  This section is really important to optimize the performance of the real time kernel in the hardware, because some of this parameters could increase not limiting the performance of the real time kernel. The next table shows the recommended configuration for the most common hardware vendors:  Option\tValue\tDescriptionWorkload Profile\tTelco Optimized\tTelco profile to optimize the performance in the hardware. Boot Performance Mode\tMax. Performance\tMaximize the performance in the boot process. Hyper- Threading (Logical Proccesor)\tEnable\tThis option enables Intel® Hyper-Threading Technology for logical processor enabling and converting processor cores (pCores) to logical cores (lCores). Virtualization Technology (XAPIC)\tEnable\tThis option is for Extended Advanced Programmable Interrupt Controller (xAPIC) support for the Intel® Virtualization Technology for Directed I/O (Intel® VT-d) feature. uncore frequency scaling\tDisable\tIf enabled, Uncore Frequency Scaling (UFS) allows the uncore to operate at a lower frequency when the Power Control Unit (PCU) has detected low utilization. Conversely, UFS allows the uncore to operate at a higher frequency when the PCU has detected high utilization. CPU P-State Control (EIST PSD Function\tHW_ALL\toptimization of the voltage and CPU fequency during operation CPU C-State Control\tDisable\tThis option is for the CPU C-State Control feature, which provides power savings by placing the processor into lower power states when the processor is idle. CPU C1E Support\tDisable\tThis option is for the CPU Enhanced Halt (C1E) feature, which provides power savings by placing the processor into a low power state when the processor is idle. AVX License Pre-Grant\tEnable\tIf enabled, this option enables the pre-grant license level selection based on workload with the AVX ICCP Pre-Grant Level option. AVX ICCP Pre- Grant Level\tLevel 5\tThis option selects a workload level for the Intel® Advanced Vector Extensions (Intel® AVX): Intel® AVX-512 Heavy AVX P1\tLevel 2\tThis option serves a dual purpose: 1 -Specifies the base P1 ratio for Intel® Streaming SIMD Extensions (Intel® SSE) or Intel® AVX workloads. 2- Pre-grants a license level based on the workload level. Energy Efficient Turbo\tDisable\tThis option allows entry into the Intel® Turbo Boost Technology frequency when the Power Control Unit (PCU) has detected high utilization. Turbo Mode\tEnable\tEnabling this Intel® Turbo Boost Technology mode setting allows the CPU cores to operate at higher than the rated frequency. GPSS timer\t0us\tThis option allows the reduction of the Global P-State Selection (GPSS) timer to be set from: 0 μs to 500 μs LLC prefetch\tEnable\tThis option enables Last Level Cache (LLC) hardware prefetch logic. Frequency Prioritization (RAPL Prioritization)\tDisable\tThis setting controls whether the Running Average Power Limit (RAPL) balancer is enabled. If enabled, it activates per core power budgeting. Hardware P-States\tNative with no Legacy Support\tWhen enabled, this option allows the hardware to choose a Performance State (P-State) based on an OS request (that is, a legacy P-State). EPP enable3\tDisable\tWhen this option is enabled, the system uses the energy performance bias register for the Energy Performance Preference (EPP) input to make decision on Performance State (P-State) or Processor Core Idle State (C-State) transitions. APS Rocketing\tDisable\tRocketing mechanism in the HWP p-state selection for pcode algorithm. Rocketing enables the core ratio to jump to max turbo instantaneously as opposed to a smooth ramp Scalability\tDisable\tCore Performance to frequency scalability based on optimizations in the CPU. Native ASPM\tDisable\tASPM off not controlled by BIOS or OS. Power Performance Tuning\tOS Controls EPB\tThis option selects the BIOS or OS that controls the Energy Performance Bias (EPB) functionality. Workload Configuration\tI/O sensitive\tThis option allows the system power and performance profile to be set to favor compute intensive workload or I/O sensitive workload. Dynamic L1\tDisable\tThis option applies only to the package-level setting to allow dynamically entering the lower power link state L1. Set Fan Profile\tPerformance\tThis option allows the fan profile to be set to Performance, Balanced, or Quiet. Cooling Configuration - Fan Speed Offset\tMedium\tThis option allows the fan speed offset to be set to Low, Medium, or High.  ","version":"Next","tagName":"h2"},{"title":"Kernel Real Time​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#kernel-real-time","content":" The real time kernel image is not necessarily better than a standard kernel. It is a different kernel tuned to a specific use case. The real time kernel is tuned for lower latency at the cost of throughput. The real time kernel is not recommended for general purpose use, but in our case, this is the recommended kernel for Telco Workloads.  There are 4 top features:  Deterministic Execution:  Get greater predictability – ensure critical business processes complete in time, every time and deliver high quality of service, even under heavy system loads. By shielding key system resources for high-priority processes, you can ensure greater predictability for time-sensitive applications.  Low Jitter:  The low jitter built upon the highly deterministic technology helps to keep applications synchronized with the real world. This helps services that need ongoing and repeated calculation.  Priority Inheritance:  Priority inheritance refers to the ability of a lower priority process to assume a higher priority when there is a higher priority process that requires the lower priority process to finish before it can accomplish its task. SUSE Linux Enterprise Real Time solves these priority inversion problems for mission-critical processes.  Thread Interrupts:  Processes running in interrupt mode in a general-purpose operating system are not preemptible. With SUSE Linux Enterprise Real Time these interrupts have been encapsulated by kernel threads, which are interruptible, and in turn allow the hard and soft interrupts to be preempted by user-defined higher priority processes.  In our case, if you have installed a real time image like SLE Micro RT, kernel real time is already installed and you don't need to install it again.  You could check it looking for the kernel and see if contains the rt string at the end of the kernel info:  uname -r 5.14.21-150400.15.11-rt   For more information about the real time kernel, please visit https://www.suse.com/products/realtime/  ","version":"Next","tagName":"h2"},{"title":"CPU Tuned Configuration​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#cpu-tuned-configuration","content":" The first thing is to create a profile for the cpu cores we want to isolate. In this case, we will isolate the cores 1-30 and 33-62.  echo &quot;export tuned_params&quot; &gt;&gt; /etc/grub.d/00_tuned echo &quot;isolated_cores=1-30,33-62&quot; &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf tuned-adm profile cpu-partitioning Cannot talk to Tuned daemon via DBus. Is Tuned daemon running? Trying to (re)start tuned... Tuned (re)started, changes applied.   Then we need to modify grub option to isolate cpu cores as well as another important parameters for the cpu usage.  Modify in /etc/default/grub the next line, to add the cpu cores to isolate. The next options are the most important to be customized with your current hardware:  parameter\tvalue\tdescriptionisolcpu\t1-30,33-62\tIsolate the cores 1-30 and 33-62 skew_tick\t1\tThis option allows the kernel to skew the timer interrupts across the isolated CPUs. nohz\ton\tThis option allows the kernel to run the timer tick on a single CPU when the system is idle. nohz_full\t1-30,33-62\tkernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation. rcu_nocbs\t1-30,33-62\tThis option allows the kernel to run the RCU callbacks on a single CPU when the system is idle. kthread_cpus\t0,31,32,63\tThis option allows the kernel to run the kthreads on a single CPU when the system is idle. irqaffinity\t0,31,32,63\tThis option allows the kernel to run the interrupts on a single CPU when the system is idle. processor.max_cstate\t1\tThis option prevents the CPU from dropping into a sleep state when idle intel_idle.max_cstate\t0\tThis option disables the intel_idle driver and allows acpi_idle to be used  With the values showed above, we are isolating 60 cores, and we are using 4 cores for the OS.  Let's modify the grub file with the previous values:  vi /boot/efi/EFI/sle_rt/grub.cfg set tuned_params=&quot;skew_tick=1 nohz=on nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 tuned.non_isolcpus=80000001,80000001 nosoftlockup&quot; vi /etc/default/grub GRUB_CMDLINE_LINUX=&quot;intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpu=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll&quot; transactional-update grub.cfg   To validate that the parameters are applied after reboot, you could check:  cat /proc/cmdline   ","version":"Next","tagName":"h2"},{"title":"Multus + Calico​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#multus--calico","content":" Multus CNI is a CNI plugin that enables attaching multiple network interfaces to pods. Multus does not replace CNI plugins, instead it acts as a CNI plugin multiplexer. Multus is useful in certain use cases, especially when pods are network intensive and require extra network interfaces that support dataplane acceleration techniques such as SR-IOV.  Multus can not be deployed standalone. It always requires at least one conventional CNI plugin that fulfills the Kubernetes cluster network requirements. That CNI plugin becomes the default for Multus, and will be used to provide the primary interface for all pods. In our case, most of the workloads in Telco will be deployed using Multus + calico.  To enable Multus on RKE2 cluster, add multus as the first list entry in the cni config key, followed by the name of the plugin you want to use alongside Multus (or none if you will provide your own default plugin). Note that multus must always be in the first position of the list. For example, to use Multus with calico as the default plugin you could specify:  # /etc/rancher/rke2/config.yaml cni: - multus - calico   This can also be specified with command-line arguments, i.e. --cni=multus,calico or --cni=multus --cni=calico.  You could also install Multus directly during the edge cluster installation:    For more information about Multus, please visit https://github.com/k8snetworkplumbingwg/multus-cni For more information about CNI plugins, please visit https://docs.rke2.io/install/network_options  ","version":"Next","tagName":"h2"},{"title":"SRIOV​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#sriov","content":" SR-IOV allows a device, such as a network adapter, to separate access to its resources among various PCIe hardware functions. There are different ways to deploy SRIOV, and in this case, we will show two different options:  Option 1: using the SRIOV CNI device plugins and a config map to configure it properly. Option 2: using the SRIOV helm chart from Rancher to make this deployment easy.  ","version":"Next","tagName":"h2"},{"title":"Option 1 - Installation of SR-IOV CNI device plugins and a config map to configure it properly​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#option-1---installation-of-sr-iov-cni-device-plugins-and-a-config-map-to-configure-it-properly","content":" Prepare the config map for the device plugin​  You could get the information to fill the config map from the lspci command:  lspci | grep -i acc 8a:00.0 Processing accelerators: Intel Corporation Device 0d5c lspci | grep -i net xr11-1:~ # lspci | grep -i net 19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11) 19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11) 19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11) 19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11) 51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02) 51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02) 51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02) 51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)   0d5d is the VF from the FEC card (take a look that it's different than the lspci | grep acc result, because it's the VF, not the PF). Normally it's the first VF of the card, so the last name will be consecutive after VF creation.  The config map consists of a JSON file that describe devices using filters to discover and creates some groups for the interfaces. The most important is to understand the filters and the groups. The filters are used to discover the devices and the groups are used to create the interfaces.  For example, we could filter using:  vendorID: 8086 (Intel)deviceID: 0d5d (FEC)driver: vfio-pci (SRIOV driver)pfNames: p2p1 (PF name)  We could also set placesholders like:  pfNames: [&quot;eth1#1,2,3,4,5,6&quot;]  Regarding the groups, we could create a group for the FEC card and another group for the Intel card even creating some prefix depending our use case:  resourceName: pci_sriov_net_bh_dpdkresourcePrefix: Rancher.io  There are a lot of combinations in order to discover and create the resource group to allocate some VFs to the pods.  For more information about the filters and groups, please visit https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin  cat &lt;&lt;EOF | k apply -f - apiVersion: v1 kind: ConfigMap metadata: name: sriovdp-config namespace: kube-system data: config.json: | { &quot;resourceList&quot;: [ { &quot;resourceName&quot;: &quot;intel_fec_5g&quot;, &quot;devicetype&quot;: &quot;accelerator&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;0d5d&quot;] } }, { &quot;resourceName&quot;: &quot;intel_sriov_odu&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;1889&quot;], &quot;drivers&quot;: [&quot;vfio-pci&quot;], &quot;pfNames&quot;: [&quot;p2p1&quot;] } }, { &quot;resourceName&quot;: &quot;intel_sriov_oru&quot;, &quot;selectors&quot;: { &quot;vendors&quot;: [&quot;8086&quot;], &quot;devices&quot;: [&quot;1889&quot;], &quot;drivers&quot;: [&quot;vfio-pci&quot;], &quot;pfNames&quot;: [&quot;p2p2&quot;] } } ] } EOF   Prepare the daemonset for the device plugin​  No changes are needed in the daemonset, so you can use the same upstream daemonset file.  cat &lt;&lt;EOF | k apply -f - --- apiVersion: v1 kind: ServiceAccount metadata: name: sriov-device-plugin namespace: kube-system --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-sriov-device-plugin-amd64 namespace: kube-system labels: tier: node app: sriovdp spec: selector: matchLabels: name: sriov-device-plugin template: metadata: labels: name: sriov-device-plugin tier: node app: sriovdp spec: hostNetwork: true nodeSelector: kubernetes.io/arch: amd64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule serviceAccountName: sriov-device-plugin containers: - name: kube-sriovdp image: ghcr.io/k8snetworkplumbingwg/sriov-network-device-plugin:latest-amd64 imagePullPolicy: IfNotPresent args: - --log-dir=sriovdp - --log-level=10 securityContext: privileged: true resources: requests: cpu: &quot;250m&quot; memory: &quot;40Mi&quot; limits: cpu: 1 memory: &quot;200Mi&quot; volumeMounts: - name: devicesock mountPath: /var/lib/kubelet/ readOnly: false - name: log mountPath: /var/log - name: config-volume mountPath: /etc/pcidp - name: device-info mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp volumes: - name: devicesock hostPath: path: /var/lib/kubelet/ - name: log hostPath: path: /var/log - name: device-info hostPath: path: /var/run/k8s.cni.cncf.io/devinfo/dp type: DirectoryOrCreate - name: config-volume configMap: name: sriovdp-config items: - key: config.json path: config.json EOF   After that you should see the pods running:  kubectl get pods -n kube-system | grep sriov kube-system kube-sriov-device-plugin-amd64-twjfl 1/1 Running 0 2m   Check the interfaces discovered and available in the nodes to be used by the pods:  kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq { &quot;cpu&quot;: &quot;64&quot;, &quot;ephemeral-storage&quot;: &quot;256196109726&quot;, &quot;hugepages-1Gi&quot;: &quot;40Gi&quot;, &quot;hugepages-2Mi&quot;: &quot;0&quot;, &quot;intel.com/intel_fec_5g&quot;: &quot;1&quot;, &quot;intel.com/intel_sriov_odu&quot;: &quot;4&quot;, &quot;intel.com/intel_sriov_oru&quot;: &quot;4&quot;, &quot;memory&quot;: &quot;221396384Ki&quot;, &quot;pods&quot;: &quot;110&quot; }   The FEC will be intel.com/intel_fec_5g and the value will be 1 The VF will be intel.com/intel_sriov_odu or intel.com/intel_sriov_oru if you deploy it with device plugin and the config map without helm charts  Important Note: If you don't get the interfaces available here, does not make sense continue with the workload, because interface will not be available for pods  ","version":"Next","tagName":"h3"},{"title":"Option 2 - Installation using Rancher using Helm chart for SR-IOV CNI and device plugins​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#option-2---installation-using-rancher-using-helm-chart-for-sr-iov-cni-and-device-plugins","content":" Get helm if not present​  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 500 get_helm.sh ./get_helm.sh   Install SRIOV​  This part could be done in two ways, using the CLI or using the Rancher UI  Install Operator from CLI  helm repo add rancher-charts https://raw.githubusercontent.com/rancher/charts/dev-v2.7/ helm install sriov-crd rancher-charts/sriov-crd helm install sriov rancher-charts/sriov -n kube-system   Install Operator from Rancher UI  Once your cluster is installed and you have access to the Rancher UI, you can install the SR-IOV Operator from the Rancher UI from the apps tab:    Check the deployed resources crd and pods​  kubectl -n sriov-network-operator get crd  kubectl -n sriov-network-operator get pods  Check the label in the nodes​  Now, if you have all resources running, the label should appears automatically in your node:  kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable feature.node.kubernetes.io/network-sriov.capable: &quot;true&quot;   if not present, you can add it manually:  kubectl label $(kubectl get nodes -oname) feature.node.kubernetes.io/network-sriov.capable=true   Review the daemonset to see the new sriov-network-config-daemon and sriov-rancher-nfd-worker as active and ready​  kubectl get daemonset -A NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-system calico-node 1 1 1 1 1 kubernetes.io/os=linux 15h cattle-sriov-system sriov-network-config-daemon 1 1 1 1 1 feature.node.kubernetes.io/network-sriov.capable=true 45m cattle-sriov-system sriov-rancher-nfd-worker 1 1 1 1 1 &lt;none&gt; 45m kube-system rke2-ingress-nginx-controller 1 1 1 1 1 kubernetes.io/os=linux 15h kube-system rke2-multus-ds 1 1 1 1 1 kubernetes.io/arch=amd64,kubernetes.io/os=linux 15h   After some minutes (can take up to 10 min to be updated) the nodes detected and configured will appear:  kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A NAMESPACE NAME AGE cattle-sriov-system xr11-2 83s   Check the interfaces detected​  the interfaces discovered should be the pci address of the network device. Check this information with lspci command in the host.  $ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml apiVersion: v1 items: - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodeState metadata: creationTimestamp: &quot;2023-06-07T09:52:37Z&quot; generation: 1 name: xr11-2 namespace: cattle-sriov-system ownerReferences: - apiVersion: sriovnetwork.openshift.io/v1 blockOwnerDeletion: true controller: true kind: SriovNetworkNodePolicy name: default uid: 80b72499-e26b-4072-a75c-f9a6218ec357 resourceVersion: &quot;356603&quot; uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad spec: dpConfigVersion: &quot;356507&quot; status: interfaces: - deviceID: &quot;1592&quot; driver: ice eSwitchMode: legacy linkType: ETH mac: 40:a6:b7:9b:35:f0 mtu: 1500 name: p2p1 pciAddress: &quot;0000:51:00.0&quot; totalvfs: 128 vendor: &quot;8086&quot; - deviceID: &quot;1592&quot; driver: ice eSwitchMode: legacy linkType: ETH mac: 40:a6:b7:9b:35:f1 mtu: 1500 name: p2p2 pciAddress: &quot;0000:51:00.1&quot; totalvfs: 128 vendor: &quot;8086&quot; syncStatus: Succeeded kind: List metadata: resourceVersion: &quot;&quot;   Note: If your interface is not detected here you should ensure that it is present in the next config map kubectl get cm supported-nic-ids -oyaml -n cattle-sriov-system if your device is not there you have to edit the config map adding the right values to be discovered (should be necessary to restart the daemonset sriov-network-config-daemon)  Create the NetworkNode Policy to configure the VFs​  Basically, you will create some VFs (numVfs) from the device (rootDevices) and will be configured with the driver (deviceType) and the MTU (mtu):  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-dpdk namespace: kube-system spec: nodeSelector: feature.node.kubernetes.io/network-sriov.capable: &quot;true&quot; resourceName: intelnicsDpdk deviceType: vfio-pci numVfs: 8 mtu: 1500 nicSelector: deviceID: &quot;1592&quot; vendor: &quot;8086&quot; rootDevices: - 0000:51:00.0 EOF   Validate configurations​  kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq { &quot;cpu&quot;: &quot;64&quot;, &quot;ephemeral-storage&quot;: &quot;256196109726&quot;, &quot;hugepages-1Gi&quot;: &quot;60Gi&quot;, &quot;hugepages-2Mi&quot;: &quot;0&quot;, &quot;intel.com/intel_fec_5g&quot;: &quot;1&quot;, &quot;memory&quot;: &quot;200424836Ki&quot;, &quot;pods&quot;: &quot;110&quot;, &quot;rancher.io/intelnicsDpdk&quot;: &quot;8&quot; }   Create the sriov network (Optional, in case we need a different network):​  cat &lt;&lt;EOF | k apply -f - apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: network-dpdk namespace: kube-system spec: ipam: | { &quot;type&quot;: &quot;host-local&quot;, &quot;subnet&quot;: &quot;192.168.0.0/24&quot;, &quot;rangeStart&quot;: &quot;192.168.0.20&quot;, &quot;rangeEnd&quot;: &quot;192.168.0.60&quot;, &quot;routes&quot;: [{ &quot;dst&quot;: &quot;0.0.0.0/0&quot; }], &quot;gateway&quot;: &quot;192.168.0.1&quot; } vlan: 500 resourceName: intelnicsDpdk EOF   Check the network created:  kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml apiVersion: v1 items: - apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk creationTimestamp: &quot;2023-06-08T11:22:27Z&quot; generation: 1 name: network-dpdk namespace: kube-system resourceVersion: &quot;13124&quot; uid: df7c89f5-177c-4f30-ae72-7aef3294fb15 spec: config: '{ &quot;cniVersion&quot;:&quot;0.3.1&quot;, &quot;name&quot;:&quot;network-dpdk&quot;,&quot;type&quot;:&quot;sriov&quot;,&quot;vlan&quot;:500,&quot;vlanQoS&quot;:0,&quot;ipam&quot;:{&quot;type&quot;:&quot;host-local&quot;,&quot;subnet&quot;:&quot;192.168.0.0/24&quot;,&quot;rangeStart&quot;:&quot;192.168.0.10&quot;,&quot;rangeEnd&quot;:&quot;192.168.0.60&quot;,&quot;routes&quot;:[{&quot;dst&quot;:&quot;0.0.0.0/0&quot;}],&quot;gateway&quot;:&quot;192.168.0.1&quot;} }' kind: List metadata: resourceVersion: &quot;&quot;   ","version":"Next","tagName":"h3"},{"title":"DPDK​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#dpdk","content":" ","version":"Next","tagName":"h2"},{"title":"Kernel parameters​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#kernel-parameters","content":" To use dpdk using some drivers we need to enable some parameters in the kernel:  parameter\tvalue\tdescriptioniommu\tpt\tThis option allows to use vfio for the dpdk interfaces intel_iommu\ton\tThis option enables to use vfio for VFs.  To enable this parameters we need to add them to the kernel command line:  vi /etc/default/grub   GRUB_CMDLINE_LINUX=&quot;intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpu=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll&quot;   Then you need to update the grub configuration and reboot the system to apply the changes:  transactional-update grub.cfg reboot   To validate that the parameters are applied after the reboot you can check the command line:  cat /proc/cmdline   ","version":"Next","tagName":"h3"},{"title":"Load vfio-pci kernel module​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#load-vfio-pci-kernel-module","content":" modprobe vfio-pci   ","version":"Next","tagName":"h3"},{"title":"Create VFs from the NICs​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#create-vfs-from-the-nics","content":" To create 4 VFs PCI addresses for example for 2 different NICs we need to execute the following commands:  echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs   ","version":"Next","tagName":"h3"},{"title":"Bind the new VFs with the vfio-pci driver​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#bind-the-new-vfs-with-the-vfio-pci-driver","content":" dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3   ","version":"Next","tagName":"h3"},{"title":"Review the configuration applied:​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#review-the-configuration-applied","content":" dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio 0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio Network devices using kernel driver =================================== 0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active* 0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci 0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci 0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci 0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci 0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci   ","version":"Next","tagName":"h3"},{"title":"Huge Pages​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#huge-pages","content":" When a process uses RAM, the CPU marks it as used by that process. For efficiency, the CPU allocates RAM in chunks—4K bytes is the default value on many platforms. Those chunks are named pages. Pages can be swapped to disk, etc.  Since the process address space is virtual, the CPU and the operating system need to remember which pages belong to which process, and where each page is stored. The more pages you have, the more time it takes to find where memory is mapped. When a process uses 1GB of memory, that's 262144 entries to look up (1GB / 4K). If one page table entry consume 8 bytes, that's 2MB (262144 * 8) to look up.  Most current CPU architectures support larger-than-default pages, which give the CPU/OS less entries to look-up.  ","version":"Next","tagName":"h2"},{"title":"Kernel parameters​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#kernel-parameters-1","content":" To enable the huge pages we should add the next kernel parameters:  parameter\tvalue\tdescriptionhugepagesz\t1G\tThis options allows to set the size of huge pages to 1G hugepages\t40\tThis is the number of hugepages defined before default_hugepagesz\t1G\tThis is the default value to get the huge pages  Modify the grub file to add them to the kernel command line:  vi /etc/default/grub   GRUB_CMDLINE_LINUX=&quot;intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpu=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll&quot;   ","version":"Next","tagName":"h3"},{"title":"Usage of huge pages​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#usage-of-huge-pages","content":" In order to use the huge pages we need to mount them:  mkdir -p /hugepages mount -t hugetlbfs nodev /hugepages   Now you could deploy your kubernetes workload creating the resources as well as the volumes:  ... resources: requests: memory: &quot;24Gi&quot; hugepages-1Gi: 16Gi intel.com/intel_sriov_oru: '4' limits: memory: &quot;24Gi&quot; hugepages-1Gi: 16Gi intel.com/intel_sriov_oru: '4' ...   ... volumeMounts: - name: hugepage mountPath: /hugepages ... volumes: - name: hugepage emptyDir: medium: HugePages ...   ","version":"Next","tagName":"h3"},{"title":"CPU Pinning Configuration​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#cpu-pinning-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#requirements","content":" You must have the CPU tuned to the performance profile covered on this sectionYou must have the RKE2 cluster kubelet configured with the cpu management arguments covered on this section  ","version":"Next","tagName":"h3"},{"title":"Use CPU Pinning on kubernetes​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#use-cpu-pinning-on-kubernetes","content":" There are three ways to use that feature using the Static Policy defined in kubelet depending on the requests and limits you define on your workload:  BestEffort QoS Class: If you don't define any request or limit for CPU, the pod will be scheduled on the first CPU available on the system.  An example to use the BestEffort QoS Class could be:  spec: containers: - name: nginx image: nginx   Burstable QoS Class: If you define a request for CPU, which is not equal to the limits, or maybe there is no CPU request.  Examples to use the Burstable QoS Class could be:  spec: containers: - name: nginx image: nginx resources: limits: memory: &quot;200Mi&quot; requests: memory: &quot;100Mi&quot;   or  spec: containers: - name: nginx image: nginx resources: limits: memory: &quot;200Mi&quot; cpu: &quot;2&quot; requests: memory: &quot;100Mi&quot; cpu: &quot;1&quot;   Guaranteed QoS Class: If you define a request for CPU, which is equal to the limits.  An example to use the Guaranteed QoS Class could be:  spec: containers: - name: nginx image: nginx resources: limits: memory: &quot;200Mi&quot; cpu: &quot;2&quot; requests: memory: &quot;200Mi&quot; cpu: &quot;2&quot;   ","version":"Next","tagName":"h3"},{"title":"NUMA Aware scheduling​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#numa-aware-scheduling","content":" Non-Uniform Memory Access or Non-Uniform Memory Architecture (NUMA) is a physical memory design used in SMP (multiprocessors) architecture, where the memory access time depends on the memory location relative to a processor. Under NUMA, a processor can access its own local memory faster than non-local memory, that is, memory local to another processor or memory shared between processors.  ","version":"Next","tagName":"h2"},{"title":"Identify NUMA nodes​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#identify-numa-nodes","content":" To identify the NUMA nodes on your system you can use the next command:  numactl --hardware available: 1 nodes (0) node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 node 0 size: 257167 MB node 0 free: 246390 MB node distances: node 0 0: 10   Note: In this case we have only one NUMA node NUMA has to enabled in the BIOS. If dmesg does not have records of numa initialization during bootup, then it is possible that NUMA related messages in the kernel ring buffer might have been overwritten.  ","version":"Next","tagName":"h3"},{"title":"VRAN Acceleration (Intel ACC100)​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#vran-acceleration-intel-acc100","content":" As communications service providers move from 4G to 5G networks, many are adopting virtualized radio access network (vRAN) architectures for higher channel capacity and easier deployment of edge-based services and applications. vRAN solutions are ideally located to deliver low-latency services with the flexibility to increase or decrease capacity based on the volume of real-time traffic and demand on the network.  ","version":"Next","tagName":"h2"},{"title":"Kernel parameters​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#kernel-parameters-2","content":" To enable the vRAN acceleration we need to enable the following kernel parameters (if not present yet):  parameter\tvalue\tdescriptioniommu\tpt\tThis option allows to use vfio for the dpdk interfaces intel_iommu\ton\tThis option enables to use vfio for VFs.  Modify the grub file to add them to the kernel command line:  vi /etc/default/grub   GRUB_CMDLINE_LINUX=&quot;intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpu=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll&quot;   Then you need to update the grub configuration and reboot the system to apply the changes:  transactional-update grub.cfg reboot   To validate that the parameters are applied after the reboot you can check the command line:  cat /proc/cmdline   ","version":"Next","tagName":"h3"},{"title":"Load igb_uio and vfio-pci kernel modules​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#load-igb_uio-and-vfio-pci-kernel-modules","content":" modprobe igb_uio modprobe vfio-pci   ","version":"Next","tagName":"h3"},{"title":"Get interface information Acc100​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#get-interface-information-acc100","content":" Maybe in some cases (depending on the OS) you should add to the path the /sbin/ for the lspci command doing: export PATH=$PATH:/sbin/  lspci | grep -i acc 8a:00.0 Processing accelerators: Intel Corporation Device 0d5c   ","version":"Next","tagName":"h3"},{"title":"Bind the PF with igb_uio module​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#bind-the-pf-with-igb_uio-module","content":" dpdk-devbind.py -b igb_uio 0000:8a:00.0   ","version":"Next","tagName":"h3"},{"title":"Create the VFs from the PF​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#create-the-vfs-from-the-pf","content":" To create 2 vfs from the PF and bind with vfio-pci follow the next steps:  echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/max_vfs dpdk-devbind.py -b vfio-pci 0000:8b:00.0   ","version":"Next","tagName":"h3"},{"title":"Configure acc100 with the proposed configuration file​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#configure-acc100-with-the-proposed-configuration-file","content":" pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg Tue Jun 6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL Tue Jun 6 10:49:20 2023:INFO:Configuration in VF mode Tue Jun 6 10:49:21 2023:INFO: ROM version MM 99AD92 Tue Jun 6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version 1302020 != 10092020 Tue Jun 6 10:49:21 2023:INFO:PF ACC100 configuration complete Tue Jun 6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!   ","version":"Next","tagName":"h3"},{"title":"Check the new VFs created from the FEC PF:​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#check-the-new-vfs-created-from-the-fec-pf","content":" dpdk-devbind.py -s ... ... ... Baseband devices using DPDK-compatible driver ============================================= 0000:8a:00.0 'Device 0d5c' drv=igb_uio unused=vfio-pci 0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=igb_uio Other Baseband devices ====================== 0000:8b:00.1 'Device 0d5d' unused=igb_uio,vfio-pci   ","version":"Next","tagName":"h3"},{"title":"Metal LB (Beta)​","type":1,"pageTitle":"SUSE Adaptive Telco Infrastructure Platform (ATIP)","url":"/docs/product/atip/features#metal-lb-beta","content":" TBC ","version":"Next","tagName":"h2"}],"options":{"indexBaseUrl":true,"id":"default"}}