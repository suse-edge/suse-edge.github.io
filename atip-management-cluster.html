<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SUSE Edge Documentation | Setting up the management cluster</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Setting up the management cluster"/>
<meta name="description" content="The management cluster is the part of SUSE Telco Cloud that is used to manage the provision and lifecycle of the runtime stacks. From a technical poi…"/>
<meta name="book-title" content="SUSE Edge Documentation"/>
<meta name="chapter-title" content="Chapter 41. Setting up the management cluster"/>
<meta name="tracker-url" content="https://github.com/suse-edge/suse-edge.github.io/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Setting up the management cluster"/>
<meta property="og:description" content="The management cluster is the part of SUSE Telco Cloud that…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Setting up the management cluster"/>
<meta name="twitter:description" content="The management cluster is the part of SUSE Telco Cloud that…"/>
<script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": ["TechArticle"],
    "image": "https://www.suse.com/assets/img/suse-white-logo-green.svg",
    
     "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "Products &amp; Solutions"
    },
    

    "headline": "Setting up the management cluster",
  
    "description": "Setting up the management cluster",
      
    "author": [
      {
        "@type": "Corporation",
        "name": "SUSE Product &amp; Solution Documentation Team",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    ],
      

    "about": [
      
    ],
  
    "sameAs": [
          "https://www.facebook.com/SUSEWorldwide/about",
          "https://www.youtube.com/channel/UCHTfqIzPKz4f_dri36lAQGA",
          "https://twitter.com/SUSE",
          "https://www.linkedin.com/company/suse"
    ],
    "publisher": {
      "@type": "Corporation",
      "name": "SUSE",
      "url": "https://documentation.suse.com",
      "logo": {
        "@type": "ImageObject",
        "url": "https://www.suse.com/assets/img/suse-white-logo-green.svg"
      }
    }
  }</script>
<link rel="prev" href="atip-requirements.html" title="Chapter 40. Requirements &amp; Assumptions"/><link rel="next" href="atip-features.html" title="Chapter 42. Telco features configuration"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/script-purejs.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="wide offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">SUSE Edge Documentation</a><span> / </span><a class="crumb" href="id-suse-telco-cloud-documentation.html">SUSE Telco Cloud Documentation</a><span> / </span><a class="crumb" href="atip-management-cluster.html">Setting up the management cluster</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">SUSE Edge Documentation</div><ol><li><a href="suse-edge-documentation.html" class=" "><span class="title-number"> </span><span class="title-name">SUSE Edge 3.4 Documentation</span></a></li><li><a href="id-quick-starts.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quick Starts</span></a><ol><li><a href="quickstart-metal3.html" class=" "><span class="title-number">1 </span><span class="title-name">BMC automated deployments with Metal<sup>3</sup></span></a></li><li><a href="quickstart-elemental.html" class=" "><span class="title-number">2 </span><span class="title-name">Remote host onboarding with Elemental</span></a></li><li><a href="quickstart-eib.html" class=" "><span class="title-number">3 </span><span class="title-name">Standalone clusters with Edge Image Builder</span></a></li><li><a href="quickstart-suma.html" class=" "><span class="title-number">4 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-components.html" class="has-children "><span class="title-number">II </span><span class="title-name">Components</span></a><ol><li><a href="components-rancher.html" class=" "><span class="title-number">5 </span><span class="title-name">Rancher</span></a></li><li><a href="components-rancher-dashboard-extensions.html" class=" "><span class="title-number">6 </span><span class="title-name">Rancher Dashboard Extensions</span></a></li><li><a href="components-rancher-turtles.html" class=" "><span class="title-number">7 </span><span class="title-name">Rancher Turtles</span></a></li><li><a href="components-fleet.html" class=" "><span class="title-number">8 </span><span class="title-name">Fleet</span></a></li><li><a href="components-slmicro.html" class=" "><span class="title-number">9 </span><span class="title-name">SUSE Linux Micro</span></a></li><li><a href="components-metal3.html" class=" "><span class="title-number">10 </span><span class="title-name">Metal<sup>3</sup></span></a></li><li><a href="components-eib.html" class=" "><span class="title-number">11 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="components-nmc.html" class=" "><span class="title-number">12 </span><span class="title-name">Edge Networking</span></a></li><li><a href="components-elemental.html" class=" "><span class="title-number">13 </span><span class="title-name">Elemental</span></a></li><li><a href="components-akri.html" class=" "><span class="title-number">14 </span><span class="title-name">Akri</span></a></li><li><a href="components-k3s.html" class=" "><span class="title-number">15 </span><span class="title-name">K3s</span></a></li><li><a href="components-rke2.html" class=" "><span class="title-number">16 </span><span class="title-name">RKE2</span></a></li><li><a href="components-suse-storage.html" class=" "><span class="title-number">17 </span><span class="title-name">SUSE Storage</span></a></li><li><a href="components-suse-security.html" class=" "><span class="title-number">18 </span><span class="title-name">SUSE Security</span></a></li><li><a href="components-metallb.html" class=" "><span class="title-number">19 </span><span class="title-name">MetalLB</span></a></li><li><a href="components-eco.html" class=" "><span class="title-number">20 </span><span class="title-name">Endpoint Copier Operator</span></a></li><li><a href="components-kubevirt.html" class=" "><span class="title-number">21 </span><span class="title-name">Edge Virtualization</span></a></li><li><a href="components-system-upgrade-controller.html" class=" "><span class="title-number">22 </span><span class="title-name">System Upgrade Controller</span></a></li><li><a href="components-upgrade-controller.html" class=" "><span class="title-number">23 </span><span class="title-name">Upgrade Controller</span></a></li><li><a href="components-suma.html" class=" "><span class="title-number">24 </span><span class="title-name">SUSE Multi-Linux Manager</span></a></li></ol></li><li><a href="id-how-to-guides.html" class="has-children "><span class="title-number">III </span><span class="title-name">How-To Guides</span></a><ol><li><a href="guides-metallb-k3s.html" class=" "><span class="title-number">25 </span><span class="title-name">MetalLB on K3s (using Layer 2 Mode)</span></a></li><li><a href="guides-metallb-k3s-l3.html" class=" "><span class="title-number">26 </span><span class="title-name">MetalLB on K3s (using Layer 3 Mode)</span></a></li><li><a href="guides-metallb-kubernetes.html" class=" "><span class="title-number">27 </span><span class="title-name">MetalLB in front of the Kubernetes API server</span></a></li><li><a href="id-air-gapped-deployments-with-edge-image-builder.html" class=" "><span class="title-number">28 </span><span class="title-name">Air-gapped deployments with Edge Image Builder</span></a></li><li><a href="guides-kiwi-builder-images.html" class=" "><span class="title-number">29 </span><span class="title-name">Building Updated SUSE Linux Micro Images with Kiwi</span></a></li><li><a href="guides-clusterclass-example.html" class=" "><span class="title-number">30 </span><span class="title-name">Using clusterclass to deploy downstream clusters</span></a></li></ol></li><li><a href="tips-and-tricks.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Tips and Tricks</span></a><ol><li><a href="tips-edge-image-builder.html" class=" "><span class="title-number">31 </span><span class="title-name">Edge Image Builder</span></a></li><li><a href="tips-elemental.html" class=" "><span class="title-number">32 </span><span class="title-name">Elemental</span></a></li></ol></li><li><a href="id-third-party-integration.html" class="has-children "><span class="title-number">V </span><span class="title-name">Third-Party Integration</span></a><ol><li><a href="integrations-nats.html" class=" "><span class="title-number">33 </span><span class="title-name">NATS</span></a></li><li><a href="id-nvidia-gpus-on-suse-linux-micro.html" class=" "><span class="title-number">34 </span><span class="title-name">NVIDIA GPUs on SUSE Linux Micro</span></a></li></ol></li><li><a href="day-2-operations.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Day 2 Operations</span></a><ol><li><a href="day2-migration.html" class=" "><span class="title-number">35 </span><span class="title-name">Edge 3.4 migration</span></a></li><li><a href="day2-mgmt-cluster.html" class=" "><span class="title-number">36 </span><span class="title-name">Management Cluster</span></a></li><li><a href="day2-downstream-clusters.html" class=" "><span class="title-number">37 </span><span class="title-name">Downstream clusters</span></a></li></ol></li><li class="active"><a href="id-suse-telco-cloud-documentation.html" class="has-children you-are-here"><span class="title-number">VII </span><span class="title-name">SUSE Telco Cloud Documentation</span></a><ol><li><a href="atip.html" class=" "><span class="title-number">38 </span><span class="title-name">SUSE Telco Cloud</span></a></li><li><a href="atip-architecture.html" class=" "><span class="title-number">39 </span><span class="title-name">Concept &amp; Architecture</span></a></li><li><a href="atip-requirements.html" class=" "><span class="title-number">40 </span><span class="title-name">Requirements &amp; Assumptions</span></a></li><li><a href="atip-management-cluster.html" class=" you-are-here"><span class="title-number">41 </span><span class="title-name">Setting up the management cluster</span></a></li><li><a href="atip-features.html" class=" "><span class="title-number">42 </span><span class="title-name">Telco features configuration</span></a></li><li><a href="atip-automated-provisioning.html" class=" "><span class="title-number">43 </span><span class="title-name">Fully automated directed network provisioning</span></a></li><li><a href="atip-lifecycle.html" class=" "><span class="title-number">44 </span><span class="title-name">Lifecycle actions</span></a></li></ol></li><li><a href="id-troubleshooting-3.html" class="has-children "><span class="title-number">VIII </span><span class="title-name">Troubleshooting</span></a><ol><li><a href="general-troubleshooting-principles.html" class=" "><span class="title-number">45 </span><span class="title-name">General Troubleshooting Principles</span></a></li><li><a href="troubleshooting-kiwi.html" class=" "><span class="title-number">46 </span><span class="title-name">Troubleshooting Kiwi</span></a></li><li><a href="troubleshooting-edge-image-builder.html" class=" "><span class="title-number">47 </span><span class="title-name">Troubleshooting Edge Image Builder (EIB)</span></a></li><li><a href="troubleshooting-edge-networking.html" class=" "><span class="title-number">48 </span><span class="title-name">Troubleshooting Edge Networking (NMC)</span></a></li><li><a href="troubleshooting-phone-home-scenarios.html" class=" "><span class="title-number">49 </span><span class="title-name">Troubleshooting Phone-Home scenarios</span></a></li><li><a href="troubleshooting-directed-network-provisioning.html" class=" "><span class="title-number">50 </span><span class="title-name">Troubleshooting Directed-network provisioning</span></a></li><li><a href="troubleshooting-other-components.html" class=" "><span class="title-number">51 </span><span class="title-name">Troubleshooting Other components</span></a></li><li><a href="collecting-diagnostics-for-support.html" class=" "><span class="title-number">52 </span><span class="title-name">Collecting Diagnostics for Support</span></a></li></ol></li><li><a href="id-appendix.html" class="has-children "><span class="title-number">IX </span><span class="title-name">Appendix</span></a><ol><li><a href="id-release-notes.html" class=" "><span class="title-number">53 </span><span class="title-name">Release Notes</span></a></li></ol></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="atip-management-cluster" data-id-title="Setting up the management cluster"><div class="titlepage"><div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">41 </span><span class="title-name">Setting up the management cluster</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><section class="sect1" id="id-introduction-2" data-id-title="Introduction"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.1 </span><span class="title-name">Introduction</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#id-introduction-2">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The management cluster is the part of SUSE Telco Cloud that is used to manage the provision and lifecycle of the runtime stacks.
From a technical point of view, the management cluster contains the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">SUSE Linux Micro</code> as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.</p></li><li class="listitem"><p><code class="literal">RKE2</code> as the Kubernetes cluster. Depending on the use case, it can be configured to use specific CNI plugins, such as <code class="literal">Multus</code>, <code class="literal">Cilium</code>, <code class="literal">Calico</code>, etc.</p></li><li class="listitem"><p><code class="literal">Rancher</code> as the management platform to manage the lifecycle of the clusters.</p></li><li class="listitem"><p><code class="literal">Metal<sup>3</sup></code> as the component to manage the lifecycle of the bare-metal nodes.</p></li><li class="listitem"><p><code class="literal">CAPI</code> as the component to manage the lifecycle of the Kubernetes clusters (downstream clusters). The <code class="literal">RKE2 CAPI Provider</code> is used to manage the lifecycle of the RKE2 clusters.</p></li></ul></div><p>With all components mentioned above, the management cluster can manage the lifecycle of downstream clusters, using a declarative approach to manage the infrastructure and applications.</p><div id="id-1.9.6.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about <code class="literal">SUSE Linux Micro</code>, see: SUSE Linux Micro (<a class="xref" href="components-slmicro.html" title="Chapter 9. SUSE Linux Micro">Chapter 9, <em>SUSE Linux Micro</em></a>)</p><p>For more information about <code class="literal">RKE2</code>, see: RKE2 (<a class="xref" href="components-rke2.html" title="Chapter 16. RKE2">Chapter 16, <em>RKE2</em></a>)</p><p>For more information about <code class="literal">Rancher</code>, see: Rancher (<a class="xref" href="components-rancher.html" title="Chapter 5. Rancher">Chapter 5, <em>Rancher</em></a>)</p><p>For more information about <code class="literal">Metal<sup>3</sup></code>, see: Metal3 (<a class="xref" href="components-metal3.html" title="Chapter 10. Metal3">Chapter 10, <em>Metal<sup>3</sup></em></a>)</p></div></section><section class="sect1" id="id-steps-to-set-up-the-management-cluster" data-id-title="Steps to set up the management cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.2 </span><span class="title-name">Steps to set up the management cluster</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#id-steps-to-set-up-the-management-cluster">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The following steps are necessary to set up the management cluster (using a single node):</p><div class="informalfigure"><div class="mediaobject"><a href="images/product-atip-mgmtcluster1.png"><img src="images/product-atip-mgmtcluster1.png" width="100%" alt="product atip mgmtcluster1" title="product atip mgmtcluster1"/></a></div></div><p>The following are the main steps to set up the management cluster using a declarative approach:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="strong"><strong>Image preparation for connected environments (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-image-preparation-connected" title="41.3. Image preparation for connected environments">Section 41.3, “Image preparation for connected environments”</a>)</strong></span>: The first step is to prepare the manifests and files with all the necessary configurations to be used in connected environments.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Directory structure for connected environments (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-directory-structure" title="41.3.1. Directory structure">Section 41.3.1, “Directory structure”</a>): This step creates a directory structure to be used by Edge Image Builder to store the configuration files and the image itself.</p></li><li class="listitem"><p>Management cluster definition file (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-image-definition-file" title="41.3.2. Management cluster definition file">Section 41.3.2, “Management cluster definition file”</a>): The <code class="literal">mgmt-cluster.yaml</code> file is the main definition file for the management cluster. It contains the following information about the image to be created:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Image Information: The information related to the image to be created using the base image.</p></li><li class="listitem"><p>Operating system: The operating system configurations to be used in the image.</p></li><li class="listitem"><p>Kubernetes: Helm charts and repositories, kubernetes version, network configuration, and the nodes to be used in the cluster.</p></li></ul></div></li><li class="listitem"><p>Custom folder (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-custom-folder" title="41.3.3. Custom folder">Section 41.3.3, “Custom folder”</a>): The <code class="literal">custom</code> folder contains the configuration files and scripts to be used by Edge Image Builder to deploy a fully functional management cluster.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Files: Contains the configuration files to be used by the management cluster.</p></li><li class="listitem"><p>Scripts: Contains the scripts to be used by the management cluster.</p></li></ul></div></li><li class="listitem"><p>Kubernetes folder (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-kubernetes-folder" title="41.3.4. Kubernetes folder">Section 41.3.4, “Kubernetes folder”</a>): The <code class="literal">kubernetes</code> folder contains the configuration files to be used by the management cluster.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Manifests: Contains the manifests to be used by the management cluster.</p></li><li class="listitem"><p>Helm: Contains the Helm values files to be used by the management cluster.</p></li><li class="listitem"><p>Config: Contains the configuration files to be used by the management cluster.</p></li></ul></div></li><li class="listitem"><p>Network folder (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-network-folder" title="41.3.5. Networking folder">Section 41.3.5, “Networking folder”</a>): The <code class="literal">network</code> folder contains the network configuration files to be used by the management cluster nodes.</p></li></ul></div></li><li class="listitem"><p><span class="strong"><strong>Image preparation for air-gap environments (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-image-preparation-airgap" title="41.4. Image preparation for air-gap environments">Section 41.4, “Image preparation for air-gap environments”</a>)</strong></span>: The step is to show the differences to prepare the manifests and files to be used in an air-gap scenario.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Modifications in the definition file (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-image-definition-file-airgap" title="41.4.1. Modifications in the definition file">Section 41.4.1, “Modifications in the definition file”</a>): The <code class="literal">mgmt-cluster.yaml</code> file must be modified to include the <code class="literal">embeddedArtifactRegistry</code> section with the <code class="literal">images</code> field set to all container images to be included into the EIB output image.</p></li><li class="listitem"><p>Modifications in the custom folder (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-custom-folder-airgap" title="41.4.2. Modifications in the custom folder">Section 41.4.2, “Modifications in the custom folder”</a>): The <code class="literal">custom</code> folder must be modified to include the resources needed to run the management cluster in an air-gap environment.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Register script: The <code class="literal">custom/scripts/99-register.sh</code> script must be removed when you use an air-gap environment.</p></li></ul></div></li><li class="listitem"><p>Modifications in the helm values folder (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-helm-values-folder-airgap" title="41.4.3. Modifications in the helm values folder">Section 41.4.3, “Modifications in the helm values folder”</a>): The <code class="literal">helm/values</code> folder must be modified to include the configuration needed to run the management cluster in an air-gap environment.</p></li></ul></div></li><li class="listitem"><p><span class="strong"><strong>Image creation (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-image-creation" title="41.5. Image creation">Section 41.5, “Image creation”</a>)</strong></span>: This step covers the creation of the image using the Edge Image Builder tool (for both, connected and air-gap scenarios). Check the prerequisites (<a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a>) to run the Edge Image Builder tool on your system.</p></li><li class="listitem"><p><span class="strong"><strong>Management Cluster Provision (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-provision" title="41.6. Provision the management cluster">Section 41.6, “Provision the management cluster”</a>)</strong></span>: This step covers the provisioning of the management cluster using the image created in the previous step (for both, connected and air-gap scenarios). This step can be done using a laptop, server, VM or any other AMD64/Intel 64 system with a USB port.</p></li></ol></div><div id="id-1.9.6.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>For more information about Edge Image Builder, see Edge Image Builder (<a class="xref" href="components-eib.html" title="Chapter 11. Edge Image Builder">Chapter 11, <em>Edge Image Builder</em></a>) and Edge Image Builder Quick Start (<a class="xref" href="quickstart-eib.html" title="Chapter 3. Standalone clusters with Edge Image Builder">Chapter 3, <em>Standalone clusters with Edge Image Builder</em></a>).</p></div></section><section class="sect1" id="mgmt-cluster-image-preparation-connected" data-id-title="Image preparation for connected environments"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.3 </span><span class="title-name">Image preparation for connected environments</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-image-preparation-connected">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Edge Image Builder is used to create the image for the management cluster, in this document we cover the minimal configuration necessary to set up the management cluster.</p><p>Edge Image Builder runs inside a container, so a container runtime is required such as <a class="link" href="https://podman.io" target="_blank">Podman</a> or <a class="link" href="https://rancherdesktop.io" target="_blank">Rancher Desktop</a>. For this guide, we assume podman is available.</p><p>Also, as a prerequisite to deploy a highly available management cluster, you need to reserve three IPs in your network:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">apiVIP</code> for the API VIP Address (used to access the Kubernetes API server).</p></li><li class="listitem"><p><code class="literal">ingressVIP</code> for the Ingress VIP Address (consumed, for example, by the Rancher UI).</p></li><li class="listitem"><p><code class="literal">metal3VIP</code> for the Metal3 VIP Address.</p></li></ul></div><section class="sect2" id="mgmt-cluster-directory-structure" data-id-title="Directory structure"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.3.1 </span><span class="title-name">Directory structure</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-directory-structure">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>When running EIB, a directory is mounted from the host, so the first thing to do is to create a directory structure to be used by EIB to store the configuration files and the image itself.
This directory has the following structure:</p><div class="verbatim-wrap"><pre class="screen">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── os-files
│ └── var
│   └── lib
│     └── rancher
│       └── rke2
│         └── server
│           └── manifests
│             └── rke2-ingress-config.yaml
├── kubernetes
│ ├── manifests
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── longhorn.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</pre></div><div id="id-1.9.6.4.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The image <code class="literal">SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso</code> must be downloaded from the <a class="link" href="https://scc.suse.com/" target="_blank">SUSE Customer Center</a> or the <a class="link" href="https://www.suse.com/download/sle-micro/" target="_blank">SUSE Download page</a>, and it must be located under the <code class="literal">base-images</code> folder.</p><p>You should check the SHA256 checksum of the image to ensure it has not been tampered with. The checksum can be found in the same location where the image was downloaded.</p><p>An example of the directory structure can be found in the <a class="link" href="https://github.com/suse-edge/atip" target="_blank">SUSE Edge GitHub repository under the "telco-examples" folder</a>.</p></div></section><section class="sect2" id="mgmt-cluster-image-definition-file" data-id-title="Management cluster definition file"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.3.2 </span><span class="title-name">Management cluster definition file</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-image-definition-file">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">mgmt-cluster.yaml</code> file is the main definition file for the management cluster. It contains the following information:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    - open-iscsi
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
  network:
    apiHost: $API_HOST
    apiVIP: $API_VIP
  nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</pre></div><p>To explain the fields and values in the <code class="literal">mgmt-cluster.yaml</code> definition file, we have divided it into the following sections.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Image section (definition file):</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</pre></div><p>where the <code class="literal">baseImage</code> is the original image you downloaded from the SUSE Customer Center or the SUSE Download page. <code class="literal">outputImageName</code> is the name of the new image that will be used to provision the management cluster.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Operating system section (definition file):</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE</pre></div><p>where the <code class="literal">installDevice</code> is the device to be used to install the operating system, the <code class="literal">username</code> and <code class="literal">encryptedPassword</code> are the credentials to be used to access the system, the <code class="literal">packageList</code> is the list of packages to be installed (<code class="literal">jq</code> is required internally during the installation process), and the <code class="literal">sccRegistrationCode</code> is the registration code used to get the packages and dependencies at build time and can be obtained from the SUSE Customer Center.
The encrypted password can be generated using the <code class="literal">openssl</code> command as follows:</p><div class="verbatim-wrap"><pre class="screen">openssl passwd -6 MyPassword!123</pre></div><p>This outputs something similar to:</p><div class="verbatim-wrap"><pre class="screen">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Kubernetes section (definition file):</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server</pre></div><p>The <code class="literal">helm</code> section contains the list of Helm charts to be installed, the repositories to be used, and the version configuration for all of them.</p><p>The <code class="literal">network</code> section contains the configuration for the network, like the <code class="literal">apiHost</code> and <code class="literal">apiVIP</code> to be used by the <code class="literal">RKE2</code> component.
The <code class="literal">apiVIP</code> should be an IP address that is not used in the network and should not be part of the DHCP pool (in case we use DHCP). Also, when we use the <code class="literal">apiVIP</code> in a multi-node cluster, it is used to access the Kubernetes API server.
The <code class="literal">apiHost</code> is the name resolution to <code class="literal">apiVIP</code> to be used by the <code class="literal">RKE2</code> component.</p><p>The <code class="literal">nodes</code> section contains the list of nodes to be used in the cluster. In this example, a single-node cluster is being used, but it can be extended to a multi-node cluster by adding more nodes to the list (by uncommenting the lines).</p><div id="id-1.9.6.4.7.19" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The names of the nodes must be unique in the cluster.</p></li><li class="listitem"><p>Optionally, use the <code class="literal">initializer</code> field to specify the bootstrap host, otherwise it will be the first node in the list.</p></li><li class="listitem"><p>The names of the nodes must be the same as the host names defined in the Network Folder (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-network-folder" title="41.3.5. Networking folder">Section 41.3.5, “Networking folder”</a>) when network configuration is required.</p></li></ul></div></div></section><section class="sect2" id="mgmt-cluster-custom-folder" data-id-title="Custom folder"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.3.3 </span><span class="title-name">Custom folder</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-custom-folder">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">custom</code> folder contains the following subfolders:</p><div class="verbatim-wrap"><pre class="screen">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">custom/files</code> folder contains the configuration files to be used by the management cluster.</p></li><li class="listitem"><p>The <code class="literal">custom/scripts</code> folder contains the scripts to be used by the management cluster.</p></li></ul></div><p>The <code class="literal">custom/files</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">basic-setup.sh</code>: contains configuration parameters for <code class="literal">Metal<sup>3</sup></code>, <code class="literal">Rancher</code> and <code class="literal">MetalLB</code>. Only modify this file if you want to change the namespaces to be used.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</pre></div></li><li class="listitem"><p><code class="literal">metal3.sh</code>: contains the configuration for the <code class="literal">Metal<sup>3</sup></code> component to be used (no modifications needed). In future versions, this script will be replaced to use instead <code class="literal">Rancher Turtles</code> to make it easy.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
	EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
	EOF
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete --ignore-not-found=true mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete --ignore-not-found=true validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">rancher.sh</code>: contains the configuration for the <code class="literal">Rancher</code> component to be used (no modifications needed).</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</pre></div></li><li class="listitem"><p><code class="literal">mgmt-stack-setup.service</code>: contains the configuration to create the systemd service to run the scripts during the first boot (no modifications needed).</p><div class="verbatim-wrap"><pre class="screen">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single one
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</pre></div></li></ul></div></li></ul></div><p>The <code class="literal">custom/scripts</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">99-alias.sh</code> script: contains the alias to be used by the management cluster to load the kubeconfig file at first boot (no modifications needed).</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</pre></div></li><li class="listitem"><p><code class="literal">99-mgmt-setup.sh</code> script: contains the configuration to copy the scripts during the first boot (no modifications needed).</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</pre></div></li><li class="listitem"><p><code class="literal">99-register.sh</code> script: contains the configuration to register the system using the SCC registration code. The <code class="literal">${SCC_ACCOUNT_EMAIL}</code> and <code class="literal">${SCC_REGISTRATION_CODE}</code> have to be set properly to register the system with your account.</p><div class="verbatim-wrap"><pre class="screen">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
	zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</pre></div></li></ul></div></section><section class="sect2" id="mgmt-cluster-kubernetes-folder" data-id-title="Kubernetes folder"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.3.4 </span><span class="title-name">Kubernetes folder</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-kubernetes-folder">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">kubernetes</code> folder contains the following subfolders:</p><div class="verbatim-wrap"><pre class="screen">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</pre></div><p>The <code class="literal">kubernetes/config</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">server.yaml</code>: By default, the <code class="literal">CNI</code> plug-in installed by default is <code class="literal">Cilium</code>, so you do not need to create this folder and file. Just in case you need to customize the <code class="literal">CNI</code> plug-in, you can use the <code class="literal">server.yaml</code> file under the <code class="literal">kubernetes/config</code> folder. It contains the following information:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
system-default-registry: registry.rancher.com</pre></div></li></ul></div><div id="id-1.9.6.4.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>This is an optional file to define certain Kubernetes customization, like the CNI plug-ins to be used or many options you can check in the <a class="link" href="https://docs.rke2.io/install/configuration" target="_blank">official documentation</a>.</p></div><p>The <code class="literal">os-files/var/lib/rancher/rke2/server/manifests</code> folder contains the following file:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">rke2-ingress-config.yaml</code>: contains the configuration to create the <code class="literal">Ingress</code> service for the management cluster (no modifications needed).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</pre></div></li></ul></div><div id="id-1.9.6.4.9.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The <code class="literal">HelmChartConfig</code> must be included via <code class="literal">os-files</code> to the <code class="literal">/var/lib/rancher/rke2/server/manifests</code> directory, not via <code class="literal">kubernetes/manifests</code> as described in previous releases.</p></div><p>The <code class="literal">kubernetes/manifests</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">neuvector-namespace.yaml</code>: contains the configuration to create the <code class="literal">NeuVector</code> namespace (no modifications needed).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</pre></div></li><li class="listitem"><p><code class="literal">ingress-l2-adv.yaml</code>: contains the configuration to create the <code class="literal">L2Advertisement</code> for the <code class="literal">MetalLB</code> component (no modifications needed).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</pre></div></li><li class="listitem"><p><code class="literal">ingress-ippool.yaml</code>: contains the configuration to create the <code class="literal">IPAddressPool</code> for the <code class="literal">rke2-ingress-nginx</code> component. The <code class="literal">${INGRESS_VIP}</code> has to be set properly to define the IP address reserved to be used by the <code class="literal">rke2-ingress-nginx</code> component.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</pre></div></li></ul></div><p>The <code class="literal">kubernetes/helm/values</code> folder contains the following files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">rancher.yaml</code>: contains the configuration to create the <code class="literal">Rancher</code> component. The <code class="literal">${INGRESS_VIP}</code> must be set properly to define the IP address to be consumed by the <code class="literal">Rancher</code> component. The URL to access the <code class="literal">Rancher</code> component will be <code class="literal">https://rancher-${INGRESS_VIP}.sslip.io</code>.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</pre></div></li><li class="listitem"><p><code class="literal">neuvector.yaml</code>: contains the configuration to create the <code class="literal">NeuVector</code> component (no modifications needed).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
registry: "registry.rancher.com"
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</pre></div></li><li class="listitem"><p><code class="literal">longhorn.yaml</code>: contains the configuration to create the <code class="literal">Longhorn</code> component (no modifications needed).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</pre></div></li><li class="listitem"><p><code class="literal">metal3.yaml</code>: contains the configuration to create the <code class="literal">Metal<sup>3</sup></code> component. The <code class="literal">${METAL3_VIP}</code> must be set properly to define the IP address to be consumed by the <code class="literal">Metal<sup>3</sup></code> component.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</pre></div></li></ul></div><div id="metal3-media-server" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The Media Server is an optional feature included in Metal<sup>3</sup> (by default is disabled). To use the Metal3 feature, you need to configure it on the previous manifest.
To use the Metal<sup>3</sup> media server, specify the following variable:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>add the <code class="literal">enable_metal3_media_server</code> to <code class="literal">true</code> to enable the media server feature in the global section.</p></li><li class="listitem"><p>include the following configuration about the media server where ${MEDIA_VOLUME_PATH} is the path to the media volume in the media (e.g <code class="literal">/home/metal3/bmh-image-cache</code>)</p><div class="verbatim-wrap highlight yaml"><pre class="screen">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</pre></div></li></ul></div><p>An external media server can be used to store the images, and in the case you want to use it with TLS, you will need to modify the following configurations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>set to <code class="literal">true</code> the <code class="literal">additionalTrustedCAs</code> in the previous <code class="literal">metal3.yaml</code> file to enable the additional trusted CAs from the external media server.</p></li><li class="listitem"><p>include the following secret configuration in the folder <code class="literal">kubernetes/manifests/metal3-cacert-secret.yaml</code> to store the CA certificate of the external media server.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</pre></div></li></ul></div><p>The <code class="literal">additional_ca_cert</code> is the base64-encoded CA certificate of the external media server. You can use the following command to encode the certificate and generate the secret doing manually:</p><div class="verbatim-wrap"><pre class="screen">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</pre></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">certmanager.yaml</code>: contains the configuration to create the <code class="literal">Cert-Manager</code> component (no modifications needed).</p><div class="verbatim-wrap highlight yaml"><pre class="screen">installCRDs: true</pre></div></li></ul></div></section><section class="sect2" id="mgmt-cluster-network-folder" data-id-title="Networking folder"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.3.5 </span><span class="title-name">Networking folder</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-network-folder">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">network</code> folder contains as many files as nodes in the management cluster. In our case, we have only one node, so we have only one file called <code class="literal">mgmt-cluster-node1.yaml</code>.
The name of the file must match the host name defined in the <code class="literal">mgmt-cluster.yaml</code> definition file into the network/node section described above.</p><p>If you need to customize the networking configuration, for example, to use a specific static IP address (DHCP-less scenario), you can use the <code class="literal">mgmt-cluster-node1.yaml</code> file under the <code class="literal">network</code> folder. It contains the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><code class="literal">${MGMT_GATEWAY}</code>: The gateway IP address.</p></li><li class="listitem"><p><code class="literal">${MGMT_DNS}</code>: The DNS server IP address.</p></li><li class="listitem"><p><code class="literal">${MGMT_MAC}</code>: The MAC address of the network interface.</p></li><li class="listitem"><p><code class="literal">${MGMT_NODE_IP}</code>: The IP address of the management cluster.</p></li></ul></div><div class="verbatim-wrap highlight yaml"><pre class="screen">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</pre></div><p>If you want to use DHCP to get the IP address, you can use the following configuration (the <code class="literal">MAC</code> address must be set properly using the <code class="literal">${MGMT_MAC}</code> variable):</p><div class="verbatim-wrap highlight yaml"><pre class="screen">## This is an example of a dhcp network configuration for a management cluster
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</pre></div><div id="id-1.9.6.4.10.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Depending on the number of nodes in the management cluster, you can create more files like <code class="literal">mgmt-cluster-node2.yaml</code>, <code class="literal">mgmt-cluster-node3.yaml</code>, etc. to configure the rest of the nodes.</p></li><li class="listitem"><p>The <code class="literal">routes</code> section is used to define the routing table for the management cluster.</p></li></ul></div></div></section></section><section class="sect1" id="mgmt-cluster-image-preparation-airgap" data-id-title="Image preparation for air-gap environments"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.4 </span><span class="title-name">Image preparation for air-gap environments</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-image-preparation-airgap">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>This section describes how to prepare the image for air-gap environments showing only the differences from the previous sections. The following changes to the previous section (Image preparation for connected environments (<a class="xref" href="atip-management-cluster.html#mgmt-cluster-image-preparation-connected" title="41.3. Image preparation for connected environments">Section 41.3, “Image preparation for connected environments”</a>)) are required to prepare the image for air-gap environments:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">mgmt-cluster.yaml</code> file must be modified to include the <code class="literal">embeddedArtifactRegistry</code> section with the <code class="literal">images</code> field set to all container images to be included into the EIB output image.</p></li><li class="listitem"><p>The <code class="literal">mgmt-cluster.yaml</code> file must be modified to include <code class="literal">rancher-turtles-airgap-resources</code> helm chart.</p></li><li class="listitem"><p>The <code class="literal">custom/scripts/99-register.sh</code> script must be removed when use an air-gap environment.</p></li></ul></div><section class="sect2" id="mgmt-cluster-image-definition-file-airgap" data-id-title="Modifications in the definition file"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.4.1 </span><span class="title-name">Modifications in the definition file</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-image-definition-file-airgap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The <code class="literal">mgmt-cluster.yaml</code> file must be modified to include the <code class="literal">embeddedArtifactRegistry</code> section.
In this section the <code class="literal">images</code> field must contain the list of all container images to be included in the output image.</p><div id="id-1.9.6.5.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>The following is an example of the <code class="literal">mgmt-cluster.yaml</code> file with the <code class="literal">embeddedArtifactRegistry</code> section included.
Make sure to the listed images contain the component versions you need.</p></div><p>The <code class="literal">rancher-turtles-airgap-resources</code> helm chart must also be added, this creates resources as described in the <a class="link" href="https://documentation.suse.com/cloudnative/cluster-api/v0.19/en/getting-started/air-gapped-environment.html" target="_blank">Rancher Turtles Airgap Documentation</a>.  This also requires a turtles.yaml values file for the rancher-turtles chart to specify the necessary configuration.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">apiVersion: 1.3
image:
  imageType: iso
  arch: x86_64
  baseImage: SL-Micro.x86_64-6.1-Base-SelfInstall-GM.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: $ROOT_PASSWORD
  packages:
    packageList:
    - jq
    sccRegistrationCode: $SCC_REGISTRATION_CODE
kubernetes:
  version: v1.33.3+rke2r1
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.18.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 107.0.0+up1.9.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: longhorn.yaml
      - name: metal3
        version: 304.0.16+up0.12.6
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: rancher-turtles
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: turtles.yaml
      - name: rancher-turtles-airgap-resources
        version: 304.0.6+up0.24.0
        repositoryName: suse-edge-charts
        targetNamespace: rancher-turtles-system
        createNamespace: true
        installationNamespace: kube-system
      - name: neuvector-crd
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 107.0.0+up2.8.7
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.12.1
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge/charts
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: $API_HOST
      apiVIP: $API_VIP
    nodes:
    - hostname: mgmt-cluster-node1
      initializer: true
      type: server
#   - hostname: mgmt-cluster-node2
#     type: server
#   - hostname: mgmt-cluster-node3
#     type: server
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.10.2-build20250611
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.7.1-build20250611
    - name: registry.rancher.com/rancher/hardened-coredns:v1.12.2-build20250611
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.8.0-build20250704
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.2.1-build20250627
    - name: registry.rancher.com/rancher/klipper-helm:v0.9.8-build20250709
    - name: registry.rancher.com/rancher/mirrored-cilium-cilium:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-cilium-operator-generic:v1.17.6
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.9.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v5.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.14.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v8.3.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.16.0-20250709
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.9.1
    - name: registry.rancher.com/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0
    - name: registry.rancher.com/rancher/neuvector-compliance-config:1.0.6
    - name: registry.rancher.com/rancher/neuvector-controller:5.4.5
    - name: registry.rancher.com/rancher/neuvector-enforcer:5.4.5
    - name: registry.rancher.com/rancher/nginx-ingress-controller:v1.12.4-hardened2
    - name: registry.suse.com/rancher/cluster-api-addon-provider-fleet:v0.11.0
    - name: registry.rancher.com/rancher/cluster-api-operator:v0.18.1
    - name: registry.rancher.com/rancher/fleet-agent:v0.13.1
    - name: registry.rancher.com/rancher/fleet:v0.13.1
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.15.7-build20250425
    - name: registry.rancher.com/rancher/rancher-webhook:v0.8.1
    - name: registry.rancher.com/rancher/rancher/turtles:v0.24.0
    - name: registry.rancher.com/rancher/rancher:v2.12.1
    - name: registry.rancher.com/rancher/shell:v0.4.1
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.16.0
    - name: registry.suse.com/rancher/cluster-api-controller:v1.10.5
    - name: registry.suse.com/rancher/cluster-api-provider-metal3:v1.10.2
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-bootstrap:v0.20.1
    - name: registry.suse.com/rancher/cluster-api-provider-rke2-controlplane:v0.20.1
    - name: registry.suse.com/rancher/hardened-sriov-network-operator:v1.5.0-build20250425
    - name: registry.suse.com/rancher/ip-address-manager:v1.10.2
    - name: registry.rancher.com/rancher/kubectl:v1.32.2
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.9.5</pre></div></section><section class="sect2" id="mgmt-cluster-custom-folder-airgap" data-id-title="Modifications in the custom folder"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.4.2 </span><span class="title-name">Modifications in the custom folder</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-custom-folder-airgap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">custom/scripts/99-register.sh</code> script must be removed when using an air-gap environment. As you can see in the directory structure, the <code class="literal">99-register.sh</code> script is not included in the <code class="literal">custom/scripts</code> folder.</p></li></ul></div></section><section class="sect2" id="mgmt-cluster-helm-values-folder-airgap" data-id-title="Modifications in the helm values folder"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">41.4.3 </span><span class="title-name">Modifications in the helm values folder</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-helm-values-folder-airgap">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The <code class="literal">turtles.yaml</code>: contains the configuration required to specify airgapped operation for Rancher Turtles, note this depends on installation of the rancher-turtles-airgap-resources chart.</p><div class="verbatim-wrap highlight yaml"><pre class="screen">cluster-api-operator:
  cluster-api:
    core:
      fetchConfig:
        selector: "{\"matchLabels\": {\"provider-components\": \"core\"}}"
    rke2:
      bootstrap:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-bootstrap\"}}"
      controlPlane:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"rke2-control-plane\"}}"
    metal3:
      infrastructure:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3\"}}"
      ipam:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"metal3ipam\"}}"
    fleet:
      addon:
        fetchConfig:
          selector: "{\"matchLabels\": {\"provider-components\": \"fleet\"}}"</pre></div></li></ul></div></section></section><section class="sect1" id="mgmt-cluster-image-creation" data-id-title="Image creation"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.5 </span><span class="title-name">Image creation</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-image-creation">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>Once the directory structure is prepared following the previous sections (for both, connected and air-gap scenarios), run the following command to build the image:</p><div class="verbatim-wrap"><pre class="screen">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/3.4/edge-image-builder:1.3.0 \
 build --definition-file mgmt-cluster.yaml</pre></div><p>This creates the ISO output image file that, in our case, based on the image definition described above, is <code class="literal">eib-mgmt-cluster-image.iso</code>.</p></section><section class="sect1" id="mgmt-cluster-provision" data-id-title="Provision the management cluster"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.6 </span><span class="title-name">Provision the management cluster</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-provision">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The previous image contains all components explained above, and it can be used to provision the management cluster using a virtual machine or a bare-metal server (using the virtual-media feature).</p></section><section class="sect1" id="mgmt-cluster-dualstack" data-id-title="Dual-stack considerations and configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">41.7 </span><span class="title-name">Dual-stack considerations and configuration</span></span> <a title="Permalink" class="permalink" href="atip-management-cluster.html#mgmt-cluster-dualstack">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a></div></div></div></div></div><p>The examples shown in the previous sections provide guidance and examples on how to set up a single-stack IPv4 management cluster. Such a management cluster is independent of the operational status of downstream clusters, which can be individually configured to operate in either IPv4/IPv6 single-stack or dual-stack configuration, once deployed. However, they way the management cluster is configured has a direct impact on the communication protocols that can be used during the provisioning phase, where both the in-band and out-of-band communications must happen according to which protocols are supported by the management cluster and downstream host. In case some or all of the BMCs and/or downstream cluster nodes are expected to use IPv6, a dual-stack setup for the management cluster is then required.</p><div id="id-1.9.6.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>Single-stack IPv6 management clusters are not yet supported.</p></div><p>In order to achieve dual-stack functionality, Kubernetes must be provided with both IPv4 and IPv6 CIDRs for PODs and Services. However, other components also require specific tuning before building the management cluster image with EIB. The Metal<sup>3</sup> provisioning services (Ironic) can be configured in different ways, depending on your infrastructure or requirements:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>The Ironic services can be configured to listen on all the interfaces on the system rather than a single IP address, thus, as long as the management cluster host(s) has both IPv4 and IPv6 addresses assigned to the relevant interface, any of them can potentially be used during the provisioning. Note that at this time only one of these addresses can be selected for the URL generation (for consumption by other services, e.g. the Baremetal Operator, BMCs, etc.); as a consequence, to enable IPv6 communications with the BMCs, the Baremetal Operator can be instructed to expose and pass on an IPv6 URL when dealing BMH definitions including an IPv6 address. In other words, when a BMC is identified as IPv6 capable, the provisioning will be performed via IPv6 only, and via IPv4 in all the other cases.</p></li><li class="listitem"><p>A single hostname, resolving to both IPv4 and IPv6, can be used by Metal<sup>3</sup> to let Ironic use those addresses for binding and URL creation. This approach allows for an easy configuration and flexible behavior (both IPv4 and IPv6 remain viable at each provisioning step), but requires an infrastructure with preexisting DNS servers, IP allocations and records already in place.</p></li></ul></div><p>In both cases, Kubernetes will need to know what CIDRs to use for both IPv4 and IPv6, so you can add the following lines to your <code class="literal">kubernetes/config/server.yaml</code> in the EIB directory, making sure to list IPv4 first:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">service-cidr: 10.96.0.0/12,fd12:4567:789c::/112
cluster-cidr: 193.168.0.0/18,fd12:4567:789b::/48</pre></div><p>Some containers leverage the host networking, so modify the network configuration for the host(s), under the <code class="literal">network</code> directory, to enable IPv6 connectivity:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">routes:
  config:
  - destination: 0.0.0.0/0
    next-hop-address: ${MGMT_GATEWAY_V4}
    next-hop-interface: eth0
  - destination: ::/0
    next-hop-address: ${MGMT_GATEWAY_V6}
    next-hop-interface: eth0
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
    - 2001:4860:4860::8888
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP_V4}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    address:
    - ip: ${MGMT_CLUSTER_IP_V6}
      prefix-length: 128
    dhcp: false
    autoconf: false
    enabled: true</pre></div><p>Replace the placeholders with the gateway IP addresses, additional DNS server (if needed), the MAC address of the network interface and the the IP addressed of the management cluster. If address autoconfiguration is preferred instead, refer to the following excerpt and just set the <code class="literal">${MGMT_MAC}</code> variable:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    enabled: true
    dhcp: true
  ipv6:
    enabled: false
    dhcp: true
    autoconf: true</pre></div><p>We can now define the remaining files for a single node configuration, starting from the first option, by creating <code class="literal">kubernetes/helm/values/metal3.yaml</code> as:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  ironicIP: ${MGMT_CLUSTER_IP_V4}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  listenOnAll: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort
metal3-baremetal-operator:
  baremetaloperator:
    externalHttpIPv6: ${MGMT_CLUSTER_IP_V6}</pre></div><p>and <code class="literal">kubernetes/helm/values/rancher.yaml</code> as:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">hostname: rancher-${MGMT_CLUSTER_IP_V4}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</pre></div><p>where <code class="literal">${MGMT_CLUSTER_IP_V4}</code> and <code class="literal">${MGMT_CLUSTER_IP_V6}</code> are the IP addresses previously assigned to the host.</p><p>Alternatively, to use the hostname in place of the IP addresses, modify <code class="literal">kubernetes/helm/values/metal3.yaml</code> to:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">global:
  provisioningHostname: `${MGMT_CLUSTER_HOSTNAME}`
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: true
  persistence:
    ironic:
      size: "5Gi"
  service:
    type: NodePort</pre></div><p>and <code class="literal">kubernetes/helm/values/rancher.yaml</code> to:</p><div class="verbatim-wrap highlight yaml"><pre class="screen">hostname: rancher-${MGMT_CLUSTER_HOSTNAME}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global:
  cattle:
    systemDefaultRegistry: "registry.rancher.com"</pre></div><p>where <code class="literal">${MGMT_CLUSTER_HOSTNAME}</code> should be a Fully Qualified Domain Name resolving to your host IP addresses.</p><p>For more information visit <a class="link" href="https://github.com/suse-edge/atip/tree/main/telco-examples/mgmt-cluster/dual-stack" target="_blank">SUSE Edge GitHub repository under the "dual-stack" folder</a>, where an example directory structure can be found.</p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="atip-requirements.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 40 </span>Requirements &amp; Assumptions</span></a> </div><div><a class="pagination-link next" href="atip-features.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 42 </span>Telco features configuration</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="section"><a href="atip-management-cluster.html#id-introduction-2"><span class="title-number">41.1 </span><span class="title-name">Introduction</span></a></span></li><li><span class="section"><a href="atip-management-cluster.html#id-steps-to-set-up-the-management-cluster"><span class="title-number">41.2 </span><span class="title-name">Steps to set up the management cluster</span></a></span></li><li><span class="section"><a href="atip-management-cluster.html#mgmt-cluster-image-preparation-connected"><span class="title-number">41.3 </span><span class="title-name">Image preparation for connected environments</span></a></span></li><li><span class="section"><a href="atip-management-cluster.html#mgmt-cluster-image-preparation-airgap"><span class="title-number">41.4 </span><span class="title-name">Image preparation for air-gap environments</span></a></span></li><li><span class="section"><a href="atip-management-cluster.html#mgmt-cluster-image-creation"><span class="title-number">41.5 </span><span class="title-name">Image creation</span></a></span></li><li><span class="section"><a href="atip-management-cluster.html#mgmt-cluster-provision"><span class="title-number">41.6 </span><span class="title-name">Provision the management cluster</span></a></span></li><li><span class="section"><a href="atip-management-cluster.html#mgmt-cluster-dualstack"><span class="title-number">41.7 </span><span class="title-name">Dual-stack considerations and configuration</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2025</span></div></div></footer></body></html>